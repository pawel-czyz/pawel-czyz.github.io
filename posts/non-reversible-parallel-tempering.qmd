---
title: "Non-reversible parallel tempering"
description: "Let's create a basic JAX implementation of a wonderful Markov chain Monte Carlo method."
author: "Paweł Czyż"
date: "8/31/2024"
# execute:
#   freeze: true
format: 
  html:
    code-fold: true
jupyter: python3
---

Markov chain Monte Carlo is wonderful. However, it often gets trapped in a mode. Consider a simple Metropolis algorithm: $q(x'\mid x) = \mathcal N\!\left(x' \mid x, \sigma^2\right)$, which for a well-tuned $\sigma$ can sample from the standard normal distribution, but can fail for a mixture, never exploring other modes:
```{python}
import numpy as np

import jax
import jax.random as jrandom
import jax.numpy as jnp

import numpyro
import numpyro.distributions as dist

import matplotlib.pyplot as plt
plt.style.use("dark_background")

def mixture2_dist() -> dist.Distribution:
  p = 0.4
  probs = jnp.asarray([p, 1.0-p])
  mus = jnp.asarray([-5., 5.])
  mixing = dist.Categorical(probs=probs)
  return dist.MixtureSameFamily(
    mixing_distribution=mixing,
    component_distribution=dist.Normal(mus, 1.0),
  )

def mixture3_dist() -> dist.Distribution:
  probs = jnp.asarray([0.1, 0.3, 0.6])
  sep = 15
  mus = jnp.asarray([-sep, 0., sep])
  mixing = dist.Categorical(probs=probs)
  return dist.MixtureSameFamily(
    mixing_distribution=mixing,
    component_distribution=dist.Normal(mus, 1.0),
  )

DISTRIBUTIONS = {
  "Normal": lambda: dist.Normal(0, 1),
  "Mixture (2 comp.)": mixture2_dist,
  "Mixture (3 comp.)": mixture3_dist,
}


def generate_kernel(
  logp_fn,
  sigma: float,
):
  def kernel(key, x):
    key1, key2 = jrandom.split(key)
    # Generate a proposal
    x_ = x + sigma * jrandom.normal(key1, shape=x.shape)
    
    # Evaluate the acceptance ratio
    log_p1 = logp_fn(x)
    log_p2 = logp_fn(x_)
    alpha = jnp.exp(log_p2 - log_p1)
    
    # Decide whether to accept the proposal
    u = jrandom.uniform(key2)
    return jax.lax.select(u <= alpha, x_, x)
  
  return kernel


def sampling_loop(
  key,
  x0,
  kernel,
  n_samples: int,
):
  def f(x, k):
    x_new = kernel(k, x)
    return x_new, x_new
  
  _, samples = jax.lax.scan(f, x0, jrandom.split(key, n_samples))
  
  return samples


SIGMAS = [0.01, 0.1, 1, 10, 100, 1000]
N_SAMPLES: int = 2_000

key = jrandom.PRNGKey(42)

xs_true_all = np.empty((len(DISTRIBUTIONS), N_SAMPLES))
xs_mcmc_all = np.zeros((len(SIGMAS), len(DISTRIBUTIONS), N_SAMPLES))

# Sample from the ground-truth distributions
for d, (_, dist_factory) in enumerate(DISTRIBUTIONS.items()):
  key, subkey = jrandom.split(key)
  xs_true = dist_factory().sample(subkey, sample_shape=(N_SAMPLES,))
  xs_true_all[d, :] = xs_true

# Sample using MCMC
for d, (_, dist_factory) in enumerate(DISTRIBUTIONS.items()):
  def logp(x):
    distribution = dist_factory()
    return distribution.log_prob(x)
  
  for s, sigma in enumerate(SIGMAS):
    kernel = generate_kernel(
      logp_fn=logp,
      sigma=sigma,
    )
    
    key, subkey = jrandom.split(key)

    x0 = jnp.asarray(0.5)
    xs_mcmc = sampling_loop(key=subkey, x0=x0, kernel=kernel, n_samples=N_SAMPLES * 2)[N_SAMPLES:]

    xs_mcmc_all[s, d, :] = xs_mcmc
```

```{python}
fig, axs = plt.subplots(
  len(SIGMAS),
  len(DISTRIBUTIONS),
  sharex="col",
  dpi=350,
)

for i, sigma in enumerate(SIGMAS):
  if sigma < 1:
    axs[i, 0].set_ylabel(f"$\\sigma={sigma:.2f}$")
  else:
    axs[i, 0].set_ylabel(f"$\\sigma={int(sigma)}$")

  for j, (dist_name, dist_factory) in enumerate(DISTRIBUTIONS.items()):
    ax = axs[i, j]

    if i == 0:
      ax.set_title(dist_name)

    # bins = jnp.linspace(jnp.quantile(xs_true, 0.01), jnp.quantile(xs_true, 0.99), 30)
    bins = 30

    ax.hist(xs_true_all[j], density=True, bins=bins, histtype="step", color="white")

    ax.hist(xs_mcmc_all[i, j], density=True, bins=bins, histtype="stepfilled", color="C3", alpha=0.4)

for ax in axs.ravel():
  ax.spines[["top", "left", "right"]].set_visible(False)
  ax.set_yticks([])
```

```{python}

fig, axs = plt.subplots(
  len(SIGMAS),
  len(DISTRIBUTIONS),
  sharex="col",
  sharey="col",
  dpi=350,
)

for i, sigma in enumerate(SIGMAS):
  if sigma < 1:
    axs[i, 0].set_ylabel(f"$\\sigma={sigma:.2f}$")
  else:
    axs[i, 0].set_ylabel(f"$\\sigma={int(sigma)}$")

  for j, (dist_name, dist_factory) in enumerate(DISTRIBUTIONS.items()):
    ax = axs[i, j]

    if i == 0:
      ax.set_title(dist_name)

    ax.plot(xs_mcmc_all[i, j], color="C3")

for ax in axs.ravel():
  ax.spines[["top", "right"]].set_visible(False)
```

We see that neither too small nor too large $\sigma$ explores a single mode efficiently, while only large $\sigma$ can pass through the low-probability region between the modes. A too-large $\sigma$ is, however, too inefficient with too few accepted samples, requiring increasing the length of the chain. 

One way to improve the exploration is to use a sequential Monte Carlo sampler, as in the [BlackJAX sampling book](https://blackjax-devs.github.io/sampling-book/algorithms/TemperedSMC.html) or [this talk from Nicholas Chopin](https://youtu.be/mOA_IKyWdkg?feature=shared).
Another solution is to use parallel tempering, which dates back to [a 1991 paper of Charles Geyer](https://www.stat.umn.edu/geyer/f05/8931/c.pdf), then termed $(MC)^3$, i.e., Metropolis-coupled Markov chain Monte Carlo.

Today we'll focus on the latter one. 

## Parallel tempering as originally designed

Consider a space $\mathcal X$ with a probability distribution of interest $p$. We have a Markov kernel $K$ allowing us to explore $\mathcal X$ locally, but which has a trouble to pass through low-density regions separating distinct modes.
This issue can be addressed by extending the original space $\mathcal X$ to a larger space $\mathcal X^{N+1} = \mathcal X \times \cdots \times \mathcal X$ and target a product distribution $\mathbf{p}(\mathbf x) =  p_0(x_1)\cdots p_{N-1}(x_{N-1}) p_{N}(x_N)$, where $p_N = p$ is the original distribution of interest and $p_0, \dotsc, p_{N-1}$ are auxiliary distributions, designed to be easier to sample from.
The main idea is that $p_0$ should be chosen so that it is known to be easy to sample and consecutive distributions, $p_{n}$ and $p_{n+1}$, should be closely related: the separate modes of $p = p_N$ can be then "connected" by going through $p_{N-1}$, $p_{N-2}$ and other distributions up to $p_0$, which is assumed to be easy to explore.

For example, a typical choice is
$$
  p_n(x) = \frac{1}{\mathcal Z(\beta_n)} \left(\frac{p(x)}{p_0(x)}\right)^{\beta_n} p_0(x) = \frac{1}{\mathcal Z(\beta_n)} p(x)^{\beta_n} p_0(x)^{1-\beta_n} 
$$

for a given annealing schedule $0 = \beta_0 < \beta_1 < \dotsc < \beta_N = 1$.
Similarly as in sequential Monte Carlo samplers[^1], the schedule does matter a lot, controlling how much consecutive distributions are related.

[^1]: I think Nicholas Chopin gave an excellent remark in one of his lectures (although I can't find the exact reference, so many apologies if I misquoted or misattributed this statement): parallel tempering and sequential Monte Carlo samplers are orthogonal to each other, in the sense that parallel tempering at any single time step keeps the particles at all possible temperatures, while sequential Monte Carlo keeps all the particles at the same temperature, and resamples them over time.

A Markov chain is now defined on $\mathcal X^{N+1}$, with a state $\mathbf{x} = (x_0, \dotsc, x_N)$. We will consider two transitions:

1. Applying Markov kernels $K_n$ to entries $x_n$, targeting distributions $\pi_n$, so that we do local exploration.
2. Swapping entries $x_{n}$ with $x_{n+1}$, so that we can pass from $x_N$ (which is targeting $\pi$) to $x_0$ (which is easy to explore) and back.

Note that if the second kind of moves were not allowed, we would have just $N+1$ independent Markov chains (each defined on the space $\mathcal X$) and targeting the $\pi_0\cdots \pi_K$ distribution "individually", although the mixing of the $K$-th chain would be very, very slow.
However, as the chains are coupled, they are no longer individually Markov.
In fact, we hope that they all reach the same stationary distribution!

To ensure that the Markov chain on $\mathcal X^{N+1}$ explores $\mathbf{p}$ properly, Charles Geyer proposed to swap components $i$ and $j$ according to a Metropolis update. If $\mathbf{x}$ is the current state and $\mathbf{x}'$ is the state with entries $i$ and $j$ swapped, the Metropolis ratio is given by
$$
  r = \frac{ \mathbf{p}(\mathbf x')}{ \mathbf{p}(\mathbf x)} = \frac{ p_i(x_j) p_j(x_i)}{ p_i(x_i) p_j(x_j)}.
$$ 

Typically, only adjacent indices are swapped, as for $i$ very distant from $j$ we expect that $r$ would close to zero.
It is also informative to write this ratio in terms of the target and the reference distributions.
As $\log p_n(x) = \beta_n \log p(x) + (1-\beta_n) \log p_0(x) - \log \mathcal Z(\beta_n)$, we have
$$\begin{align*}
\log r &= \beta_i (\log p(x_j) - \log p(x_i)) + (1-\beta_i) (\log p_0(x_j) - \log p_0(x_i)) \\
&+ \beta_j( \log p(x_i) - \log p(x_j) ) + (1-\beta_j)( \log p_0(x_i) - \log p_0(x_j) ) \\
&= -(\beta_i - \beta_j) \left( \log \frac{p(x_i)}{p_0(x_i)} - \log \frac{p(x_j)}{p_0(x_j)} \right)  
\end{align*}
$$

This is a very convenient formula if $p_0$ corresponds to the prior distribution and $p$ is the posterior distribution, as their ratio is then simply the likelihood[^2].

[^2]: It is tempting to use priors $p_0$ which are easy to sample from, but for which $\log p_0(x)$ can be intractable. However, that one has to evaluate $\log p_0(x)$ to construct the Markov kernels targeting intermediate distributions $p_n$ for $0 < n < N$. Only for swapping the chains we can rely just on the likelihood.

This is enough theory for now – let's implement parallel tempering in JAX.

### JAX implementation

We need to make some design choices. We keep the state $\mathbf{x} = (x_0, \dotsc, x_N)$ as a matrix $(N+1)\times (\mathrm{dim}\, \mathcal X)$. As we have access to two log-probability functions ($p = p_N$ and the reference distribution $p_0$), we need to construct intermediate log-probability functions given an annealing schedule $0 = \beta_0 < \beta_1 < \cdots < \beta_N$.
To make everything vectorisable, let's construct the kernels $K_n = \mathcal{K}(\phi_n)$ using a function generating kernels $\mathcal{K}$ and kernel-specific parameters $\phi_n$.

Hence, this implementation is not as general as possible, but it should be compatible with JAX vectorised operations.

To swap the chains, we do a "full sweep", attemping to swap $x_0 \leftrightarrow x_1$, then $x_1\leftrightarrow x_2$, and up to $x_{N-1}\leftrightarrow x_N$. 

```{python}

def generate_independent_annealed_kernel(
  log_prob,
  log_ref,
  annealing_schedule,
  kernel_generator,
  params,
) -> tuple:
  """Generates the kernels via the kernel generator given appropriate parameters.

  Args:
    log_prob: log_prob of the target distribution
    log_ref: log_prob of the easy-to-sample reference distribution
    annealing_schedule: annealing schedule such that `annealing_schedule[0] = 0.0` and `annealing_schedule[-1] = 1`
    kernel_generator: `kernel_generator(log_p, param)` returns a transition kernel of signature `kernel(key, state) -> new_state`
    params: parameters for the transition kernels. Note that `len(annealing_schedule) = len(params)`
  """
  if len(annealing_schedule) != len(params):
    raise ValueError("Parameters have to be of the same length as the annealing schedule")
  n_chains = len(annealing_schedule)

  def transition_kernel(key, state, beta, param):
    def log_p(y):
      return beta * log_prob(y) + (1.0 - beta) * log_ref(y)
    return kernel_generator(log_p, param)(key, state)

  def kernel(key, state_joint):
    key_vec = jrandom.split(key, n_chains)
    return jax.vmap(transition_kernel, in_axes=(0, 0, 0, 0))(key_vec, state_joint, annealing_schedule, params)

  return kernel

def generate_swap_chains_decision_kernel(
  log_prob,
  log_ref,
  annealing_schedule,
):
  def log_p(y, beta):
    return beta * log_prob(y) + (1.0 - beta) * log_ref(y)

  def swap_decision(key, state, i: int, j: int) -> bool:
    beta1, beta2 = annealing_schedule[i], annealing_schedule[j]
    x1, x2 = state[i], state[j]
    log_numerator = log_p(x1, beta2) + log_p(x2, beta1) 
    log_denominator = log_p(x1, beta1) + log_p(x2, beta2)
    log_r = log_numerator - log_denominator

    r = jnp.exp(log_r)
    return jrandom.uniform(key) < r

  return swap_decision


def generate_full_sweep_swap_kernel(
  log_prob,
  log_ref,
  annealing_schedule,
):
  """Applies a full sweep, attempting to swap chains 0 <-> 1, then 1 <-> 2 etc. one-after-another.
  """
  n_chains = len(annealing_schedule)

  if n_chains < 2:
    raise ValueError("At least two chains are needed.")

  swap_decision_fn = generate_swap_chains_decision_kernel(
    log_prob=log_prob,
    log_ref=log_ref,
    annealing_schedule=annealing_schedule,
  )

  def kernel(key, state):
    def f(state, i: int):
      subkey = jrandom.fold_in(key, i)
      decision = swap_decision_fn(subkey, state=state, i=i, j=i+1)
      
      # Candidate state: we swap values at i and i+1 positions
      swapped_state = state.at[i].set(state[i+1])
      swapped_state = swapped_state.at[i+1].set(state[i])

      new_state = jax.lax.select(decision, swapped_state, state)
      return new_state, None
    
    final_state, _ = jax.lax.scan(f, state, jnp.arange(n_chains - 1))
    return final_state
  
  return kernel


def compose_kernels(kernels: list):
  """Composes kernels, applying them in order."""
  def kernel(key, state):
    for ker in kernels:
      key, subkey = jrandom.split(key)
      state = ker(subkey, state)

    return state

  return kernel
```

We need also some annealing schedules:

```{python}

def annealing_constant(n_chains: int, base: float = 1.0):
  """Constant annealing schedule, should be avoided."""
  return base * jnp.ones(n_chains)

def annealing_linear(n_chains: int):
  """Linear annealing schedule, should be avoided."""
  return jnp.linspace(0.0, 1.0, n_chains)

def annealing_exponential(n_chains: int, base: float = 2.0**0.5):
  """

  Args:
    n_chains: number of chains in the schedule
    base: annealing base, float larger than 1
  """
  if base <= 1:
    raise ValueError("Base should be larger than 1.")

  if n_chains < 2:
    raise ValueError("At least two chains are required.")
  elif n_chains == 2:
    return jnp.array([0.0, 1.0])
  else:
    x = jnp.append(jnp.power(base, -jnp.arange(n_chains - 1)), 0.0)
    return x[::-1]


```

Now we can re-run the previous experiment, but with the usual random walk kernel being replaced with parallel tempering:

```{python}

xs_pt_all = np.empty((len(SIGMAS), len(DISTRIBUTIONS), N_SAMPLES))

_REFERENCE_SCALE = 30

def log_ref(x):
  return dist.Normal(0, _REFERENCE_SCALE).log_prob(x)

n_chains = 10
betas = annealing_exponential(n_chains, 1.1)


for d, (_, dist_factory) in enumerate(DISTRIBUTIONS.items()):
  def log_prob(x):
    distribution = dist_factory()
    return distribution.log_prob(x)
  
  for s, sigma in enumerate(SIGMAS):
    sigmas = sigma * jnp.ones_like(betas)
    sigmas = sigmas.at[0].set(_REFERENCE_SCALE)  # We know how to sample from the reference distribution 

    K_ind = generate_independent_annealed_kernel(
      log_prob=log_prob,
      log_ref=log_ref,
      annealing_schedule=betas,
      kernel_generator=generate_kernel,
      params=sigmas,
    )

    K_swap = generate_full_sweep_swap_kernel(
      log_prob=log_prob,
      log_ref=log_ref,
      annealing_schedule=betas,
    )
    K_pt = compose_kernels([K_ind, K_swap])

    key, subkey = jrandom.split(key)
    x0 = 0.5 * jnp.ones(n_chains)
    xs_pt = sampling_loop(key=subkey, x0=x0, kernel=K_pt, n_samples=N_SAMPLES * 2)[N_SAMPLES:]

    xs_pt_all[s, d, :] = xs_pt[:, -1]
```

```{python}
fig, axs = plt.subplots(
  len(SIGMAS),
  len(DISTRIBUTIONS),
  sharex="col",
  dpi=350,
)

for i, sigma in enumerate(SIGMAS):
  if sigma < 1:
    axs[i, 0].set_ylabel(f"$\\sigma={sigma:.2f}$")
  else:
    axs[i, 0].set_ylabel(f"$\\sigma={int(sigma)}$")

  for j, (dist_name, dist_factory) in enumerate(DISTRIBUTIONS.items()):
    ax = axs[i, j]

    if i == 0:
      ax.set_title(dist_name)

    bins = 30

    ax.hist(xs_true_all[j], density=True, bins=bins, histtype="step", color="white")

    ax.hist(xs_pt_all[i, j], density=True, bins=bins, histtype="stepfilled", color="C3", alpha=0.4)

for ax in axs.ravel():
  ax.spines[["top", "left", "right"]].set_visible(False)
  ax.set_yticks([])
```

```{python}
fig, axs = plt.subplots(
  len(SIGMAS),
  len(DISTRIBUTIONS),
  sharex="col",
  sharey="col",
  dpi=350,
)

for i, sigma in enumerate(SIGMAS):
  if sigma < 1:
    axs[i, 0].set_ylabel(f"$\\sigma={sigma:.2f}$")
  else:
    axs[i, 0].set_ylabel(f"$\\sigma={int(sigma)}$")

  for j, (dist_name, dist_factory) in enumerate(DISTRIBUTIONS.items()):
    ax = axs[i, j]

    if i == 0:
      ax.set_title(dist_name)

    ax.plot(xs_pt_all[i, j], color="C3")

for ax in axs.ravel():
  ax.spines[["top", "right"]].set_visible(False)
```

This looks much better! However, to obtain these results I needed to tweak the parameters quite a bit. The most important points seem to be the following:

1. The reference distribution should cover a large enough area, so that the modes are covered. Using a reference distribution with a very small standard deviation resulted in non-mixing chain. Using a too "wide" reference distribution resulted in worse mixing, but the problems were less severe than when I used too narrow reference distribution. 
2. Ensuring that local exploration is somewhat efficient is very important. When I used $\sigma$ to sample from $p_0$ (rather than using random walk with an appropriate scale), I could not get a good mixing of the chain.
3. Schedule does matter: sometimes changing the base in the exponential annealing schedule qualitatively changed the results. It however did not seem to be as crucial as using a wide-enough reference which can be explored properly.

### How can we do better?
It would be nice to have a good default for the annealing schedule. Also, if one has multiple machines, it would also be interesting to parallelise the computation: for example, applying the  kernels $N$ kernels to the components $x_n$ can be parallelised and we could also think about parallelising the swap moves. 

## Non-reversible parallel tempering

In [*Non-reversible parallel tempering: a scalable highly parallel MCMC scheme
*](https://arxiv.org/abs/1905.02939), Saifuddin Syed, Alexandre Bouchard-Côté, George Deligiannidis and Arnaud Doucet propose an interesting alternative to the sampling scheme described above.

Consider a *distributed* swapping scheme, distributed across many machines (even though, we will stay with JAX parallelism).
To make it distributed, one may not want to sequentially attempt swapping $i \leftrightarrow i+1$ for all $i=0, \dotsc, N-1$, but rather employ *either* an:

1. *Even move*: swap the states $2k \leftrightarrow 2k+1$, which can be done simultaneously for all $k$.
2. *Odd move*: swap the states $2k-1 \leftrightarrow 2k$, which also can be done simultaneously for all $k$.

Note that both moves are different from what we did above: each full sweep swapped the states consequtively.
In particular, there was a chance (very small, though) to travel from $p_0$ to $p$ in one full sweep (incurring $N-1$ swaps).
Currently $x_0$ can either be swapped with $x_1$ (even move is accepted) or be left in place in one step (odd move or rejected even move).

The authors consider alternating between these moves basing on the following:

1. Stochastic even-odd swap (SEO): the even move is proposed with chance $p$ and the odd move is proposed with chance $1-p$ (in the paper $p=0.5$, providing equal chance).
2. Deterministic even-odd swap (DEO): even time steps result in even moves and odd time steps result in odd moves.

### JAX implementation

Let's quickly implement DEO in JAX:

```{python}

def controlled_swapping(X, M):
  """Swaps particular entries of `X`, as described by binary mask `M`.

  Args:
    X: array of shape (n_chains, dim)
    M: binary mask of shape (n_chains - 1,) controlling which chains should be swapped. We have `M[i] = 1` if `X[i]` and `X[i+1]` should be swapped.
    
  Note:
    Consecutive values 1 in `M` are not allowed.
    Namely, it cannot hold that `M[i] = M[i+1] = 1`.
  """
  def f(Y, i):
    # If M[i] = 1, we swap X[i] with X[i+1]
    # Otherwise we leave it as it was
    
    # Y[i]
    value = jax.lax.select(M[i], X[i + 1], X[i])
    # Y[i + 1]
    value_next = jax.lax.select(M[i], X[i], X[i + 1])
        
    Y = Y.at[i].set(value)
    Y = Y.at[i + 1].set(value_next)
        
    return Y, None

  # Run the scan over the range of M
  Y, _ = jax.lax.scan(f, X, jnp.arange(M.shape[0]))
  return Y


def deo_extended_kernel(
  log_prob,
  log_ref,
  annealing_schedule,
):
  def log_p(y, beta):
    return beta * log_prob(y) + (1.0 - beta) * log_ref(y)

  log_p_vmap = jax.vmap(log_p, in_axes=(0, 0))

  def extended_kernel(
    key,
    state,
    timestep: int,
  ) -> tuple:
    """Extended deterministic even-odd swap kernel, which
    for even timesteps makes even swaps (2i <-> 2i+1)
    and for odd timesteps makes odd swaps (2i-1 <-> 2i)

    Args:
      key: random key
      state: state
      timestep: timestep number, used to decide whether to make even or odd move

    Returns:
      new_state, the same shape as `state`
      rejection_rates, shape (n_chains-1,)
    """
    n_chains = state.shape[0]

    idx1 = jnp.arange(n_chains-1)
    idx2 = idx_i + 1

    xs1 = state[idx1]
    xs2 = state[idx2]

    betas1 = annealing_schedule[idx1]
    betas2 = annealing_schedule[idx2]

    log_numerators = log_p_vmap(xs1, betas2) + log_p_vmap(xs2, betas1) 
    log_denominators = log_p_vmap(xs1, betas1) + log_p_vmap(xs2, betas2)
    log_accept = log_numerator - log_denominator
    accept_prob = jnp.minimum(jnp.exp(log_r), 1.0)
    rejection_rates = 1.0 - accept_prob

    # Where the swaps would be accepted through M-H
    accept_mask = jrandom.bernoulli(key, p=accept_prob)
    # Where the swaps can be accepted due to even-odd moves
    even_odd_mask = jnp.mod(idx1, 2) == jnp.mod(timestep, 2)
    total_mask = accept_mask & even_odd_mask

    # Now the tricky part: we need to execute the swaps
    new_state = controlled_swapping(state, total_mask)
    return new_state, rejection_rates

  return extended_kernel


def deo_sampling_loop(
  key,
  x0,
  kernel_local,
  kernel_deo,
  n_samples: int,
):
  def f(x, timestep: int):
    subkey = jrandom.fold_in(key, timestep)

    key_local, key_deo = jrandom.split(subkey)

    # Apply local exploration kernel
    x = kernel_local(key_local, x)

    # Apply the DEO swap
    x, rejection_rates = kernel_deo(
      key_deo,
      x,
      timestep,
    )

    return x, (x, rejection_rates)
  
  _, (samples, rejection_rates) = jax.lax.scan(f, x0, jrandom.split(key, n_samples))
  
  return samples, rejection_rates
```

```{python}

xs_pt_all = np.empty((len(SIGMAS), len(DISTRIBUTIONS), N_SAMPLES))

_REFERENCE_SCALE = 30

def log_ref(x):
  return dist.Normal(0, _REFERENCE_SCALE).log_prob(x)

n_chains = 10
betas = annealing_exponential(n_chains, 1.1)


for d, (_, dist_factory) in enumerate(DISTRIBUTIONS.items()):
  def log_prob(x):
    distribution = dist_factory()
    return distribution.log_prob(x)
  
  for s, sigma in enumerate(SIGMAS):
    sigmas = sigma * jnp.ones_like(betas)
    sigmas = sigmas.at[0].set(_REFERENCE_SCALE)  # We know how to sample from the reference distribution 

    K_ind = generate_independent_annealed_kernel(
      log_prob=log_prob,
      log_ref=log_ref,
      annealing_schedule=betas,
      kernel_generator=generate_kernel,
      params=sigmas,
    )

    K_swap = generate_full_sweep_swap_kernel(
      log_prob=log_prob,
      log_ref=log_ref,
      annealing_schedule=betas,
    )
    K_pt = compose_kernels([K_ind, K_swap])

    key, subkey = jrandom.split(key)
    x0 = 0.5 * jnp.ones(n_chains)
    xs_pt = sampling_loop(key=subkey, x0=x0, kernel=K_pt, n_samples=N_SAMPLES * 2)[N_SAMPLES:]

    xs_pt_all[s, d, :] = xs_pt[:, -1]
```

```{python}
fig, axs = plt.subplots(
  len(SIGMAS),
  len(DISTRIBUTIONS),
  sharex="col",
  dpi=350,
)

for i, sigma in enumerate(SIGMAS):
  if sigma < 1:
    axs[i, 0].set_ylabel(f"$\\sigma={sigma:.2f}$")
  else:
    axs[i, 0].set_ylabel(f"$\\sigma={int(sigma)}$")

  for j, (dist_name, dist_factory) in enumerate(DISTRIBUTIONS.items()):
    ax = axs[i, j]

    if i == 0:
      ax.set_title(dist_name)

    bins = 30

    ax.hist(xs_true_all[j], density=True, bins=bins, histtype="step", color="white")

    ax.hist(xs_pt_all[i, j], density=True, bins=bins, histtype="stepfilled", color="C3", alpha=0.4)

for ax in axs.ravel():
  ax.spines[["top", "left", "right"]].set_visible(False)
  ax.set_yticks([])
```

```{python}
fig, axs = plt.subplots(
  len(SIGMAS),
  len(DISTRIBUTIONS),
  sharex="col",
  sharey="col",
  dpi=350,
)

for i, sigma in enumerate(SIGMAS):
  if sigma < 1:
    axs[i, 0].set_ylabel(f"$\\sigma={sigma:.2f}$")
  else:
    axs[i, 0].set_ylabel(f"$\\sigma={int(sigma)}$")

  for j, (dist_name, dist_factory) in enumerate(DISTRIBUTIONS.items()):
    ax = axs[i, j]

    if i == 0:
      ax.set_title(dist_name)

    ax.plot(xs_pt_all[i, j], color="C3")

for ax in axs.ravel():
  ax.spines[["top", "right"]].set_visible(False)
```



### Why non-reversible parallel tempering?

Interestingly, DEO has better performance than SEO, which are termed, respectively, non-reversible and reversible parallel tempering schemes.
The discussion whether to use non-reversible or reversible kernels has a long history and I still find it mysterious.
Probably it is worth to write a separate blog post on the topic, but:

  - This wonderful 2000 paper from [Persi Diaconis, Susan Holmes and Radford Neal](https://doi.org/10.1214/aoap/1019487508) shows examples where non-reversible methods are more efficient than reversible ones. 
  - In a [great 2016 paper from Gareth Roberts and Jeffrey Rosenthal](https://doi.org/10.5539/ijsp.v5n1p51) there are examples where "systematic scan" Gibbs samplers (which often are non-reversible, although not always: recall kernels of the form $K_1 K_2 K_1$) can outperform "random scan" (always reversible) Gibbs samplers. Examples with the other behaviour are also provided.
  - In a 2016 [C. Andrieu's paper](https://www.jstor.org/stable/26363466) there is a theorem showing that for two kernels fulfilling some technical assumptions, systematic scans are more efficient than random ones. I think this may perhaps offer an interesting perspective on why DEO may be more efficient than SEO.  

This discussion whether reversible or non-reversible scheme could be used is one way of looking at the problem. Another is through the perspective of reducing a diffusive random walk behaviour by introducing a momentum variable.
Momentum is a common theme in computational statistics and machine learning, with examples such as [MALA](https://en.wikipedia.org/wiki/Metropolis-adjusted_Langevin_algorithm) and [Hamiltonian Monte Carlo](https://en.wikipedia.org/wiki/Hamiltonian_Monte_Carlo) in Markov chain Monte Carlo world or [stochastic gradient descent with momentum](https://en.wikipedia.org/wiki/Stochastic_gradient_descent#Momentum) in optimisation.

I think studying the behaviour of the DEO scheme is an important contribution of this paper, but there several more: 

1. By introducing and studying the index process, the authors devise the DEO sampling scheme together with a method for *choosing the annealing schedule* basing on preliminary runs.
2. The proposed sampling scheme is highly parallelisable and can be used in distributed computing environments. [Pigeons.jl](https://pigeons.run/) serves as a proof that this solution is very practical.


I think I'll skip the precise description of the index process, replacing it with a picture I based on the figures from the paper. We will simulate the path of the chain under the sequential scheme we studied before, SEO and DEO:

```{python}
def _find_trajectory(states, tracked: int = None):
  n_chains = states.shape[1]

  if tracked is None:
    tracked = n_chains // 2

  return jnp.einsum("ng,g->n", states == tracked, jnp.arange(n_chains))

def generate_figure_momentum(p: float = 0.85, n_chains: int = 5, n_timesteps: int = 30):
  rng = np.random.default_rng(42)

  fig, axs = plt.subplots(3, 1, sharex=True, sharey=True)

  x_axis = np.arange(n_timesteps)

  for ax in axs:
    for chain in range(n_chains):
      ax.scatter(x_axis, chain * np.ones_like(x_axis), c="w", s=3)
    ax.spines[["top", "right"]].set_visible(False)

  # Sample full sweep
  state = jnp.arange(n_chains)
  states = [state]
  for timestep in range(1, n_timesteps):
    for i in range(n_chains - 1):
      if rng.binomial(1, p):
        new_state = state.at[i].set(state[i+1])
        new_state = new_state.at[i+1].set(state[i])
        state = new_state
    states.append(state)

  states = jnp.stack(states)
  trajectory = _find_trajectory(states)
  
  ax = axs[0]
  ax.plot(x_axis, trajectory)
  ax.set_title("Full sweep")

  # Sample SEO
  state = jnp.arange(n_chains)
  states = [state]
  for timestep in range(1, n_timesteps):
    mode = rng.binomial(1, 0.5)
    for i in range(n_chains - 1):
      if (i % 2 == mode) and rng.binomial(1, p):
        new_state = state.at[i].set(state[i+1])
        new_state = new_state.at[i+1].set(state[i])
        state = new_state
    states.append(state)

  states = jnp.stack(states)
  trajectory = _find_trajectory(states)
  
  ax = axs[1]
  ax.plot(x_axis, trajectory)
  ax.set_title("Reversible stochastic even-odd swaps (SEO)")

  # Sample DEO
  state = jnp.arange(n_chains)
  states = [state]
  for timestep in range(1, n_timesteps):
    for i in range(n_chains - 1):
      if (i % 2 == timestep % 2) and rng.binomial(1, p):
        new_state = state.at[i].set(state[i+1])
        new_state = new_state.at[i+1].set(state[i])
        state = new_state
    states.append(state)

  states = jnp.stack(states)
  trajectory = _find_trajectory(states)
  
  ax = axs[2]
  ax.plot(x_axis, trajectory)
  ax.set_title("Non-reversible deterministic even-odd swaps (DEO)")

  fig.tight_layout()
  return fig

fig = generate_figure_momentum(p=0.8, n_chains=4, n_timesteps=20)
```

```{python}
fig = generate_figure_momentum(p=0.9, n_chains=10, n_timesteps=50)
```

Definitely, SEO has a trouble going between the reference and the target distribution.
On the other hand, in these simulations (which are very simplistic, though!), DEO does not show a clear advantage over the full sweep.
Let's see, however, how to tune the annealing schedule, which has a wonderful theory outlined in Section 4 of the paper.

### Annealing schedule optimisation

Assuming efficient local exploration of individual components, the authors build a theory how quickly the chain can cycle between $p_0$ and $p=p_N$.
The key quantity is the *instateneous rejection rate* function
$$
  \lambda(\beta) = \frac{1}{2} \mathbb E_{X, Y \sim_\mathrm{i.i.d.} p_\beta } \left[\left| \log \frac{ p(X) p_0(Y)}{ p(Y) p_0(X) } \right|\right],
$$

which depends on the annealing parameter $\beta$ (which controls the measure over which we integrate), but also on how different $p$ and $p_0$ are.
Define
$$
  \Lambda(\beta) = \int_{0}^{\beta} \lambda(\beta') \,\mathrm{d}\beta'
$$

If $\tilde \Lambda = \Lambda(1)$, then for small $\max_i |\beta_i - \beta_{i+1}|$ it holds that the round-trip rate for SEO is about
$$
  f_\mathrm{SEO} \approx \frac{1}{2N + 2\tilde \Lambda}
$$

and for DEO
$$
  f_\mathrm{DEO} \approx \frac{1}{2 \cdot \left(1+\tilde \Lambda\right)}.
$$

In particular, using a large $N$ for SEO leads to diffusive behaviour with close-to-zero round-trip rate. For DEO using larger $N$ does not cause this behaviour, but the round-trip rate is controlled by the communication barrier $\tilde \Lambda$, which depends on $p_0$ and $p$.
Hence, for large $\tilde\Lambda$ many, many iterations may be necessary to obtain enough round trips and good mixing.

Interestingly, $\Lambda$ can be estimated for a fine-grained annealing schedule.
It turns out that if $\rho(\beta, \beta')$ is the expected rejection rate of swapping the chains between $\beta$ and $\beta'$ (so that, of course, $\rho(\beta, \beta') = \rho(\beta', \beta)$), then
$$
  \rho(\beta, \beta') = | \Lambda(\beta) - \Lambda(\beta') | + O(|\beta - \beta'|^3).
$$

In particular,
$$
  \tilde \Lambda \approx \sum_{i=0}^{N-1} \rho(\beta_{i}, \beta_{i+1}),
$$

where the error is of order $O\!\left(N \cdot \max_i |\beta_i - \beta_{i+1}|\right)$.

In other words, $\Lambda(\beta)$ can be estimated from the rejection probabilities.

To optimise the schedule, the authors note that the round-trip rate under DEO is given by
$$
  f = \frac{1}{2\left(1 + \sum_{i=0}^{N-1} \frac{\rho(\beta_{i}, \beta_{i+1})}{ 1-\rho(\beta_i, \beta_{i+1}) } \right)}
$$

for any schedule. (Note that the approximation of $f_\mathrm{DEO}$ when differences $\beta_i - \beta_{i+1}$ are small, can be read from this formula: all $\rho$ are small, so we can ignore terms $1-\rho$ and then we obtain $\tilde \Lambda$).

As we want to maximise $f$, we need to find a schedule minimising the denominator. On the other hand, there is a constraint that for any fine-grained schedule it holds that

$$
  \tilde \Lambda \approx \sum_{i=0}^{N-1} \rho(\beta_{i}, \beta_{i+1}),
$$

so that this is a constrained optimisation problem.
It turns out that the optimum is attained when $\rho(\beta_i, \beta_{i+1})$ are all equal.  Using the relationship between $\rho$ and differences in $\Lambda$, we see that we should aim at
$$
  \Lambda(\beta_i) \approx \frac{i}{N} \tilde \Lambda.
$$

#### JAX implementation

```{python}
from scipy.interpolate import PchipInterpolator
from scipy.optimize import bisect

rejection_rates = jnp.arange(1, 6)  # n_chains: 6

def estimate_lambda_values(rejection_rates, offset: float = 1e-3):
  rejection_rates = jnp.maximum(rejection_rates, offset)
  extended = jnp.concatenate((jnp.zeros(1), rejection_rates))
  return jnp.cumsum(extended)


def get_lambda_function(
  annealing_schedule,
  lambda_values,
):
  return PchipInterpolator(annealing_schedule, lambda_values)


def annealing_optimal(
  n_chains: int,
  previous_schedule,
  rejection_rates,
  _offset: float = 1e-3,
):
  lambda_values = estimate_lambda_values(rejection_rates, offset=_offset)
  lambda_fn = get_lambda_function(
    previous_schedule,
    lambda_values,
  )

  lambda1 = lambda_values[-1]

  new_schedule = [0.0]

  for k in range(1, n_chains - 1):
    def fn(x):
      desired_value = k * lambda1 / (n_chains - 1)
      return lambda_fn(x) - desired_value
    
    new_point = bisect(
      fn,
      new_schedule[-1],
      1.0,
    )

    if new_point >= 1.0:
      raise ValueError("Encountered value 1.0.")

    new_schedule.append(new_point)

  new_schedule.append(1.0)

  if len(new_schedule) != n_chains:
    raise Exception("This should not happen.")

  return jnp.asarray(new_schedule, dtype=float)

new_sch = annealing_optimal(
  10,
  jnp.linspace(0, 1, 8),
  0.1 * jnp.ones(7),
)

print(new_sch)

```


### Summary

I am excited about parallel tempering. 