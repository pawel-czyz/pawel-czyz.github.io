{
  "hash": "6bdd57a44e9dfc47a2d143eb1f0682ed",
  "result": {
    "markdown": "---\ntitle: Starting (on finite domains) with Gaussian processes\ndescription: 'Gaussian processes are wonderful. Let''s take a look at the machinery behind them in the simplest case: when they are just a multivariate normal distribution.'\nauthor: Paweł Czyż\ndate: 2/12/2024\nexecute:\n  freeze: true\nformat:\n  html:\n    code-fold: true\n---\n\nLet $Y = (Y_1, \\dotsc, Y_n)^T$ be a random variable distributed according to the multivariate normal distribution $\\mathcal N(\\mu, \\Sigma)$, where $\\mu\\in \\mathbb R^n$ and $\\Sigma$ is a real symmetric positive-definite[^1] $n\\times n$ matrix.\n\n[^1]: For every non-zero $x\\in \\mathbb R^n$ we have $x^T\\Sigma x > 0$, where the inequality is strict. As $\\Sigma$ is a real symmetric matrix, one of the versions of the spectral theorem yields a decomposition $\\Sigma = R^TDR$ for a diagonal matrix $D$ and orthogonal $R$. Hence, equivalently, we all eigenvalues have to be positive. See [this link](https://en.wikipedia.org/wiki/Definite_matrix) for more discussion. \n\n\nWe will think of this distribution in the following manner: we have a domain $\\mathcal X = \\{1, 2, \\dotsc, n\\}$ and for each $x\\in \\mathcal X$ we have a random variable $Y_i$ and the joint distribution $P(Y_1, \\dotsc, Y_n)$ is multivariate normal.\n\nAssume that we have measured $Y_x$ variables for indices $x_1, \\dotsc, x_k$, with corresponding values $Y_{x_1}=y_1, \\dotsc, Y_{x_k}=y_k$, and we are interested in predicting the values at locations $x'_1, \\dotsc, x'_q$, i.e., modelling the conditional probability distribution\n$$\nP(Y_{x'_1}, \\dotsc, Y_{x'_q} \\mid Y_{x_1}=y_1, \\dotsc, Y_{x_k}=y_k).\n$$\n\nWe will also allow $k=0$, i.e., we would like to access marginal distributions.\nThis can be treated as an extension of the problems answered by bend and mix models we studied [here](../publications/pmi-profiles.qmd).\n\n## Formalising the problem\n\nTo formalise the problem a bit:\n\n::: {.callout-note icon=false}\n\n## Conditional calculation\n\n Consider a set of measured values $M = \\{(x_1, y_1), \\dotsc, (x_k, y_k)\\}$ and a non-empty query set $Q = \\{x'_1, \\dotsc, x'_q\\} \\subseteq \\mathcal X$.\n \n We assume that $Q\\cap M_x = \\varnothing$ and $|M_x| = |M|$, where $M_x = \\{x \\in \\mathcal X \\mid (x, y)\\in M \\text{ for some } y \\}$.\n\nWe would like to be able to sample from the conditional probability distribution\n$$\nP(Y_{x_1'}, \\dotsc, Y_{x_q'} \\mid Y_{x_1}=y_1, \\dotsc, Y_{x_k}=y_k)\n$$\nas well as to evaluate the (log-)density at any point.\n\nWe allow $M=\\varnothing$, which corresponds then to marginal distributions.\n\n:::\n\nThis problem can be solved for multivariate normal distributions by noticing that all conditional (and marginal) distributions will also be multivariate normal.\nLet's introduce some notation.\n\nFor a tuple $\\pi = (\\pi_1, \\dotsc, \\pi_m) \\in \\mathcal X^m$ such that $\\pi_i\\neq \\pi_j$ for $i\\neq j$, we will write $Y_\\pi$ for a random vector $(Y_{\\pi_1}, \\dotsc, Y_{\\pi_m})$.\nNote that this operation can be implemented using a linear mapping $A_\\pi \\colon \\mathbb R^n\\to \\mathbb R^m$ with \n$$\nA_\\pi \\begin{pmatrix} Y_1 \\\\ \\vdots \\\\ Y_n \\end{pmatrix} = \\begin{pmatrix}\n  Y_{\\pi_1} \\\\ \\vdots \\\\ Y_{\\pi_m}\n \\end{pmatrix}\n$$\nand $(A_\\pi)_{oi} = \\mathbf 1[ i = \\pi_o]$.\nHence, $Y_\\pi$ vector is distributed according to $\\mathcal N(A_\\pi\\mu, A_\\pi\\Sigma A_\\pi^T)$.\n\nThe above operation suffices for calculating arbitrary marginal distributions and distributions corresponding to permuting the components.\n\nConsider now the case where we want to calculate a \"true\" conditional distribution (i.e., with $M\\neq \\varnothing$), so the marginalisation does no suffice.\n\nWe can use the tuple $\\pi = (x_1', \\dotsc, x_q', x_1, \\dotsc, x_k)$ to select the right variables and reorder them into a $q$-dimensional block of unobserved (\"query\") variables and a $k$-dimensional block of observed (\"key\") variables[^2].\n\n[^2]: I like to call them \"query\", \"keys\" and \"values\" vectors, which makes the language a bit more similar to [transformers](kernel-regression-transformer.qmd).\nFrom that we just need [one conditioning operation](https://en.wikipedia.org/wiki/Multivariate_normal_distribution#Conditional_distributions):\n\n::: {#tip-cond .callout-note icon=\"false\"}\n## Conditioning multivariate normal\n\nLet $Y=(Y_1, Y_2) \\in \\mathbb R^{k}\\times \\mathbb R^{n-k}$ be a random vector split into blocks of dimensions $k$ and $n-k$. If $Y\\sim \\mathcal N(\\mu, \\Sigma)$, where\n$$\n\\mu = (\\mu_1, \\mu_2)\n$$\nand\n$$\n\\Sigma = \\begin{pmatrix}\n  \\Sigma_{11} & \\Sigma_{12} \\\\\n  \\Sigma_{21} & \\Sigma_{22}\n\\end{pmatrix},\n$$\n\nthen for every $y \\in \\mathbb R^{n-k}$ it holds that\n\n$$\nY_1 \\mid Y_2=y \\sim \\mathcal N(\\mu', \\Sigma'),\n$$\nwhere\n$$\n  \\mu' = \\mu_1 + {\\color{Apricot}\\Sigma_{12}\\Sigma_{22}^{-1}}(y-\\mu_2)\n$$\nand\n$$\n\\Sigma' = \\Sigma_{11} - {\\color{Apricot}\\Sigma_{12} \\Sigma_{22}^{-1}} \\Sigma_{21}.\n$$\n:::\n\nWe see that in both formulae the *matrix of regression coefficients*[^3] $\\color{Apricot}\\Sigma_{12}\\Sigma_{22}^{-1}$ appears.\nWe will discuss calculation of this term [below](#digression-matrix-of-regression-coefficients).\n\n[^3]: Why is it called in this manner? What are the slopes of $\\mu'$ change, when we vary observed values $y$?\n\n\n## Let's write some code\n\nNow let's implement a prototype:\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport numpy as np\nimport numpy.linalg as npla\nfrom jaxtyping import Float, Int, Array\n\nfrom scipy import stats\nimport scipy.linalg as spla\n\n\nclass MultivariateNormal:\n  def __init__(\n    self,\n    mu: Float[Array, \" dim\"],\n    cov: Float[Array, \"dim dim\"],\n  ) -> None:\n    eigvals, _ = npla.eig(cov)\n    if np.min(eigvals) <= 0:\n      raise ValueError(f\"Covariance should be positive-definite.\")\n    \n    self.mu = np.asarray(mu)\n    self.cov = np.asarray(cov)\n    dim = self.mu.shape[0]\n\n    assert self.mu.shape == (dim,)\n    assert self.cov.shape == (dim, dim)\n\n  @property\n  def dim(self) -> int:\n    return self.mu.shape[0]\n\n  def sample(\n    self,\n    rng: np.random.Generator,\n    size: int = 1,\n  ) -> np.ndarray:\n    return rng.multivariate_normal(\n      mean=self.mu, cov=self.cov, size=size,\n    )\n\n  def logpdf(self, y: Float[Array, \" dim\"]) -> float:\n    return stats.multivariate_normal.logpdf(\n      y,\n      mean=self.mu,\n      cov=self.cov,\n      allow_singular=False,\n    )\n\n\ndef _contruct_projection_matrix(\n  n: int,\n  indices: Int[Array, \" k\"],\n) -> Int[Array, \"k n\"]:\n  indices = np.asarray(indices, dtype=int)  \n\n  # Output dimension\n  k = indices.shape[0]\n  if np.unique(indices).shape[0] != k:\n    raise ValueError(\"Indices should be unique.\")\n\n  inp = np.arange(n, dtype=int)\n  \n  ret = np.asarray(inp[None, :] == indices[:, None], dtype=int)\n  assert ret.shape == (k, n)\n  return ret\n\n\ndef select(\n  dist: MultivariateNormal,\n  indices: Int[Array, \" k\"],\n) -> MultivariateNormal:\n  proj = np.asarray(\n    _contruct_projection_matrix(\n      n=dist.dim,\n      indices=indices,\n    ),\n    dtype=float,\n  )\n  \n  new_mu = np.einsum(\"oi,i -> o\", proj, dist.mu)\n  new_cov = np.einsum(\"oi,iI,OI -> oO\", proj, dist.cov, proj)\n  \n  return MultivariateNormal(\n    mu=new_mu,\n    cov=new_cov,\n  )\n\ndef _regression_coefs(\n  sigma12: Float[Array, \"Q K\"],\n  sigma22: Float[Array, \"K K\"],\n) -> Float[Array, \"Q K\"]:\n  return spla.solve(sigma22, sigma12.T).T\n\ndef _condition_gaussian(\n  dist: MultivariateNormal,\n  m: int,\n  y: Float[Array, \" vals\"]\n) -> MultivariateNormal:\n  assert y.shape[0] == dist.dim - m\n\n  mu1 = dist.mu[:m]\n  mu2 = dist.mu[m:]\n  sigma11 = dist.cov[:m, :m]\n  sigma12 = dist.cov[:m, m:]\n  sigma22 = dist.cov[m:, m:]\n\n  reg = _regression_coefs(\n    sigma12=sigma12, sigma22=sigma22,\n  )\n\n  mu_ = mu1 + reg @ (y - mu2)\n  sigma_ = sigma11 - reg @ sigma12.T\n\n  return MultivariateNormal(\n    mu=mu_, cov=sigma_,\n  )\n\ndef condition(\n  dist: MultivariateNormal,\n  query: Int[Array, \" Q\"],\n  key: Int[Array, \" K\"],\n  values: Float[Array, \" K\"],\n) -> MultivariateNormal:\n  q, k = query.shape[0], key.shape[0]\n  assert values.shape == (k,), \"Values have wrong shape\"\n\n  total_index = np.concatenate((query, key))\n\n  if np.unique(total_index).shape[0] != k + q:\n    raise ValueError(\"Indices must be unique.\")\n  if np.min(total_index) < 0 or np.max(total_index) >= dist.dim:\n    raise ValueError(\"Indices must be from the set 0, 1, ..., dim-1.\")\n\n  ordered_dist = select(dist, indices=total_index)\n\n  if k == 0:\n    return ordered_dist\n  else:\n    return _condition_gaussian(\n      dist=ordered_dist,\n      m=q,\n      y=values,\n    )\n```\n:::\n\n\nNow we can do conditioning.\nFor example, imagine that we have $\\mu = 0$ and we know $\\Sigma$: $Y_1$ correlates with $Y_3$, and $Y_2$ anticorrelates with $Y_4$ and $Y_5$ doesn't correlate with anything else. \nWe measure $Y_1$ and $Y_2$, so we can use this correlation structure to impute $Y_3$, $Y_4$ and $Y_5$.\n\nFirst, let's plot the covariance matrix and the samples:\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nplt.style.use(\"dark_background\")\n\nmixing = np.asarray([\n  [1.5, -0.7, 1.5, 0, 0],\n  [0, 1, 0, -1, 0],\n  [0, 0, 0, 0, 1],\n], dtype=float)\n\ncov = np.einsum(\"io,iO->oO\", mixing, mixing) + 0.1 * np.eye(5)\n\nfig, axs = plt.subplots(1, 2, figsize=(4.5, 2), dpi=200)\nticklabels = [f\"$Y_{i}$\" for i in range(1, 5+1)]\n\nax = axs[0]\nsns.heatmap(cov, ax=ax, xticklabels=ticklabels, yticklabels=ticklabels, vmin=-1.5, vmax=1.5, center=0, square=True, annot=True, fmt=\".1f\")\n\ndist = MultivariateNormal(mu=np.zeros(5), cov=cov)\nrng = np.random.default_rng(42)\nsamples = dist.sample(rng, size=10)\n\nax = axs[1]\nfor sample in samples:\n  x_ax = np.arange(1, 6)\n  ax.plot(x_ax, sample, alpha=0.5, c=\"C1\")\n  ax.scatter(x_ax, sample, alpha=0.5, c=\"C1\", s=4)\n\nax.spines[['top', 'right']].set_visible(False)\nax.set_xlim(0.9, 5.1)\nax.set_xticks(x_ax, ticklabels)\n\nfig.tight_layout()\n```\n\n::: {.cell-output .cell-output-display}\n![](conditioning-multivariate-normal_files/figure-html/cell-3-output-1.png){}\n:::\n:::\n\n\nImagine now that we observed $Y_1=1.5$ and $Y_2=1$.\nWe expect that $Y_3$ should move upwards (the posterior should be shifted so that most of the mass is above $0$), $Y_4$ to go downwards and $Y_5$ to stay as it was.\nLet's plot covariance matrix and draws from the conditional posterior $P(Y_3, Y_4, Y_5\\mid Y_1=1.5, Y_2=1)$: \n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\ny_obs = np.asarray([1.5, 1])\ncond = condition(\n  dist=dist,\n  query=np.asarray([2, 3, 4]),\n  key=np.asarray([0, 1]),\n  values=y_obs,\n)\n\nfig, axs = plt.subplots(1, 2, figsize=(4.5, 2), dpi=200)\n\nax = axs[0]\nsns.heatmap(\n  cond.cov,\n  ax=ax,\n  xticklabels=ticklabels[2:],\n  yticklabels=ticklabels[2:],\n  vmin=-1.5,\n  vmax=1.5,\n  center=0,\n  annot=True,\n  square=True,\n)\n\nax = axs[1]\nax.spines[['top', 'right']].set_visible(False)\nax.set_xlim(0.9, 5.1)\nax.set_xticks(x_ax)\n\nax.scatter([1, 2], y_obs, c=\"maroon\", s=4)\n\nsamples = cond.sample(rng, size=10)\nfor sample in samples:\n  ax.plot([3, 4, 5], sample, alpha=0.5, c=\"C1\")\n  ax.scatter([3, 4, 5], sample, alpha=0.5, c=\"C1\", s=4)\n\nax.set_xticks(x_ax, ticklabels)\n\nfig.tight_layout()\n```\n\n::: {.cell-output .cell-output-display}\n![](conditioning-multivariate-normal_files/figure-html/cell-4-output-1.png){}\n:::\n:::\n\n\nWe see that there is slight anticorrelation between $Y_3$ and $Y_4$: by sampling from the conditional distribution we obtain a coherent sample.\nThis is different than drawing independent samples from $P(Y_3 \\mid Y_1=y_1, Y_2=y_2)$ and $P(Y_4\\mid Y_1=y_1, Y_2=y_2)$.\nPerhaps it'll be easier to visualise it on a scatter plot:\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\nfig, axs = plt.subplots(1, 2, figsize=(4.5, 3), dpi=200, sharex=True, sharey=True)\nsamples = cond.sample(rng, size=15_000)\n\nax = axs[0]\nax.scatter(samples[:, 0], samples[:, 1], s=2, alpha=0.01)\nax.set_title(\"Joint sample\")\nax.set_xlabel(r\"$Y_3$\")\nax.set_ylabel(r\"$Y_4$\")\n\nax = axs[1]\nsamples2 = cond.sample(rng, size=samples.shape[0])\nax.scatter(samples[:, 0], samples2[:, 1], s=2, alpha=0.01)\nax.set_title(\"Independent\")\nax.set_xlabel(r\"$Y_3$\")\nax.set_ylabel(r\"$Y_4$\")\n\ncorr = cond.cov[0, 1] / np.sqrt(cond.cov[0, 0] * cond.cov[1, 1])\nfig.suptitle(r\"$\\text{Corr}(Y_3, Y_4) = $\" + f\"{corr:.2f}\")\nfig.tight_layout()\n\nfor ax in axs:\n  ax.spines[['top', 'right']].set_visible(False)\n```\n\n::: {.cell-output .cell-output-display}\n![](conditioning-multivariate-normal_files/figure-html/cell-5-output-1.png){}\n:::\n:::\n\n\nOk, it's hard to see, but visible – the (negative) correlation is just quite weak in this case.\n\nLet's do the last visualisation before we move to Gaussian processes. As mentioned, the magical thing is the access to the whole posterior distribution $P(Y_3, Y_4, Y_5 \\mid Y_1=y_1, Y_2=y_2)$: we can evaluate arbitrary probabilities and sample consistent vectors from this distribution.\nWe can visualise samples, but sometimes a simpler summary statistic would be useful. Each of the distributions $P(Y_i \\mid Y_1=y_1, Y_2=y_2)$ is one-dimensional Gaussian, so we can plot its mean and standard deviation. Or, even better, let's plot $\\mu_i\\pm 2\\sigma_i$ to see where [approximately 95% of probability lies](https://en.wikipedia.org/wiki/68%E2%80%9395%E2%80%9399.7_rule).\n\nWe'll plot these regions both before and after conditioning:\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\nfig, axs = plt.subplots(1, 2, figsize=(4, 1.5), dpi=170, sharex=True, sharey=True)\n\n# Before conditioning\nax = axs[0]\n\nax.plot(np.arange(1, 1+5), np.zeros(5), linestyle=\"--\", c=\"gray\", alpha=0.5, linewidth=0.8)\n\nax.errorbar(\n  np.arange(1, 1+5),\n  dist.mu,\n  yerr=2 * np.sqrt(np.diagonal(dist.cov)),\n  fmt=\"o\",\n)\n\n# After conditioning\nax = axs[1]\nax.plot(np.arange(1, 1+5), np.zeros(5), linestyle=\"--\", c=\"gray\", alpha=0.5, linewidth=0.8)\nax.scatter([1, 2], y_obs, c=\"maroon\", s=4)\nax.errorbar(\n  [3, 4, 5],\n  cond.mu,\n  yerr=2 * np.sqrt(np.diagonal(cond.cov)),\n  fmt=\"o\",\n)\n\nfor ax in axs:\n  ax.spines[['top', 'right']].set_visible(False)\n  ax.set_xlim(0.8, 5.3)\n  ax.set_xticks(x_ax, ticklabels)\n\nfig.tight_layout()\n```\n\n::: {.cell-output .cell-output-display}\n![](conditioning-multivariate-normal_files/figure-html/cell-6-output-1.png){}\n:::\n:::\n\n\nAs mentioned, these plots don't really allow us to look at correlations between different variables, but they are still useful: we can easily see that the posterior of $Y_3$ moved upwards and $Y_4$ moved downwards!\nVariable $Y_5$, which is independent of $(Y_1, Y_2, Y_3, Y_4)$, doesn't change: if we want to know it, we just have to measure it.\n\n## Gaussian processes\n\nFor $\\mathcal X = \\{1, \\dotsc, n\\}$ we considered an indexed collection of random variables $\\{Y_x\\}_{x\\in \\mathcal X}$. Let's call it a *stochastic process*. \n\nThis stochastic process has the property that the joint distribution over all variables was multivariate normal.\nFrom that we could deduce that distributions $P(Y_{x_1}, \\dotsc, Y_{x_m})$ were again multivariate normal, what in turn allowed us to do prediction via conditioning (which resulted, again, in multivariate normal distributions).\n\nLet's move beyond a finite dimension: take $\\mathcal X=\\mathbb R$ and consider a stochastic process $\\{Y_x\\}_{x\\in \\mathcal X}$.\nWe will say that it's a *Gaussian process* if for every finite set $\\{x_1, \\dotsc, x_m\\}\\subseteq \\mathcal X$ the joint distribution $P(Y_{x_1}, \\dotsc, Y_{x_m})$ is multivariate normal.\nMore generally, we can take other domains $\\mathcal X$ (e.g., $\\mathbb R^n$) and speak of *Gaussian random fields*.\n\nIn either case, the trick is that we never work with infinitely many random variables at once: for example, if we observe values $y_1, \\dotsc, y_k$ at locations $x_1, \\dotsc,  x_k$ and we want to predict the values at points $x'_1, \\dotsc, x'_q$, we will construct the joint multivariate normal distribution\n$P(Y_{x_1}, \\dotsc, Y_{x_m}, Y_{x'_1}, \\dotsc, Y_{x'_q})$ and condition on observed values to get the conditional distribution\n$P(Y_{x'_1}, \\dotsc, Y_{x'_q} \\mid Y_{x_1} = y_1, \\dotsc, Y_{x_k}=y_k)$.\n\nNow the questions is: how can we define a consistent stochastic process with these great properties?\nWhen $\\mathcal X$ was finite, we could just define the joint probability distribution over all variables via mean and covariance. But now $\\mathcal X$ is not finite!\n\nConsider therefore two functions, giving the mean and covariance: $m \\colon \\mathcal X\\to \\mathbb R$ and $k\\colon \\mathcal X\\to \\mathcal X\\to \\mathbb R^+$.\nThe premise is to build multivariate normal distributions $P(Y_{x_1}, \\dotsc, Y_{x_m})$ by using the mean vector $\\mu_i = m(x_i)$ and covariance matrix $\\Sigma_{ij} = k(x_i, x_j)$.\n\nFirst of all, we see that not all covariance functions are suitable: we want covariance matrices to be symmetric and positive-definite, so we should use [positive-definite kernels](https://en.wikipedia.org/wiki/Positive-definite_kernel).\n\nSecondly, we don't know if these probability distributions can be coherently glued to a stochastic process.\nThe answer to this problem is provided by [Daniell-Kolmogorov extension theorem](https://en.wikipedia.org/wiki/Kolmogorov_extension_theorem), which says when a family of probability distributions can be coherently glued yielding a stochastic process.\nIn this case parameterising covariances via $\\Sigma_{ij}=k(x_i, x_j)$ has the properties mentioned in the theorem. On the other hand, parameterising precision matrices via $k(x_i, x_j)$ doesn't generally yield a coherent stochastic process.\n\nThere are many important technical details, which I should mention here. Instead, I'll refer to [a great introduction to Gaussian processes](https://dansblog.netlify.app/posts/2021-11-03-yes-but-what-is-a-gaussian-process-or-once-twice-three-times-a-definition-or-a-descent-into-madness/yes-but-what-is-a-gaussian-process-or-once-twice-three-times-a-definition-or-a-descent-into-madness) at [Dan Simpson's blog](https://dansblog.netlify.app/), and implement something.\n\n## Modelling functions\n\nThere are many libraries for working with Gaussian processes, including [GPJax](https://github.com/JaxGaussianProcesses/GPJax), [GPyTorch](https://gpytorch.ai/) and [GPy](https://gpytorch.ai/).\nWe will however just use the code developed above, plus some simple covariance functions.\n\nOur task will be the following: we are given some function on the interval $(0, 1)$.\nWe observe some values $M=\\{(x_1, y_1), \\dotsc, (x_k, y_k)\\}$ inside the intervals $(0, u)$ and $(1-u, 1)$ and we want to predict the function behaviour in the interval $(u, 1-u)$, from which we do not have any data.\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\nimport dataclasses\n\n@dataclasses.dataclass\nclass Task:\n  xs_all: Float[Array, \" points\"]\n  ys_all: Float[Array, \" points\"]\n  xs_obs: Float[Array, \" key\"]\n  ys_obs: Float[Array, \" key\"]\n  xs_query: Float[Array, \" query\"]\n\ndef create_task(\n  f,\n  thresh: float = 0.25,\n  k_2: int = 5,\n  n_query: int = 101,\n) -> Task:\n  assert 0.02 < thresh < 0.98, \"Threshold should be in (0.02, 0.98)\"\n\n  xs_all = np.linspace(0.01, 1)\n  \n  xs_obs = np.concatenate((np.linspace(0.01, thresh, k_2), np.linspace(1 - thresh, 0.99, k_2)))\n  xs_query = np.linspace(thresh + 0.01, 0.99 - thresh, n_query)\n\n  return Task(\n    xs_all=xs_all,\n    ys_all=f(xs_all),\n    xs_obs=xs_obs,\n    ys_obs=f(xs_obs),\n    xs_query=xs_query,\n  )\n\ndef plot_task(ax: plt.Axes, task: Task):\n  ax.plot(\n    task.xs_all, task.ys_all, linestyle=\"--\", c=\"maroon\", linewidth=1.0, alpha=0.8\n  )\n  ax.scatter(task.xs_obs, task.ys_obs, s=8, c=\"maroon\")\n  ax.spines[['top', 'right']].set_visible(False)\n  ax.set_xlim(-0.02, 1.02)\n\n\nfig, ax = plt.subplots(figsize=(2.2, 1.4), dpi=200)\n\ntask1 = create_task(lambda x: np.cos(2*np.pi * x))\nplot_task(ax, task1)\nfig.tight_layout()\n```\n\n::: {.cell-output .cell-output-display}\n![](conditioning-multivariate-normal_files/figure-html/cell-7-output-1.png){}\n:::\n:::\n\n\nWe can approach this problem in two ways: first, we can impute missing values evaluated at some points.\n\nFor example, we can define a grid over $(u, 1-u)$ with $q$ query points $x'_1, \\dotsc, x'_q$ and sample from the conditional distribution $P(Y_{x'_1}, \\dotsc, Y_{x'_q} \\mid Y_{x_1}=y_1, \\dotsc, Y_{x_k}=y_k)$ several times.\nThis is one good way of plotting, showing us the behaviour of the whole sample at once.\n\nAnother way of plotting, which we also have already seen, is to take a single point $x'$ and look at the normal distribution $P(Y_{x'} \\mid Y_{x_1}=y_1, \\dotsc, Y_{x_k}=y_k)$, summarized by the mean and standard deviation: we can plot $\\mu(x') \\pm 2\\sigma(x')$ as a function of $x'$ (similarly to the finite-dimensional case).\nThis approach doesn't allow us to look at joint behaviour at different locations, but is quite convenient to summarise uncertainty at a single specific point.\nFor example, this may be informative enough to determine a good location of the next sample to collect in Bayesian optimisation framework (unless one wants to consider multiple points).\n\nLet's implement an example kernel and plot predictions in both ways:\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\ndef kernel(x, x_):\n  return np.exp(-20 * np.square(x-x_))\n\nfig, axs = plt.subplots(1, 2, figsize=(2*3, 2), dpi=120, sharex=True, sharey=True)\n\nfor ax in axs:\n  plot_task(ax, task1)\n\nxs_eval = np.concatenate((task1.xs_query, task1.xs_obs))\n\ncov = kernel(xs_eval[:, None], xs_eval[None, :]) + 1e-6 * np.eye(len(xs_eval))\ndist = MultivariateNormal(np.zeros_like(xs_eval), cov)\n\ncond = condition(\n  dist=dist,\n  query=np.arange(len(task1.xs_query)),\n  key=np.arange(len(task1.xs_query), len(xs_eval)),\n  values=task1.ys_obs\n)\n\nrng = np.random.default_rng(1)\nsamples = cond.sample(rng, size=30)\n\nax = axs[0]\nfor sample in samples:\n  ax.plot(task1.xs_query, sample, alpha=0.8, color=\"C1\", linewidth=0.1)\n\n\nax = axs[1]\nax.plot(task1.xs_query, cond.mu, c=\"C1\", linestyle=\":\")\nuncert = 2 * np.sqrt(np.diagonal(cond.cov))\nax.fill_between(task1.xs_query, cond.mu - uncert, cond.mu + uncert, color=\"C1\", alpha=0.2)\n\nfig.tight_layout()\n```\n\n::: {.cell-output .cell-output-display}\n![](conditioning-multivariate-normal_files/figure-html/cell-8-output-1.png){}\n:::\n:::\n\n\nNice! I've been thinking about showing how different kernels result in differing predictions.\nBut this post is already a bit too long, so I may write another one on this topic.\nIn any case, there's a [Kernel Cookbook](https://www.cs.toronto.edu/~duvenaud/cookbook/) created by David Duvenaud.\n\n## Appendix\n\n### Matrix of regression coefficients\n\nLet's take a quick look at the *matrix of regression coefficients*, $\\Sigma_{12}\\Sigma_{22}^{-1}$.\n\nWe could implement it via inversion, but [there is a better solution](https://peterroelants.github.io/posts/gaussian-process-tutorial/#Predictions-from-posterior).\n\nNamely, note that if $X=\\Sigma_{12} \\Sigma_{22}^{-1}$, then\n\n$$\n  \\Sigma_{22} X^T = \\Sigma_{22} \\Sigma_{22}^{-1} \\Sigma_{12}^T = \\Sigma_{12}^T\n$$\n\nHence, $X^T$ is a solution to a matrix equation, which we can implement using [`scipy.linalg.solve`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.linalg.solve.html).\nThis is considered a better practice as it increases the numerical precision and can be faster (which is visible for large matrices; for small matrices the solution using matrix inversion was often faster).\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\nimport time\nimport numpy as np\nimport numpy.linalg as npla\nimport scipy.linalg as spla\n\n\ndef calc_inv(sigma_12, sigma_22):\n  return sigma_12 @ npla.inv(sigma_22)\n\n\ndef calc_solve(sigma_12, sigma_22):\n  return spla.solve(sigma_22, sigma_12.T).T\n\nrng = np.random.default_rng(42)\n\nn_examples = 3\nsize = 20\nm = 10\n\n_coefs = rng.normal(size=(n_examples, size, size))\nsigmas = np.einsum(\"kab,kac->kbc\", _coefs, _coefs)\nfor sigma in sigmas:\n  assert np.min(npla.eigvals(sigma)) > 0\n\n  sigma_12 = sigma[:m, m:]\n  sigma_22 = sigma[m:, m:]\n\n  sol1 = calc_inv(sigma_12, sigma_22)\n  sol2 = calc_solve(sigma_12, sigma_22)\n\n  assert np.allclose(sol1, sol2), \"Solutions differ.\"\n```\n:::\n\n\n",
    "supporting": [
      "conditioning-multivariate-normal_files"
    ],
    "filters": [],
    "includes": {}
  }
}