{
  "hash": "98b49af64e8da091c9c446bbe98665ff",
  "result": {
    "markdown": "---\ntitle: The mutual information saga\ndescription: ''\nauthor: Paweł Czyż\ndate: 20/11/2023\ncategories:\n  - Mutual information\n  - Monte Carlo\n  - Clustering\n  - Bayesian statistics\nbibliography: references.bib\nexecute:\n  freeze: auto\ndraft: true\nformat:\n  html:\n    code-fold: true\n---\n\nWhere should I start this story? Probably a good origin will be in the [Laboratory of Modeling in Biology and Medicine](https://pmbm.ippt.pan.pl/web/Main_Page) in Warsaw, where [Tomasz Lipniacki](http://www.ippt.pan.pl/en/staff/tlipnia) and [Marek Kochańczyk](https://pmbm.ippt.pan.pl/web/Marek_Kocha%C5%84czyk) decided to mentor two students who just completed high-school education, namely the younger versions of [Frederic](https://pmbm.ippt.pan.pl/web/Frederic_Grabowski) and myself.\n\nThey were great mentors and they got us interested in information theory: given two random variables $X$ and $Y$, mutual information $I(X; Y)$ how dependent they are. Namely, $I(X; Y) = 0$ if and only if they are independent and contrary to e.g., correlation, mutual information works for any kind of variables (no matter whether they are continuous or discrete and what dimensionality they have!) and can capture nonlinear dependencies as well.\n\nWe worked on cell signalling, where $X$ was a discrete input given to the cell and $Y$ was the measured continuous output. I'm very grateful for this opportunity: it showed me how to do research finishing with [a paper](https://royalsocietypublishing.org/doi/10.1098/rsif.2018.0792), and perhaps more importantly, got me interested in information theory and biology.\n\nAnyway, years have passed, I became older and got an opportunity to supervise the thesis of a very bright Master's student, [Anej](https://anejsvete.github.io/). Anej did excellent work trying to apply mutual information to learned latent spaces. Perhaps the project proposal I submitted was not completely off – it was concurrently investigated in Section 4.2 of the [k-sliced mutual information](https://arxiv.org/abs/2206.08526) paper, which has a lot of cool theoretical results I haven't thought of! – but I definitely had not done the homework properly. Namely, different mutual information estimators gave *very* different estimates, which was concerning, even when considered distributions were low-dimensional.\n\nI did the only rational thing in this situation, meaning that I ran asking for help to two mutual information experts, Frederic and [Alex](https://www.a-marx.com/). Together, we started questioning authorities, namely:\n\n1. How do we really know that mutual information estimators work? The evidence that they work is collected by investigating low-dimensional distributions and moderate-dimensional multivariate normal distributions.\n2. Is it possible to construct more expressive distributions with known ground-truth mutual information?\n3. And, the most important question to me (in the end I'm an imposter in machine learning world, having primary background in differential geometry), was: how invariant are the estimators to diffeomorphisms? Namely, if $f$ and $g$ are diffeomorphisms, then $I(X; Y) = I(f(X); g(Y))$. Do the numerical estimates have the same property?\n\nQuestions 1. and 2. are related. But so are 2. and 3.! Namely, consider two random variables $X$ and $Y$, such that we can easily sample points $(x_1, y_1), \\dotsc, (x_n, y_n)$ from the joint distribution $P_{XY}$. If $f$ and $g$ are diffeomorphisms, we can apply them to obtain a sample $(f(x_1), g(y_1)), \\dotsc, (f(x_n), g(y_n))$ from $P_{f(X)g(Y)}$ and the mutual information has not changed: if we know $I(X; Y)$, then we also know\n$$I(f(X); g(Y)) = I(X; Y).$$\n\nThis invariance property holds for variables valued in standard Borel spaces and general continuous injective mappings (see [Theorem 2.1 here](https://openreview.net/forum?id=25vRtG56YH)), which is apparently a well-known fact, but it still took us quite some time to prove it (M.S. Pinsker's [Information and information stability of random variables and processes](https://books.google.pl/books/about/Information_and_Information_Stability_of.html?id=seEyAAAAMAAJ) proved to be an invaluable resource).\n\nWe started programming, in the meantime learning [Snakemake](https://snakemake.readthedocs.io/), and after five months we had a ready manuscript investigating the performance of several mutual information estimators on different distributions.\nIt was titled \"Are mutual information estimators homeomorphism-invariant?\", which signifies which question I found the most important.\n\nWell, I was wrong: when the paper got rejected from ICML we realized that the most important aspect of our work was really 1., rather than 3. It became clear after Frederic pointed out to me the following fact: if $M$ is a smooth manifold of dimension at least 2, then for two set of distinct points $\\{a_1, \\dotsc, a_n\\}$ and $\\{b_1, \\dotsc, b_n\\}$ there exists a diffeomorphism $u\\colon M\\to M$ such that $b_i = u(a_i)$ (the proof of this fact can be e.g., found in P.W. Michor's and Cornelia Vizman's _$n$-transitivity of certain diffeomorphisms groups_).\nIn particular, if the estimates were diffeomorphism-invariant, for any finite set of points $(x_1, y_1), \\dotsc, (x_n, y_n)$ we could arrange them into absolutely any shape without changing the estimate. Hence, all diffeomorphism-invariant mutual information estimators have to ignore any input data points, always returning a pre-specified constant.\n\nWe improved the experiments in the paper and changed the story to _Beyond normal: on the evaluation of the mutual information estimators_ (let me state two obvious things. First: \"Beyond\" refers to the fact that we can transform multivariate normal distributions to obtain more expressive distributions. Second: [Next to Normal](https://en.wikipedia.org/wiki/Next_to_Normal), which is a wonderful musical, may have had the title copyrighted), which was accepted to NeurIPS.\n\nOf course, we were very happy. But there were some important aspects that deserved to be studied a bit more.\n\n## The trouble\n\n",
    "supporting": [
      "on-beyond-normal_files"
    ],
    "filters": [],
    "includes": {}
  }
}