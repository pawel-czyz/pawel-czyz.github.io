{
  "hash": "4fce343a23f78ac6399d81daab701d2c",
  "result": {
    "markdown": "---\ntitle: Non-reversible parallel tempering\ndescription: Markov chain got stuck in a single mode? Parallel tempering comes to help.\nauthor: Paweł Czyż\ndate: 9/1/2024\nexecute:\n  freeze: true\nformat:\n  html:\n    code-fold: true\n---\n\nI have revived my interest in [Ising models](distinct-ising-models.qmd), which can be now trained using [discrete Fisher divergence](discrete-intractable-likelihood.qmd).\nHowever, once the model is trained, I would like to generate synthetic samples and evaluate the quality of the fit.\n\nThe standard solution for sampling from such distributions is our usual suspect, Markov chain Monte Carlo (MCMC).\nHowever, MCMC can get \"trapped\" in a single mode of the distribution and never escape it within the (finite) simulation time.\nIsing models are somewhat hard to visualise, so let's focus on some one-dimensional problem and the simple Metropolis algorithm with Gaussian random walk proposals, namely $q(x'\\mid x) = \\mathcal N\\!\\left(x' \\mid x, \\sigma^2\\right)$.\n\nWe can use MCMC to obtain samples for the following problems:\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport numpy as np\n\nimport jax\nimport jax.random as jrandom\nimport jax.numpy as jnp\nfrom jaxtyping import Float, Int, Array\n\nimport numpyro\nimport numpyro.distributions as dist\n\nimport matplotlib.pyplot as plt\nplt.style.use(\"dark_background\")\n\nfrom typing import Any\nfrom collections import OrderedDict\n\nRandomKey = jax.Array\nKernel = callable  # kernel(key, x) -> new_x\nKernelParam = Any\n\nclass JAXRNG:\n    \"\"\"JAX stateful random number generator.\n\n    Example:\n      key = jax.random.PRNGKey(5)\n      rng = JAXRNG(key)\n      a = jax.random.bernoulli(rng.key, shape=(10,))\n      b = jax.random.bernoulli(rng.key, shape=(10,))\n    \"\"\"\n\n    def __init__(self, key: RandomKey) -> None:\n        \"\"\"\n        Args:\n            key: initialization key\n        \"\"\"\n        self._key = key\n\n    @property\n    def key(self) -> RandomKey:\n        \"\"\"Generates a new key.\"\"\"\n        key, subkey = jax.random.split(self._key)\n        self._key = key\n        return subkey\n\n    def __repr__(self) -> str:\n        \"\"\"Used by the repr() method.\"\"\"\n        return f\"{type(self).__name__}(key={self._key})\"\n\n    def __str__(self) -> str:\n        \"\"\"Used by the str() method.\"\"\"\n        return repr(self)\n\n\ndef mcmc_sampling_loop(\n  key: RandomKey,\n  x0: Float[Array, \" *dim\"],\n  kernel: Kernel,\n  n_samples: int,\n  warmup: int = 2_000,\n) -> Float[Array, \"n_samples *dim\"]:\n  \"\"\"Markov chain Monte Carlo sampling loop.\"\"\"\n  def f(x, subkey):\n    x_new = kernel(subkey, x)\n    return x_new, x_new\n  \n  key_warmup, key_sampling = jrandom.split(key)\n  # Run warm-up: update the starting point, but do not collect samples:\n  x0, _ = jax.lax.scan(f, x0, jrandom.split(key_warmup, warmup))\n  # Collect the samples:\n  _, samples = jax.lax.scan(f, x0, jrandom.split(key_sampling, n_samples))\n  \n  return samples\n\n\ndef sample_exact(\n  key: RandomKey,\n  distributions: OrderedDict,\n  n_samples: int,\n) -> Float[Array, \"n_distributions n_samples\"]:\n  \"\"\"Samples from the ground-truth distributions using (exact) ancestral sampling in NumPyro.\n  \n  Args:\n    key: JAX random key\n    distributions: an ordered dictionary mapping names to distribution factories. For example, `OrderedDict({\"name\": factory})`, where `factory()` returns a NumPyro distribution.\n    n_samples: number of samples to collect  \n  \"\"\"\n  xs_all = np.empty((len(distributions), n_samples))\n\n  for idx, (_, dist_factory) in enumerate(distributions.items()):\n    key, subkey = jrandom.split(key)\n    distrib = dist_factory()\n    xs = distrib.sample(subkey, sample_shape=(n_samples,))\n    \n    xs_all[idx, :] = np.array(xs)\n\n  return xs_all\n\n\ndef make_multirun(\n  key: RandomKey,\n  sampling_fn,\n  distributions: OrderedDict,\n  params: list,\n) -> Float[Array, \"params distributions *samples\"]:\n  \"\"\"This function applies a sampling function over all distributions and parameters.\n\n  Args:\n    sampling_fn: function used to provide samples. It has the signature\n        sampling_fn(key, log_prob, param) -> Float[Array, \" *samples\"]\n      where \"*samples\" encodes all the dimension of the sample\n  \"\"\"\n  all_samples = []\n  for param in params:\n    samples_param = []\n    for _, dist_factory in distributions.items():\n    # Define log-PDF\n      def log_p(x):\n        distribution = dist_factory()\n        return distribution.log_prob(x)\n      \n      key, subkey = jrandom.split(key)\n\n      samples = np.array(sampling_fn(key, log_p, param))\n      # Append the samples\n      samples_param.append(samples)\n    # Now add the row\n    all_samples.append(np.array(samples_param))\n\n  return np.array(all_samples)\n\n\ndef mcmc_multirun(\n  key: RandomKey,\n  kernel_generator,\n  distributions: OrderedDict,\n  params: list[KernelParam],\n  n_samples: int,\n  warmup: int,\n  x0: float = 0.5\n) -> Float[Array, \"params distributions n_samples\"]:\n  \"\"\"A high-level function running an array of MCMC samplers\n  over different distributions and parameter settings.\n  \"\"\"\n  def sampling_fn(key, log_prob, param):\n    kernel = kernel_generator(log_prob, param)\n    return mcmc_sampling_loop(\n      key=key, x0=jnp.asarray(x0), kernel=kernel, n_samples=n_samples, warmup=warmup\n    )\n\n  return make_multirun(\n    key=key, sampling_fn=sampling_fn, distributions=distributions, params=params\n  )\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n/home/pawel/micromamba/envs/data-science/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n```\n:::\n:::\n\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\ndef make_multipanel_figure(\n  plotting_fn,\n  distributions: OrderedDict,\n  params: list[KernelParam],\n  samples_exact: Float[Array, \"distributions n_samples_exact\"] | None,\n  samples_mcmc: Float[Array, \"params distributions n_samples_mcmc\"],\n  n_spines: int = 1,\n  sharex=\"col\",\n  dpi=350,\n  param_name = \"\",\n):\n  def _param_to_str(p):\n    if isinstance(p, float):\n      return f\"{p:.1f}\"\n    else:\n      return str(p)\n\n  fig, axs = plt.subplots(\n    len(params),\n    len(distributions),\n    sharex=sharex,\n    dpi=dpi,\n  )\n\n  for param_idx, param_value in enumerate(params):\n    if param_name is None:\n      axs[param_idx, 0].set_ylabel(_param_to_str(param_value))\n    else:\n      axs[param_idx, 0].set_ylabel(f\"{param_name}={_param_to_str(param_value)}\")\n    \n    for dist_idx, (dist_name, _) in enumerate(DISTRIBUTIONS.items()):\n      ax = axs[param_idx, dist_idx]\n\n      if param_idx == 0:\n        ax.set_title(dist_name)\n\n      if samples_exact is None:\n        se = None\n      else:\n        se = samples_exact[dist_idx]\n\n      plotting_fn(\n        ax,\n        se,\n        samples_mcmc[param_idx, dist_idx]\n      )\n\n  for ax in axs.ravel():\n    if n_spines == 1:\n      ax.spines[[\"top\", \"left\", \"right\"]].set_visible(False)\n      ax.set_yticks([])\n    elif n_spines == 2:\n      ax.spines[[\"top\", \"right\"]].set_visible(False)\n\n  fig.tight_layout()\n\n\ndef plot_hist_panel_function(\n  ax: plt.Axes,\n  exact_samples: Float[Array, \" n_samples_exact\"],\n  mcmc_samples: Float[Array, \" n_samples_mcmc\"],\n  bins: int = 30\n):\n  ax.hist(exact_samples, density=True, bins=bins, histtype=\"step\", color=\"white\")\n  ax.hist(mcmc_samples, density=True, bins=bins, histtype=\"stepfilled\", color=\"C3\", alpha=0.4)\n\n\ndef plot_trace_panel_function(\n  ax: plt.Axes,\n  exact_samples: Float[Array, \" n_samples_exact\"],\n  mcmc_samples: Float[Array, \" n_samples_mcmc\"],\n):\n  ax.plot(mcmc_samples, color=\"C3\")\n```\n:::\n\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\ndef mixture2_dist() -> dist.Distribution:\n  p = 0.3\n  probs = jnp.asarray([p, 1.0 - p])\n  sep = 25\n  mus = jnp.asarray([-sep, sep], dtype=float)\n  mixing = dist.Categorical(probs=probs)\n  return dist.MixtureSameFamily(\n    mixing_distribution=mixing,\n    component_distribution=dist.Normal(mus, 1.0),\n  )\n\n\ndef mixture3_dist() -> dist.Distribution:\n  probs = jnp.asarray([0.15, 0.3, 0.55])\n  sep = 35\n  mus = jnp.asarray([-sep, 0., sep])\n  mixing = dist.Categorical(probs=probs)\n  return dist.MixtureSameFamily(\n    mixing_distribution=mixing,\n    component_distribution=dist.Normal(mus, 1.0),\n  )\n\nRNG_JAX = JAXRNG(jrandom.PRNGKey(42))\nRNG_NPY = np.random.default_rng(101)\n\nDISTRIBUTIONS = {\n  \"Normal\": lambda: dist.Normal(0, 1),\n  \"Mixture (2 comp.)\": mixture2_dist,\n  \"Mixture (3 comp.)\": mixture3_dist,\n}\nSAMPLES_EXACT = sample_exact(\n  key=RNG_JAX.key,\n  distributions=DISTRIBUTIONS,\n  n_samples=3_000,\n)\n\ndef generate_kernel(\n  logp_fn,\n  sigma: float,\n) -> Kernel:\n  \"\"\"Generates a random-walk kernel.\n\n  Args:\n    logp_fn: function mapping a point to log-density\n    sigma: scale of the random walk\n  \"\"\"\n  def kernel(\n    key: RandomKey,\n    x: Float[Array, \" dim\"],\n  ) -> Float[Array, \" dim\"]:\n    key1, key2 = jrandom.split(key)\n    # Generate a proposal\n    x_ = x + sigma * jrandom.normal(key1, shape=x.shape)\n    \n    # Evaluate the acceptance ratio\n    log_p1 = logp_fn(x)\n    log_p2 = logp_fn(x_)\n    alpha = jnp.exp(log_p2 - log_p1)\n    \n    # Decide whether to accept the proposal\n    u = jrandom.uniform(key2)\n    return jax.lax.select(u <= alpha, x_, x)\n  \n  return kernel\n\nSIGMAS = [0.1, 1, 10, 100]\n\ndef plot_histograms(\n  samples_exact,\n  samples_mcmc,\n  params,\n  param_name,\n):\n  return make_multipanel_figure(\n    plotting_fn=plot_hist_panel_function,\n    distributions=DISTRIBUTIONS,\n    params=params,\n    samples_exact=samples_exact,\n    samples_mcmc=samples_mcmc,\n    n_spines=1,\n    param_name=param_name,\n  )\n\ndef plot_traces(\n  samples_mcmc,\n  params,\n  param_name,\n):\n  return make_multipanel_figure(\n    plotting_fn=plot_trace_panel_function,\n    distributions=DISTRIBUTIONS,\n    params=params,\n    samples_exact=None,\n    samples_mcmc=samples_mcmc,\n    n_spines=2,\n    param_name=param_name,\n  )\n\n_samples_mcmc = mcmc_multirun(\n  key=RNG_JAX.key,\n  kernel_generator=generate_kernel,\n  distributions=DISTRIBUTIONS,\n  params=SIGMAS,\n  n_samples=8_000,\n  warmup=5_000,\n  x0=0.5,\n)\n\nplot_histograms(SAMPLES_EXACT, _samples_mcmc, params=SIGMAS, param_name=\"$\\\\sigma$\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nAn NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](non-reversible-parallel-tempering_files/figure-html/cell-4-output-2.png){}\n:::\n:::\n\n\nSome problems are already visible, but let's take a look at the trace plots, tracking how the parameter has changed over time:\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\nplot_traces(_samples_mcmc, params=SIGMAS, param_name=\"$\\\\sigma$\")\n```\n\n::: {.cell-output .cell-output-display}\n![](non-reversible-parallel-tempering_files/figure-html/cell-5-output-1.png){}\n:::\n:::\n\n\nIt turns out that:\n\n1. Too small $\\sigma$ results in large autocorrelation, making the effective sample size limited.\n2. Medium $\\sigma$ can explore a single mode effectively. However, they cannot travel reasonably often switch between the modes and attribute wrong posterior probabilities to them.\n3. Large $\\sigma$ allow one to switch between the modes quite often. However, for the simplest unimodal problem they suffer from low acceptance rate: many proposals are rejected.\n\nPerfectly, we would use a sampling scheme allowing for both efficient local exploration and frequent jumps between the modes.\nThere are different reasonable strategies:\n\n1. We could devise a better kernel by using e.g., compositions or mixtures of kernels with different $\\sigma$. Or use a more sophisticated strategy of combining kernels, such as [delayed rejection of P.J. Green and A. Mira (2001)](https://www.jstor.org/stable/2673700).\n2. When working with continuous distributions defined on Euclidean spaces (which is the case here, but not for sampling from the Ising model), we could consider novel diffusion-based techniques such as the ones proposed by [X. Huang et al. (2024)](https://openreview.net/forum?id=kIPEyMSdFV) or [L. Grenioux et al. (2024)](https://arxiv.org/abs/2402.10758).\n3. Again in Euclidean spaces, memorise the modes and use a combination of local and jump moves, such as in [E. Pompe et al. (2018)](https://arxiv.org/abs/1812.02609) or connect the modes via [wormhole Hamiltonian Monte Carlo of S. Lan et al. (2013)](https://arxiv.org/abs/1306.0063).\n4. Use sequential Monte Carlo (SMC) samplers, as in the [BlackJAX sampling book](https://blackjax-devs.github.io/sampling-book/algorithms/TemperedSMC.html) or [this talk from Nicholas Chopin](https://youtu.be/mOA_IKyWdkg?feature=shared). SMC samplers can be used on any space, provided that one has efficient locally-exploring kernels.\n5. Another solution (historically existing before the SMC samplers) is to use *parallel tempering*, which dates back to [a 1991 paper of Charles Geyer](https://www.stat.umn.edu/geyer/f05/8931/c.pdf), then termed $(MC)^3$, i.e., Metropolis-coupled Markov chain Monte Carlo.\n\nToday we focus on the last method and look at a new variant of it.\n\n## Parallel tempering as originally designed\n\nConsider a space $\\mathcal X$ with a probability distribution of interest $p$ (by abuse of notation we write $p$ both for the probability distribution and for its density with respect to some convenient measure on $\\mathcal X$). \nWe have a Markov kernel $K$ allowing us to explore $\\mathcal X$ locally, but which has a trouble to pass through low-density regions separating distinct modes.\n\nThis issue can be addressed by extending the original space $\\mathcal X$ to a larger space $\\mathcal X^{N+1} = \\mathcal X \\times \\cdots \\times \\mathcal X$ and targeting a product distribution $\\mathbf{p}(\\mathbf x) =  p_0(x_0)\\cdots p_{N-1}(x_{N-1}) p_{N}(x_N)$, where $p_N = p$ is the original distribution of interest and $p_0, \\dotsc, p_{N-1}$ are auxiliary distributions, designed to be easier to sample from.\nThe main idea is that $p_0$ should be chosen so that it is known to be easy to sample (e.g., i.i.d. samples are easy to generate) and the consecutive distributions, $p_{i}$ and $p_{i+1}$, should be closely related: the separate modes of $p = p_N$ can be then \"connected\" by going through $p_{N-1}, p_{N-2}, \\dotsc$ to $p_0$, which is then to sample from, and back.\n\nFor example, a typical choice for a sequence $p_0, \\dotsc, p_N=p$ is to use an annealing schedule $0 = \\beta_0 < \\beta_1 < \\dotsc < \\beta_N = 1$ and employ the following distribution:\n$$\n  p_n(x) = \\frac{1}{\\mathcal Z(\\beta_n)} \\left(\\frac{p(x)}{p_0(x)}\\right)^{\\beta_n} p_0(x) = \\frac{1}{\\mathcal Z(\\beta_n)} p(x)^{\\beta_n} p_0(x)^{1-\\beta_n} \n$$\n\nSimilarly as in SMC samplers, the schedule does matter a lot, controlling how much consecutive distributions are related.\nHowever, there is an important difference between SMC samplers and parallel tempering: they are orthogonal to each other, in the sense that parallel tempering at any single time keeps the states at *across all the temperatures*, while SMC samplers always have all the particles at the same temperature, which then rises over time.\n(I think this observation was made by Nicholas Chopin in one of his lecture, although I can't find the exact reference. Many apologies for misquoting or misattributing this statement)\n\nA Markov chain is now defined on $\\mathcal X^{N+1}$, with a state $\\mathbf{x} = (x_0, \\dotsc, x_N)$. We consider two transitions:\n\n1. Applying Markov kernels $K_n$ to entries $x_i$, targeting distributions $p_i$, so that we do local exploration.\n2. Swapping entries $x_{i}$ with $x_{i+1}$, so that we can eventually pass from $x_N$ (which is targeting $\\pi$) to $x_0$ (which is easy to explore) and back.\n\nNote that if the second kind of moves were not allowed, we would have just $N+1$ independent Markov chains (each defined on the space $\\mathcal X$) and targeting the $\\mathbf{p}$ distribution \"individually\": the first chain would be efficient (exploring $p_0$) and the last one would mix very, very slow.\nAs the chains are coupled, they are not individually Markov anymore and they travel between different tempered distributions (hopefully eventually reaching the same distribution).\nWe can use them then to extract samples just from the $p=p_N$ distribution. \n\nTo ensure that the Markov chain on $\\mathcal X^{N+1}$ explores $\\mathbf{p}$ properly, [Charles Geyer proposed](https://www.stat.umn.edu/geyer/f05/8931/c.pdf) to swap components $i$ and $j$ according to a Metropolis update. If $\\mathbf{x}$ is the current state and $\\mathbf{x}'$ is the state with entries $i$ and $j$ swapped, the Metropolis ratio is given by\n$$\n  r(i, j) = \\frac{ \\mathbf{p}(\\mathbf x')}{ \\mathbf{p}(\\mathbf x)} = \\frac{ p_i(x_j) p_j(x_i)}{ p_i(x_i) p_j(x_j)}.\n$$ \n\nTypically, only adjacent indices are swapped, as for $i$ very distant from $j$ we expect that $r$ would close to zero.\nIt is also informative to write this ratio in terms of the target and the reference distributions.\nAs $\\log p_n(x) = \\beta_n \\log p(x) + (1-\\beta_n) \\log p_0(x) - \\log \\mathcal Z(\\beta_n)$, we have\n$$\\begin{align*}\n\\log r &= \\beta_i (\\log p(x_j) - \\log p(x_i)) + (1-\\beta_i) (\\log p_0(x_j) - \\log p_0(x_i)) \\\\\n&+ \\beta_j( \\log p(x_i) - \\log p(x_j) ) + (1-\\beta_j)( \\log p_0(x_i) - \\log p_0(x_j) ) \\\\\n&= -(\\beta_i - \\beta_j) \\left( \\log \\frac{p(x_i)}{p_0(x_i)} - \\log \\frac{p(x_j)}{p_0(x_j)} \\right)  \n\\end{align*}\n$$\n\nThis is a very convenient formula if $p_0$ corresponds to the prior distribution and $p$ is the posterior distribution, as their ratio is then simply the likelihood[^2].\n\n[^2]: It is tempting to use priors $p_0$ which are easy to sample from, but for which $\\log p_0(x)$ can be intractable. However, that one has to evaluate $\\log p_0(x)$ to construct the Markov kernels targeting intermediate distributions $p_n$ for $0 < n < N$. Only for swapping the chains we can rely just on the likelihood.\n\nThis is enough theory for now – let's implement parallel tempering in JAX.\n\n### JAX implementation\n\nWe need to make some design choices. We keep the state $\\mathbf{x} = (x_0, \\dotsc, x_N)$ as a matrix $(N+1)\\times (\\mathrm{dim}\\, \\mathcal X)$. As we have access to two log-probability functions ($p = p_N$ and the reference distribution $p_0$), we need to construct intermediate log-probability functions given an annealing schedule $0 = \\beta_0 < \\beta_1 < \\cdots < \\beta_N$.\nTo make everything vectorisable, let's construct the individual kernels $K_n$ as $K_n = \\mathcal{K}(\\phi_n)$ using a factory function $\\mathcal{K}$ and kernel-specific parameters $\\phi_n$.\n\nHence, this implementation is not as general as possible, but it should be compatible with JAX vectorised operations.\n\nTo swap the chains, we do a \"full sweep\", attemping to swap $x_0 \\leftrightarrow x_1$, then $x_1\\leftrightarrow x_2$, and up to $x_{N-1}\\leftrightarrow x_N$.\nThis choice is actually important, as we will later see. \n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\ndef generate_independent_annealed_kernel(\n  log_prob,\n  log_ref,\n  annealing_schedule,\n  kernel_generator,\n  params,\n) -> tuple:\n  \"\"\"Generates the kernels via the kernel generator given appropriate parameters.\n\n  Args:\n    log_prob: log_prob of the target distribution\n    log_ref: log_prob of the easy-to-sample reference distribution\n    annealing_schedule: annealing schedule such that `annealing_schedule[0] = 0.0` and `annealing_schedule[-1] = 1`\n    kernel_generator: `kernel_generator(log_p, param)` returns a transition kernel of signature `kernel(key, state) -> new_state`\n    params: parameters for the transition kernels. Note that `len(annealing_schedule) = len(params)`\n  \"\"\"\n  if len(annealing_schedule) != len(params):\n    raise ValueError(\"Parameters have to be of the same length as the annealing schedule\")\n  n_chains = len(annealing_schedule)\n\n  def transition_kernel(key, state, beta, param):\n    def log_p(y):\n      return beta * log_prob(y) + (1.0 - beta) * log_ref(y)\n    return kernel_generator(log_p, param)(key, state)\n\n  def kernel(key, state_joint):\n    key_vec = jrandom.split(key, n_chains)\n    return jax.vmap(transition_kernel, in_axes=(0, 0, 0, 0))(key_vec, state_joint, annealing_schedule, params)\n\n  return kernel\n\ndef generate_swap_chains_decision_kernel(\n  log_prob,\n  log_ref,\n  annealing_schedule,\n):\n  def log_p(y, beta):\n    return beta * log_prob(y) + (1.0 - beta) * log_ref(y)\n\n  def swap_decision(key, state, i: int, j: int) -> bool:\n    beta1, beta2 = annealing_schedule[i], annealing_schedule[j]\n    x1, x2 = state[i], state[j]\n    log_numerator = log_p(x1, beta2) + log_p(x2, beta1) \n    log_denominator = log_p(x1, beta1) + log_p(x2, beta2)\n    log_r = log_numerator - log_denominator\n\n    r = jnp.exp(log_r)\n    return jrandom.uniform(key) < r\n\n  return swap_decision\n\n\ndef generate_full_sweep_swap_kernel(\n  log_prob,\n  log_ref,\n  annealing_schedule,\n):\n  \"\"\"Applies a full sweep, attempting to swap chains 0 <-> 1, then 1 <-> 2 etc. one-after-another.\n  \"\"\"\n  n_chains = len(annealing_schedule)\n\n  if n_chains < 2:\n    raise ValueError(\"At least two chains are needed.\")\n\n  swap_decision_fn = generate_swap_chains_decision_kernel(\n    log_prob=log_prob,\n    log_ref=log_ref,\n    annealing_schedule=annealing_schedule,\n  )\n\n  def kernel(key, state):\n    def f(state, i: int):\n      subkey = jrandom.fold_in(key, i)\n      decision = swap_decision_fn(subkey, state=state, i=i, j=i+1)\n      \n      # Candidate state: we swap values at i and i+1 positions\n      swapped_state = state.at[i].set(state[i+1])\n      swapped_state = swapped_state.at[i+1].set(state[i])\n\n      new_state = jax.lax.select(decision, swapped_state, state)\n      return new_state, None\n    \n    final_state, _ = jax.lax.scan(f, state, jnp.arange(n_chains - 1))\n    return final_state\n  \n  return kernel\n\n\ndef compose_kernels(kernels: list):\n  \"\"\"Composes kernels, applying them in order.\"\"\"\n  def kernel(key, state):\n    for ker in kernels:\n      key, subkey = jrandom.split(key)\n      state = ker(subkey, state)\n\n    return state\n\n  return kernel\n```\n:::\n\n\nWe need also some annealing schedules:\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\ndef annealing_constant(n_chains: int, base: float = 1.0):\n  \"\"\"Constant annealing schedule, should be avoided.\"\"\"\n  return base * jnp.ones(n_chains)\n\ndef annealing_linear(n_chains: int):\n  \"\"\"Linear annealing schedule, should be avoided.\"\"\"\n  return jnp.linspace(0.0, 1.0, n_chains)\n\ndef annealing_exponential(n_chains: int, base: float = 2.0**0.5):\n  \"\"\"Annealing parameters form a geometric series (apart from beta[0] = 0).\n\n  Args:\n    n_chains: number of chains in the schedule\n    base: geometric progression base, float larger than 1\n  \"\"\"\n  if base <= 1:\n    raise ValueError(\"Base should be larger than 1.\")\n\n  if n_chains < 2:\n    raise ValueError(\"At least two chains are required.\")\n  elif n_chains == 2:\n    return jnp.array([0.0, 1.0])\n  else:\n    x = jnp.append(jnp.power(base, -jnp.arange(n_chains - 1)), 0.0)\n    return x[::-1]\n```\n:::\n\n\nNow we can re-run the previous experiment, but with the usual random-walk MCMC replaced with parallel tempering:\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\ndef pt_multirun_sigmas(\n  key: RandomKey,\n  sigmas: list[float],\n  distributions: OrderedDict,\n  n_samples: int,\n  warmup: int,\n  reference_scale: float = 20.0,\n  n_chains: int = 10,\n  schedule_const: float = 1.1,\n  x0: float = 0.5,\n):\n  def log_ref(x):\n    return dist.Normal(0, reference_scale).log_prob(x)\n  \n  def sampling_fn(key, log_prob, sigma):\n    betas = annealing_exponential(n_chains, schedule_const)\n\n    # We know how to sample from the reference distribution \n    sigmas = sigma * jnp.ones_like(betas)\n    sigmas = sigmas.at[0].set(reference_scale)\n\n    K_ind = generate_independent_annealed_kernel(\n      log_prob=log_prob,\n      log_ref=log_ref,\n      annealing_schedule=betas,\n      kernel_generator=generate_kernel,\n      params=sigmas,\n    )\n    K_swap = generate_full_sweep_swap_kernel(\n      log_prob=log_prob,\n      log_ref=log_ref,\n      annealing_schedule=betas,\n    )\n    K_combined = compose_kernels([K_ind, K_swap])\n\n    return mcmc_sampling_loop(\n      key=key,\n      x0=x0 * jnp.ones(n_chains),\n      kernel=K_combined,\n      n_samples=n_samples,\n      warmup=warmup)\n\n  return make_multirun(\n    key,\n    sampling_fn=sampling_fn,\n    distributions=distributions,\n    params=sigmas,\n  )\n\n_samples_pt = pt_multirun_sigmas(\n  key=RNG_JAX.key,\n  sigmas=SIGMAS,\n  distributions=DISTRIBUTIONS,\n  n_samples=4_000,\n  warmup=2000,\n)\n\n\nplot_histograms(SAMPLES_EXACT, _samples_pt[..., -1], params=SIGMAS, param_name=\"$\\\\sigma$\")\n```\n\n::: {.cell-output .cell-output-display}\n![](non-reversible-parallel-tempering_files/figure-html/cell-8-output-1.png){}\n:::\n:::\n\n\nMuch better! Let's take a look at the traces:\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\nplot_traces(_samples_pt[..., -1], params=SIGMAS, param_name=\"$\\\\sigma$\")\n```\n\n::: {.cell-output .cell-output-display}\n![](non-reversible-parallel-tempering_files/figure-html/cell-9-output-1.png){}\n:::\n:::\n\n\nIn fact, we can plot the traces corresponding to the auxiliary distributions. This distribution is closer to the reference, about in the middle:\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\n_middle_chain = _samples_pt.shape[-1] // 2\nplot_traces(_samples_pt[..., _middle_chain], params=SIGMAS, param_name=\"$\\\\sigma$\")\n```\n\n::: {.cell-output .cell-output-display}\n![](non-reversible-parallel-tempering_files/figure-html/cell-10-output-1.png){}\n:::\n:::\n\n\nAnd this one is just the reference distribution, from which we know how to sample well:\n\n::: {.cell execution_count=10}\n``` {.python .cell-code}\nplot_traces(_samples_pt[..., 0], params=SIGMAS, param_name=\"$\\\\sigma$\")\n```\n\n::: {.cell-output .cell-output-display}\n![](non-reversible-parallel-tempering_files/figure-html/cell-11-output-1.png){}\n:::\n:::\n\n\n### What if?\n\nLet's understand parallel tempering better. The method has quite a few hyperparameters and we can understand what happens if we change them.\n\n#### Reference distribution\nConsider a \"too narrow\" reference distribution, which puts low mass in the region where the modes of the target distribution arise (in the multimodal case).\n\n::: {.cell execution_count=11}\n``` {.python .cell-code}\n_samples_pt = pt_multirun_sigmas(\n  key=RNG_JAX.key,\n  sigmas=SIGMAS,\n  distributions=DISTRIBUTIONS,\n  n_samples=4_000,\n  warmup=2000,\n  reference_scale=1.0,\n)\n\nplot_histograms(SAMPLES_EXACT, _samples_pt[..., -1], params=SIGMAS, param_name=\"$\\\\sigma$\")\n```\n\n::: {.cell-output .cell-output-display}\n![](non-reversible-parallel-tempering_files/figure-html/cell-12-output-1.png){}\n:::\n:::\n\n\n::: {.cell execution_count=12}\n``` {.python .cell-code}\nplot_traces(_samples_pt[..., -1], params=SIGMAS, param_name=\"$\\\\sigma$\")\n```\n\n::: {.cell-output .cell-output-display}\n![](non-reversible-parallel-tempering_files/figure-html/cell-13-output-1.png){}\n:::\n:::\n\n\nWe see some problems arising here.\nOn the other hand, if choose a very wide distribution...\n\n::: {.cell execution_count=13}\n``` {.python .cell-code}\n_samples_pt = pt_multirun_sigmas(\n  key=RNG_JAX.key,\n  sigmas=SIGMAS,\n  distributions=DISTRIBUTIONS,\n  n_samples=4_000,\n  warmup=2000,\n  reference_scale=300.0,\n)\n\nplot_histograms(SAMPLES_EXACT, _samples_pt[..., -1], params=SIGMAS, param_name=\"$\\\\sigma$\")\n```\n\n::: {.cell-output .cell-output-display}\n![](non-reversible-parallel-tempering_files/figure-html/cell-14-output-1.png){}\n:::\n:::\n\n\n::: {.cell execution_count=14}\n``` {.python .cell-code}\nplot_traces(_samples_pt[..., -1], params=SIGMAS, param_name=\"$\\\\sigma$\")\n```\n\n::: {.cell-output .cell-output-display}\n![](non-reversible-parallel-tempering_files/figure-html/cell-15-output-1.png){}\n:::\n:::\n\n\n... the behaviour of parallel tempering is somewhat better.\nOf course, not as good as when the reference $p_0$ was closer to the target $p=p_N$, but my current intuition is that it is better to use a \"too wide\" reference, rather than \"too narrow\".\n\n#### Number of chains\n\nLet's try a smaller number of chains:\n\n::: {.cell execution_count=15}\n``` {.python .cell-code}\n_samples_pt = pt_multirun_sigmas(\n  key=RNG_JAX.key,\n  sigmas=SIGMAS,\n  distributions=DISTRIBUTIONS,\n  n_samples=4_000,\n  warmup=2000,\n  n_chains=4,\n)\n\nplot_histograms(SAMPLES_EXACT, _samples_pt[..., -1], params=SIGMAS, param_name=\"$\\\\sigma$\")\n```\n\n::: {.cell-output .cell-output-display}\n![](non-reversible-parallel-tempering_files/figure-html/cell-16-output-1.png){}\n:::\n:::\n\n\n::: {.cell execution_count=16}\n``` {.python .cell-code}\nplot_traces(_samples_pt[..., -1], params=SIGMAS, param_name=\"$\\\\sigma$\")\n```\n\n::: {.cell-output .cell-output-display}\n![](non-reversible-parallel-tempering_files/figure-html/cell-17-output-1.png){}\n:::\n:::\n\n\nThis seems to be not bad, although the autocorrelation between samples is larger. \n\nOn the other hand, if we use a larger number of chains...\n\n::: {.cell execution_count=17}\n``` {.python .cell-code}\n_samples_pt = pt_multirun_sigmas(\n  key=RNG_JAX.key,\n  sigmas=SIGMAS,\n  distributions=DISTRIBUTIONS,\n  n_samples=4_000,\n  warmup=2000,\n  n_chains=50,\n)\n\nplot_histograms(SAMPLES_EXACT, _samples_pt[..., -1], params=SIGMAS, param_name=\"$\\\\sigma$\")\n```\n\n::: {.cell-output .cell-output-display}\n![](non-reversible-parallel-tempering_files/figure-html/cell-18-output-1.png){}\n:::\n:::\n\n\n::: {.cell execution_count=18}\n``` {.python .cell-code}\nplot_traces(_samples_pt[..., -1], params=SIGMAS, param_name=\"$\\\\sigma$\")\n```\n\n::: {.cell-output .cell-output-display}\n![](non-reversible-parallel-tempering_files/figure-html/cell-19-output-1.png){}\n:::\n:::\n\n\n... we see excellent results. As we will see below, there exist parallel tempering schemes in which a large number of chains results in a bad performance. However, the \"full sweep\" variant does not seem to suffer from this issue.\n(I may need to revise this intuition one day, when precise theory is available, but right now I am happy with it).\n\n#### Annealing schedule\n\nFinally, let's take a look at the annealing schedule:\n\n::: {.cell execution_count=19}\n``` {.python .cell-code}\nfor base in [1.01, 1.1, 1.2, 1.4]:\n  schedule = np.asarray(annealing_exponential(n_chains=10, base=base)).tolist()\n  schedule_str = \", \".join(map(lambda x: f\"{x:.1f}\", schedule))\n  print(f\"{base}:\\t{schedule_str}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n1.01:\t0.0, 0.9, 0.9, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0\n1.1:\t0.0, 0.5, 0.5, 0.6, 0.6, 0.7, 0.8, 0.8, 0.9, 1.0\n1.2:\t0.0, 0.2, 0.3, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 1.0\n1.4:\t0.0, 0.1, 0.1, 0.1, 0.2, 0.3, 0.4, 0.5, 0.7, 1.0\n```\n:::\n:::\n\n\nLet's use the constant 1.2:\n\n::: {.cell execution_count=20}\n``` {.python .cell-code}\n_samples_pt = pt_multirun_sigmas(\n  key=RNG_JAX.key,\n  sigmas=SIGMAS,\n  distributions=DISTRIBUTIONS,\n  n_samples=4_000,\n  warmup=2000,\n  schedule_const=1.2,\n)\n\nplot_histograms(SAMPLES_EXACT, _samples_pt[..., -1], params=SIGMAS, param_name=\"$\\\\sigma$\")\n```\n\n::: {.cell-output .cell-output-display}\n![](non-reversible-parallel-tempering_files/figure-html/cell-21-output-1.png){}\n:::\n:::\n\n\n::: {.cell execution_count=21}\n``` {.python .cell-code}\nplot_traces(_samples_pt[..., -1], params=SIGMAS, param_name=\"$\\\\sigma$\")\n```\n\n::: {.cell-output .cell-output-display}\n![](non-reversible-parallel-tempering_files/figure-html/cell-22-output-1.png){}\n:::\n:::\n\n\nPerformance does not seem to differ by much.\n\nEven if we use constant 2...\n\n::: {.cell execution_count=22}\n``` {.python .cell-code}\n_samples_pt = pt_multirun_sigmas(\n  key=RNG_JAX.key,\n  sigmas=SIGMAS,\n  distributions=DISTRIBUTIONS,\n  n_samples=4_000,\n  warmup=2000,\n  schedule_const=2.0,\n)\n\nplot_histograms(SAMPLES_EXACT, _samples_pt[..., -1], params=SIGMAS, param_name=\"$\\\\sigma$\")\n```\n\n::: {.cell-output .cell-output-display}\n![](non-reversible-parallel-tempering_files/figure-html/cell-23-output-1.png){}\n:::\n:::\n\n\n::: {.cell execution_count=23}\n``` {.python .cell-code}\nplot_traces(_samples_pt[..., -1], params=SIGMAS, param_name=\"$\\\\sigma$\")\n```\n\n::: {.cell-output .cell-output-display}\n![](non-reversible-parallel-tempering_files/figure-html/cell-24-output-1.png){}\n:::\n:::\n\n\n... the performance is reasonable. This is interesting. \n\n### Summary\n\nMy (current and subjective) intuition is that:\n  - The most important thing is to ensure efficient sampling from the reference, $p_0$, which should be somewhat close to $p$.\n  - It is better to use a bit \"too wide\" $p_0$ rather than a \"too narrow\" one.\n  - We need efficient local exploration. Parallel tempering can however improve on these aspects.\n  - The annealing schedule and the number of chains also do matter. I feel that the points considered above may be a bit more important for the final performance than tuning the optimisation schedule (e.g., 10-20 chains and around 1.1-1.2 annealing constant seem to be quite reasonable defaults), but choosing the number of chains and annealing schedule properly does matter. Also, we can often easily change the number of chains and the annealing schedule, while changing $p_0$ can be much more tricky (especially that in Bayesian inference problems, it often is taken to be the prior distribution, which cannot be changed arbitrarily).\n\nThese intuitions are based on \"easy\" low-dimensional problems considered above. However, for modern high-dimensional problems with many modes these intuitions may not generalise well. It is also much harder to ensure then that $p_0$ is reasonably wide and that the kernels are efficient enough.\n\nIt would be nice to have a good default for the annealing schedule.\nAnother aspect, which we do not consider above, but is important for very high-dimensional problems, is to use distributed computing on multiple machines or parallel computing on multiple cores.\nWhile it is easy to parallelise application of the kernels $K_n$ to distinct components $x_n$, our current \"full sweep\" swapping strategy has to be executed iteratively.\n\n## Non-reversible parallel tempering\n\nIn [*Non-reversible parallel tempering: a scalable highly parallel MCMC scheme\n*](https://arxiv.org/abs/1905.02939), Saifuddin Syed, Alexandre Bouchard-Côté, George Deligiannidis and Arnaud Doucet propose an interesting alternative to the sampling scheme described above, suitable for distributed computing.\nI enjoyed reading the paper very much, but there is also [an excellent lecture available](https://youtu.be/9gOssrhN3EA?feature=shared).\n\nConsider a distributed swapping scheme in which we do not want to sequentially attempt swapping chains $i \\leftrightarrow i+1$ sequentially for all $i=0, \\dotsc, N-1$, but rather employ *either* an:\n\n1. *Even move*: attempt swapping the states $2k \\leftrightarrow 2k+1$ (which can be done simultaneously for all $k$ on different machines).\n2. *Odd move*: attempt swapping the states $2k-1 \\leftrightarrow 2k$ (which also can be done simultaneously for all $k$ on different machines).\n\nNote that both moves are different from what we did above: each full sweep swapped the states consequtively.\nIn particular, there was a chance (very small, though) to travel from $p_0$ to $p_N$ in one full sweep (incurring $N-1$ swaps).\nCurrently $x_0$ can either be swapped with $x_1$ (even move is accepted) or be left in place in one step (the odd move is executed or the even move is rejected).\n\nThe authors consider alternating between these moves basing on the following:\n\n1. Stochastic even-odd swap (SEO): an unbiased coin is tossed to decide whether to execute the even or the odd move.\n2. Deterministic even-odd swap (DEO): even time steps result in even moves and odd time steps result in odd moves.\n\nIt turns out that SEO is a very inefficient choice when a large number of chains is used and DEO is much more preferred (see also [this section](#why-non-reversible-parallel-tempering)). \n\n### JAX implementation of DEO\n\nLet's implement DEO in JAX. \n\nOur first task is to execute even and odd moves, which deserves a subsection on its own.\n\n#### Controlled swapping problem\n\nWe have a state $\\mathbf{x} = (x_0, \\dotsc, x_N)$ and we want to execute some moves.\nLet's keep the information about the swaps in a binary mask matrix\n$\\mathbf{m} = (m_0, \\dotsc, m_{N-1})$\nsuch that $m_i = 1$ if and only if we want to swap $x_{i} \\leftrightarrow x_{i+1}$ (and $m_i = 0$ otherwise).\n\nAn even move in which (somewhat unlikely) all proposals are accepted has then a mask $\\mathbf{m}_\\text{even} = (1, 0, 1, 0, \\cdots)$ and an odd move has a mask $\\mathbf{m}_\\text{odd} = (0, 1, 0, 1, \\cdots)$.\nHowever, as only some moves have been accepted, some ones can be replaced by zeros. Namely, if we have a binary matrix $\\mathbf{m}_\\text{accept}$, we have to take the entry-wise AND operation. For example, $\\mathbf{m} := \\mathbf{m}_\\text{accept}\\, \\&\\, \\mathbf{m}_\\text{even}$ for accepted even moves. \n\nNote that it is not possible to have two consecutive ones.\n\nI find applying the swaps according to $\\mathbf{m}$ rather tricky. Consider the following algorithm:\n\n```\nfn swapping_naive(x[], m[]) -> y[]:\n  for i = 0, ..., N-1:\n    if m[i] = 0:\n      y[i]   := x[i]\n      y[i+1] := x[i+1]\n    else:                 // m[i] = 1\n      y[i]   := x[i+1]\n      y[i+1] := x[i]\n```\n\nDoes it work? \n\nNo.\nFor the input data\n\n```\nm := [1, 0]\nx := [a, b, c]\n```\nwe want `y = [b, a, c]`.\nHowever, we have:\n```\ny := [b, a]       // i = 0, m[0] = 1\ny := [b, b, c]    // i = 1, m[1] = 0\n```\n\nThe issue was that `y[1]` was updated both at `i=0` and `i=1` stages. Let's think how we can improve this:\n\n```\nfn swapping_good(x[], m[]) -> y[]:\n  y := copy(x)\n\n  for i = 0, ..., N-1:\n    if m[i] = 1:\n      y[i]   := x[i+1]\n      y[i+1] := x[i]\n    else:                // m[i] = 0\n      y[i]   := y[i]     // Note that we do not change the values\n      y[i+1] := y[i+1]   // simply copying them over\n```\n\nIn this case, the algorithm works as following:\n\n```\ny = [a, b, c]   // Before the loop\ny = [b, a, c]   // i = 0, m[0] = 1\ny = [b, a, c]   // i = 1, m[1] = 0\n```\n\nLet's prove that this algorithm is indeed correct by showing that each `y[i]` attains the correct value. We consider three cases: \n\n**Case `y[0]`**: we have `y[0] := x[0]` at the beginning. The only moment when it can be modified is at step `i = 0`. We have `y[0] := y[0] = x[0]` if `m[0] = 0` and `y[0] := x[0+1] = x[1]` if `m[0] = 1`.\n\n**Case `y[N]`**: similarly as above, `y[N] := x[N]` at the beginning and the only moment when it can be modified is at `i = N-1` step.\nIf `m[N-1] = 0`, then `y[N] = x[N]` and if `m[N-1] = 1`, then we overwrite the value to `y[N] :=  x[N-1]`.\n\n**Case `y[j]` for `0 < j < N`**: at the beginning `y[j] := x[j]` and can be modified only at steps `i=j-1` and `i=j`. We have three cases:\n\n  - `m[j-1, j] = [0, 0]`. Then, `y[j] = x[j]` as it has not been modified at either step.\n  - `m[j-1, j] = [1, 0]`. Then `y[j] := x[j-1]` at `i=j-1` and stays unchanged at `i=j`, so in the end sequence we have `y[j] = x[j-1]`.\n  - `m[j-1, j] = [0, 1]`. Then, `y[j] = x[j]` at step `i=j-1`. Then, at step `i=j` we have `y[j] := x[j+1]`.\n\nNote that it is important that consecutive ones are not allowed.\n\n::: {.cell execution_count=24}\n``` {.python .cell-code}\ndef _test_controlled_swapping(func):\n  test_cases = [\n    # Triples (x, m, y)\n    ([1, 2], [0,], [1, 2]),\n    ([1, 2], [1,], [2, 1]),\n    ([1, 2, 3], [0, 0], [1, 2, 3]),\n    ([1, 2, 3], [1, 0], [2, 1, 3]),\n    ([1, 2, 3], [0, 1], [1, 3, 2]),\n    ([1, 2, 3, 4], [1, 0, 1], [2, 1, 4, 3]),\n    ([1, 2, 3, 4], [0, 1, 0], [1, 3, 2, 4]),\n  ]\n  for x, m, y in test_cases:\n    y_ = func(jnp.asarray(x), jnp.asarray(m))\n    y_ = np.asarray(y_).tolist()\n    if tuple(y_) != tuple(y):\n      raise ValueError(f\"f(x={x}, m={m}) = {y_}. Expected {y}.\")\n\ndef controlled_swapping_scan(\n  x: Float[Array, \"n_chains *dims\"],\n  m: Int[Array, \" n_chains-1\"],\n) -> Float[Array, \"n_chains *dims\"]:\n  \"\"\"Swaps the entries of `x`, as described by binary mask `m`.\n\n  Args:\n    x: array of shape (n_chains, dim)\n    m: binary mask of shape (n_chains - 1,) controlling which chains should be swapped. We have `m[i] = 1` if `x[i]` and `x[i+1]` should be swapped.\n    \n  Note:\n    Consecutive values 1 in `m` are not allowed.\n    Namely, it cannot hold that `m[i] = m[i+1] = 1`.\n  \"\"\"\n  def f(y, i):\n    value = jax.lax.select(m[i], x[i + 1], y[i])       # y[i]\n    value_next = jax.lax.select(m[i], x[i], y[i + 1])  # y[i + 1]\n        \n    y = y.at[i].set(value)\n    y = y.at[i + 1].set(value_next)\n        \n    return y, None\n\n  # Run the scan over the range of M\n  y, _ = jax.lax.scan(f, x, jnp.arange(m.shape[0]))\n  \n  return y\n\n_test_controlled_swapping(controlled_swapping_scan)\n```\n:::\n\n\nThis implementation seems to work and shows how powerful `[jax.lax.scan](https://jax.readthedocs.io/en/latest/_autosummary/jax.lax.scan.html)` can be.\nHowever, JAX is built to accelerate linear algebra and perhaps we can come up with an appropriately vectorised operation.\nLet's go through the three cases once again.\n\n**Case `y[0]`**: we want `x[0]` if `m[0] = 0` and `x[1]` if `m[1] = 1`. In other words, we have `y[0] = x[m[0]]`.\n\n**Case `y[N]`**: we want `x[N-1]` if `m[N-1] = 1` and `x[N]` if `m[N-1] = 0`. Hence, `y[N] = x[N - m[N-1]]`. \n\n**Case `y[j]` for `0 < j < N`**: as before, we have three cases, controlled by `m[j-1]` and `m[j]`. I claim that `y[j] = x[j + m[j] - m[j-1]]`. For both zeros, we do not swap anything and have `y[j] = x[j]`. For `m[j-1, j] = [0, 1]` we want to have `y[j] = x[j+1]` and for `m[j-1, j] = [1, 0]` we have `y[j] = x[j-1]`.\n\nThe implementation is now trivial:\n\n::: {.cell execution_count=25}\n``` {.python .cell-code}\ndef _create_indices(m):\n  N = m.shape[0] + 1\n  base_indices = jnp.arange(1, N-1)  # Length N-2\n  ind_middle = base_indices + m[1:] - m[:-1]\n\n  ind = jnp.concatenate((\n        jnp.array([m[0]]),\n        ind_middle,\n        jnp.array([N - 1 - m[-1]])  \n  ))\n\n  return ind\n\ndef controlled_swapping(\n  x: Float[Array, \"n_chains *dims\"],\n  m: Int[Array, \" n_chains-1\"],\n) -> Float[Array, \"n_chains *dims\"]:\n  \"\"\"Swaps the entries of `x`, as described by binary mask `m`.\n\n  Args:\n    x: array of shape (n_chains, dim)\n    m: binary mask of shape (n_chains - 1,) controlling which chains should be swapped. We have `m[i] = 1` if `x[i]` and `x[i+1]` should be swapped.\n    \n  Note:\n    Consecutive values 1 in `m` are not allowed.\n    Namely, it cannot hold that `m[i] = m[i+1] = 1`.\n  \"\"\"\n  indices = _create_indices(m)\n  return x[indices, ...]\n\n_test_controlled_swapping(controlled_swapping)\n```\n:::\n\n\n#### DEO swapping kernel and the sampling loop\n\nAt this stage, we can implement the swapping kernel.\nNote that this kernel has a different signature than usual, additionally taking the timestep as input and calculating the rejection rates, which will turn out to be useful [when we optimise the annealing schedule](#annealing-schedule-optimisation).\nDue to the fact that the swaps are not \"interfering\", we can use vectorised operations.\n\n::: {.cell execution_count=26}\n``` {.python .cell-code}\ndef generate_deo_extended_kernel(\n  log_prob,\n  log_ref,\n  annealing_schedule,\n):\n  def log_p(y, beta):\n    return beta * log_prob(y) + (1.0 - beta) * log_ref(y)\n\n  log_p_vmap = jax.vmap(log_p, in_axes=(0, 0))\n\n  def extended_kernel(\n    key,\n    state,\n    timestep: int,\n  ) -> tuple:\n    \"\"\"Extended deterministic even-odd swap kernel, which\n    for even timesteps makes even swaps (2i <-> 2i+1)\n    and for odd timesteps makes odd swaps (2i-1 <-> 2i)\n\n    Args:\n      key: random key\n      state: state\n      timestep: timestep number, used to decide whether to make even or odd move\n\n    Returns:\n      new_state, the same shape as `state`\n      rejection_rates, shape (n_chains-1,)\n    \"\"\"\n    n_chains = state.shape[0]\n\n    idx1 = jnp.arange(n_chains - 1)\n    idx2 = idx1 + 1\n\n    xs1 = state[idx1]\n    xs2 = state[idx2]\n\n    betas1 = annealing_schedule[idx1]\n    betas2 = annealing_schedule[idx2]\n\n    log_numerator = log_p_vmap(xs1, betas2) + log_p_vmap(xs2, betas1) \n    log_denominator = log_p_vmap(xs1, betas1) + log_p_vmap(xs2, betas2)\n    log_accept = log_numerator - log_denominator\n    accept_prob = jnp.minimum(jnp.exp(log_accept), 1.0)\n    rejection_rates = 1.0 - accept_prob\n\n    # Where the swaps would be accepted through M-H\n    accept_mask = jrandom.bernoulli(key, p=accept_prob)\n    # Where the swaps can be accepted due to even-odd moves\n    even_odd_mask = jnp.mod(idx1, 2) == jnp.mod(timestep, 2)\n    total_mask = accept_mask & even_odd_mask\n\n    # Now the tricky part: we need to execute the swaps\n    new_state = controlled_swapping(state, total_mask)\n    return new_state, rejection_rates\n\n  return extended_kernel\n\n\ndef deo_sampling_loop(\n  key: RandomKey,\n  x0,\n  kernel_local,\n  kernel_deo,\n  n_samples: int,\n  warmup: int,\n) -> tuple:\n  \"\"\"The sampling loop for DEO parallel tempering.\n\n  Returns:\n    samples\n    rejection_rates\n  \"\"\"\n  def f(x, timestep: int):\n    subkey = jrandom.fold_in(key, timestep)\n\n    key_local, key_deo = jrandom.split(subkey)\n\n    # Apply local exploration kernel\n    x = kernel_local(key_local, x)\n\n    # Apply the DEO swap\n    x, rejection_rates = kernel_deo(\n      key_deo,\n      x,\n      timestep,\n    )\n\n    return x, (x, rejection_rates)\n  \n  # Run warmup\n  x0, _ = jax.lax.scan(f, x0, jnp.arange(warmup))\n\n  # Collect samples\n  _, (samples, rejection_rates) = jax.lax.scan(f, x0, jnp.arange(n_samples))\n  \n  return samples, rejection_rates\n\n\ndef nonreversible_pt_multirun_sigmas(\n  key: RandomKey,\n  sigmas: list[float],\n  distributions: OrderedDict,\n  n_samples: int,\n  warmup: int,\n  reference_scale: float = 20.0,\n  n_chains: int = 10,\n  schedule_const: float = 1.1,\n  x0: float = 0.5,\n  annealing_schedule: Float[Array, \" n_chains\"] = None,\n):\n  if annealing_schedule is None:\n    betas = annealing_exponential(n_chains=n_chains, base=schedule_const)\n  else:\n    betas = jnp.asarray(annealing_schedule)\n\n  def log_ref(x):\n    return dist.Normal(0, reference_scale).log_prob(x)\n  \n  def sampling_fn(key, log_prob, sigma):\n    # We know how to sample from the reference distribution \n    sigmas = sigma * jnp.ones_like(betas)\n    sigmas = sigmas.at[0].set(reference_scale)\n\n    K_ind = generate_independent_annealed_kernel(\n      log_prob=log_prob,\n      log_ref=log_ref,\n      annealing_schedule=betas,\n      kernel_generator=generate_kernel,\n      params=sigmas,\n    )\n    K_deo = generate_deo_extended_kernel(\n      log_prob=log_prob,\n      log_ref=log_ref,\n      annealing_schedule=betas,\n    )\n\n    key, subkey = jrandom.split(key)\n    samples, rejections = deo_sampling_loop(\n      key=subkey,\n      x0=x0 * jnp.ones(n_chains, dtype=float),\n      kernel_local=K_ind,\n      kernel_deo=K_deo,\n      n_samples=n_samples,\n      warmup=warmup,\n    )\n    return samples\n\n  return make_multirun(\n    key,\n    sampling_fn=sampling_fn,\n    distributions=distributions,\n    params=sigmas,\n  )\n```\n:::\n\n\n::: {.cell execution_count=27}\n``` {.python .cell-code}\n_samples_npt = nonreversible_pt_multirun_sigmas(\n  key=RNG_JAX.key,\n  sigmas=SIGMAS,\n  distributions=DISTRIBUTIONS,\n  n_samples=4000,\n  warmup=2000,\n)\n\nplot_histograms(SAMPLES_EXACT, _samples_npt[..., -1], params=SIGMAS, param_name=\"$\\\\sigma$\")\n```\n\n::: {.cell-output .cell-output-display}\n![](non-reversible-parallel-tempering_files/figure-html/cell-28-output-1.png){}\n:::\n:::\n\n\n::: {.cell execution_count=28}\n``` {.python .cell-code}\nplot_traces(_samples_npt[..., -1], params=SIGMAS, param_name=\"$\\\\sigma$\")\n```\n\n::: {.cell-output .cell-output-display}\n![](non-reversible-parallel-tempering_files/figure-html/cell-29-output-1.png){}\n:::\n:::\n\n\nThis seems pretty good to me!\n\n### Why non-reversible parallel tempering?\n\nInterestingly, DEO has better performance than SEO, which are termed in the paper, respectively, non-reversible and reversible parallel tempering schemes.\nThe discussion whether to use non-reversible or reversible kernels has a long history and I still find the topic mysterious.\nProbably it is worth to write a separate blog post on the topic, but:\n\n  - This wonderful 2000 paper from [Persi Diaconis, Susan Holmes and Radford Neal](https://doi.org/10.1214/aoap/1019487508) shows examples where non-reversible methods are more efficient than reversible ones. \n  - In a [great 2016 paper from Gareth Roberts and Jeffrey Rosenthal](https://doi.org/10.5539/ijsp.v5n1p51) there are examples where \"systematic scan\" Gibbs samplers (which often are non-reversible, although not always: recall palindromic kernels of the form $K_1 K_2 K_1$) can outperform \"random scan\" (always reversible) Gibbs samplers. Examples with the *opposite behaviour* are also provided.\n  - In a 2016 [C. Andrieu's paper](https://www.jstor.org/stable/26363466) there is a theorem showing that for two kernels fulfilling some technical assumptions, systematic scans are more efficient than random ones. This could perhaps offer an orthogonal perspective on why DEO is more efficient than SEO, but I am not sure.\n\nThis discussion whether reversible or non-reversible scheme could be used is one way of looking at the problem. Another is through the perspective of reducing the diffusive random walk behaviour by introducing a momentum variable.\nMomentum is a common theme in computational statistics and machine learning, with examples such as [MALA](https://en.wikipedia.org/wiki/Metropolis-adjusted_Langevin_algorithm) and [Hamiltonian Monte Carlo](https://en.wikipedia.org/wiki/Hamiltonian_Monte_Carlo) in Markov chain Monte Carlo world or [stochastic gradient descent with momentum](https://en.wikipedia.org/wiki/Stochastic_gradient_descent#Momentum) in optimisation.\n\nDEO reduces random walk in swapping the chains (which is studied through the perspective of an *index process*) and can be thought as of introducing a (discrete) momentum variable.\nI will skip the precise description of the index process, replacing it with a picture I based on the figures from the paper. We will simulate the path of the chain under the sequential scheme we studied before, SEO and DEO:\n\n::: {.cell execution_count=29}\n``` {.python .cell-code}\ndef _find_trajectory(states, tracked: int = None):\n  n_chains = states.shape[1]\n\n  if tracked is None:\n    tracked = n_chains // 2\n\n  return jnp.einsum(\"ng,g->n\", states == tracked, jnp.arange(n_chains))\n\ndef generate_figure_momentum(p: float = 0.85, n_chains: int = 5, n_timesteps: int = 30):\n  rng = np.random.default_rng(8)\n\n  fig, axs = plt.subplots(3, 1, sharex=True, sharey=True)\n\n  x_axis = np.arange(n_timesteps)\n\n  for ax in axs:\n    for chain in range(n_chains):\n      ax.scatter(x_axis, chain * np.ones_like(x_axis), c=\"w\", s=3)\n    ax.spines[[\"top\", \"right\"]].set_visible(False)\n\n  # Sample full sweep\n  state = jnp.arange(n_chains)\n  states = [state]\n  for timestep in range(1, n_timesteps):\n    for i in range(n_chains - 1):\n      if rng.binomial(1, p):\n        new_state = state.at[i].set(state[i+1])\n        new_state = new_state.at[i+1].set(state[i])\n        state = new_state\n    states.append(state)\n\n  states = jnp.stack(states)\n  trajectory = _find_trajectory(states)\n  \n  ax = axs[0]\n  ax.plot(x_axis, trajectory)\n  ax.set_title(\"Full sweep\")\n\n  # Sample SEO\n  state = jnp.arange(n_chains)\n  states = [state]\n  for timestep in range(1, n_timesteps):\n    mode = rng.binomial(1, 0.5)\n    for i in range(n_chains - 1):\n      if (i % 2 == mode) and rng.binomial(1, p):\n        new_state = state.at[i].set(state[i+1])\n        new_state = new_state.at[i+1].set(state[i])\n        state = new_state\n    states.append(state)\n\n  states = jnp.stack(states)\n  trajectory = _find_trajectory(states)\n  \n  ax = axs[1]\n  ax.plot(x_axis, trajectory)\n  ax.set_title(\"Reversible stochastic even-odd swaps (SEO)\")\n\n  # Sample DEO\n  state = jnp.arange(n_chains)\n  states = [state]\n  for timestep in range(1, n_timesteps):\n    for i in range(n_chains - 1):\n      if (i % 2 == timestep % 2) and rng.binomial(1, p):\n        new_state = state.at[i].set(state[i+1])\n        new_state = new_state.at[i+1].set(state[i])\n        state = new_state\n    states.append(state)\n\n  states = jnp.stack(states)\n  trajectory = _find_trajectory(states)\n  \n  ax = axs[2]\n  ax.plot(x_axis, trajectory)\n  ax.set_title(\"Non-reversible deterministic even-odd swaps (DEO)\")\n\n  fig.tight_layout()\n  return fig\n\nfig = generate_figure_momentum(p=0.8, n_chains=4, n_timesteps=20)\n```\n\n::: {.cell-output .cell-output-display}\n![](non-reversible-parallel-tempering_files/figure-html/cell-30-output-1.png){width=662 height=470}\n:::\n:::\n\n\n::: {.cell execution_count=30}\n``` {.python .cell-code}\nfig = generate_figure_momentum(p=0.9, n_chains=10, n_timesteps=50)\n```\n\n::: {.cell-output .cell-output-display}\n![](non-reversible-parallel-tempering_files/figure-html/cell-31-output-1.png){width=662 height=470}\n:::\n:::\n\n\nDefinitely, SEO has a trouble going between the reference and the target distribution.\nOn the other hand, in these simulations (which are very simplistic, though!), DEO does not show a clear advantage over the full sweep (other than being more parallelisable).\nNote that both SEO and DEO can move an index only by $\\pm 1$, while the full sweep can increase the index arbitrarily. However, it also decreases the indices at most by $1$.\nIt may perhaps be interesting to consider a *deterministic forward-backward full sweep scheme*, in which even timesteps make a full forward sweep (as we did in the first prototype) and odd timesteps make a backward sweep.\n\nI think studying the behaviour of the DEO scheme is an important contribution of this paper, but there several more: \n\n1. By introducing and studying the index process, the authors devise the DEO sampling scheme together with a method for *choosing the annealing schedule* basing on preliminary runs.\n2. The proposed sampling scheme is highly parallelisable and can be used in distributed computing environments. The experiments in the paper cover many complex high-dimensional distributions. Moreover, a new Julia package, [Pigeons.jl](https://pigeons.run/) makes application of non-reversible parallel tempering practical in the distributed setting.\n\nLet's see, however, how to tune the annealing schedule, which has a wonderful theory outlined in Section 4 of the paper.\n\n### Annealing schedule optimisation\n\nAssuming efficient local exploration of individual components, the authors build a theory how quickly the chain can cycle between $p_0$ and $p=p_N$.\nThe key quantity is the *instateneous rejection rate* function\n$$\n  \\lambda(\\beta) = \\frac{1}{2} \\mathbb E_{X, Y \\sim_\\mathrm{i.i.d.} p_\\beta } \\left[\\left| \\log \\frac{ p(X) p_0(Y)}{ p(Y) p_0(X) } \\right|\\right],\n$$\n\nwhich depends on the annealing parameter $\\beta$ (which controls the measure over which we integrate), but also on how different $p$ and $p_0$ are.\nDefine\n$$\n  \\Lambda(\\beta) = \\int_{0}^{\\beta} \\lambda(\\beta') \\,\\mathrm{d}\\beta'.\n$$\n\nIf $\\tilde \\Lambda = \\Lambda(1)$, then for small $\\max_i |\\beta_i - \\beta_{i+1}|$ it holds that the \"round-trip rate\", describing how often going from $p_0$ to $p$ and back, for SEO is about\n$$\n  f_\\mathrm{SEO} \\approx \\frac{1}{2N + 2\\tilde \\Lambda}.\n$$\n\nUsing a large $N$ for SEO leads to diffusive behaviour with close-to-zero round-trip rate!\nI find this result amazing. What is even more interesting, for DEO:\n$$\n  f_\\mathrm{DEO} \\approx \\frac{1}{2 \\cdot \\left(1+\\tilde \\Lambda\\right)}.\n$$\n\nDEO with large $N$ does not have the diffusive behaviour, with the round-trip rate being controlled by the *communication barrier* $\\tilde \\Lambda$, which depends on $p_0$ and $p$.\nHence, for large $\\tilde\\Lambda$ many, many iterations may be necessary to obtain enough round trips and good mixing.\n\nInterestingly, the $\\Lambda$ function can be estimated from a run using a fine-grained annealing schedule.\nIt turns out that if $\\rho(\\beta, \\beta')$ is the expected rejection rate of swapping the chains between $\\beta$ and $\\beta'$ (so that, of course, $\\rho(\\beta, \\beta') = \\rho(\\beta', \\beta)$), then\n$$\n  \\rho(\\beta, \\beta') = | \\Lambda(\\beta) - \\Lambda(\\beta') | + O(|\\beta - \\beta'|^3).\n$$\n\nIn particular,\n$$\n  \\tilde \\Lambda \\approx \\sum_{i=0}^{N-1} \\rho(\\beta_{i}, \\beta_{i+1}),\n$$\n\nwhere the error is of order $O\\!\\left(N \\cdot \\left(\\max_i |\\beta_i - \\beta_{i+1}|\\right)^3 \\right)$.\n\nIn other words, $\\Lambda(\\beta)$ can be estimated from the rejection probabilities.\n\nTo optimise the schedule, the authors note that the round-trip rate under DEO is given by\n$$\n  f = \\frac{1}{2\\left(1 + \\sum_{i=0}^{N-1} \\frac{\\rho(\\beta_{i}, \\beta_{i+1})}{ 1-\\rho(\\beta_i, \\beta_{i+1}) } \\right)}\n$$\n\nfor any schedule. (Note that the approximation of $f_\\mathrm{DEO}$ when differences $|\\beta_i - \\beta_{i+1}|$ are small, can be read from this formula: all $\\rho$ are small, so we can ignore terms $1-\\rho$ and then we obtain $\\tilde \\Lambda$).\n\nAs we want to maximise $f$, we need to find a schedule minimising the denominator. On the other hand, there is a constraint that for any fine-grained schedule it holds that\n\n$$\n  \\tilde \\Lambda \\approx \\sum_{i=0}^{N-1} \\rho(\\beta_{i}, \\beta_{i+1}),\n$$\n\nso that this is a constrained optimisation problem.\nIt turns out that the optimum is attained when $\\rho(\\beta_i, \\beta_{i+1})$ are all equal. \nUsing the relationship between $\\rho$ and differences in $\\Lambda$, we see that we should aim at\n$$\n  \\Lambda(\\beta_i) \\approx \\frac{i}{N} \\tilde \\Lambda.\n$$\n\n#### JAX implementation\n\n::: {.cell execution_count=31}\n``` {.python .cell-code}\nfrom scipy.interpolate import PchipInterpolator\nfrom scipy.optimize import bisect\n\ndef estimate_lambda_values(rejection_rates, offset: float = 1e-3):\n  # Make sure that the estimated rejection rates are non-zero\n  rejection_rates = jnp.maximum(rejection_rates, offset)\n  # We have Lambda(0) = 0 and then estimate the rest by cumulative sums\n  extended = jnp.concatenate((jnp.zeros(1), rejection_rates))\n  return jnp.cumsum(extended)\n\n\ndef get_lambda_function(\n  annealing_schedule,\n  lambda_values,\n):\n  \"\"\"Approximates the Lambda function from several estimates at the schedule by interpolating the values with a monotonic cubic spline (as advised in the paper).\"\"\"\n  return PchipInterpolator(annealing_schedule, lambda_values)\n\n\ndef annealing_optimal(\n  n_chains: int,\n  previous_schedule,\n  rejection_rates,\n  _offset: float = 1e-3,\n):\n  \"\"\"Finds the optimal annealing schedule basing on the approximation of the Lambda function.\"\"\"\n  lambda_values = estimate_lambda_values(rejection_rates, offset=_offset)\n  lambda_fn = get_lambda_function(\n    previous_schedule,\n    lambda_values,\n  )\n\n  lambda1 = lambda_values[-1]\n\n  new_schedule = [0.0]\n\n  for k in range(1, n_chains - 1):\n    def fn(x):\n      desired_value = k * lambda1 / (n_chains - 1)\n      return lambda_fn(x) - desired_value\n    \n    new_point = bisect(\n      fn,\n      new_schedule[-1],\n      1.0,\n    )\n\n    if new_point >= 1.0:\n      raise ValueError(\"Encountered value 1.0.\")\n\n    new_schedule.append(new_point)\n\n  new_schedule.append(1.0)\n\n  if len(new_schedule) != n_chains:\n    raise Exception(\"This should not happen.\")\n\n  return jnp.asarray(new_schedule, dtype=float)\n```\n:::\n\n\nLet's apply these utilities to suggest better annealing schedules for one of the problems above.\n\n::: {.cell execution_count=32}\n``` {.python .cell-code}\ndef run_nonversible_pt(\n  key: RandomKey,\n  sigma: float,\n  distribution_factory,\n  n_samples: int,\n  warmup: int,\n  annealing_schedule: Float[Array, \" n_chains\"],\n  reference_scale: float = 20.0,\n  x0: float = 0.5,\n):\n  betas = jnp.asarray(annealing_schedule)\n  n_chains = betas.shape[0]\n\n  def log_ref(x):\n    return dist.Normal(0, reference_scale).log_prob(x)\n  \n  def sampling_fn(key, log_prob, sigma):\n    # We know how to sample from the reference distribution \n    sigmas = sigma * jnp.ones_like(betas)\n    sigmas = sigmas.at[0].set(reference_scale)\n\n    K_ind = generate_independent_annealed_kernel(\n      log_prob=log_prob,\n      log_ref=log_ref,\n      annealing_schedule=betas,\n      kernel_generator=generate_kernel,\n      params=sigmas,\n    )\n    K_deo = generate_deo_extended_kernel(\n      log_prob=log_prob,\n      log_ref=log_ref,\n      annealing_schedule=betas,\n    )\n\n    key, subkey = jrandom.split(key)\n    samples, rejections = deo_sampling_loop(\n      key=subkey,\n      x0=x0 * jnp.ones(n_chains, dtype=float),\n      kernel_local=K_ind,\n      kernel_deo=K_deo,\n      n_samples=n_samples,\n      warmup=warmup,\n    )\n    return samples, rejections\n\n  def log_prob(x):\n    return distribution_factory().log_prob(x)\n\n  return sampling_fn(key, log_prob, sigma)\n```\n:::\n\n\nWe use 20 chains and an exponential schedule, supposed to decay too quickly:\n\n::: {.cell execution_count=33}\n``` {.python .cell-code}\n_N_CHAINS = 20\n_CURRENT_DIST = mixture3_dist\n\ninitial_schedule = annealing_exponential(n_chains=_N_CHAINS, base=3.0)\nsamples, rejections = run_nonversible_pt(\n  key=RNG_JAX.key,\n  sigma=1.0,\n  distribution_factory=_CURRENT_DIST,\n  n_samples=5_000,\n  warmup=2_000,\n  annealing_schedule=initial_schedule,\n)\n```\n:::\n\n\nLet's calculate a new schedule and collect a new sample:\n\n::: {.cell execution_count=34}\n``` {.python .cell-code}\n_mean_rejections = rejections.mean(axis=0)\n_lambda_vals = estimate_lambda_values(_mean_rejections)\n_lambda_fn = get_lambda_function(initial_schedule, _lambda_vals)\n\nnew_schedule = annealing_optimal(\n  initial_schedule.shape[0],\n  initial_schedule,\n  _mean_rejections,\n)\n\nnew_samples, new_rejections = run_nonversible_pt(\n  key=RNG_JAX.key,\n  sigma=1.0,\n  distribution_factory=_CURRENT_DIST,\n  n_samples=5_000,\n  warmup=2_000,\n  annealing_schedule=new_schedule,\n)\n\n_new_mean_rejections = new_rejections.mean(axis=0)\n_new_lambda_vals = estimate_lambda_values(_new_mean_rejections)\n_new_lambda_fn = get_lambda_function(new_schedule, _new_lambda_vals)\n\nvery_new_schedule = annealing_optimal(\n  new_schedule.shape[0],\n  new_schedule,\n  _new_mean_rejections,\n)\n```\n:::\n\n\nWe have now:\n\n  - two samples: using initial (very quickly decaying schedule) and an optimised one,\n  - two estimates of the $\\Lambda(\\beta)$ function (each estimate depends on the schedule and the rejection rates collected during sampling),\n  - three schedules: the initial one (decaying one), the optimised schedule (for which we also collected a sample), and a third, \"very optimised\", schedule (which we estimated using the second one). \n\nWe hope that:\n\n1. The second sample will be better than the first one (as the schedule now should be better).\n2. The $\\Lambda(\\beta)$ estimates will somewhat agree.\n3. The second schedule will be much different than the second one. On the other hand, we can hope that the third schedule will be close to the second one (as it depends on the $\\Lambda$ function and we hope that the estimates are reasonable).\n\nLet's see how this works:\n\n::: {.cell execution_count=35}\n``` {.python .cell-code}\nfig, axs = plt.subplots(4, 2, figsize=(2 * 3, 4 * 2), dpi=350, sharex=\"row\", sharey=\"row\")\n\n_sample_true = _CURRENT_DIST().sample(RNG_JAX.key, (5_000,)) \n_BINS = np.concatenate([\n  np.linspace(-40, -20, 30),\n  np.linspace(-10, 10, 30),\n  np.linspace(20, 40, 30),\n])\n\naxs[0, 0].set_ylabel(\"Samples\")\naxs[1, 0].set_ylabel(\"Trace\")\naxs[2, 0].set_ylabel(\"$\\\\Lambda(\\\\beta)$ function\")\naxs[3, 0].set_ylabel(\"After opt.\")\n\nfor ax, smp, title in zip(axs[0, :], [samples, new_samples], [\"Initial schedule\", \"New schedule\"]):\n  ax.hist(_sample_true, histtype=\"step\", color=\"w\", bins=_BINS, density=True)\n  ax.hist(smp[..., -1], bins=_BINS, density=True)\n  ax.spines[[\"top\", \"left\", \"right\"]].set_visible(False)\n  ax.set_yticks([])\n  ax.set_title(title)\n\nfor ax, smp in zip(axs[1, :], [samples, new_samples]):\n  ax.plot(smp[..., -1], color=\"C3\")\n  ax.spines[[\"top\", \"right\"]].set_visible(False)\n  ax.set_xlabel(\"Time\")\n\nfor ax, fn in zip(axs[2, :], [_lambda_fn, _new_lambda_fn]):\n  x_ax = np.linspace(0, 1, 30)\n  ax.plot(x_ax, fn(x_ax), c=\"w\")\n  ax.spines[[\"top\", \"right\"]].set_visible(False)\n  ax.set_xlabel(\"$\\\\beta$\")\n\nfor ax, [x_sch, y_sch] in zip(\n  axs[3, :],\n  [\n    [initial_schedule, new_schedule],\n    [new_schedule, very_new_schedule],\n  ]\n):\n  ax.set_xlabel(\"Schedule before opt.\")\n  ax.spines[[\"top\", \"right\"]].set_visible(False)\n  ax.scatter(\n    x_sch,\n    y_sch,\n    c=\"C1\",\n    s=3**2,\n  )\n  ax.plot(x_sch, x_sch, linestyle=\":\", c=\"w\", linewidth=0.8)\n\nfig.tight_layout()\n```\n\n::: {.cell-output .cell-output-display}\n![](non-reversible-parallel-tempering_files/figure-html/cell-36-output-1.png){}\n:::\n:::\n\n\nThis looks pretty good to me!\nWe see that the mixing has improved and the sample is of better quality. The $\\Lambda(\\beta)$ differs a bit (resulting in a refined optimised schedule), but I would argue that this disagreement is reasonable. It also nice to see that the third schedule is close to the second one.\nIn the paper, it is suggested to run the optimisation multiple times and the authors propose solutions how to allocate the computational budget within the preliminary runs and the last sampling phase.\n\n### Summary\n\nI am very excited about parallel tempering! Some related thoughts:\n\n- Can we design other update schemes than DEO, which round-trip rate could be improved $\\Lambda$? How to calculate the round-trip rate for a \"full sweep\" update scheme? Could alternating \"full sweep forward\" and \"full sweep backward\" improve the performance of the algorithm, or somewhat degenerate to the diffuse behaviour?\n- DEO removing dependency on $N$ from SEO reminds me of [preconditioned Crank-Nicolson algorithm](https://en.wikipedia.org/wiki/Preconditioned_Crank%E2%80%93Nicolson_algorithm), which works better than the random-walk Metropolis in high dimensions. Is it possible to somewhat formalise a possible connection between these ideas?\n- How to use parallel tempering when working with Gibbs samplers? Building a bridge through tempering between prior and posterior can break the conjugacy employed at some steps.  Also, Gibbs samplers do not need to explore the prior (acting as the reference distribution) effectively enough.\n- How well can the *data point tempering* work, where the data set is artificially shrunk? This could be a potential bridge for Gibbs samplers (as we can simply consider different Gibbs samplers conditioned on subsets of the data sets as the local exploration kernels), but in the context of SMC samplers, there is [a great 2007 paper](https://doi.org/10.1007/s11222-007-9028-9) from Ajay Jasra, David Stephens and Chris Holmes, showing that (at least for SMC samplers) the data point tempering seems to work worse than the likelihood tempering.\n- Regarding the choice of the tempering scheme, [there is a more general likelihood tempering scheme to bridge the prior and the posterior](https://proceedings.mlr.press/v139/syed21a.html). It looks very interesting, but I have not read this paper yet.\n\n",
    "supporting": [
      "non-reversible-parallel-tempering_files"
    ],
    "filters": [],
    "includes": {}
  }
}