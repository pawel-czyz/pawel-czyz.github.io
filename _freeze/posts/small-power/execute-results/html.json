{
  "hash": "b74aa377d7261d361b85dbce55551fd0",
  "result": {
    "markdown": "---\ntitle: From $t$-test to \"This is what 'power=0.06' looks like\"\ndescription: We take a close look at the derivation of the $t$-test and reproduce plots used by Andrew Gelman to show implications of low-powered studies.\nauthor: Paweł Czyż\ndate: 2/2/2024\nexecute:\n  freeze: true\nformat:\n  html:\n    code-fold: true\n---\n\nAndrew Gelman wrote an [amazing blogpost](https://statmodeling.stat.columbia.edu/2014/11/17/power-06-looks-like-get-used/), where he argues that with large noise-to-signal ratio (low power studies) statistically significant results:\n\n  - Often have exagerrated effect size.\n  - Have large chance of being the wrong sign. Art Owen [made an additional plot](https://statmodeling.stat.columbia.edu/2023/09/07/this-is-what-power-06-looks-like-visualized-by-art-owen/) demonstrating this effect. \n\nThis is especially troublesome because as there is a tradition of publishing only statistically significant results[^1], many of the reported effects will have the wrong sign or be seriously exaggerated.\n\n[^1]: A practice, which can result in [p-hacking](https://en.wikipedia.org/wiki/Data_dredging) and [HARKing](https://en.wikipedia.org/wiki/HARKing). On a related manner see [this post on negative results](https://darrendahly.github.io/post/negative_results/) and [the paper on \"the garden of forking paths\"](http://www.stat.columbia.edu/~gelman/research/unpublished/p_hacking.pdf).\n\nBut we'll take a look at these phenomena later. First, let's review what the $t$-test, confidence intervals, and statistical power are.\n\n## A quick overview of $t$-test\n\nRecall that if we have access to i.i.d. random variables  \n$$\nX_1, \\dotsc, X_n \\sim \\mathcal N(\\mu, \\sigma^2),\n$$\nwe introduce sample mean \n$$\nM = \\frac{X_1 + \\cdots + X_n}{n}\n$$\nand sample standard deviation:\n$$\nS = \\sqrt{ \\frac{1}{n-1} \\sum_{i=1}^n \\left(X_i - M\\right)^2}.\n$$\n\nIt follows that\n$$\nM\\sim \\mathcal N(\\mu, \\sigma^2/n)\n$$\nand\n$$\nS^2\\cdot (n-1)/\\sigma^2 \\sim \\chi^2_{n-1}\n$$\nare independent.\nWe can construct a [pivot](https://en.wikipedia.org/wiki/Pivotal_quantity)\n$$\nT_\\mu = \\frac{M-\\mu}{S / \\sqrt{n}},\n$$\nwhich is distributed according to Student's $t$ distribution[^2] with $n-1$ degrees of freedom, $t_{n-1}$.\n\n[^2]: Student's $t$ distribution with $k$ degrees of freedom arises as the distribution of the variable $T = A/\\sqrt{B / k}$, where $A\\sim \\mathcal N(0, 1)$ and $B\\sim \\chi^2_k$ are independent.\nIn this case, $\\frac{M-\\mu}{\\sigma/\\sqrt{n}}\\sim N(0, 1)$ and $S^2\\cdot (n-1)/ \\sigma^2 \\sim \\chi^2_{n-1}$ are independent. Parameter $\\sigma$ cancels out.  \n\nChoose a number $0 < \\alpha < 1$ and write $u$ for the solution to the equation\n$$\nP(T_\\mu \\in (-u, u)) = 1-\\alpha.\n$$\n\nThe $t$ distribution is continuous and symmetric around $0$, so that\n$$\n\\mathrm{CDF}(-u) = 1 - \\mathrm{CDF}(u)\n$$\nand\n$$\nP(T_\\mu \\in (-u, u)) = \\mathrm{CDF}(u) - \\mathrm{CDF}(-u) = 2\\cdot \\mathrm{CDF}(u) - 1.\n$$\n\nFrom this we have\n$$\n\\mathrm{CDF}(u) = 1 - \\alpha / 2.\n$$\n\nHence, we have\n$$\nP(\\mu \\in ( M - \\delta, M + \\delta )) = 1 - \\alpha,\n$$\nwhere\n$$\n\\delta = u \\cdot\\frac{S}{\\sqrt{n}}\n$$\nis the half-width of the confidence interval.\n\nIn this way we have constructed a confidence interval with coverage $1-\\alpha$. Note that $u$ coresponds to the $(1-\\alpha/2)$th quantile.\nFor example, if we want coverage of $95\\%$, we take $\\alpha=5\\%$\nand $u$ will be the 97.5th percentile.\n\n### Hypothesis testing\n\nConsider a hypothesis $H_0\\colon \\mu = 0$.\n\nWe will reject $H_0$ if $0$ is outside of the confidence interval defined above.\nNote that\n$$\nP( 0 \\in (M-\\delta, M + \\delta)  \\mid H_0 ) = 1-\\alpha\n$$\nand\n$$\nP(0 \\not\\in (M-\\delta, M + \\delta) \\mid H_0 ) = 1 - (1-\\alpha) = \\alpha,\n$$\nmeaning that such a test will have false discovery rate $\\alpha$.\n\n### Interlude: $p$-values\n\nThe above test is often presented in terms of $p$-values.\nDefine the $t$-statistic\n$$\nT_0 = \\frac{M}{S/\\sqrt{n}},\n$$\nwhich does not require the knowledge of a true $\\mu$.\nNote that if $H_0$ is true, then $T_0$ will be distributed according to $t_{n-1}$ distribution. (If $H_0$ is false, then it won't be. We will take a closer look at this case when we [discuss power](#statistical-power)).\n\nWe have $T_0 \\in (-u, u)$ if and only if $0$ is inside the confidence interval defined above: we can reject $H_0$ whenever $|T_0| \\ge u$.\nAs the procedure hasn't really changed, we also have \n$$\nP(T_0 \\in (-u, u) \\mid H_0 ) = P( |T_0| < u \\mid H_0 ) = 1-\\alpha\n$$\nand this test again has false discovery rate $\\alpha$.\n\nNow define\n$$\n\\Pi = 2\\cdot (1 - \\mathrm{CDF}( |T_0| ) ) = 2\\cdot \\mathrm{CDF}(-|T_0|).\n$$\nNote that we have\n\n$$\\begin{align*}\n  \\Pi < \\alpha &\\Leftrightarrow 2\\cdot (1-\\mathrm{CDF}(|T_0|)) < 2\\cdot (1-\\mathrm{CDF}(u)) \\\\\n  &\\Leftrightarrow 1- \\mathrm{CDF}(|T_0|) < 1 - \\mathrm{CDF}(u) \\\\\n  &\\Leftrightarrow \\mathrm{CDF}(-|T_0|) < \\mathrm{CDF}(-u)\\\\\n  &\\Leftrightarrow -|T_0| < -u \\\\\n  &\\Leftrightarrow |T_0| > u,\n\\end{align*}\n$$\n\nso that we can compare the $p$-value $\\Pi$ against false discovery rate $\\alpha$.\n\nBoth comparisons are equivalent and equally hard to compute: the hard part of comparing $|T_0|$ against $u$ is calculation of $u$ from $\\alpha$ (which requires the access to the CDF).\nThe hard part of comparing the $p$-value $\\Pi$ against $\\alpha$ is calculation of $\\Pi$ from $|T_0|$ (which requires the access to the CDF).\nI should also add that, as we will see below, the CDF is actually easy to access in Python.  \n\nLet's also observe that we have\n$$\nP( \\Pi \\le \\alpha \\mid H_0 ) = P( |T_0| \\ge u \\mid H_0 ) = \\alpha,\n$$\nmeaning that if the null hypothesis is true, then $\\Pi$ has the uniform distribution over the interval $(0, 1)$.\n\n### Summary\n\n- Choose a coverage level $1-\\alpha$.\n- Collect a sample of size $n$.\n- Calculate $u$ as using $\\mathrm{CDF}(u)=1-\\alpha/2$, or equivalently, $\\alpha = 2\\cdot ( 1 - \\mathrm{CDF}(u) )$, where we use the CDF of the Student distribution with $n-1$ degrees of freedom.\n- Calculate sample mean and sample standard deviation, $M$ and $S$.\n- Construct the confidence interval as $M\\pm uS/\\sqrt{n}$.\n- To test for $H_0\\colon \\mu = 0$ check if $\\mu_0$ is outside of the confidence interval (to reject $H_0$).\n- Alternatively, construct $T_0 = M\\sqrt{n}/S$ and compare $|T_0| > u$ (to reject $H_0$).\n- Alternatively, construct $\\Pi = 2\\cdot (1-\\mathrm{CDF}(|T_0|))$ and check whether $\\Pi < \\alpha$ (to reject $H_0$).\n\n### A bit of code\nLet's implement the above formulae using [SciPy's $t$ distribution](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.t.html) to keep things as related to the formulae above as possible.\nAny discrepancies will suggest an error in the derivations or a bug in the code. \n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport numpy as np\n\nfrom scipy import stats\n\ndef alpha_to_u(alpha: float, nobs: int) -> float:\n  if np.min(alpha) <= 0 or np.max(alpha) >= 1:\n    raise ValueError(\"Alpha has to be inside (0, 1).\")\n  return stats.t.ppf(1 - alpha / 2, df=nobs - 1)\n\n\ndef u_to_alpha(u: float, nobs: int) -> float:\n  if np.min(u) <= 0:\n    raise ValueError(\"u has to be positive\")\n  return 2 * (1 - stats.t.cdf(u, df=nobs - 1))\n\n# Let's test whether the functions seem to be compatible\nfor alpha in [0.01, 0.05, 0.1, 0.99]:\n  for nobs in [5, 10]:\n    u = alpha_to_u(alpha, nobs=nobs)\n    a = u_to_alpha(u, nobs=nobs)\n\n    if abs(a - alpha) > 1e-4:\n      raise ValueError(f\"Discrepancy for {nobs=} {alpha=}\")\n\n\ndef calculate_t(xs: np.ndarray):\n  # Sample mean and sample standard deviation\n  n = len(xs)\n  m = np.mean(xs)\n  s = np.std(xs, ddof=1)\n\n  # Calculate the t value assuming the null hypothesis mu = 0\n  t = m / (s / np.sqrt(n))\n  return t\n\ndef calculate_p_value_from_t(t: float, nobs: int) -> float:\n  return 2 * (1 - stats.t.cdf(np.abs(t), df=nobs-1))\n\ndef calculate_p_value_from_data(xs: np.ndarray) -> float:\n  n = len(xs)\n  t = calculate_t(xs)\n  return calculate_p_value_from_t(t=t, nobs=n)\n\ndef calculate_ci_delta_from_params(\n  s: float,\n  nobs: int,\n  alpha: float,\n) -> tuple[float, float]:\n  u = alpha_to_u(alpha, nobs=nobs)\n  delta = u * s / np.sqrt(nobs)\n  return delta\n\ndef calculate_ci_delta_from_data(xs: np.ndarray, alpha: float) -> float:\n  m = np.mean(xs)\n  s = np.std(xs, ddof=1)\n  delta = calculate_ci_delta_from_params(s=s, nobs=len(xs), alpha=alpha)\n  return delta\n\ndef calculate_confidence_interval_from_data(\n  xs: np.ndarray,\n  alpha: float,\n) -> tuple[float, float]:\n  delta = calculate_ci_delta_from_data(xs, alpha=alpha)\n  return (m-delta, m+delta)\n```\n:::\n\n\nWe have three equivalent tests for rejecting $H_0$. Let's see how they perform on several samples.\nWe'll simulate $N$ times a fresh data set $X_1, \\dotsc, X_n$ from $\\mathcal N\\left(0, \\sigma^2\\right)$ and calculate the confidence interval, the $T_0$ statistic and the $p$-value for each of these.\nWe will order the samples by increasing $p$-value (equivalently, with decreasing $|T_0|$ statistic), to make dependencies in the plot easier to see.\n\nAdditionally, we will choose relatively large $\\alpha$ and mark the regions such that the test does not reject $H_0$. In terms of confidence intervals there is no such region, as each sample which has its own confidence interval, which can contain $0$ or not.\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\nplt.style.use(\"dark_background\")\n\nrng = np.random.default_rng(42)\n\nnsimul = 100\nnobs = 5\n\nsamples = rng.normal(loc=0, scale=1.0, size=(nsimul, nobs))\np_values = np.asarray([calculate_p_value_from_data(x) for x in samples])\nindex = np.argsort(p_values)\n\n\nalpha: float = 0.1\nu_thresh = alpha_to_u(alpha, nobs=nobs)\n\nsamples = samples[index, :]\np_values = p_values[index]\nt_stats = np.asarray([calculate_t(x) for x in samples])\nsample_means = np.mean(samples, axis=1)\nci_deltas = np.asarray([calculate_ci_delta_from_data(x, alpha=alpha) for x in samples])\n\nx_axis = np.arange(1, nsimul + 1)\nfig, axs = plt.subplots(3, 1, figsize=(3, 4), dpi=150, sharex=True)\n\n# P-values plot\nax = axs[0]\nax.fill_between(x_axis, alpha, 0, color=\"lime\", alpha=0.2)\nax.plot(x_axis, np.linspace(0, 1, num=len(x_axis)), linestyle=\"--\", linewidth=0.5, color=\"white\")\nax.scatter(x_axis, p_values, c=\"yellow\", s=1)\nax.set_xlim(-0.5, nsimul + 0.5)\nax.set_ylabel(\"$p$-value\")\n\nax.axvline(alpha * nsimul + 0.5, color=\"maroon\", linestyle=\"--\", linewidth=0.5)\n\n# T statistics\nax = axs[1]\nax.fill_between(x_axis, -u_thresh, u_thresh, color=\"lime\", alpha=0.2)\nax.scatter(x_axis, t_stats, c=\"yellow\", s=1)\nax.plot(x_axis, np.zeros_like(x_axis), c=\"white\", linestyle=\"--\", linewidth=0.5)\nax.set_ylabel(\"$t$ statistic\")\n\nax.axvline(alpha * nsimul + 0.5, color=\"maroon\", linestyle=\"--\", linewidth=0.5)\n\n# Confidence intervals\nax = axs[2]\nax.set_xlabel(\"Sample index\")\nax.plot(x_axis, np.zeros_like(x_axis), c=\"white\", linestyle=\"--\", linewidth=0.5)\nax.errorbar(x_axis, sample_means, yerr=ci_deltas, fmt=\"o\", markersize=1, linewidth=0.5, c=\"yellow\")\nax.set_ylabel(\"Conf. int.\")\n\nax.axvline(alpha * nsimul + 0.5, color=\"maroon\", linestyle=\"--\", linewidth=0.5)\n\nfor ax in axs:\n  ax.spines[['top', 'right']].set_visible(False)\n\nfig.tight_layout()\n```\n\n::: {.cell-output .cell-output-display}\n![](small-power_files/figure-html/cell-3-output-1.png){}\n:::\n:::\n\n\nLet's also quickly verify that, indeed, the distribution of $p$-values is uniform over $(0, 1)$ (which, in a way, can be already deduced from the plot with ordered $p$ values) and that the distribution of the $t$ statistic is indeed $t_{n-1}$.\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nfig, axs = plt.subplots(1, 3, figsize=(5, 2), dpi=150)\n\nax = axs[0]\nax.set_title(\"CDF ($p$-value)\")\nx_ax = np.linspace(0, 1, 5)\nax.plot(x_ax, x_ax, color=\"white\", linestyle=\"--\", linewidth=1)\nax.ecdf(p_values, c=\"maroon\")\n\nax = axs[1]\nax.set_title(\"CDF ($t$-stat.)\")\nx_ax = np.linspace(-3.5, 3.5, 101)\nax.plot(x_ax, stats.t.cdf(x_ax, df=nobs-1), color=\"white\", linestyle=\"--\", linewidth=1)\nax.ecdf(t_stats, c=\"maroon\")\n\nax = axs[2]\nax.set_title(\"CDF (mean)\")\nx_ax = np.linspace(-1.2, 1.2, 101)\nax.plot(x_ax, stats.norm.cdf(x_ax, scale=1/np.sqrt(nobs)), color=\"white\", linestyle=\"--\", linewidth=1)\nax.ecdf(sample_means, c=\"maroon\")\n\nfor ax in axs:\n  ax.spines[['top', 'right']].set_visible(False)\n\nfig.tight_layout()\n```\n\n::: {.cell-output .cell-output-display}\n![](small-power_files/figure-html/cell-4-output-1.png){}\n:::\n:::\n\n\n## Statistical power\n\nAbove we have seen a procedure used to reject the null hypothesis $H_0\\colon \\mu = 0$ either by constructing the confidence interval or defining the variable $T_0$ with the property that\n$P( |T_0| > u \\mid H_0) = \\alpha.$\n\nConsider now the data coming from a distribution with any other $\\mu$, i.e., we will not condition on $H_0$ anymore.\nTo make this explicit in the notation, we will condition on $H_\\mu$, rather than $H_0$.\n\nHow does the distribution of $T_0$ look like now? Recall that have independent variables\n$$\n\\frac{M-\\mu}{\\sigma/\\sqrt{n}} \\sim \\mathcal N(0, 1)\n$$\nand\n$$\nS^2\\cdot (n-1)/\\sigma^2 \\sim \\chi^2_{n-1}\n$$\nso that we have, of course,\n$$\nT_\\mu = \\frac{M-\\mu}{S/\\sqrt{n}} =  \n\\frac{ \\frac{M-\\mu}{\\sigma/\\sqrt{n}}}{\\sqrt{ \\frac{ S^2\\cdot (n-1)/\\sigma^2 }{n-1} }}\n\\sim t_{n-1}.\n$$\nFor $T_0$ we have\n$$\nT_0 = \\frac{ \\frac{M-\\mu}{\\sigma/\\sqrt{n}} + \\frac{\\mu}{\\sigma/\\sqrt{n}}}{\\sqrt{ \\frac{ S^2\\cdot (n-1)/\\sigma^2 }{n-1} }}\n$$\nwhich is distributed according to the [noncentral $t$ distribution](https://en.wikipedia.org/wiki/Noncentral_t-distribution) with noncentrality parameter $\\theta = \\mu\\sqrt{n} / \\sigma$.\nNote that this distribution is generally asymmetric and different from the location-scale generalisation of the (central) $t$ distribution.\nLet's write $F_{n-1, \\theta}$ for the CDF of this function. We will reject the null hypothesis $H_0$ with probability\n$$\\begin{align*}\nP(|T_0| > u \\mid H_\\mu) &= P(T_0 > u \\mid H_\\mu ) + P(T_0 < -u \\mid H_\\mu) \\\\\n&= 1 - P(T_0 < u \\mid H_\\mu) + P(T_0 < -u \\mid H_\\mu) \\\\\n&= 1 - F_{n-1,\\theta}(u) + F_{n-1,\\theta}(-u),\n\\end{align*}\n$$\n\nwhich gives us *statistical power of the test*.\n\nWe see that power depends on chosen $\\alpha$ (as it controls $u$, the value we compare against the $|T_0|$ statistic), on $n$ (as it controls both the number of degrees of freedom in the CDF $F_{n-1,\\theta}$ and the noncentrality parameter $\\theta$) and on the effect size, by which we understand the [standardized mean difference](https://en.wikipedia.org/wiki/Effect_size#Standardized_mean_difference), i.e., $\\mu/\\sigma$.\n\n### Another bit of code\n\nLet's implement power calculation. In fact, [statsmodels](https://www.statsmodels.org/dev/generated/statsmodels.stats.power.TTestPower.html) has very convenient utilities for power calculation, so in practice implementing it is never needed:\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\nfrom statsmodels.stats.power import TTestPower\n\ndef calculate_power(\n  effect_size: float,\n  nobs: int,\n  alpha: float,\n) -> float:\n  theta = np.sqrt(nobs) * effect_size\n  u = alpha_to_u(alpha, nobs=nobs)\n\n  def cdf(x: float) -> float:\n    return stats.nct.cdf(x, df=nobs-1, nc=theta)\n  \n  return 1 - cdf(u) + cdf(-u)\n\nfor nobs in [5, 10]:\n  for alpha in [0.01, 0.05, 0.1]:\n    for effect_size in [0, 0.1, 1.0, 3.0]:\n      power = calculate_power(\n        effect_size=effect_size, nobs=nobs, alpha=alpha,\n      )\n      power_ = TTestPower().power(\n        effect_size=effect_size, nobs=nobs, alpha=alpha, alternative=\"two-sided\")\n      \n      if abs(power - power_) > 0.001:\n        raise ValueError(f\"For {nobs=} {alpha=} {effect_size=} we noted discrepancy {power} != {power_}\")\n```\n:::\n\n\nLet's quickly check how power depends on the effect size in the following setting.\nWe collect $X_1, \\dotsc, X_n\\sim \\mathcal N(\\mu, 1^2)$, so that standardized mean difference is $\\mu = \\mu/1$ and we will use standard $\\alpha = 5\\%$.\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\nfig, axs = plt.subplots(1, 3, figsize=(7, 3), dpi=150, sharey=True)\n\nax = axs[0]\nmus = np.linspace(0, 1.5)\nax.plot(\n  mus, calculate_power(effect_size=mus, nobs=10, alpha=0.05)\n)\nax.set_xlabel(r\"$\\mu/\\sigma$\")\nax.set_ylabel(\"Power\")\n\nax = axs[1]\nns = np.arange(5, 505)\nfor eff in [0.1, 0.5, 1.0]:\n  ax.plot(\n    ns, calculate_power(effect_size=eff, nobs=ns, alpha=0.05),\n    label=f\"$\\mu/\\sigma={eff:.1f}$\"\n  )\nax.set_xscale(\"log\")\nn_labels = np.asarray([5, 10, 20, 100, 500])\nax.set_xticks(n_labels, n_labels)\nax.set_xlabel(\"$n$\")\n\nax = axs[2]\nalphas = np.linspace(0.01, 0.99, 99)\nfor eff in [0.1, 0.5, 1.0]:\n  ax.plot(\n    alphas, calculate_power(effect_size=eff, nobs=5, alpha=alphas),\n    label=f\"$\\mu/\\sigma={eff:.1f}$\"\n  )\nax.legend(frameon=False)\nax.set_xlabel(r\"$\\alpha$\")\n\nfor ax in axs:\n  ax.spines[['top', 'right']].set_visible(False)\n\nfig.tight_layout()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n/home/pawel/micromamba/envs/data-science/lib/python3.10/site-packages/scipy/stats/_continuous_distns.py:7313: RuntimeWarning: invalid value encountered in _nct_cdf\n  return np.clip(_boost._nct_cdf(x, df, nc), 0, 1)\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](small-power_files/figure-html/cell-6-output-2.png){}\n:::\n:::\n\n\n## Consequences of low power\n\nConsider a study with $\\sigma=1$, $\\mu=0.1$ and $n=10$. At $\\alpha = 5\\%$ we have power of:\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\npower = calculate_power(effect_size=0.1, nobs=10, alpha=0.05)\nif abs(power - 0.06) > 0.005:\n  raise ValueError(f\"We want power to be around 6%\")\nprint(f\"Power: {100 * power:.2f}%\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nPower: 5.93%\n```\n:::\n:::\n\n\nwhich is similar to the value used in Andrew Gelman's post.\nLet's simulate a lot of data sets and confirm that the power of the test is indeed around 6%:\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\nnsimul = 200_000\nnobs = 10\nalpha = 0.05\n\ntrue_mean = 0.1\n\nsamples = rng.normal(loc=true_mean, scale=1.0, size=(nsimul, nobs))\np_values = np.asarray([calculate_p_value_from_data(x) for x in samples])\n\nprint(f\"Power from simulation: {100 * np.mean(p_values < alpha):.2f}%\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nPower from simulation: 5.97%\n```\n:::\n:::\n\n\n(As a side note, I already regret not implementing $p$-value calculation in JAX – I can't use `vmap`!)\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\nfig, axs = plt.subplots(1, 2, figsize=(5, 2), dpi=150, sharex=True)\n\nbins = np.linspace(-1.5, 1.5, 31)\n\nax = axs[0]\nsample_means = np.mean(samples, axis=1)\nax.hist(\n  sample_means,\n  bins=bins,\n  histtype=\"step\",\n)\nax.axvline(true_mean)\nax.set_title(\"Histogram of sample means\")\nax.set_xlabel(\"Sample mean\")\nax.set_ylabel(\"PDF\")\n\nax = axs[1]\nx_ax = np.linspace(-1.5, 1.5, 51)\nax.plot(\n  x_ax, stats.norm.cdf(x_ax, loc=0.1, scale=1/np.sqrt(nobs))\n)\nax.ecdf(sample_means)\nax.set_xlabel(\"Sample mean\")\nax.set_ylabel(\"CDF\")\nax.set_title(\"Empirical CDF\")\n\nfor ax in axs:\n  ax.spines[['top', 'right']].set_visible(False)\n\nfig.tight_layout()\n```\n\n::: {.cell-output .cell-output-display}\n![](small-power_files/figure-html/cell-9-output-1.png){}\n:::\n:::\n\n\nWe see that sample mean, represented by the random variable $M$, is indeed distributed according to $\\mathcal N\\left(0.1, \\left(1/\\sqrt{10}\\right)^2\\right)$.\nThe standard error of the mean, $1/\\sqrt{10}\\approx 0.31$ is three times larger than the population mean $\\mu=0.1$, so we have a lot of noise here.\n\nLet's see what happens if a \"statistically significant\" result is somehow obtained.\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\nstat_signif_samples = samples[p_values < alpha, :]\n\nfig, ax = plt.subplots(figsize=(2, 2), dpi=150)\n\nsample_means = np.mean(stat_signif_samples, axis=1)\nax.hist(\n  sample_means,\n  bins=bins,\n  histtype=\"step\",\n)\nax.axvline(true_mean)\nax.set_title(\"Stat. signif. results\")\nax.set_xlabel(\"Sample mean\")\nax.set_ylabel(\"PDF\")\n\n\nax.spines[['top', 'right']].set_visible(False)\nfig.tight_layout()\n```\n\n::: {.cell-output .cell-output-display}\n![](small-power_files/figure-html/cell-10-output-1.png){}\n:::\n:::\n\n\nConditioning only on statistically significant results, let's take a look at:\n\n  - how many of them have sample mean of wrong sign,\n  - how many of them have sample mean seriously exaggerated (e.g., at least 5 times),\n\nsimilarly as [Andrew Gelman](https://statmodeling.stat.columbia.edu/2014/11/17/power-06-looks-like-get-used/) did in his blog post:\n\n::: {.cell execution_count=10}\n``` {.python .cell-code}\nfrac_wrong_sign = np.mean(sample_means < 0)\nprint(f\"Wrong sign: {100*frac_wrong_sign:.1f}%\")\n\nfrac_exaggerated = np.mean(sample_means >= 5 * true_mean)\nprint(f\"Exaggerated (5x): {100 * frac_exaggerated:.1f}%\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nWrong sign: 20.8%\nExaggerated (5x): 69.5%\n```\n:::\n:::\n\n\nOh, that's not good! We see that a statistically significant result (which itself has only 6% occurrence probability, if the study is executed properly), will have about 20% chance of being of a wrong sign and around 2/3 chance of being quite exaggerated.\n\nActually, Andrew Gelman looked at results exaggerated 9 times. Let's make a plot summarizing these probabilities:\n\n::: {.cell execution_count=11}\n``` {.python .cell-code}\nfig, ax = plt.subplots(figsize=(2, 2), dpi=150)\n\nratio_exag = np.linspace(1, 11, 31)\nfrac_exag = [np.mean(sample_means >= r * true_mean) for r in ratio_exag]\n\nax.plot(\n  ratio_exag,\n  frac_exag,\n)\nax.set_ylim(0, 1)\nax.set_xlabel(\"Exag. factor\")\nax.set_ylabel(\"Probability\")\nxticks = [1, 3, 5, 7, 9]\nax.set_xticks(xticks, xticks)\n\nax.spines[['top', 'right']].set_visible(False)\nfig.tight_layout()\n```\n\n::: {.cell-output .cell-output-display}\n![](small-power_files/figure-html/cell-12-output-1.png){}\n:::\n:::\n\n\nFinally, there is a great plot by [Art Owen](https://statmodeling.stat.columbia.edu/2023/09/07/this-is-what-power-06-looks-like-visualized-by-art-owen/) showing how confidence intervals of statistically significant results look like when power is low. Let's quickly reproduce it:\n\n::: {.cell execution_count=12}\n``` {.python .cell-code}\nci_deltas = np.asarray([calculate_ci_delta_from_data(x, alpha=alpha) for x in stat_signif_samples])\n\nn_to_plot = min(50, len(stat_signif_samples))\n\nx_axis = np.arange(1, n_to_plot + 1)\nfig, ax = plt.subplots(1, figsize=(3, 2), dpi=150)\n\nax.set_xlabel(\"Sample index\")\n\nax.plot(x_axis, np.zeros_like(x_axis), c=\"white\", linestyle=\"--\", linewidth=0.5)\nax.plot(x_axis, np.zeros_like(x_axis) + true_mean, c=\"maroon\", linestyle=\"-\", linewidth=0.5)\n\nax.errorbar(x_axis, sample_means[:n_to_plot], yerr=ci_deltas[:n_to_plot], fmt=\"o\", markersize=1, linewidth=0.5, c=\"yellow\")\nax.set_ylabel(\"Conf. int.\")\n\nax.spines[['top', 'right']].set_visible(False)\nfig.tight_layout()\n```\n\n::: {.cell-output .cell-output-display}\n![](small-power_files/figure-html/cell-13-output-1.png){}\n:::\n:::\n\n\nOf course, the confidence intervals cannot contain $0$ (otherwise the results wouldn't have been statistically significant).\nHow many of them contain $\\mu=0.1$?\nIn case we look at all the results (including the majority of nonsignificant results), $1-\\alpha=95\\%$ of confidence intervals contains the true value.\nHowever, once we restrict our attention to only significant results, this coverage drops to\n\n::: {.cell execution_count=13}\n``` {.python .cell-code}\nfrac_contain = np.mean(\n  (sample_means - ci_deltas < true_mean) \n  & (true_mean < sample_means + ci_deltas)\n)\nprint(f\"Coverage: {100 * frac_contain:.1f}%\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCoverage: 37.2%\n```\n:::\n:::\n\n\n## Summary\n\nLow-powered studies have, of course, high probability of not rejecting $H_0$ when the alternative is true.\nBut even when $H_0$ is rejected, estimated mean will be often of wring sign or exaggerated.\nThis indeed shows that principled experimental design and power analysis are crucial to execute before any data collection! \n\n",
    "supporting": [
      "small-power_files"
    ],
    "filters": [],
    "includes": {}
  }
}