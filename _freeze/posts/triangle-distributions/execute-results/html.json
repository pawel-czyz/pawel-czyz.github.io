{
  "hash": "39b778d494ed3b228bf71f9ade39f866",
  "result": {
    "markdown": "---\ntitle: Two distributions on a triangle\ndescription: 'Sometimes if you misunderstand something, you can have two interesting distributions, rather than only one.'\nauthor: Paweł Czyż\ndate: 8/31/2023\ncategories:\n  - Mutual Information\nexecute:\n  freeze: true\nformat:\n  html:\n    code-fold: true\n---\n\nFrederic, Alex and I have been discussing some experiments related to [our work on mutual information estimators](https://arxiv.org/abs/2306.11078) and Frederic suggested to look at one distribution.\nI misunderstood what he meant, but this mistake turned out to be quite an interesting object.\n\nSo let's take a look at two distributions defined over a triangle\n$$T = \\{ (x, y)\\in (0, 1)\\times (0, 1) \\mid y < x \\}$$\nand calculate their mutual information.\n\n## Uniform joint\nConsider a probability distribution with constant probability density function (PDF) of the joint distribution:\n$$p_{XY}(x, y) = 2 \\cdot \\mathbf{1}[y<x].$$\n\nWe have\n$$p_X(x) = \\int\\limits_0^x p_{XY}(x, y)\\, \\mathrm{d}y = 2x$$\nand\n$$ p_Y(y) = \\int\\limits_0^1 p_{XY}(x, y) \\mathbf{1}[y < x]  \\, \\mathrm{d}x = \\int\\limits_y^1 p_{XY}(x, y) \\, \\mathrm{d}x = 2(1-y).$$\n\nHence, pointwise mutual information is given by\n$$ i(x, y) = \\log \\frac{ p_{XY}(x, y) }{p_X(x) \\, p_Y(y) } = \\log \\frac{1}{2x(1-y)}$$\nand mutual information is\n\n$$I(X; Y) = \\int\\limits_0^1 \\mathrm{d}x \\int\\limits_x^1 i(x, y)\\, p_{XY}(x, y) \\mathrm{d}y = 1-\\log 2 \\approx 0.307.$$\n\nFinally, let's visualise this distribution to numerically validate the formulae above:\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nfrom typing import Protocol\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nplt.style.use(\"dark_background\")\n\n\nclass Distribution(Protocol):\n  def sample(self, rng, n_samples: int) -> np.ndarray:\n    pass\n\n  def p_xy(self, x: np.ndarray, y: np.ndarray) -> np.ndarray:\n    pass\n\n  def p_x(self, x: np.ndarray) -> np.ndarray:\n    pass\n\n  def p_y(self, y: np.ndarray) -> np.ndarray:\n    pass\n\n  def pmi(self, x: np.ndarray, y: np.ndarray) -> np.ndarray:\n    pass\n\n  @property\n  def mi(self) -> float:\n    pass\n\n\nclass UniformJoint(Distribution):\n  def sample(self, rng, n_samples):\n    samples = rng.uniform(low=1e-9, size=(3 * n_samples, 2))\n    samples = np.asarray(list(filter(lambda point: point[1] < point[0], samples)))\n    if len(samples) < n_samples:\n      samples = self.sample(rng, n_samples)\n    \n    assert len(samples) >= n_samples\n    return samples[:n_samples, ...]\n\n  def p_xy(self, x, y):\n    return np.where(y < x, 2.0, 0.0)\n\n  def p_x(self, x):\n    return 2*x\n\n  def p_y(self, y):\n    return 2*(1-y)\n\n  def pmi(self, x, y):\n    return np.where(y < x, -np.log(2*x*(1-y)), np.nan)\n\n  @property\n  def mi(self):\n    return 0.307\n\n\ndef visualise_dist(\n  rng,\n  dist: Distribution,\n  n_samples: int = 15_000,\n) -> plt.Figure:\n  fig, axs = plt.subplots(2, 3, figsize=(3*2.2, 2*2.2))\n\n  samples = dist.sample(rng, n_samples=n_samples)\n\n  t_axis = np.linspace(1e-9, 1 - 1e-9, 51)\n\n  X, Y = np.meshgrid(t_axis, t_axis)\n\n  # Visualise joint probability\n  ax = axs[0, 0]\n  ax.scatter(samples[:, 0], samples[:, 1], rasterized=True, alpha=0.3, s=0.2, marker=\".\")\n  ax.set_xlim(0, 1)\n  ax.set_ylim(0, 1)\n  ax.set_title(\"Samples from $P_{XY}$\")\n  ax.set_xlabel(\"$x$\")\n  ax.set_ylabel(\"$y$\")\n\n  ax = axs[1, 0]\n  ax.imshow(dist.p_xy(X, Y), origin=\"lower\", extent=[0, 1, 0, 1], cmap=\"magma\")\n  ax.set_title(\"PDF $p_{XY}$\")\n  ax.set_xlabel(\"$x$\")\n  ax.set_ylabel(\"$y$\")\n\n  # Visualise marginal distributions\n  ax = axs[0, 1]\n  ax.set_xlim(0, 1)\n  ax.hist(samples[:, 0], bins=np.linspace(0, 1, 51), density=True, alpha=0.2, rasterized=True)\n  ax.plot(t_axis, dist.p_x(t_axis))\n  ax.set_xlabel(\"$x$\")\n  ax.set_title(\"PDF $p_X$\")\n\n  ax = axs[1, 1]\n  ax.set_xlim(0, 1)\n  ax.hist(samples[:, 1], bins=np.linspace(0, 1, 51), density=True, alpha=0.2, rasterized=True)\n  t_axis = np.linspace(0, 1, 51)\n  ax.plot(t_axis, dist.p_y(t_axis))\n  ax.set_xlabel(\"$y$\")\n  ax.set_title(\"PDF $p_Y$\")\n\n  # Visualise PMI\n  ax = axs[0, 2]\n  ax.set_xlim(0, 1)\n  ax.set_ylim(0, 1)\n  ax.imshow(dist.pmi(X, Y), origin=\"lower\", extent=[0, 1, 0, 1], cmap=\"magma\")\n  ax.set_title(\"PMI\")\n  ax.set_xlabel(\"$x$\")\n  ax.set_ylabel(\"$y$\")\n\n  ax = axs[1, 2]\n  pmi_profile = dist.pmi(samples[:, 0], samples[:, 1])\n  mi = np.mean(pmi_profile)\n  ax.set_title(f\"PMI histogram. MI={dist.mi:.2f}\")  \n  ax.axvline(mi, color=\"navy\", linewidth=1)\n  ax.axvline(dist.mi, color=\"salmon\", linewidth=1, linestyle=\"--\")\n  ax.hist(pmi_profile, bins=np.linspace(-2, 5, 21), density=True)\n  ax.set_xlabel(\"PMI value\")\n\n  return fig\n\nrng = np.random.default_rng(42)\ndist = UniformJoint()\n\nfig = visualise_dist(rng, dist)\nfig.tight_layout()\n```\n\n::: {.cell-output .cell-output-display}\n![](triangle-distributions_files/figure-html/cell-2-output-1.png){width=651 height=409}\n:::\n:::\n\n\n## Uniform margin\n\nThe above distribution is interesting, but when I heard about the distribution over the triangle, I actually had the following generative model in mind:\n$$\\begin{align*}\n  X &\\sim \\mathrm{Uniform}(0, 1),\\\\\n  Y \\mid X=x &\\sim \\mathrm{Uniform}(0, x).\n\\end{align*}$$\n\nWe have $p_X(x) = 1$ and therefore\n$$p_{XY}(x, y) = p_{Y\\mid X}(y\\mid x) = \\frac{1}{x}\\,\\mathbf{1}[y < x].$$\n\nAgain, this distribution is defined on the triangle $T$, although now the joint is *not* uniform.\n\nWe have\n$$ p_Y(y) = \\int\\limits_y^1  \\frac{1}{x} \\, \\mathrm{d}x = -\\log y$$\nand\n$$i(x, y) = \\log \\frac{1}{-x \\log y} = -\\log \\big(x\\cdot (-\\log y)\\big )\n= - \\left(\\log(x) + \\log(-\\log y) \\right) = -\\log x - \\log(-\\log y).$$\nThis expression suggests that if $p_Y(y)$ were uniform on $(0, 1)$ (but it is not), the pointwise mutual information $i(x, Y)$ would be distributed according to [Gumbel distribution](https://en.wikipedia.org/wiki/Gumbel_distribution#Random_variate_generation).\n\nThe mutual information\n$$\n  I(X; Y) = -\\int\\limits_0^1 \\mathrm{d}y \\int\\limits_y^1 \\frac{ \\log x + \\log(-\\log y)}{x} \\, \\mathrm{d}x = \\frac{1}{2} \\int\\limits_0^1 \\log y \\cdot \\log \\left(y \\log ^2 y\\right) \\, \\mathrm{d}y = \\gamma \\approx 0.577 \n$$\nis in this case the [Euler--Mascheroni constant](https://en.wikipedia.org/wiki/Euler%27s_constant). I don't know how to do this integral, but both Mathematica and Wolfram Alpha seem to be quite confident in it.\n\nPerhaps it shouldn't be too surprising as $\\gamma$ can appears in expressions involving mean of the Gumbel distribution.\nHowever, I'd like to understand this connection better.\n\nPerhaps another time; let's finish this post with another visualisation:\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nclass UniformMargin(Distribution):\n  def sample(self, rng, n_samples: int) -> np.ndarray:\n    x = rng.uniform(size=(n_samples,))\n    y = rng.uniform(high=x)\n    return np.hstack([x.reshape((-1, 1)), y.reshape((-1, 1))])\n\n  def p_xy(self, x: np.ndarray, y: np.ndarray) -> np.ndarray:\n    return np.where(y < x, np.reciprocal(x), np.nan)\n\n  def p_x(self, x: np.ndarray) -> np.ndarray:\n    return np.full_like(x, fill_value=1.0)\n\n  def p_y(self, y: np.ndarray) -> np.ndarray:\n    return -np.log(y)\n\n  def pmi(self, x: np.ndarray, y: np.ndarray) -> np.ndarray:\n    return np.where(y < x, -np.log(-x * np.log(y)), np.nan)\n\n  @property\n  def mi(self):\n    return 0.577\n\n\nrng = np.random.default_rng(42)\ndist = UniformMargin()\n\nfig = visualise_dist(rng, dist)\nfig.tight_layout()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n/tmp/ipykernel_95941/2727834072.py:14: RuntimeWarning: divide by zero encountered in log\n  return -np.log(y)\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](triangle-distributions_files/figure-html/cell-3-output-2.png){width=651 height=409}\n:::\n:::\n\n\n",
    "supporting": [
      "triangle-distributions_files"
    ],
    "filters": [],
    "includes": {}
  }
}