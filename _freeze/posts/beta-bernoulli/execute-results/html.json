{
  "hash": "de9031193be59a92291fa62378022f63",
  "result": {
    "markdown": "---\ntitle: Beta-Bernoulli distribution\ndescription: 'Bernoulli, binomial, beta-binomial... Why don''t we talk about the beta-Bernoulli distribution?'\nauthor: Paweł Czyż\ndate: 2/19/2024\nexecute:\n  freeze: true\nformat:\n  html:\n    code-fold: true\n---\n\nI have [already demonstrated](almost-binomial-markov-chain.qmd) that I don't know how to properly toss a coin.\nLet's do it again.\n\n## Famous Bernoulli and the company\n\n### Counting the distributions\nBefore we go any further: how many distributions can produce outcomes from the set $\\{0, 1\\}$?\n\nIn other words, we have a measurable space $\\{0, 1\\}$ (where all four subsets are measurable) and we would like to know how many probability measures exist on this space.\nWe have $P(\\varnothing) = 0$ and $P(\\{0, 1\\}) = 1$ straight from the definition of a probability measure.\nAs we need to have $P(\\{0\\}) + P(\\{1\\}) = 1$ we see that there is a bijection between these probability measures and numbers from the set $[0, 1]$, given by $P_b(\\{1\\}) = b$ for any *bias* $b\\in [0, 1]$.\nThis distribution is called $\\mathrm{Bernoulli}(b)$ and it's easy to prove that if $X\\sim \\mathrm{Bernoulli}(b)$, then\n$$\n\\mathbb E[X] = P(X=1) = b.\n$$\n\nHence, the first moment fully determines any distribution on $\\{0, 1\\}$.\n\nThe derivation of variance is quite elegant, once one notices that if $X\\sim \\mathrm{Bernoulli}(b)$, then also $X^2\\sim \\mathrm{Bernoulli}(b)$, because it has to be *some* Bernoulli distribution and $P(X^2=1) = P(X=1)$.\nThen, we have:\n$$\n\\mathbb V[X] = \\mathbb E[X^2] - \\mathbb E[X]^2 = b - b^2 = b(1-b).\n$$\n\nBy considering both cases, we see that for every outcome $x \\in \\{0, 1\\}$, the likelihood is given by\n$$\n\\mathrm{Bernoulli}(x\\mid b) = b^x(1-b)^{1-x}.\n$$\n\n### A few coins\nThis characterization of distributions over $\\{0, 1\\}$ is very powerful.\n\nConsider the following problem: we have $K$ coins with biases $b_1, \\dotsc, b_K$ and we throw a loaded die which can give $K$ different outcomes, each with probability $d_1, \\dotsc, d_K$ (which, of course, sum up to $1$) to decide which coin we will toss.\nWhat is the outcome of this distribution?\nIt is, of course, a coin toss outcome, which is a number from the set $\\{0, 1\\}$.\nHence, this has to be some Bernoulli distribution.\nWhich one?\nBernoulli distributions are fully determined by their expectations, and the expectation in this case is given by a weighted average\n$\\bar b = b_1 d_1 + \\cdots + b_K d_K$.\nIn other words, we can replace a loaded die and $K$ biased coins with just a single biased coin.\n\nTo have more equations, the first procedure corresponds to\n$$\\begin{align*}\nD &\\sim \\mathrm{Categorical}(d_1, \\dotsc, d_K)\\\\\nX \\mid D &\\sim \\mathrm{Bernoulli}(b_D)\n\\end{align*}\n$$\n\nand the second one to\n$$\nY \\sim \\mathrm{Bernoulli}(\\bar b),\n$$\n\nwith $\\bar b = b_1d_1 + \\cdots + b_Kd_K$.\n\nBoth of these distributions are the same, i.e., $\\mathrm{law}\\, X = \\mathrm{law}\\, Y$.\n\nIn particular, the likelihood has to be the same, proving an equality\n$$\n\\sum_{k=1}^K d_k\\, b_k^x(1-b_k)^{1-x} = \\bar b^x(1-\\bar b)^{1-x}\n$$\n\nfor every $x\\in \\{0, 1\\}$.\n\n### Even more coins\nEven more interesting, consider infinitely many coins with different biases, which are chosen according to a beta distribution.\nOnce the coin is chosen, we toss it:\n$$\\begin{align*}\nB &\\sim \\mathrm{Beta}(\\alpha, \\beta)\\\\\nX\\mid B &\\sim \\mathrm{Bernoulli}(B) \n\\end{align*}\n$$\n\nThis is a continuous mixture, which we might call $\\mathrm{BetaBernoulli}(\\alpha, \\beta)$... if it weren't just a plain Bernoulli distribution with bias\n$$\n\\mathbb E[X] = \\mathbb E[\\mathbb E[X\\mid B]] = \\mathbb E[B] = \\frac{\\alpha}{\\alpha + \\beta}.\n$$\n\n### Noisy communication channel\nLet's consider an example involving plain Bernoulli distributions and a noisy communication channel.\n\nLet $X\\sim \\mathrm{Bernoulli}(b)$ be an input variable.\nThe binary output, $Y$, is a noisy version of $X$, with $\\alpha$ controlling the false positive rate and $\\beta$ the false negative rate:\n$$\nP(Y = 1 \\mid X=1) = 1-\\beta, \\quad P(Y=1\\mid X=0) = \\alpha.\n$$\n\nWe can write this model as:\n$$\\begin{align*}\nX &\\sim \\mathrm{Bernoulli}(b)\\\\\nY \\mid X &\\sim \\mathrm{Bernoulli}( X(1-\\beta) + (1-X) \\alpha)\n\\end{align*}\n$$\n\nIn fact, we have [already seen](#finite-mixture) this example: we can treat $X$ as a special case of a loaded dice, indexing a finite mixture with just two components.\nHence, the marginal distribution of $Y$ is\n$$\nY \\sim \\mathrm{Bernoulli}(b(1-\\beta) + (1-b) \\alpha).\n$$\n\n\n### Tossing multiple coins\n\nWe know that a single coin toss characterises all probability distributions on the set $\\{0, 1\\}$.\nHowever, once we consider $N\\ge 2$ coin tosses, yielding outcomes in the set $\\{0, 1, 2, \\dotsc, N\\}$, many *different distributions* will appear.\n\nWe mentioned some of these distributions in [this post](almost-binomial-markov-chain.qmd), but just for completeness, [at the end](#list-of-distributions) there is a list of standard distributions.\n\nSimilarly, if one is interested in modeling binary vectors, which are from the set $\\{0, 1\\}\\times \\{0, 1\\} \\cdots \\{0, 1\\}$, many different distributions will appear.\nLet's analyse an example below.\n\n## Two deceptively similar distributions\n\n### Denoising problem\nWe have a fixed bit $T \\in \\{0, 1\\}$ and we observe its noisy measurements, with false negative rate $\\beta$ and false positive rate $\\alpha$ (recall [this section](#noisy-communication-channel)).\nWe will write $c_0 = \\alpha$ and $c_1 = 1-\\beta$ and put some prior on $T$:\n$$\\begin{align*}\n  \\theta &\\sim \\mathrm{Beta}(\\alpha, \\beta)\\\\\n  T\\mid \\theta &\\sim \\mathrm{Bernoulli}(\\theta)\\\\\n  X_n\\mid T &\\sim \\mathrm{Bernoulli}( c_0(1-T) + c_1T) \\text{ for } n = 1, \\cdots, N\n\\end{align*}\n$$\n\nLet's use shorthand notation $\\mathbf{X} = (X_1, \\dotsc, X_N)$ and note that the likelihood is:\n$$\\begin{align*}\n  P(\\mathbf{X} \\mid T) &= \\prod_{n=1}^N P(X_n \\mid b(T) ) \\\\\n  &= \\prod_{n=1}^N b(T) ^{X_n} \\left(1-b(T)\\right)^{1-X_n} \\\\\n  &= b(T)^S \\left(1-b(T)\\right)^{N-S}\n\\end{align*}\n$$\n\nwhere $b(T) = c_0(1-T) + c_1T$ and $S = X_1 + \\cdots + X_N$.\n\nAfter reading the [discussion above](#famous-bernoulli-and-the-company), there is a natural question: why is $\\theta$ introduced at all?\nFor a simple denoising question, i.e., finding $P(T\\mid \\mathbf{X}) \\propto P(\\mathbf{X} \\mid T) P(T)$, this parameter is not needed at all: $P(T)$ is just a Bernoulli variable, with bias parameter $\\bar \\theta = \\alpha/(\\alpha + \\beta)$.\nThen, \n$$\n  P(T=1\\mid \\mathbf{X}) = \\frac{ c_1^S (1-c_1)^{N-S} \\cdot \\bar \\theta }{ c_1^S(1-c_1)^{N-S} \\cdot \\bar \\theta + c_0^S(1-c_0)^{N-S}\\cdot (1-\\bar \\theta) }. \n$$\n\nLet's quickly implement this formula and see how the posterior looks if we start with an unbiased coin (i.e., $\\bar \\theta=0.5$), observe $S = N - 1$ successes, and we vary the noise level $\\alpha = \\beta = c_0 = 1 - c_1$:\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nfrom typing import Callable, NamedTuple\n\nimport jax\nimport jax.numpy as jnp\n\nimport numpyro\nimport numpyro.distributions as dist\nimport matplotlib.pyplot as plt\nplt.style.use(\"dark_background\")\n\n\ndef _binomial_loglikelihood(n: int, s: int, bias: float) -> float:\n  return s * jnp.log(bias) + (n - s) * jnp.log1p(-bias)\n\n\nclass LikelihoodArgs(NamedTuple):\n  n: int  # Number of throws\n  s: int  # Number of successes, 0 <= s <= n\n  c0: float  # Probability of observing success if T = 0 (false positive rate) \n  c1: float  # Probability of observing success if T = 1 (true positive rate)\n\n\nclass BetaPriorArgs(NamedTuple):\n  alpha: float\n  beta: float\n\n  @property\n  def mean(self) -> float:\n    return self.alpha / (self.alpha + self.beta)\n\n\ndef posterior_t(\n  bias: float,\n  like: LikelihoodArgs,\n) -> float:\n  \"\"\"Calculates P(T | data),\n  where the prior is given by T ~ Bernoulli(bias).\"\"\"\n  n, s, c0, c1 = like.n, like.s, like.c0, like.c1\n  log_0 = _binomial_loglikelihood(n=n, s=s, bias=c0) + jnp.log1p(-bias)\n  log_1 = _binomial_loglikelihood(n=n, s=s, bias=c1) + jnp.log(bias)\n\n  return jax.nn.softmax(jnp.array([log_0, log_1]))\n\nfig, axs = plt.subplots(\n  1, 3, dpi=120, figsize=(8, 2.4), sharex=True, sharey=True,\n)\n\nfor ax in axs:\n  ax.spines[['top', 'right']].set_visible(False)\n  ax.set_xticks([0, 1])\n  ax.set_xlim([-0.5, 1.5])\n  ax.set_ylim([-0.1, 1.1])\n  ax.set_yticks([0, 0.25, 0.5, 0.75, 1])\n\n\nfor ax, n in zip(axs, [1, 3, 5]):\n  ax.set_title(f\"$N = {n}$, $S = {n-1}$\")\n  for i, noise in enumerate([0.01, 0.1, 0.25, 0.45]):\n    like = LikelihoodArgs(\n      n=n,\n      s=n - 1,\n      c0=noise,\n      c1=1-noise,\n    )\n    posterior = posterior_t(bias=0.5, like=like)\n\n    ax.plot([0, 1], posterior, label=f\"{noise:.2f}\", c=f\"C{i}\")\n    ax.scatter([0, 1], posterior, c=f\"C{i}\", s=4)\n  \nax = axs[-1]\nax.legend(frameon=False, bbox_to_anchor=(1.05, 1))\nfig.tight_layout()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n/home/pawel/micromamba/envs/data-science/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\nAn NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](beta-bernoulli_files/figure-html/cell-2-output-2.png){}\n:::\n:::\n\n\nThis formula is elegant: we can use it to solve the denoising problem, i.e., to infer the value of the $T$ variable.\nWhy did we introduce the $\\theta$ variable with its own prior?\nWe may be interested not only in inferring a missing coin toss, but also in learning about the unknown bias $\\theta$ of the coin from this experiment.\n\nFor example, if $T$ were directly observed, we would have $P(\\theta \\mid T) = \\mathrm{Beta}(\\theta \\mid \\alpha + T,  \\beta + (1-T) )$:\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nfig, axs = plt.subplots(1, 3, figsize=(5, 2.3), dpi=200, sharex=True, sharey=True)\n\nfor ax in axs:\n  ax.spines[['top', 'right']].set_visible(False)\n  ax.set_xlim([-0.02, 1.02])\n\nbins = jnp.linspace(0, 1, 31)\n\nkey = jax.random.PRNGKey(1)\nkey, *subkeys = jax.random.split(key, 4)\n\nax = axs[0]\nax.set_title(\"Prior\\nBeta(1, 1)\")\nsamples = dist.Beta(1, 1).sample(subkeys[0], sample_shape=(10_000,))\nax.hist(samples, bins=bins, density=True)\n\nax = axs[1]\nax.set_title(\"Posterior for $T=0$\\nBeta(1, 2)\")\nsamples = dist.Beta(1, 2).sample(subkeys[0], sample_shape=(10_000,))\nax.hist(samples, bins=bins, density=True)\n\nax = axs[2]\nax.set_title(\"Posterior for $T=1$\\nBeta(2, 1)\")\nsamples = dist.Beta(2, 1).sample(subkeys[0], sample_shape=(10_000,))\nax.hist(samples, bins=bins, density=True)\n\nfig.tight_layout()\n```\n\n::: {.cell-output .cell-output-display}\n![](beta-bernoulli_files/figure-html/cell-3-output-1.png){}\n:::\n:::\n\n\nAs $T$ is not directly observed, we have to look at $P(\\theta \\mid \\mathbf{X})$:\n$$\\begin{align*}\n  P(\\theta \\mid \\mathbf{X}) &= \\sum_{t} P(\\theta, T=t \\mid \\mathbf{X}) \\\\\n  &= \\sum_{t} P(\\theta \\mid T=t,  \\mathbf{X})P(T=t\\mid \\mathbf{X})\\\\\n  &= \\sum_{t} P(\\theta \\mid T=t) P(T=t\\mid \\mathbf{X}).\n\\end{align*}\n$$\n\nThis is therefore a mixture of $\\mathrm{Beta}(\\alpha+1, \\beta)$ and $\\mathrm{Beta}(\\alpha, \\beta + 1)$ distributions:\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\n_noise = 0.2\n\ndef create_posterior_theta(\n  prior: BetaPriorArgs,\n  like: LikelihoodArgs,\n):\n  mixing = dist.Categorical(\n    probs=posterior_t(\n      bias=prior.mean, like=like\n  ))\n  return dist.MixtureSameFamily(\n    mixing_distribution=mixing,\n    component_distribution=dist.Beta(\n      # alpha\n      concentration1=jnp.array([prior.alpha, prior.alpha + 1]),\n      # beta\n      concentration0=jnp.array([prior.beta + 1, prior.beta]),\n    )\n  )\n\nfig, axs = plt.subplots(1, 3, figsize=(5, 2.3), dpi=200, sharex=True, sharey=True)\n\nfor ax in axs:\n  ax.spines[['top', 'right']].set_visible(False)\n  ax.set_xlim([-0.02, 1.02])\n\nkey, *subkeys = jax.random.split(key, 4)\n\nax = axs[0]\nax.set_title(\"$N=S=1$\")\nd = create_posterior_theta(\n  prior=BetaPriorArgs(alpha=1.0, beta=1.0),\n  like=LikelihoodArgs(\n    n=1, s=1,\n    c0=_noise, c1=1-_noise,\n  )\n)\nsamples = d.sample(subkeys[0], sample_shape=(10_000,))\nax.hist(samples, bins=bins, density=True)\n\nax = axs[1]\nax.set_title(\"$N=S=2$\")\nd = create_posterior_theta(\n  prior=BetaPriorArgs(alpha=1.0, beta=1.0),\n  like=LikelihoodArgs(\n    n=2, s=2,\n    c0=_noise, c1=1-_noise,\n  )\n)\nsamples = d.sample(subkeys[0], sample_shape=(10_000,))\nax.hist(samples, bins=bins, density=True)\n\nax = axs[2]\nax.set_title(\"$N=S=100$\")\nd = create_posterior_theta(\n  prior=BetaPriorArgs(alpha=1.0, beta=1.0),\n  like=LikelihoodArgs(\n    n=100, s=100,\n    c0=_noise, c1=1-_noise,\n  )\n)\nsamples = d.sample(subkeys[0], sample_shape=(10_000,))\nax.hist(samples, bins=bins, density=True)\n\nfig.tight_layout()\n```\n\n::: {.cell-output .cell-output-display}\n![](beta-bernoulli_files/figure-html/cell-4-output-1.png){}\n:::\n:::\n\n\nImportantly, we see that the posterior on $\\theta$ will not shrink even for large $N$: increasing $N$ affects how well we can determine the outcome of the toss $T$.\nBut even if we observe $T$ perfectly, it's only one toss, so it does not give too much information on the bias $\\theta$.\n\nLet's now look at this problem also from a bit different perspective.\nIf we want to update $\\theta$ directly from the observed data $\\mathbf{X}$, we can marginalise $T$ out in the likelihood:\n$$\\begin{align*}\n  P(\\mathbf{X} \\mid \\theta ) &= \\sum_{t} P(\\mathbf{X} \\mid T=t) P(T=t\\mid \\theta) \\\\\n  &= \\theta P(\\mathbf{X} \\mid T=1) + (1-\\theta) P( \\mathbf{X}\\mid T=0) \\\\\n  &= \\theta c_1^{S}(1-c_1)^{N-S} + (1-\\theta) c_0^S(1-c_0)^{N-S}.\n\\end{align*}\n$$\n\nWe have now only one continuous variable and we could use Hamiltonian Monte Carlo to sample from the distribution $P(\\theta \\mid \\mathbf{X})$.\nWe have already determined it analytically, as a mixture of beta distributions, but let's quickly compare the results:\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\nfrom numpyro.infer import MCMC, NUTS\n\n\ndef create_model(like: LikelihoodArgs, prior: BetaPriorArgs):\n  theta = numpyro.sample(\"theta\", \n    dist.Beta(concentration1=prior.alpha, concentration0=prior.beta)\n  )\n  log1 = _binomial_loglikelihood(n=like.n, s=like.s, bias=like.c1) + jnp.log(theta)\n  log0 = _binomial_loglikelihood(n=like.n, s=like.s, bias=like.c0) + jnp.log1p(-theta)\n  numpyro.factor(\"loglikelihood\", jnp.logaddexp(log1, log0))\n\n\nprior_args = BetaPriorArgs(\n  alpha=1.0,\n  beta=1.0,\n)\n\nlike_args = LikelihoodArgs(\n  n=3,\n  s=2,\n  c0=0.1,\n  c1=0.9,\n)\n\nfig, axs = plt.subplots(1, 3, figsize=(5, 2.3), dpi=200, sharex=True, sharey=True)\nfor ax in axs:\n  ax.spines[['top', 'right']].set_visible(False)\n  ax.set_xlim([-0.02, 1.02])\n\nkey, *subkeys = jax.random.split(key, 4)\n\n\nax = axs[0]\nax.set_title(\"$P(\\\\theta \\\\mid T=1)$\")\nsamples = dist.Beta(2, 1).sample(subkeys[0], sample_shape=(100_000,))\nax.hist(samples, bins=bins, density=True, color=\"C2\")\n\nax = axs[1]\nax.set_title(\"$P(\\\\theta\\\\mid \\\\mathbf{X})$\\n(explicit)\")\nd = create_posterior_theta(\n  like=like_args,\n  prior=prior_args,\n)\nsamples = d.sample(subkeys[1], sample_shape=(100_000,))\nax.hist(samples, bins=bins, density=True)\n\nax = axs[2]\nax.set_title(\"$P(\\\\theta\\\\mid \\\\mathbf{X})$\\n(HMC)\")\n\nnuts_kernel = NUTS(create_model)\nmcmc = MCMC(nuts_kernel, num_warmup=1000, num_samples=10_000, num_chains=2)\nmcmc.run(subkeys[2], like=like_args, prior=prior_args)\nsamples = mcmc.get_samples()[\"theta\"]\nax.hist(samples, bins=bins, density=True)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n/tmp/ipykernel_11590/1934137946.py:51: UserWarning: There are not enough devices to run parallel chains: expected 2 but got 1. Chains will be drawn sequentially. If you are running MCMC in CPU, consider using `numpyro.set_host_device_count(2)` at the beginning of your program. You can double-check how many devices are available in your system using `jax.local_device_count()`.\n  mcmc = MCMC(nuts_kernel, num_warmup=1000, num_samples=10_000, num_chains=2)\n\r  0%|          | 0/11000 [00:00<?, ?it/s]\rwarmup:   0%|          | 1/11000 [00:00<2:48:23,  1.09it/s, 3 steps of size 1.40e+01. acc. prob=0.98]\rwarmup:   6%|▌         | 642/11000 [00:01<00:11, 863.61it/s, 1 steps of size 1.58e+00. acc. prob=0.79]\rsample:  12%|█▏        | 1288/11000 [00:01<00:05, 1765.94it/s, 1 steps of size 9.13e-01. acc. prob=0.91]\rsample:  18%|█▊        | 1948/11000 [00:01<00:03, 2670.27it/s, 1 steps of size 9.13e-01. acc. prob=0.91]\rsample:  24%|██▎       | 2601/11000 [00:01<00:02, 3484.36it/s, 3 steps of size 9.13e-01. acc. prob=0.91]\rsample:  30%|██▉       | 3261/11000 [00:01<00:01, 4205.38it/s, 1 steps of size 9.13e-01. acc. prob=0.92]\rsample:  36%|███▌      | 3911/11000 [00:01<00:01, 4774.65it/s, 3 steps of size 9.13e-01. acc. prob=0.91]\rsample:  42%|████▏     | 4572/11000 [00:01<00:01, 5254.93it/s, 3 steps of size 9.13e-01. acc. prob=0.91]\rsample:  47%|████▋     | 5217/11000 [00:01<00:01, 5580.35it/s, 1 steps of size 9.13e-01. acc. prob=0.91]\rsample:  53%|█████▎    | 5858/11000 [00:01<00:00, 5771.69it/s, 1 steps of size 9.13e-01. acc. prob=0.91]\rsample:  59%|█████▉    | 6499/11000 [00:01<00:00, 5951.85it/s, 1 steps of size 9.13e-01. acc. prob=0.91]\rsample:  65%|██████▌   | 7157/11000 [00:02<00:00, 6131.07it/s, 3 steps of size 9.13e-01. acc. prob=0.91]\rsample:  71%|███████   | 7801/11000 [00:02<00:00, 6191.86it/s, 3 steps of size 9.13e-01. acc. prob=0.91]\rsample:  77%|███████▋  | 8442/11000 [00:02<00:00, 6142.81it/s, 3 steps of size 9.13e-01. acc. prob=0.91]\rsample:  82%|████████▏ | 9072/11000 [00:02<00:00, 6188.51it/s, 3 steps of size 9.13e-01. acc. prob=0.91]\rsample:  88%|████████▊ | 9711/11000 [00:02<00:00, 6247.36it/s, 3 steps of size 9.13e-01. acc. prob=0.91]\rsample:  94%|█████████▍| 10366/11000 [00:02<00:00, 6336.80it/s, 3 steps of size 9.13e-01. acc. prob=0.91]\rsample: 100%|██████████| 11000/11000 [00:02<00:00, 4188.79it/s, 1 steps of size 9.13e-01. acc. prob=0.91]\n\r  0%|          | 0/11000 [00:00<?, ?it/s]\rwarmup:   6%|▌         | 665/11000 [00:00<00:01, 6642.86it/s, 3 steps of size 1.31e+00. acc. prob=0.79]\rsample:  12%|█▏        | 1330/11000 [00:00<00:01, 6536.01it/s, 3 steps of size 1.01e+00. acc. prob=0.89]\rsample:  18%|█▊        | 1984/11000 [00:00<00:01, 6444.17it/s, 1 steps of size 1.01e+00. acc. prob=0.89]\rsample:  24%|██▍       | 2629/11000 [00:00<00:01, 6401.43it/s, 1 steps of size 1.01e+00. acc. prob=0.89]\rsample:  30%|██▉       | 3270/11000 [00:00<00:01, 6373.51it/s, 3 steps of size 1.01e+00. acc. prob=0.89]\rsample:  36%|███▌      | 3917/11000 [00:00<00:01, 6403.57it/s, 3 steps of size 1.01e+00. acc. prob=0.89]\rsample:  42%|████▏     | 4568/11000 [00:00<00:00, 6437.18it/s, 1 steps of size 1.01e+00. acc. prob=0.89]\rsample:  47%|████▋     | 5219/11000 [00:00<00:00, 6457.23it/s, 3 steps of size 1.01e+00. acc. prob=0.89]\rsample:  53%|█████▎    | 5869/11000 [00:00<00:00, 6468.53it/s, 1 steps of size 1.01e+00. acc. prob=0.89]\rsample:  59%|█████▉    | 6516/11000 [00:01<00:00, 6441.67it/s, 3 steps of size 1.01e+00. acc. prob=0.89]\rsample:  65%|██████▌   | 7189/11000 [00:01<00:00, 6529.38it/s, 3 steps of size 1.01e+00. acc. prob=0.89]\rsample:  71%|███████▏  | 7858/11000 [00:01<00:00, 6576.41it/s, 3 steps of size 1.01e+00. acc. prob=0.89]\rsample:  77%|███████▋  | 8522/11000 [00:01<00:00, 6593.16it/s, 3 steps of size 1.01e+00. acc. prob=0.89]\rsample:  84%|████████▎ | 9197/11000 [00:01<00:00, 6638.41it/s, 3 steps of size 1.01e+00. acc. prob=0.89]\rsample:  90%|████████▉ | 9868/11000 [00:01<00:00, 6658.19it/s, 1 steps of size 1.01e+00. acc. prob=0.89]\rsample:  96%|█████████▌| 10540/11000 [00:01<00:00, 6674.45it/s, 3 steps of size 1.01e+00. acc. prob=0.89]\rsample: 100%|██████████| 11000/11000 [00:01<00:00, 6546.98it/s, 3 steps of size 1.01e+00. acc. prob=0.89]\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=4}\n```\n(array([0.24749999, 0.28499999, 0.31799995, 0.39600002, 0.43200003,\n        0.5084998 , 0.55500003, 0.64650004, 0.63000004, 0.77400005,\n        0.81750005, 0.79799933, 0.86250005, 0.86850005, 1.00800096,\n        1.08899909, 1.08749909, 1.2570012 , 1.22699898, 1.25250119,\n        1.26899894, 1.34400128, 1.36049886, 1.4324988 , 1.53300146,\n        1.48199876, 1.48800142, 1.60799866, 1.74450166, 1.6785016 ]),\n array([0.        , 0.03333334, 0.06666667, 0.10000001, 0.13333334,\n        0.16666667, 0.20000002, 0.23333335, 0.26666668, 0.30000001,\n        0.33333334, 0.36666667, 0.40000004, 0.43333337, 0.4666667 ,\n        0.5       , 0.53333336, 0.56666672, 0.60000002, 0.63333338,\n        0.66666669, 0.70000005, 0.73333335, 0.76666671, 0.80000007,\n        0.83333337, 0.86666673, 0.90000004, 0.9333334 , 0.9666667 ,\n        1.        ]),\n <BarContainer object of 30 artists>)\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](beta-bernoulli_files/figure-html/cell-5-output-3.png){}\n:::\n:::\n\n\nHMC agrees well with the mixture of beta distributions. We see that $P(\\theta \\mid \\mathbf{X})$ is slightly more diffuse compared with the case when $T$ is directly observed.\n\n### A bit different model\n\nIn the model above, for any single observation $X_n$ we had\n$$\nP(X_n\\mid \\theta) = \\theta c_1^{X_n}(1-c_1)^{1-X_n} + (1-\\theta) c_0^{X_n}(1-c_0)^{1-X_n}.\n$$\n\nHowever, the joint likelihood was given by\n\n$$\\begin{align*}\nP(\\mathbf{X} \\mid \\theta) &= \\theta \\prod_{n} c_1^{X_n}(1-c_1)^{1-X_n} + (1-\\theta) \\prod_{n} c_0^{X_n}(1-c_0)^{1-X_n} \\\\\n&= \\theta c_1^S (1-c_1)^{N-S} + (1-\\theta)c_0^S(1-c_0)^S,\n\\end{align*}\n$$\n\nwhich is *not* the product of $\\prod_n P(X_n\\mid \\theta)$, because all the variables $X_n$ were noisy observations of a single coin toss outcome $T$.\n\nLet's now consider a deceptively similar model:\n$$\\begin{align*}\n  \\phi &\\sim \\mathrm{Beta}(\\alpha, \\beta)\\\\\n  U_n \\mid \\phi &\\sim \\mathrm{Bernoulli}(\\phi) \\\\\n  Y_n \\mid U_n &\\sim \\mathrm{Bernoulli}(c_0(1-U_n) + c_1U_n)\n\\end{align*}\n$$\n\nIn this case, for each observation $Y_n$, we have *a new coin toss* $U_n$.\n\nWe have $U_n\\sim \\mathrm{Bernoulli}(\\alpha/(\\alpha + \\beta))$, so that $\\mathrm{law}\\, U_n = \\mathrm{law}\\, T$.\nSimilarly, $P(Y_n \\mid \\phi)$ and $P(X_n \\mid \\theta)$ will be very similar:\n$$\\begin{align*}\nP(Y_n \\mid \\phi) &= \\sum_{u} P( Y_n \\mid U_n=u ) P(U_n=u \\mid \\phi) \\\\\n&= \\phi c_1^{Y_n}(1-c_1)^{1-Y_n} + (1-\\phi) c_0^{Y_n} (1-c_1)^{1-Y_n}.\n\\end{align*}\n$$\n\nWe see that for $X_n = Y_n$ and $\\theta = \\phi$ the expressions are exactly the same.\n\nFor $N=1$ there is no real difference between these two models.\nHowever, for $N\\ge 2$ a difference appears, because throws $U_n$ are independent and we have\n$$\nP(\\mathbf{Y} \\mid \\phi) = \\prod_{n} \\left( \\phi c_1^{Y_n}(1-c_1)^{1-Y_n} + (1-\\phi) c_0^{Y_n} (1-c_1)^{1-Y_n} \\right).\n$$\n\nThis is substantially *different* from $P(\\mathbf{X}\\mid \\theta)$.\n\nPerhaps the following perspective is useful: the new model, with variables $U_n$ marginalised out, corresponds to the following:\n\n$$\\begin{align*}\n  \\phi &\\sim \\mathrm{Beta}(\\alpha, \\beta)\\\\\n  Y_n \\mid \\phi &\\sim \\mathrm{Bernoulli}(c_0(1-\\phi) + c_1\\phi)\n\\end{align*}\n$$\n\nwhich is different from the model with a shared latent variable $T$:\n\n$$\\begin{align*}\n  \\theta &\\sim \\mathrm{Beta}(\\alpha, \\beta)\\\\\n  T\\mid \\theta &\\sim \\mathrm{Bernoulli}(\\theta)\\\\\n  X_n \\mid T &\\sim \\mathrm{Bernoulli}(c_0(1-T) + c_1T)\n\\end{align*}\n$$\n\nHence, although the likelihood functions agree for any *single observation*, i.e., for every $x\\in \\{0, 1\\}$, the likelihood functions $P(X_n=x\\mid \\theta)$ and $P(Y_n=x\\mid \\phi)$ are the same, the likelihood functions constructed using *all* observed variables, $P(\\mathbf{X}\\mid \\theta)$ and $P(\\mathbf{Y}\\mid \\phi)$, are usually *different*.\n\nAlso, the posteriors $P(\\theta \\mid  \\mathbf{X})$ and $P(\\phi \\mid \\mathbf{Y})$ do differ: $\\phi$ treats each outcome $Y_n$ independently, so that the posterior can shrink quickly if $N$ is large. \n\nCompare this with the posterior on $\\theta$, which assumes that all $X_n$ are noisy versions of a single throw $T$, so it knows that there is little information about $\\theta$ even if $N$ is large: the posterior will always be a mixture of $\\mathrm{Beta}(\\alpha+1, \\beta)$ and $\\mathrm{Beta}(\\alpha, \\beta+1)$.\nIn particular, if $\\alpha = \\beta = 1$, the posterior on $\\theta$ will still be very diffuse.\n\nFor example, consider $T = 1$.\nWe toss the coin $N = 10$ times, but due to the noise $c_0 = 1 - c_1 = 0.1$ we observed $S=8$.\nWe have the following $P(T=1\\mid \\mathbf{X})$:\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\nprior = BetaPriorArgs(1.0, 1.0)\nlike = LikelihoodArgs(\n  n=10,\n  s=8,\n  c0=0.1,\n  c1=0.9,\n)\n\nposterior_t_value = posterior_t(\n  bias=prior.mean,\n  like=like,\n)\n\nprint(f\"P(T=1 | X) = {posterior_t_value[1]:.7f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nP(T=1 | X) = 0.9999981\n```\n:::\n:::\n\n\nHow would the posteriors on $\\theta$ and $\\phi$ look like? We will plot the above value as a dashed line:\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\ndef new_model(prior: BetaPriorArgs, like: LikelihoodArgs):\n  phi = numpyro.sample(\"phi\", dist.Beta(prior.alpha, prior.beta))\n\n  bias = like.c0 * (1 - phi) + like.c1 * phi\n  numpyro.factor(\"loglikelihood\",\n    _binomial_loglikelihood(n=like.n, s=like.s, bias=bias)\n  )\n\n\nfig, axs = plt.subplots(1, 3, figsize=(5, 2.3), dpi=200, sharex=True, sharey=True)\nfor ax in axs:\n  ax.spines[['top', 'right']].set_visible(False)\n  ax.set_xlim([-0.02, 1.02])\n  ax.axvline(posterior_t_value[1], c=\"white\", linestyle=\"--\", linewidth=2)\n\nkey, *subkeys = jax.random.split(key, 4)\n\nax = axs[0]\nax.set_title(\"$P(\\\\theta \\\\mid T=1)$\")\nsamples = dist.Beta(2, 1).sample(subkeys[0], sample_shape=(100_000,))\nax.hist(samples, bins=bins, density=True, color=\"C2\")\n\nax = axs[1]\nax.set_title(\"$P(\\\\theta\\\\mid \\\\mathbf{X}=\\\\mathbf{x})$\")\nd = create_posterior_theta(\n  like=like_args,\n  prior=prior_args,\n)\nsamples = d.sample(subkeys[1], sample_shape=(100_000,))\nax.hist(samples, bins=bins, density=True)\n\nax = axs[2]\nax.set_title(\"$P(\\\\phi\\\\mid \\\\mathbf{Y}=\\\\mathbf{x})$\")\n\nnuts_kernel = NUTS(new_model)\nmcmc = MCMC(nuts_kernel, num_warmup=1000, num_samples=5_000, num_chains=2)\nmcmc.run(subkeys[2], like=like, prior=prior)\nsamples = mcmc.get_samples()[\"phi\"]\nax.hist(samples, bins=bins, density=True, color=\"C4\")\n\nfig.tight_layout()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n/tmp/ipykernel_11590/3633385242.py:36: UserWarning: There are not enough devices to run parallel chains: expected 2 but got 1. Chains will be drawn sequentially. If you are running MCMC in CPU, consider using `numpyro.set_host_device_count(2)` at the beginning of your program. You can double-check how many devices are available in your system using `jax.local_device_count()`.\n  mcmc = MCMC(nuts_kernel, num_warmup=1000, num_samples=5_000, num_chains=2)\n\r  0%|          | 0/6000 [00:00<?, ?it/s]\rwarmup:   0%|          | 1/6000 [00:00<1:28:40,  1.13it/s, 3 steps of size 8.10e+00. acc. prob=0.68]\rwarmup:  11%|█         | 655/6000 [00:00<00:05, 908.21it/s, 3 steps of size 1.76e+00. acc. prob=0.79]\rsample:  22%|██▏       | 1312/6000 [00:01<00:02, 1844.48it/s, 7 steps of size 7.26e-01. acc. prob=0.93]\rsample:  33%|███▎      | 1972/6000 [00:01<00:01, 2753.18it/s, 3 steps of size 7.26e-01. acc. prob=0.92]\rsample:  44%|████▍     | 2632/6000 [00:01<00:00, 3577.19it/s, 3 steps of size 7.26e-01. acc. prob=0.92]\rsample:  55%|█████▍    | 3290/6000 [00:01<00:00, 4280.01it/s, 3 steps of size 7.26e-01. acc. prob=0.92]\rsample:  66%|██████▌   | 3951/6000 [00:01<00:00, 4863.02it/s, 3 steps of size 7.26e-01. acc. prob=0.92]\rsample:  77%|███████▋  | 4603/6000 [00:01<00:00, 5298.99it/s, 3 steps of size 7.26e-01. acc. prob=0.92]\rsample:  88%|████████▊ | 5264/6000 [00:01<00:00, 5655.87it/s, 3 steps of size 7.26e-01. acc. prob=0.92]\rsample:  99%|█████████▉| 5930/6000 [00:01<00:00, 5936.81it/s, 3 steps of size 7.26e-01. acc. prob=0.92]\rsample: 100%|██████████| 6000/6000 [00:01<00:00, 3335.62it/s, 3 steps of size 7.26e-01. acc. prob=0.92]\n\r  0%|          | 0/6000 [00:00<?, ?it/s]\rwarmup:  11%|█▏        | 677/6000 [00:00<00:00, 6768.72it/s, 1 steps of size 5.60e-01. acc. prob=0.79]\rsample:  23%|██▎       | 1354/6000 [00:00<00:00, 6768.16it/s, 3 steps of size 7.83e-01. acc. prob=0.92]\rsample:  34%|███▍      | 2034/6000 [00:00<00:00, 6781.42it/s, 1 steps of size 7.83e-01. acc. prob=0.92]\rsample:  45%|████▌     | 2714/6000 [00:00<00:00, 6787.05it/s, 7 steps of size 7.83e-01. acc. prob=0.92]\rsample:  57%|█████▋    | 3393/6000 [00:00<00:00, 6778.94it/s, 3 steps of size 7.83e-01. acc. prob=0.92]\rsample:  68%|██████▊   | 4072/6000 [00:00<00:00, 6782.54it/s, 3 steps of size 7.83e-01. acc. prob=0.92]\rsample:  79%|███████▉  | 4752/6000 [00:00<00:00, 6786.99it/s, 7 steps of size 7.83e-01. acc. prob=0.92]\rsample:  91%|█████████ | 5431/6000 [00:00<00:00, 6787.16it/s, 1 steps of size 7.83e-01. acc. prob=0.92]\rsample: 100%|██████████| 6000/6000 [00:00<00:00, 6781.78it/s, 1 steps of size 7.83e-01. acc. prob=0.92]\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](beta-bernoulli_files/figure-html/cell-7-output-2.png){}\n:::\n:::\n\n\nAs we expected, the posterior on $\\phi$ is much more precise than posterior on $\\theta$.\nIt also is shifted towards the value $T=1$, so (in this case) it behaves as sort of an approximation to $T$.\n\n## Which model is better for denoising?\nBoth models actually answer different questions: the first model tries to estimate $T$, a single coin toss, and slightly updates the information about this coin bias, $\\theta$.\n\nThe second model assumes independent coin tosses, where the bias is controlled by $\\phi$.\nAs such, it can quickly shrink the posterior on $\\phi$.\nMoreover, it can be used to answer the question to impute individual coin tosses, $P(\\mathbf{U} \\mid \\mathbf{Y})$.\n\nLet's think what could happen if we fitted each model to the data generated from the other one: this is working with misspecified models (in the $\\mathcal M$-complete setting).\n\nConsider a setting where we have a lot of data, $N\\gg 1$ and the false positive and false negative rates are small, with $c_0 \\ll c_1$. If the data come from the second model, with individual variables $U_n\\sim \\mathrm{Bernoulli}(\\phi)$, and we have $Y_n\\approx U_n$, then the posterior on $T$ will have most mass on the maximum likelihood solution: either $0$ (which should happen for $\\phi \\ll 0.5$) or $1$ (for $\\phi \\gg 0.5$).\nThis model will be underfitting and the predictive distribution from this model will be quite bad: new $Y_{N+1}$ would again be an approximation to $U_{N+1}$, which is sampled from $\\mathrm{Bernoulli}(\\phi)$, but the model would just return a noisy version of the inferred $T$.\n\nOn the other hand, if we have a lot of data from the first model (with a single $T$ variable) and we fit the second model, the posterior on $\\phi$ may have most of the mass near $0$ or $1$, depending on the true value of $T$.\nHence, although $U_n\\sim \\mathrm{Bernoulli}(\\phi)$ are sampled independently, once $\\phi$ is near the true value of $T$ ($0$ or $1$), they can all be approximately equal to $T$.\n\nSo, when there is a lot of data, noting where most of the mass of $\\phi$ lies can be a good approximation to the maximum likelihood of $T$.\n\nOf course, these $N\\gg 1$ settings only tell what happens when we have a lot of data and we didn't discuss the uncertainty: can we use $\\phi$ to get well-calibrated uncertainty on $T$?\n\nI generally expect that the $\\phi$ model can be a bit better in terms of handling slight misspecification, but doing inference directly on $T$ will provide better results in terms of uncertainty quantification for small $N$.\nBut this is just a hypothesis: extensive simulations are not for today.\n\n\n## Appendix\n\n### List of distributions\n\n#### Binomial distribution\nThe simplest choice: we have a coin with bias $b$ and we toss it $N$ times:\n$$\\begin{align*}\n  X_n &\\sim \\mathrm{Bernoulli}(b) \\text{ for } n = 1, \\dotsc, N\\\\\n  S &= X_1 + \\cdots + X_N\n\\end{align*}\n$$\n\nThen, we have $S \\sim \\mathrm{Binomial}(N, b)$.\nAs the individual throws are independent, it's easy to prove that\n$$\n\\mathbb E[S] = Nb, \\quad \\mathbb V[S] = Nb(1-b).\n$$\n\n#### Finite mixture of binomial distributions\nAs [above](#a-few-coins), consider $K$ coins with biases $b_1, \\dotsc, b_K$ and a dice used to choose the coin which will be tossed.\nThis is a finite mixture of binomial distributions:\n$$\\begin{align*}\n  D &\\sim \\mathrm{Categorical}(d_1, \\dotsc, d_K)\\\\\n  S \\mid D &\\sim \\mathrm{Binomial}(N, b_D)\n\\end{align*}\n$$\n\nIn this case the expectation is exactly what one can expect:\n$$\n\\mathbb E[S] = \\sum_{k=1}^K d_k \\cdot Nb_k = N\\bar b,\n$$\nwhere $\\bar b = d_1b_1 + \\cdots + d_Kb_K$.\n\nHowever, the formula for variance is more complex: from [the law of total variance](https://en.wikipedia.org/wiki/Law_of_total_variance):\n\n$$\\begin{align*}\n\\mathbb V[S] &= \\mathbb E[\\mathbb V[S\\mid B]] + \\mathbb V[ \\mathbb E[S\\mid B] ] \\\\\n&= \\sum_{k=1}^K d_k N b_k(1-b_k) + \\mathbb V[NB(1-B)]\n\\end{align*}\n$$\n\n#### Beta-binomial distribution\n\nSimilarly as [here](#even-more-coins), we can consider an infinite collection of coins, chosen from the beta distribution. Once we pick a coin, we toss it $N$ times:\n\n$$\\begin{align*}\n  B &\\sim \\mathrm{Beta}(\\alpha, \\beta)\\\\\n  S \\mid B &\\sim \\mathrm{Binomial}(N, B)\n\\end{align*}\n$$\n\nThe marginal distribution is called the [beta-binomial distribution](https://en.wikipedia.org/wiki/Beta-binomial_distribution):\n$$\nS \\sim \\mathrm{BetaBinomial}(N, \\alpha, \\beta).\n$$\n\nIt's easy to prove that\n$$\n\\mathbb E[S] = N \\frac{\\alpha}{\\alpha + \\beta},\n$$\nbut I don't know an easy derivation of the formula for the variance:\n$$\n\\mathbb V[S] = Nb(1-b)\\cdot (1 + (N-1)\\rho),\n$$\nwhere\n$b=\\alpha/(\\alpha + \\beta)$ and $\\rho=1/(1 + \\alpha + \\beta)$.\n\nHence, choosing the coin first incurs additional variance (compared to the binomial distribution).\n\n#### Poisson binomial distribution\n\nThat was quite a few examples. Let's do one more: the [Poisson binomial distribution](https://en.wikipedia.org/wiki/Poisson_binomial_distribution), because it is fun.\n\nIn this case one has $N$ coins with biases $b_1, \\dotsc, b_N$ and tosses each of them exactly once:\n$$\\begin{align*}\n  X_n &\\sim \\mathrm{Bernoulli}(b_n) \\text{ for } n=1, \\dotsc, N\\\\\n  S &= X_1 + \\cdots + X_N.\n\\end{align*}\n$$\n\nWe see that if all biases are equal, this reduces to the binomial distribution. However, this one is more flexible, as the expectation and variance are given now by\n$$\n  \\mathbb E[S] = \\sum_{n=1}^N b_n, \\quad \\mathbb  V[S] = \\sum_{n=1}^N b_n(1-b_n).\n$$\n\n### Beta-Bernoulli sparsity magic\n\nConsider the following prior on coefficients in a linear model:\n$$\\begin{align*}\n  \\gamma &\\sim \\mathrm{Beta}(\\alpha, \\beta)\\\\\n  \\theta_k \\mid \\gamma &\\sim \\gamma\\, Q_0 + (1-\\gamma)\\, Q_1 \\text{ for } k = 1, \\dotsc, K\n\\end{align*}\n$$\n\nwhere $Q_1$ is e.g., a $\\mathrm{Normal}(0, 10^2)$ distribution corresponding to \"slab\" component and $Q_0$, e.g., $\\mathrm{Normal}\\left(0, 0.01^2\\right)$ is the \"spike\" component.\n\nIntuitively, we expect that fraction $\\gamma$ of the parameters will be shrunk to $0$ by the spike component $Q_0$ and the rest (the $1-\\gamma$ fraction) of the parameters will actually be used to predict values.\n\nMichael Betancourt wrote an [amazing tutorial](https://betanalpha.github.io/assets/case_studies/modeling_sparsity.html) in which he introduces local latent variables, $\\lambda_k$, that control whether $\\theta_k$ should be shrunk or not:\n\n$$\\begin{align*}\n  \\lambda_k &\\sim \\mathrm{Beta}(\\alpha, \\beta) \\text{ for } k = 1, \\dotsc, K\\\\\n  \\theta_k \\mid \\lambda_k &\\sim \\lambda_k \\, Q_0 + (1-\\lambda_k)\\, Q_1 \\text{ for } k = 1, \\dotsc, K.\n\\end{align*}\n$$\n\nUsing small letters for PDFs, we can marginalize variables $\\lambda_k$ as follows:\n$$\n  p(\\mathbf{\\theta}) = \\prod_{k=1}^K p(\\theta_k) = \\prod_{k=1}^K \\left( \\int p(\\theta_k \\mid \\lambda_k) \\, \\mathrm{d}P(\\lambda_k) \\right)\n$$\nand\n$$\\begin{align*}\n  p(\\theta_k) &= \\int p(\\theta_k \\mid \\lambda_k) \\, \\mathrm{d}P(\\lambda_k) \\\\\n  &= q_0(\\theta_k) \\int \\lambda_k\\, \\mathrm{Beta}(\\lambda_k \\mid \\alpha, \\beta) \\, \\mathrm{d}\\lambda_k + q_1(\\theta_k) \\int (1-\\lambda_k)\\, \\mathrm{Beta}(\\lambda_k \\mid \\alpha, \\beta)\\, \\mathrm{d} \\lambda_k \\\\\n  &= q_0(\\theta_k) \\frac{\\alpha}{\\alpha + \\beta} + q_1(\\theta_k) \\left( 1 - \\frac{\\alpha}{\\alpha + \\beta} \\right),\n\\end{align*}\n$$\nso that\n$$\n  \\theta_k \\sim \\gamma\\, Q_0 + (1-\\gamma)\\, Q_1,\n$$\n\nwhere $\\gamma = \\alpha / (\\alpha + \\beta)$.\n\nBeta-Bernoulli distribution offers the following perspective: draw latent indicator variables $T_k \\mid \\lambda_k \\sim \\mathrm{Bernoulli}(\\lambda_k)$, so that $\\theta_k \\mid T_k \\sim T_k\\, Q_0 + (1-T_k) \\, Q_1$.\n\nWe recognize that $T_k \\sim \\mathrm{BetaBernoulli}(\\alpha, \\beta)$ which is just $\\mathrm{Bernoulli}(\\gamma)$ for $\\gamma =\\alpha/(\\alpha+\\beta)$. By integrating out $T_k$ variables (which is just trivial summation!), we have\n$$\n  \\theta_k \\sim \\gamma\\, Q_0 + (1-\\gamma)\\, Q_1.\n$$\n\n",
    "supporting": [
      "beta-bernoulli_files"
    ],
    "filters": [],
    "includes": {}
  }
}