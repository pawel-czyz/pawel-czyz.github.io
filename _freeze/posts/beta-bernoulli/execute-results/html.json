{
  "hash": "b12676e43a15b56d66a6244f7721fc93",
  "result": {
    "markdown": "---\ntitle: Beta-Bernoulli distribution\ndescription: 'Bernoulli, binomial, beta-binomial... Why don''t we talk about the beta-Bernoulli distribution?'\nauthor: Paweł Czyż\ndate: 2/19/2024\nexecute:\n  freeze: true\nformat:\n  html:\n    code-fold: true\n---\n\nI have [already demonstrated](almost-binomial-markov-chain.qmd) that I don't know how to properly toss a coin. Let's do that again.\n\n## Famous Bernoulli and the company\n\n### Counting the distributions\nBefore we go any further: how many distributions can give outcomes from the set $\\{0, 1\\}$?\n\nIn other words, we have a measurable space $\\{0, 1\\}$ (such that all four subsets are measurable) and we would like to know how many probability measures exist on this space.\nWe have $P(\\varnothing) = 0$ and $P(\\{0, 1\\}) = 1$ straight from the definition of a probability measure.\nAs we need to have $P(\\{0\\}) + P(\\{1\\}) = 1$ we see that that there is a bijection between these probability measures and numbers from the set $[0, 1]$, given by $P_b(\\{1\\}) = b$ for any *bias* $b\\in [0, 1]$.\nThis distribution is called $\\mathrm{Bernoulli}(b)$ and it's easy to prove that if $X\\sim \\mathrm{Bernoulli}(b)$, then\n$$\n\\mathbb E[X] = P(X=1) = b.\n$$\n\nHence, the first moment fully determines any distribution on $\\{0, 1\\}$.\n\nDerivation of variance is quite elegant, once one notices that if $X\\sim \\mathrm{Bernoulli}(b)$, then also $X^2\\sim \\mathrm{Bernoulli}(b)$, because it has to be *some* Bernoulli and $P(X^2=1) = P(X=1)$.\nThen:\n$$\n\\mathbb V[X] = \\mathbb E[X^2] - \\mathbb E[X]^2 = b - b^2 = b(1-b).\n$$\n\nBy considering both cases, we see that for every outcome $x\\in \\{0, 1\\}$, the likelihood is given by\n$$\n\\mathrm{Bernoulli}(x\\mid b) = b^x(1-b)^{1-x}.\n$$\n\n### A few coins\nThis characterization of distributions over $\\{0, 1\\}$ is very powerful.\n\nConsider the following problem: we have $K$ coins with biases $b_1, \\dotsc, b_K$ and we throw a loaded dice which can give $K$ different outcomes, each with probability $d_1, \\dotsc, d_K$ (which, of course, sum up to $1$) to decide which coin we will toss.\nWhat is the outcome of this distribution? It's, of course, a coin toss outcome, that is a number from the set $\\{0, 1\\}$.\nHence, this has to be some Bernoulli distribution.\nWhich one? Bernoulli distributions are fully determined by their expectations and the expectation in this case is given by a weighted average\n$\\bar b = b_1 d_1 + \\cdots + b_K d_K$.\nIn other words, we can replace a loaded dice and $K$ biased coins with just a single biased coin.\n\nTo have more equations, the first procedure corresponds to\n$$\\begin{align*}\nD &\\sim \\mathrm{Categorical}(d_1, \\dotsc, d_K)\\\\\nX \\mid D &\\sim \\mathrm{Bernoulli}(b_D)\n\\end{align*}$$\nand the second one to\n$$\nY \\sim \\mathrm{Bernoulli}(\\bar b),\n$$\nwith $\\bar b = b_1d_1 + \\cdots + b_Kd_K$.\n\nBoth of these distributions are the same, i.e., $\\mathrm{law}\\, X = \\mathrm{law}\\, Y$.\n\nIn particular, the likelihood has to be the same, proving an equality\n$$\n\\sum_{k=1}^K d_k\\, b_k^x(1-b_k)^{1-x} = \\bar b^x(1-\\bar b)^{1-x}\n$$\nfor every $x\\in \\{0, 1\\}$.\n\n### Even more coins\nEven more interestingly, consider infinitely many coins with different biases, which are chosen according to the beta distribution. Once the coin is chosen, we toss it:\n$$\\begin{align*}\nB &\\sim \\mathrm{Beta}(\\alpha, \\beta)\\\\\nX\\mid B &\\sim \\mathrm{Bernoulli}(B) \n\\end{align*}\n$$\n\nThis is a continuous mixture, which we could perhaps call $\\mathrm{BetaBernoulli}(\\alpha, \\beta)$... if it wasn't just a plain Bernoulli distribution with bias\n$$\n\\mathbb E[X] = \\mathbb E[\\mathbb E[X\\mid B]] = \\mathbb E[B] = \\frac{\\alpha}{\\alpha + \\beta}.\n$$\n\n### Noisy communication channel\nLet's consider an example involving plain Bernoulli distributions and a noisy communication channel.\n\nLet $X\\sim \\mathrm{Bernoulli}(b)$ be an input variable.\nThe binary output, $Y$, is a noisy version of $X$, with $\\alpha$ controlling the false positive rate and $\\beta$ the false negative rate:\n$$\nP(Y = 1 \\mid X=1) = 1-\\beta, \\quad P(Y=1\\mid X=0) = \\alpha.\n$$\n\nWe can write this model as:\n$$\\begin{align*}\nX &\\sim \\mathrm{Bernoulli}(b)\\\\\nY \\mid X &\\sim \\mathrm{Bernoulli}( X(1-\\beta) + (1-X) \\alpha)\n\\end{align*}\n$$\n\nIn fact, we have [already seen](#finite-mixture) this example: we can treat $X$ as a special case of a loaded dice, indexing a finite mixture with just two components.\nHence, the marginal distribution of $Y$ is\n$$\nY \\sim \\mathrm{Bernoulli}(b(1-\\beta) + (1-b) \\alpha).\n$$\n\n\n### Tossing multiple coins\n\nWe know that a single coin toss characterises all probability distributions on the set $\\{0, 1\\}$.\nHowever, once we consider $N\\ge 2$ coin tosses, yielding outcomes in the set $\\{0, 1, 2, \\dotsc, N\\}$, many *different distributions* will appear.\n\nWe mentioned some of these distributions in [this post](almost-binomial-markov-chain.qmd), but just for completeness [at the end](#list-of-distributions) there is a list of standard distributions.\n\nSimilarly, if one is interested in modelling binary vectors, which are from the set $\\{0, 1\\}\\times \\{0, 1\\} \\cdots \\{0, 1\\}$, many different distributions will appear.\nLet's analyse an example below.\n\n## Two deceptively similar distributions\n\n### Denoising problem\nWe have a fixed bit $T \\in \\{0, 1\\}$ and we observe its noisy measurements, with false negatives rate $\\beta$ and false positive rate $\\alpha$ (recall [this section](#noisy-communication-channel)).\nWe will write $c_0 = \\alpha$ and $c_1 = 1-\\beta$ and put some prior on $T$:\n$$\\begin{align*}\n  \\theta &\\sim \\mathrm{Beta}(\\alpha, \\beta)\\\\\n  T\\mid \\theta &\\sim \\mathrm{Bernoulli}(\\theta)\\\\\n  X_n\\mid T &\\sim \\mathrm{Bernoulli}( c_0(1-T) + c_1T) \\text{ for } n = 1, \\cdots, N\n\\end{align*}\n$$\n\nLet's use shorthand notation $\\mathbf{X} = (X_1, \\dotsc, X_N)$ and note that the likelihood is:\n$$\\begin{align*}\n  P(\\mathbf{X} \\mid T) &= \\prod_{n=1}^N P(X_n \\mid b(T) ) \\\\\n  &= \\prod_{n=1}^N b(T) ^{X_n} \\left(1-b(T)\\right)^{1-X_n} \\\\\n  &= b(T)^S \\left(1-b(T)\\right)^{N-S}\n\\end{align*}\n$$\n\nwhere $b(T) = c_0(1-T) + c_1T$ and $S = X_1 + \\cdots + X_N$.\n\nAfter reading the [discussion above](#famous-bernoulli-and-the-company), there is a natural question: why is $\\theta$ introduced at all?\nFor a simple denoising question, i.e., finding $P(T\\mid \\mathbf{X}) \\propto P(\\mathbf{X} \\mid T) P(T)$ this parameter is not needed at all: $P(T)$ is just a Bernoulli variable, with bias parameter $\\bar \\theta = \\alpha/(\\alpha + \\beta)$.\nThen, \n$$\n  P(T=1\\mid \\mathbf{X}) = \\frac{ c_1^S (1-c_1)^{N-S} \\cdot \\bar \\theta }{ c_1^S(1-c_1)^{N-S} \\cdot \\bar \\theta + c_0^S(1-c_0)^{N-S}\\cdot (1-\\bar \\theta) }. \n$$\n\nThis is, of course, simple and correct. The reason for introducing $\\theta$ is that we are not only interested in inferring a missing coin throw, but also in learning about the unknown bias $\\theta$ of the coin from this experiment.\nFor example, if $T$ was directly observed, we would have $P(\\theta \\mid T) = \\mathrm{Beta}(\\theta \\mid \\alpha + T,  \\beta + (1-T) )$.\n\nAs $T$ is not directly observed, we have to look at $P(\\theta \\mid \\mathbf{X})$:\n$$\\begin{align*}\n  P(\\theta \\mid \\mathbf{X}) &= \\sum_{t} P(\\theta, T=t \\mid \\mathbf{X}) \\\\\n  &= \\sum_{t} P(\\theta \\mid T=t,  \\mathbf{X})P(T=t\\mid \\mathbf{X})\\\\\n  &= \\sum_{t} P(\\theta \\mid T=t) P(T=t\\mid \\mathbf{X}),\n\\end{align*}\n$$\nwhich is therefore a mixture of $\\mathrm{Beta}(\\alpha+1, \\beta)$ and $\\mathrm{Beta}(\\alpha, \\beta + 1)$ distributions.\n\nBut let's look at it from a bit different perspective.\nLet's marginalise $T$ out and formulate likelihood in terms of $\\theta$:\n$$\\begin{align*}\n  P(\\mathbf{X} \\mid \\theta ) &= \\sum_{t} P(\\mathbf{X} \\mid T=t) P(T=t\\mid \\theta) \\\\\n  &= \\theta P(\\mathbf{X} \\mid T=0) + (1-\\theta) P( \\mathbf{X}\\mid T=1) \\\\\n  &= \\theta c_1^{S}(1-c_1)^{N-S} + (1-\\theta) c_0^S(1-c_0)^{N-S}.\n\\end{align*}\n$$\n\nWe have now only one continuous variable and we could use Hamiltonian Monte Carlo to sample from the distribution $P(\\theta \\mid \\mathbf{X})$.\n(But we have already determined it analytically.)\n\n### A bit different model\n\nIn the model above for any single observation $X_n$ we had\n$$\nP(X_n\\mid \\theta) = \\theta c_1^{X_n}(1-c_1)^{1-X_n} + (1-\\theta) c_0^{X_n}(1-c_0)^{1-X_n}.\n$$\n\nHowever, the joint likelihood was given by\n\n$$\\begin{align*}\nP(\\mathbf{X} \\mid \\theta) &= \\theta \\prod_{n} c_1^{X_n}(1-c_1)^{1-X_n} + (1-\\theta) \\prod_{n} c_0^{X_n}(1-c_0)^{1-X_n} \\\\\n&= \\theta c_1^S (1-c_1)^{N-S} + (1-\\theta)c_0^S(1-c_0)^S,\n\\end{align*}\n$$\n\nwhich is *not* the product of $\\prod_n P(X_n\\mid \\theta)$, because all variables $X_n$ were noisy observations of a single coin toss outcome $T$.\n\nLet's now consider a deceptively similar model:\n$$\\begin{align*}\n  \\phi &\\sim \\mathrm{Beta}(\\alpha, \\beta)\\\\\n  U_n \\mid \\phi &\\sim \\mathrm{Bernoulli}(\\phi) \\\\\n  Y_n \\mid U_n &\\sim \\mathrm{Bernoulli}(c_0(1-U_n) + c_1U_n)\n\\end{align*}\n$$\n\nIn this case for each observation $Y_n$ we have *a new coin toss* $U_n$.\n\nWe have $U_n\\sim \\mathrm{Bernoulli}(\\alpha/(\\alpha + \\beta))$, so that $\\mathrm{law}\\, U_n = \\mathrm{law}\\, T$.\nSimilarly, $P(Y_n \\mid \\phi)$ and $P(X_n \\mid \\theta)$ will be very similar:\n$$\\begin{align*}\nP(Y_n \\mid \\phi) &= \\sum_{u} P( Y_n \\mid U_n=u ) P(U_n=u \\mid \\phi) \\\\\n&= \\phi c_1^{Y_n}(1-c_1)^{1-Y_n} + (1-\\phi) c_0^{Y_n} (1-c_1)^{1-Y_n}.\n\\end{align*}\n$$\n\nWe see that for $X_n = Y_n$ and $\\theta = \\phi$ the expressions are exactly the same.\n\nFor $N=1$ there is no real difference between these two models.\nHowever, for $N\\ge 2$ a difference appears, because throws $U_n$ are independent and we have\n$$\nP(\\mathbf{Y} \\mid \\phi) = \\prod_{n} \\left( \\phi c_1^{Y_n}(1-c_1)^{1-Y_n} + (1-\\phi) c_0^{Y_n} (1-c_1)^{1-Y_n} \\right).\n$$\n\nThis is substantially *different* from $P(\\mathbf{X}\\mid \\theta)$.\n\nPerhaps the following perspective is useful: the new model, with variables $U_n$ marginalised out, corresponds to the following:\n\n$$\\begin{align*}\n  \\phi &\\sim \\mathrm{Beta}(\\alpha, \\beta)\\\\\n  Y_n \\mid \\phi &\\sim \\mathrm{Bernoulli}(c_0(1-\\phi) + c_1\\phi)\n\\end{align*}\n$$\n\nwhich is different from the model with a shared latent variable $T$:\n\n$$\\begin{align*}\n  \\theta &\\sim \\mathrm{Beta}(\\alpha, \\beta)\\\\\n  T\\mid \\theta &\\sim \\mathrm{Bernoulli}(\\theta)\\\\\n  Y_n \\mid T &\\sim \\mathrm{Bernoulli}(c_0(1-T) + c_1T)\n\\end{align*}\n$$\n\nHence, although the likelihood functions agree for any single observation, i.e., $P(X_n\\mid \\theta)$ and $P(Y_n\\mid \\phi)$ are \"the same\" (up to relabeling), the likelihood functions constructed using all observed variables, $P(\\mathbf{X}\\mid \\theta)$ and $P(\\mathbf{Y}\\mid \\phi)$, are usually very different.\n\nAlso, the posteriors $P(\\theta \\mid  \\mathbf{X})$ and $P(\\phi \\mid \\mathbf{Y})$ will differ: $\\phi$ treats each outcome $Y_n$ independently, so that the posterior can shrink quickly if $N$ is large. \n\nThe posterior on $\\theta$ assumes that all $X_n$ are noisy versions of a single throw $T$, so it knows that there's not that much information about $\\theta$ even if $N$ is very large: the posterior will always be a mixture of $\\mathrm{Beta}(\\alpha+1, \\beta)$ and $\\mathrm{Beta}(\\alpha, \\beta+1)$.\nIn particular, if $\\alpha = \\beta = 1$ the posterior on $\\theta$ will still be very diffuse.\n\n## Which model is better for denoising?\nBoth models actually answer different questions: the first model tries to estimate $T$, a single coin toss, and slightly updates the information about this coin bias, $\\theta$.\n\nThe second model assumes independent coin tosses, where the bias is controlled by $\\phi$.\nAs such, it can quickly shrink the posterior on $\\phi$.\nMoreover, it can be used to answer the question to impute individual coin tosses $P(\\mathbf{U} \\mid \\mathbf{Y})$.\n\nLet's think what could happen if we fitted each model to the data generated from the other one: this is working with misspecified models (in the $\\mathcal M$-complete setting).\n\nConsider a setting where we have a lot of data, $N\\gg 1$ and the false positive and false negative rates are small, with $c_0 \\ll c_1$. If the data come from the second model, with individual variables $U_n\\sim \\mathrm{Bernoulli}(\\phi)$, and we have $Y_n\\approx U_n$, then the posterior on $T$ will have most mass on the maximum likelihood solution: either $0$ (which should happen for $\\phi \\ll 0.5$) or $1$ (for $\\phi \\gg 0.5$).\nThis model will be underfitting and the predictive distribution from this model will be quite bad: new $Y_{N+1}$ would again be an approximation to $U_{N+1}$, which is sampled from $\\mathrm{Bernoulli}(\\phi)$, but the model would just return a noisy version of the inferred $T$.\n\nOn the other hand, if we have a lot of data from the first model (with a single $T$ variable) and we fit the second model, the posterior on $\\phi$ may have most of the mass near $0$ or $1$, depending on the true value of $T$.\nHence, although $U_n\\sim \\mathrm{Bernoulli}(\\phi)$ are sampled independently, once $\\phi$ is near the true value of $T$ ($0$ or $1$), they can all be approximately equal to $T$.\n\nSo, when there is a lot of data, noting where most of the mass of $\\phi$ lies can be a good approximation to the maximum likelihood of $T$.\n\nOf course, these $N\\gg 1$ settings only tell what happens when we have a lot of data and we didn't discuss the uncertainty: can we use $\\phi$ to get well-calibrated uncertainty on $T$?\n\nI generally expect that the $\\phi$ model can be a bit better in terms of handling slight misspecification, but doing inference directly on $T$ will provide better results in terms of uncertainty quantification for small $N$.\nBut this is just a hypothesis: the simulations are not for today.\n\n## Appendix\n\n### List of distributions\n\n#### Binomial distribution\nThe simplest choice: we have a coin with bias $b$ and we toss it $N$ times:\n$$\\begin{align*}\n  X_n &\\sim \\mathrm{Bernoulli}(b) \\text{ for } n = 1, \\dotsc, N\\\\\n  S &= X_1 + \\cdots + X_N\n\\end{align*}\n$$\n\nThen, we have $S \\sim \\mathrm{Binomial}(N, b)$.\nAs the individual throws are independent, it's easy to prove that\n$$\n\\mathbb E[S] = Nb, \\quad \\mathbb V[S] = Nb(1-b).\n$$\n\n#### Finite mixture of binomial distributions\nAs [above](#a-few-coins), consider $K$ coins with biases $b_1, \\dotsc, b_K$ and a dice used to choose the coin which will be tossed.\nThis is a finite mixture of binomial distributions:\n$$\\begin{align*}\n  D &\\sim \\mathrm{Categorical}(d_1, \\dotsc, d_K)\\\\\n  S \\mid D &\\sim \\mathrm{Binomial}(N, b_D)\n\\end{align*}\n$$\n\nIn this case the expectation is exactly what one can expect:\n$$\n\\mathbb E[S] = \\sum_{k=1}^K d_k \\cdot Nb_k = N\\bar b,\n$$\nwhere $\\bar b = d_1b_1 + \\cdots + d_Kb_K$.\n\nHowever, the formula for variance is more complex: from [the law of total variance](https://en.wikipedia.org/wiki/Law_of_total_variance):\n\n$$\\begin{align*}\n\\mathbb V[S] &= \\mathbb E[\\mathbb V[S\\mid B]] + \\mathbb V[ \\mathbb E[S\\mid B] ] \\\\\n&= \\sum_{k=1}^K d_k N b_k(1-b_k) + \\mathbb V[NB(1-B)]\n\\end{align*}\n$$\n\n#### Beta-binomial distribution\n\nSimilarly as [here](#even-more-coins), we can consider an infinite collection of coins, chosen from the beta distribution. Once we pick a coin, we toss it $N$ times:\n\n$$\\begin{align*}\n  B &\\sim \\mathrm{Beta}(\\alpha, \\beta)\\\\\n  S \\mid B &\\sim \\mathrm{Binomial}(N, B)\n\\end{align*}\n$$\n\nThe marginal distribution is called the [beta-binomial distribution](https://en.wikipedia.org/wiki/Beta-binomial_distribution):\n$$\nS \\sim \\mathrm{BetaBinomial}(N, \\alpha, \\beta).\n$$\n\nIt's easy to prove that\n$$\n\\mathbb E[S] = N \\frac{\\alpha}{\\alpha + \\beta},\n$$\nbut I don't know an easy derivation of the formula for the variance:\n$$\n\\mathbb V[S] = Nb(1-b)\\cdot (1 + (N-1)\\rho),\n$$\nwhere\n$b=\\alpha/(\\alpha + \\beta)$ and $\\rho=1/(1 + \\alpha + \\beta)$.\n\nHence, choosing the coin first incurs additional variance (compared to the binomial distribution).\n\n#### Poisson binomial distribution\n\nThat was quite a few examples. Let's do one more: the [Poisson binomial distribution](https://en.wikipedia.org/wiki/Poisson_binomial_distribution), because it is fun.\n\nIn this case one has $N$ coins with biases $b_1, \\dotsc, b_N$ and tosses each of them exactly once:\n$$\\begin{align*}\n  X_n &\\sim \\mathrm{Bernoulli}(b_n) \\text{ for } n=1, \\dotsc, N\\\\\n  S &= X_1 + \\cdots + X_N.\n\\end{align*}\n$$\n\nWe see that if all biases are equal, this reduces to the binomial distribution. However, this one is more flexible, as the expectation and variance are given now by\n$$\n  \\mathbb E[S] = \\sum_{n=1}^N b_n, \\quad \\mathbb  V[S] = \\sum_{n=1}^N b_n(1-b_n).\n$$\n\n### Beta-Bernoulli sparsity magic\n\nConsider the following prior on coefficients in a linear model:\n$$\\begin{align*}\n  \\gamma &\\sim \\mathrm{Beta}(\\alpha, \\beta)\\\\\n  \\theta_k \\mid \\gamma &\\sim \\gamma\\, Q_0 + (1-\\gamma)\\, Q_1 \\text{ for } k = 1, \\dotsc, K\n\\end{align*}\n$$\n\nwhere $Q_1$ is e.g., a $\\mathrm{Normal}(0, 10^2)$ distribution corresponding to \"slab\" component and $Q_0$, e.g., $\\mathrm{Normal}\\left(0, 0.01^2\\right)$ is the \"spike\" component.\n\nIntuitively, we expect that fraction $\\gamma$ of the parameters will be shrunk to $0$ by the spike component $Q_0$ and the rest (the $1-\\gamma$ fraction) of the parameters will be actually used to predict values.\n\nMichael Betancourt wrote an [amazing tutorial](https://betanalpha.github.io/assets/case_studies/modeling_sparsity.html) in which he introduces local latent variables, $\\lambda_k$, individually controlling whether $\\theta_k$ should be shrunk or not:\n\n$$\\begin{align*}\n  \\lambda_k &\\sim \\mathrm{Beta}(\\alpha, \\beta) \\text{ for } k = 1, \\dotsc, K\\\\\n  \\theta_k \\mid \\lambda_k &\\sim \\lambda_k \\, Q_0 + (1-\\lambda_k)\\, Q_1 \\text{ for } k = 1, \\dotsc, K.\n\\end{align*}\n$$\n\nUsing small letters for PDFs, we can marginalise variables $\\lambda_k$ as follows:\n$$\n  p(\\mathbf{\\theta}) = \\prod_{k=1}^K p(\\theta_k) = \\prod_{k=1}^K \\left( \\int p(\\theta_k \\mid \\lambda_k) \\, \\mathrm{d}P(\\lambda_k) \\right)\n$$\nand\n$$\\begin{align*}\n  p(\\theta_k) &= \\int p(\\theta_k \\mid \\lambda_k) \\, \\mathrm{d}P(\\lambda_k) \\\\\n  &= q_0(\\theta_k) \\int \\lambda_k\\, \\mathrm{Beta}(\\lambda_k \\mid \\alpha, \\beta) \\, \\mathrm{d}\\lambda_k + q_1(\\theta_k) \\int (1-\\lambda_k)\\, \\mathrm{Beta}(\\lambda_k \\mid \\alpha, \\beta)\\, \\mathrm{d} \\lambda_k \\\\\n  &= q_0(\\theta_k) \\frac{\\alpha}{\\alpha + \\beta} + q_1(\\theta_k) \\left( 1 - \\frac{\\alpha}{\\alpha + \\beta} \\right),\n\\end{align*}\n$$\nso that\n$$\n  \\theta_k \\sim \\gamma\\, Q_0 + (1-\\gamma)\\, Q_1,\n$$\n\nwhere $\\gamma = \\alpha / (\\alpha + \\beta)$.\n\nBeta-Bernoulli distribution offers the following perspective: draw latent indicator variables $T_k \\mid \\lambda_k \\sim \\mathrm{Bernoulli}(\\lambda_k)$, so that $\\theta_k \\mid T_k \\sim T_k\\, Q_0 + (1-T_k) \\, Q_1$.\n\nWe recognise that $T_k \\sim \\mathrm{BetaBernoulli}(\\alpha, \\beta)$ which is just $\\mathrm{Bernoulli}(\\gamma)$ for $\\gamma =\\alpha/(\\alpha+\\beta)$. By integrating out $T_k$ variables (which is just trivial summation!) we have\n$$\n  \\theta_k \\sim \\gamma\\, Q_0 + (1-\\gamma)\\, Q_1.\n$$\n\n",
    "supporting": [
      "beta-bernoulli_files"
    ],
    "filters": [],
    "includes": {}
  }
}