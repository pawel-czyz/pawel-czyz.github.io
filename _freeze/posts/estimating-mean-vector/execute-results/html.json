{
  "hash": "cc172ed334d4a02969fc6f259a2263a0",
  "result": {
    "markdown": "---\ntitle: Estimating the mean vector\ndescription: Let's estimate the mean vector from multivariate normal data.\nauthor: Paweł Czyż\ndate: 8/16/2024\nexecute:\n  freeze: true\nformat:\n  html:\n    code-fold: true\n---\n\nI recently ended up building another Gibbs sampler[^1]. I had $N$ vectors $(Y_n)$ such that each vector $Y_n = (Y_{n1}, \\dotsc, Y_{nG})$ was assumed to come from the multivariate normal distribution:\n\n$$\nY_n\\mid \\mu \\sim \\mathcal N(\\mu, \\Sigma),\n$$\n\nwhere $\\Sigma$ is a known $G\\times G$ covariance matrix and $\\mu \\sim \\mathcal N(0, B)$ is the unknown population mean, given a multivariate normal prior.\nIn this case, it is important that we know $\\Sigma$ and that $B$ is a fixed matrix, which was not necessarily build using $\\Sigma$: the [Wikipedia derivation](https://en.wikipedia.org/wiki/Bayesian_multivariate_linear_regression#Conjugate_prior_distribution) for Bayesian multivariate linear regression (which is a more general case) uses a different prior.\nI searched the internet for some time and I found a nice project, [The Book of Statistical Proofs](https://statproofbook.github.io/P/mblr-prior), but I still could not find the derivation adressing the simple case above.\n\n[^1]: Probably I [shouldn't have](https://elevanth.org/blog/2017/11/28/build-a-better-markov-chain/), but I had to use a sparse prior over the space of positive definite matrices and I don't know how to run Hamiltonian Monte Carlo with these choices...\n\nLet's quickly derive it. Define $\\nu(x) = \\exp(-x/2)$, which has two key properties. First, $\\nu(x)\\cdot \\nu(y) = \\nu(x + y)$.\nSecond, \n$$\\begin{align*}\n  \\mathcal N(x\\mid m, V) &\\propto \\nu\\big( (x-m)^T V^{-1}(x-m) \\big)\\\\\n  &\\propto \\nu( x^TV^{-1}x - 2m^TV^{-1}x),\n\\end{align*}\n$$\n\nwhich shows us how to recognise the mean and the covariance matrix of a multivariate normal distribution.\n\nLet's define $\\bar Y = N^{-1}\\sum_{n=1}^N Y_n$ to be the mean vector and $V = (B^{-1} + N\\Sigma^{-1})^{-1}$ to be an auxiliary matrix. (We see that $V^{-1}$ looks like sum of precision matrices, so may turn out to be some precision matrix!).\nThe posterior on $\\mu$ is given by\n$$\\begin{align*}\n  p\\big(\\mu \\mid (Y_n), \\Sigma, B\\big) &\\propto  \\mathcal N( \\mu\\mid 0, B) \\cdot \\prod_{n=1}^N \\mathcal N(Y_n\\mid \\mu, \\Sigma) \\\\\n  &\\propto \\nu( \\mu^T B^{-1}\\mu )\\cdot \\nu\\left( \\sum_{n=1}^N (Y_n - \\mu)^T \\Sigma^{-1} (Y_n - \\mu)  \\right) \\\\\n  &\\propto \\nu\\left( \n    \\mu^T \\left(B^{-1} + N \\Sigma^{-1}\\right)\\mu - 2 N \\bar Y^T \\Sigma^{-1} \\mu\n    \\right) \\\\\n  & \\propto \\nu\\left(\n    \\mu^T V^{-1} \\mu - 2 N \\bar Y^T \\Sigma^{-1} (V V^{-1}) \\mu\n  \\right) \\\\\n  & \\propto \\nu\\left(\n    \\mu^T V^{-1} \\mu - 2\\left(N \\bar Y^T \\Sigma^{-1} V\\right) V^{-1} \\mu\n  \\right).\n\\end{align*}\n$$\n\nLet's define $m^T = N\\bar Y^T \\Sigma^{-1} V$, so that $m = N \\cdot V \\Sigma^{-1} \\bar Y$.\nIn turn, we have \n$p\\big(\\mu \\mid (Y_n), \\Sigma, B\\big) = \\mathcal N(\\mu \\mid m, V)$.\n\nIt looks a bit surprising that we have $m$ being proportional to $N$: we would expect that for $N\\gg 1$ we would have $m\\approx \\bar Y$. However, this is fine as for $N\\gg 1$ we have $V \\approx N^{-1}\\Sigma$ and $m\\approx \\bar Y$.\nFor a small sample size, however, the prior regularises the estimate.\n\nLet's implement these equations in JAX:\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nfrom typing import Callable\n\nimport jax\nimport jax.numpy as jnp\nimport jax.random as jrandom\n\nimport blackjax\nfrom jaxtyping import Float, Array\n\n\ndef normal_logp(\n  x: Float[Array, \" G\"],\n  mean: Float[Array, \" G\"],\n  precision: Float[Array, \"G G\"],\n) -> Float[Array, \"\"]:\n  y = x - mean\n  return -0.5 * jnp.einsum(\"g,gh,h->\", y, precision, y)\n\n\ndef logposterior_fn(\n  data: Float[Array, \"N G\"],\n  precision_prior: Float[Array, \"G G\"],\n  precision_likelihood: Float[Array, \"G G\"],\n) -> Callable[[Float[Array, \" G\"]], Float[Array, \"\"]]:\n  def fn(mu: Float[Array, \" G\"]) -> Float[Array, \"\"]:\n    logprior = normal_logp(\n      x=mu,\n      mean=jnp.zeros_like(mu),\n      precision=precision_prior,\n    )\n    loglike = jnp.sum(\n      jax.vmap(\n        normal_logp,\n        in_axes=(0, None, None),)(\n          data,\n          mu,\n          precision_likelihood,\n        )\n    )\n    return logprior + loglike\n  \n  return fn\n\n\ndef get_y_bar(data: Float[Array, \"N G\"]) -> Float[Array, \" G\"]:\n  return jnp.mean(data, axis=0)\n\n\ndef posterior_precision(\n  data: Float[Array, \"N G\"],\n  precision_prior: Float[Array, \"G G\"],\n  precision_likelihood: Float[Array, \"G G\"],\n):\n  N = data.shape[0]\n  return precision_prior + N * precision_likelihood\n\n\ndef posterior_mean(\n  data: Float[Array, \"N G\"],\n  precision_prior: Float[Array, \"G G\"],\n  precision_likelihood: Float[Array, \"G G\"],\n):\n  N = data.shape[0]\n  posterior_cov = jnp.linalg.inv(\n    posterior_precision(\n      data=data,\n      precision_prior=precision_prior,\n      precision_likelihood=precision_likelihood,\n    )\n  )\n  return (N * posterior_cov) @ precision_likelihood  @  get_y_bar(data)\n\n\ndef posterior_sample(\n  key,\n  data: Float[Array, \"N G\"],\n  precision_prior: Float[Array, \"G G\"],\n  precision_likelihood: Float[Array, \"G G\"],\n  size: int = 1_000,\n):\n  N = data.shape[0]\n\n  m = posterior_mean(\n    data=data,\n    precision_prior=precision_prior,\n    precision_likelihood=precision_likelihood,\n  )\n  V = jnp.linalg.inv(posterior_precision(\n    data=data,\n    precision_prior=precision_prior,\n    precision_likelihood=precision_likelihood,\n  ))\n\n  return jrandom.multivariate_normal(\n    key, mean=m, cov=V, shape=(size,)\n  )\n```\n:::\n\n\nWe start by generating some data points:\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nn_samples = 4_000\ndata_size: int = 3\n\ncorr = 0.95\nSigma = jnp.asarray([\n  [1.0, 2 * corr],\n  [2 * corr, 2.0**2 * 1.0],\n])\n\nB = 1.0**2 * jnp.eye(2)\n\nmu = jnp.asarray([0.0, 1.5])\n\nkey = jrandom.PRNGKey(42)\nkey, subkey = jrandom.split(key)\n\ndata = jrandom.multivariate_normal(key, mu, Sigma, shape=(data_size,))\n```\n:::\n\n\nNow let's do inference in three different ways:\n\n1. Sample directly from multivariate normal using the formula derived above.\n2. Use the NUTS sampler from the [BlackJAX package](https://github.com/blackjax-devs/blackjax).\n3. Assume a somewhat wrong $\\Sigma$ matrix, ignoring the offdiagonal terms and retaining only the diagonal ones.\n\nAdditionally, we will plot a sample from the prior.\nOn top of that we plot three points: the ground-truth vector $\\mu^*$, data mean $\\bar Y$, and the plotted (prior or an appropriate posterior) distribution mean[^2].\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\nplt.style.use(\"dark_background\")\n\n\n# Sample from the prior\nkey, subkey = jrandom.split(key)\nprior = jrandom.multivariate_normal(\n  subkey,\n  mean=jnp.zeros(2),\n  cov=B,\n  shape=(n_samples,)\n)\n\n# Sample from the posterior using analytic formula\nkey, subkey = jrandom.split(key)\nposterior = posterior_sample(\n  subkey,\n  data=data,\n  precision_prior=jnp.linalg.inv(B),\n  precision_likelihood=jnp.linalg.inv(Sigma),\n  size=n_samples,\n)\n\n\n# Sample from the posterior using BlackJAX\nlogdensity_fn = logposterior_fn(\n  data=data,\n  precision_prior=jnp.linalg.inv(B),\n  precision_likelihood=jnp.linalg.inv(Sigma),\n)\n\nnuts = blackjax.nuts(\n  logdensity_fn,\n  1e-2,\n  jnp.ones(2),\n)\n\nn_warmup = 2_000\n\nstate = nuts.init(jnp.zeros_like(mu))\nstep_fn = jax.jit(nuts.step)\n\nkey, subkey = jrandom.split(key)\nfor i in range(n_warmup):\n    nuts_key = jrandom.fold_in(subkey, i)\n    state, _ = step_fn(nuts_key, state)\n\nposterior_blackjax = []\nkey, subkey = jrandom.split(key)\nfor i in range(n_samples):\n    nuts_key = jrandom.fold_in(subkey, i)\n    state, _ = step_fn(nuts_key, state)\n    posterior_blackjax.append(state.position)\n\nposterior_blackjax = jnp.asarray(posterior_blackjax)\n\n# Assume that errors are uncorrelated and use analytic formula\nkey, subkey = jrandom.split(key)\nposterior_ind = posterior_sample(\n  subkey,\n  data=data,\n  precision_prior=jnp.linalg.inv(B),\n  precision_likelihood=jnp.diag(1.0 / jnp.diagonal(Sigma)),\n  size=5_000,\n)\n\n\ndef _annotate(ax, x, y, marker, color, label=None):\n  ax.scatter([x], [y], s=6**2, c=color, marker=marker, label=label)\n\ndef annotate_axis(ax):\n  _annotate(ax, mu[0], mu[1], marker=\"x\", color=\"r\", label=\"$\\\\mu^*$\")\n  _annotate(ax, data.mean(axis=0)[0], data.mean(axis=0)[1], marker=\"+\", color=\"yellow\", label=\"$\\\\bar Y$\")\n\n\nfig, axs = plt.subplots(2, 2, sharex=True, sharey=True, dpi=200)\n\nax = axs[0, 0]\nax.set_title(\"Prior\")\nax.scatter(prior[:, 0], prior[:, 1], s=1, c=\"lightblue\", alpha=0.3)\n_annotate(ax, mu[0], mu[1], marker=\"x\", color=\"r\")\n_annotate(ax, 0.0, 0.0, marker=\"*\", color=\"salmon\")\n\nax = axs[0, 1]\nax.set_title(\"Posterior (uncorrelated $\\\\Sigma$)\")\nax.scatter(posterior_ind[:, 0], posterior_ind[:, 1], s=1, c=\"blue\", alpha=0.3)\nax.scatter([mu[0]], [mu[1]], s=10, c=\"red\", marker=\"x\")\nannotate_axis(ax)\n_annotate(ax, posterior_ind[:, 0].mean(), posterior_ind[:, 1].mean(), marker=\"*\", color=\"salmon\")\n\n\nax = axs[1, 0]\nax.set_title(\"Posterior (analytic)\")\nax.scatter(posterior[:, 0], posterior[:, 1], s=1, c=\"blue\", alpha=0.3)\nax.scatter([mu[0]], [mu[1]], s=10, c=\"red\", marker=\"x\")\nannotate_axis(ax)\n_annotate(ax, posterior[:, 0].mean(), posterior[:, 1].mean(), marker=\"*\", color=\"salmon\")\n\nax = axs[1, 1]\nax.set_title(\"Posterior (BlackJAX)\")\nax.scatter(posterior_blackjax[:, 0], posterior_blackjax[:, 1], s=1, c=\"blue\", alpha=0.3)\nax.scatter([mu[0]], [mu[1]], s=10, c=\"red\", marker=\"x\")\nannotate_axis(ax)\n_annotate(ax, posterior_blackjax[:, 0].mean(), posterior_blackjax[:, 1].mean(), marker=\"*\", color=\"salmon\", label=\"Mean\")\nax.legend(frameon=False)\n\n\nfor ax in axs.ravel():\n  ax.set_xlabel(\"$\\\\mu_1$\")\n  ax.set_ylabel(\"$\\\\mu_2$\")\n  ax.spines[[\"top\", \"right\"]].set_visible(False)\n\nfig.tight_layout()\n```\n\n::: {.cell-output .cell-output-display}\n![](estimating-mean-vector_files/figure-html/cell-4-output-1.png){}\n:::\n:::\n\n\nLooks like BlackJAX and analytic formula give the same posterior, so perhaps there is no mistake in the algebra.\nWe also see that using a proper $\\Sigma$ should help us estimate the mean vector better and that using the prior should regularise the inference.\n\nLet's do several repetitions of this experiment and evaluate the distance from the point estimate to the ground-truth value:\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\ndef distance(x1, x2):\n  return jnp.sqrt(jnp.sum(jnp.square(x1 - x2)))\n\n\ndef make_repetition(key, data_size: int):\n  key1, key2 = jrandom.split(key, 2)\n  mu_true = jrandom.multivariate_normal(key1, jnp.zeros(2), B)\n\n  data = jrandom.multivariate_normal(\n    key2, mu_true, Sigma, shape=(data_size,)\n  )\n\n  y_bar = get_y_bar(data)\n  \n  mu_expected = posterior_mean(\n    data=data,\n    precision_prior=jnp.linalg.inv(B),\n    precision_likelihood=jnp.linalg.inv(Sigma),\n  )\n\n  mu_diagonal = posterior_mean(\n    data=data,\n    precision_prior=jnp.linalg.inv(B),\n    precision_likelihood=jnp.diag(1.0 / jnp.diagonal(Sigma)),\n  )\n\n  return {\n    \"prior\": distance(jnp.zeros(2), mu_true),\n    \"posterior\": distance(mu_expected, mu_true),\n    \"data\": distance(y_bar, mu_true),\n    \"diagonal\": distance(mu_diagonal, mu_true),\n  }\n\nn_reps = 2_000\n\ndef make_plots(key, axs, data_size: int):\n  reps = [make_repetition(k, data_size=data_size) for k in jrandom.split(key, n_reps)]\n\n  bins = jnp.linspace(0, 4, 20)\n\n  def plot(ax, name, color):\n    ax.hist(\n      [r[name] for r in reps],\n      color=color,\n      density=True,\n      bins=bins,\n    )\n\n  ax = axs[0]\n  ax.set_title(f\"$N={data_size}$\")\n  ax.set_xlabel(\"Prior mean\")\n  plot(ax, \"prior\", \"white\")\n\n  ax = axs[1]\n  ax.set_xlabel(\"Posterior mean\")\n  plot(ax, \"posterior\", \"bisque\")\n\n  ax = axs[2]\n  ax.set_xlabel(\"Data mean\")\n  plot(ax, \"data\", \"darkorange\")\n\n  ax = axs[3]\n  ax.set_xlabel(\"Diagonal model\")\n  plot(ax, \"diagonal\", \"purple\")\n\n\nfig, axs = plt.subplots(4, 4, sharex=True, sharey=\"row\")\n\nfor i, size in enumerate([2, 10, 50, 250]):\n  key, subkey = jrandom.split(key)\n  make_plots(\n    subkey,\n    axs=axs[:, i],\n    data_size=size,\n  )\n\nfor ax in axs.ravel():\n  ax.spines[[\"top\", \"right\", \"left\"]].set_visible(False)\n  ax.set_yticks([])\n\nfig.suptitle(\"Error\")\nfig.tight_layout()\n```\n\n::: {.cell-output .cell-output-display}\n![](estimating-mean-vector_files/figure-html/cell-5-output-1.png){width=657 height=475}\n:::\n:::\n\n\nWe see what should expected:\n\n1. Every method that uses data (i.e., everything apart from the prior) has improved performance when $N$ increases.\n2. For small sample sizes, using plain data can result in larger errors and a reasonable prior can regularise the posterior mean, so that the error is smaller.\n3. Given enough data, the performance of posterior mean and using the data mean looks quite similar.\n\nAdditionally, we see that a model assuming diagonal $\\Sigma$ (i.e., ignoring the correlations) also has performance quite similar to the true one.\n\nThis \"performance looks similar\" can actually be somewhat misleading: each of this distributions has quite large variance, so minor differences can be unobserved.\n\nLet's now repeat this experiment, but this time plotting the *difference* between distances, so that we can see any difference better. Namely, for the method $M$ and and the $s$-th simulation, write $d^{(M)}_s$ for the obtained distance.\nNow, instead of plotting the data sets $\\{ d^{(M_1)}_{1}, \\dotsc, d^{(M_1)}_S\\}$ and $\\{ d^{(M_2)}_{1}, \\dotsc, d^{(M_2)}_S\\}$, we can plot the differences $\\{ d^{(M_2)}_{1} - d^{(M_1)}_{1}, \\dotsc, d^{(M_2)}_{S} - d^{(M_1)}_{S} \\}$.\n\nLet's use the posterior mean in the right model (potentially the best solution) as the baseline and compare it with three other models.\nIn each of the plots, the samples on the right of zero, represent positive difference, i.e., the case when the baseline method (in our case the posterior in the right model) was better than the considered alternative.\nApart from raw samples, let's plot the mean of such distribution (and, intuitively, we should expect it to be larger than zero) and report the percentage of samples on the right from zero.\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\nn_reps = 3_000\n\ndef compare_with_diagonal(key, data_size: int):\n  key1, key2 = jrandom.split(key, 2)\n  mu_true = jrandom.multivariate_normal(key1, jnp.zeros(2), B)\n\n  data = jrandom.multivariate_normal(\n    key2, mu_true, Sigma, shape=(data_size,)\n  )\n\n  y_bar = get_y_bar(data)\n  \n  mu_posterior = posterior_mean(\n    data=data,\n    precision_prior=jnp.linalg.inv(B),\n    precision_likelihood=jnp.linalg.inv(Sigma),\n  )\n  mu_diagonal = posterior_mean(\n    data=data,\n    precision_prior=jnp.linalg.inv(B),\n    precision_likelihood=jnp.diag(1.0 / jnp.diagonal(Sigma)),\n  )\n\n  baseline = distance(mu_posterior, mu_true)\n\n  return {\n    \"delta_prior\": distance(jnp.zeros(2), mu_true) - baseline,\n    \"delta_diagonal\": distance(mu_diagonal, mu_true) - baseline,\n    \"delta_data\": distance(y_bar, mu_true) - baseline,\n  }\n\n\ndef make_plots(key, axs, data_size: int):\n  reps = [compare_with_diagonal(k, data_size=data_size) for k in jrandom.split(key, n_reps)]\n\n  bins = jnp.linspace(-2, 2, 20)\n\n  def plot(ax, name, color):\n    samples = jnp.array([r[name] for r in reps])\n    ax.hist(\n      samples,\n      color=color,\n      density=True,\n      bins=bins,\n    )\n    p_worse = float(100 * jnp.mean(samples > 0))\n    ax.axvline(jnp.mean(samples), linestyle=\":\", color=\"salmon\")\n    ax.axvline(0.0, linestyle=\":\", color=\"white\")\n    ax.annotate(f\"{p_worse:.0f}%\", xy=(0.05, 0.5), xycoords='axes fraction')\n\n  ax = axs[0]\n  ax.set_title(f\"$N={data_size}$\")\n  ax.set_xlabel(\"Prior mean\")\n  plot(ax, \"delta_prior\", \"white\")\n\n  ax = axs[1]\n  ax.set_xlabel(\"Data mean\")\n  plot(ax, \"delta_data\", \"darkorange\")\n\n  ax = axs[2]\n  ax.set_xlabel(\"Diagonal model\")\n  plot(ax, \"delta_diagonal\", \"purple\")\n\n\nfig, axs = plt.subplots(3, 4, sharex=True, sharey=\"row\")\n\nfor i, size in enumerate([2, 10, 50, 250]):\n  key, subkey = jrandom.split(key)\n  make_plots(\n    subkey,\n    axs=axs[:, i],\n    data_size=size,\n  )\n\nfor ax in axs.ravel():\n  ax.spines[[\"top\", \"right\", \"left\"]].set_visible(False)\n  ax.set_yticks([])\n\nfig.suptitle(\"Extra error over the baseline\")\nfig.tight_layout()\n```\n\n::: {.cell-output .cell-output-display}\n![](estimating-mean-vector_files/figure-html/cell-6-output-1.png){width=661 height=475}\n:::\n:::\n\n\nAs expected, a well-specified Bayesian model performs the best. However, having \"enough\" data points one can use the data mean as well (or the misspecified model without off-diagonal terms in the covariance).\nAn interesting question would be to check how this \"enough\" depends on the dimensionality of the problem.\n\n",
    "supporting": [
      "estimating-mean-vector_files"
    ],
    "filters": [],
    "includes": {}
  }
}