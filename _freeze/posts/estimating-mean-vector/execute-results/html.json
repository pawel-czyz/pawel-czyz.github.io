{
  "hash": "923acb6ddd3ff8b72f548704c7e74b68",
  "result": {
    "markdown": "---\ntitle: Estimating the mean vector\ndescription: Let's estimate the mean vector from multivariate normal data.\nauthor: Paweł Czyż\ndate: 8/16/2024\nexecute:\n  freeze: true\nformat:\n  html:\n    code-fold: true\n---\n\nI recently ended up building another Gibbs sampler[^1]. I had $N$ vectors $(Y_n)$ such that each vector $Y_n = (Y_{n1}, \\dotsc, Y_{nG})$ was assumed to come from the multivariate normal distribution:\n\n$$\nY_n\\mid \\mu \\sim \\mathcal N(\\mu, \\Sigma),\n$$\n\nwhere $\\Sigma$ is a known $G\\times G$ covariance matrix and $\\mu \\sim \\mathcal N(0, B)$ is the unknown population mean, given a multivariate normal prior.\nIn this case, it is important that we know $\\Sigma$ and that $B$ is a fixed matrix, which was not necessarily build using $\\Sigma$: the [Wikipedia derivation](https://en.wikipedia.org/wiki/Bayesian_multivariate_linear_regression#Conjugate_prior_distribution) for Bayesian multivariate linear regression (which is a more general case) uses a different prior.\nI searched the internet for some time and I found a nice project, [The Book of Statistical Proofs](https://statproofbook.github.io/P/mblr-prior), but I still could not find the derivation adressing the simple case above.\n\n[^1]: Yes, probably I [shouldn't have](https://elevanth.org/blog/2017/11/28/build-a-better-markov-chain/), but I had to use a sparse prior over the space of positive definite matrices and I don't know how to run Hamiltonian Monte Carlo with these choices...\n\nLet's quickly derive it. Define $\\nu(x) = \\exp(-x/2)$, which has two key properties. First, $\\nu(x)\\cdot \\nu(y) = \\nu(x + y)$.\nSecond, \n$$\\begin{align*}\n  \\mathcal N(x\\mid m, V) &\\propto \\nu\\big( (x-m)^T V^{-1}(x-m) \\big)\\\\\n  &\\propto \\nu( x^TV^{-1}x - 2m^TV^{-1}x),\n\\end{align*}\n$$\n\nwhich shows us how to recognise the mean and the covariance matrix of a multivariate normal distribution.\n\nLet's define $\\bar Y = N^{-1}\\sum_{n=1}^N Y_n$ to be the mean vector and $V = (B^{-1} + N\\Sigma^{-1})^{-1}$ to be an auxiliary matrix. (We see that $V^{-1}$ looks like sum of precision matrices, so may turn out to be some precision matrix!).\nThe posterior on $\\mu$ is given by\n$$\\begin{align*}\n  p\\big(\\mu \\mid (Y_n), \\Sigma, B\\big) &\\propto  \\mathcal N( \\mu\\mid 0, B) \\cdot \\prod_{n=1}^N \\mathcal N(Y_n\\mid \\mu, \\Sigma) \\\\\n  &\\propto \\nu( \\mu^T B^{-1}\\mu )\\cdot \\nu\\left( \\sum_{n=1}^N (Y_n - \\mu)^T \\Sigma^{-1} (Y_n - \\mu)  \\right) \\\\\n  &\\propto \\nu\\left( \n    \\mu^T \\left(B^{-1} + N \\Sigma^{-1}\\right)\\mu - 2 N \\bar Y^T \\Sigma^{-1} \\mu\n    \\right) \\\\\n  & \\propto \\nu\\left(\n    \\mu^T V^{-1} \\mu - 2 N \\bar Y^T \\Sigma^{-1} (V V^{-1}) \\mu\n  \\right) \\\\\n  & \\propto \\nu\\left(\n    \\mu^T V^{-1} \\mu - 2\\left(N \\bar Y^T \\Sigma^{-1} V\\right) V^{-1} \\mu\n  \\right).\n\\end{align*}\n$$\n\nLet's define $m^T = N\\bar Y^T \\Sigma^{-1} V$, so that $m = N \\cdot V \\Sigma^{-1} \\bar Y$.\nIn turn, we have \n$p\\big(\\mu \\mid (Y_n), \\Sigma, B\\big) = \\mathcal N(\\mu \\mid m, V)$.\n\nIt looks a bit surprising that we have $m$ being proportional to $N$: we would expect that for $N\\gg 1$ we would have $m\\approx \\bar Y$. However, this is fine as for $N\\gg 1$ we have $V \\approx N^{-1}\\Sigma$ and $m\\approx \\bar Y$.\nFor a small sample size, however, the prior regularises the estimate.\n\nLet's implement these equations in JAX:\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nfrom typing import Callable\n\nimport jax\nimport jax.numpy as jnp\nimport jax.random as jrandom\n\nimport blackjax\nfrom jaxtyping import Float, Array\n\n\ndef normal_logp(\n  x: Float[Array, \" G\"],\n  mean: Float[Array, \" G\"],\n  precision: Float[Array, \"G G\"],\n) -> Float[Array, \"\"]:\n  y = x - mean\n  return -0.5 * jnp.einsum(\"g,gh,h->\", y, precision, y)\n\n\ndef logposterior_fn(\n  data: Float[Array, \"N G\"],\n  precision_prior: Float[Array, \"G G\"],\n  precision_likelihood: Float[Array, \"G G\"],\n) -> Callable[[Float[Array, \" G\"]], Float[Array, \"\"]]:\n  def fn(mu: Float[Array, \" G\"]) -> Float[Array, \"\"]:\n    logprior = normal_logp(\n      x=mu,\n      mean=jnp.zeros_like(mu),\n      precision=precision_prior,\n    )\n    loglike = jnp.sum(\n      jax.vmap(\n        normal_logp,\n        in_axes=(0, None, None),)(\n          data,\n          mu,\n          precision_likelihood,\n        )\n    )\n    return logprior + loglike\n  \n  return fn\n\n\ndef get_y_bar(data: Float[Array, \"N G\"]) -> Float[Array, \" G\"]:\n  return jnp.mean(data, axis=0)\n\n\ndef posterior_precision(\n  data: Float[Array, \"N G\"],\n  precision_prior: Float[Array, \"G G\"],\n  precision_likelihood: Float[Array, \"G G\"],\n):\n  N = data.shape[0]\n  return precision_prior + N * precision_likelihood\n\n\ndef posterior_mean(\n  data: Float[Array, \"N G\"],\n  precision_prior: Float[Array, \"G G\"],\n  precision_likelihood: Float[Array, \"G G\"],\n):\n  N = data.shape[0]\n  posterior_cov = jnp.linalg.inv(\n    posterior_precision(\n      data=data,\n      precision_prior=precision_prior,\n      precision_likelihood=precision_likelihood,\n    )\n  )\n  return (N * posterior_cov) @ precision_likelihood  @  get_y_bar(data)\n\n\ndef posterior_sample(\n  key,\n  data: Float[Array, \"N G\"],\n  precision_prior: Float[Array, \"G G\"],\n  precision_likelihood: Float[Array, \"G G\"],\n  size: int = 1_000,\n):\n  N = data.shape[0]\n\n  m = posterior_mean(\n    data=data,\n    precision_prior=precision_prior,\n    precision_likelihood=precision_likelihood,\n  )\n  V = jnp.linalg.inv(posterior_precision(\n    data=data,\n    precision_prior=precision_prior,\n    precision_likelihood=precision_likelihood,\n  ))\n\n  return jrandom.multivariate_normal(\n    key, mean=m, cov=V, shape=(size,)\n  )\n```\n:::\n\n\nWe start by generating some data points:\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nn_samples = 4_000\ndata_size: int = 3\n\ncorr = 0.95\nSigma = jnp.asarray([\n  [1.0, 2 * corr],\n  [2 * corr, 2.0**2 * 1.0],\n])\n\nB = 1.0**2 * jnp.eye(2)\n\nmu = jnp.asarray([0.0, 1.5])\n\nkey = jrandom.PRNGKey(42)\nkey, subkey = jrandom.split(key)\n\ndata = jrandom.multivariate_normal(key, mu, Sigma, shape=(data_size,))\n```\n:::\n\n\nNow let's do inference in three different ways:\n\n1. Sample directly from multivariate normal using the formula derived above.\n2. Use the NUTS sampler from the [BlackJAX package](https://github.com/blackjax-devs/blackjax).\n3. Assume a somewhat wrong $\\Sigma$ matrix, ignoring the offdiagonal terms and retaining only the diagonal ones.\n\nAdditionally, we will plot a sample from the prior.\nOn top of that we plot three points: the ground-truth vector $\\mu^*$, data mean $\\bar Y$, and the plotted (prior or an appropriate posterior) distribution mean.\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\n# Prior\nkey, subkey = jrandom.split(key)\nprior = jrandom.multivariate_normal(\n  subkey,\n  mean=jnp.zeros(2),\n  cov=B,\n  shape=(n_samples,)\n)\n\n# Sample using analytic formula\nkey, subkey = jrandom.split(key)\nposterior = posterior_sample(\n  subkey,\n  data=data,\n  precision_prior=jnp.linalg.inv(B),\n  precision_likelihood=jnp.linalg.inv(Sigma),\n  size=n_samples,\n)\n\n\nlogdensity_fn = logposterior_fn(\n  data=data,\n  precision_prior=jnp.linalg.inv(B),\n  precision_likelihood=jnp.linalg.inv(Sigma),\n)\n\nnuts = blackjax.nuts(\n  logdensity_fn,\n  1e-2,\n  jnp.ones(2),\n)\n\nn_warmup = 2_000\n\nstate = nuts.init(jnp.zeros_like(mu))\nstep_fn = jax.jit(nuts.step)\n\nkey, subkey = jrandom.split(key)\nfor i in range(n_warmup):\n    nuts_key = jrandom.fold_in(subkey, i)\n    state, _ = step_fn(nuts_key, state)\n\nposterior_blackjax = []\nkey, subkey = jrandom.split(key)\nfor i in range(n_samples):\n    nuts_key = jrandom.fold_in(subkey, i)\n    state, _ = step_fn(nuts_key, state)\n    posterior_blackjax.append(state.position)\n\nposterior_blackjax = jnp.asarray(posterior_blackjax)\n\n# Assume that errors are uncorrelated\nkey, subkey = jrandom.split(key)\nposterior_ind = posterior_sample(\n  subkey,\n  data=data,\n  precision_prior=jnp.linalg.inv(B),\n  precision_likelihood=jnp.diag(1.0 / jnp.diagonal(Sigma)),\n  size=5_000,\n)\n\nimport matplotlib.pyplot as plt\nplt.style.use(\"dark_background\")\n\ndef _annotate(ax, x, y, marker, color, label=None):\n  ax.scatter([x], [y], s=6**2, c=color, marker=marker, label=label)\n\ndef annotate_axis(ax):\n  _annotate(ax, mu[0], mu[1], marker=\"x\", color=\"r\", label=\"$\\\\mu^*$\")\n  _annotate(ax, data.mean(axis=0)[0], data.mean(axis=0)[1], marker=\"+\", color=\"yellow\", label=\"$\\\\bar Y$\")\n\n\nfig, axs = plt.subplots(2, 2, sharex=True, sharey=True, dpi=200)\n\nax = axs[0, 0]\nax.set_title(\"Prior\")\nax.scatter(prior[:, 0], prior[:, 1], s=1, c=\"lightblue\", alpha=0.3)\n_annotate(ax, mu[0], mu[1], marker=\"x\", color=\"r\")\n_annotate(ax, 0.0, 0.0, marker=\"*\", color=\"salmon\")\n\nax = axs[0, 1]\nax.set_title(\"Posterior (uncorrelated $\\\\Sigma$)\")\nax.scatter(posterior_ind[:, 0], posterior_ind[:, 1], s=1, c=\"blue\", alpha=0.3)\nax.scatter([mu[0]], [mu[1]], s=10, c=\"red\", marker=\"x\")\nannotate_axis(ax)\n_annotate(ax, posterior_ind[:, 0].mean(), posterior_ind[:, 1].mean(), marker=\"*\", color=\"salmon\")\n\n\nax = axs[1, 0]\nax.set_title(\"Posterior (analytic)\")\nax.scatter(posterior[:, 0], posterior[:, 1], s=1, c=\"blue\", alpha=0.3)\nax.scatter([mu[0]], [mu[1]], s=10, c=\"red\", marker=\"x\")\nannotate_axis(ax)\n_annotate(ax, posterior[:, 0].mean(), posterior[:, 1].mean(), marker=\"*\", color=\"salmon\")\n\nax = axs[1, 1]\nax.set_title(\"Posterior (BlackJAX)\")\nax.scatter(posterior_blackjax[:, 0], posterior_blackjax[:, 1], s=1, c=\"blue\", alpha=0.3)\nax.scatter([mu[0]], [mu[1]], s=10, c=\"red\", marker=\"x\")\nannotate_axis(ax)\n_annotate(ax, posterior_blackjax[:, 0].mean(), posterior_blackjax[:, 1].mean(), marker=\"*\", color=\"salmon\", label=\"Mean\")\nax.legend(frameon=False)\n\n\nfor ax in axs.ravel():\n  ax.set_xlabel(\"$\\\\mu_1$\")\n  ax.set_ylabel(\"$\\\\mu_2$\")\n  ax.spines[[\"top\", \"right\"]].set_visible(False)\n\nfig.tight_layout()\n```\n\n::: {.cell-output .cell-output-display}\n![](estimating-mean-vector_files/figure-html/cell-4-output-1.png){}\n:::\n:::\n\n\nUsing a proper $\\Sigma$ should help us estimate the mean vector better.\nMoreover, using the prior should regularise the inference.\nLet's do several repetitions of this experiment and evaluate the distance:\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\ndef distance(x1, x2):\n  return jnp.sqrt(jnp.sum(jnp.square(x1 - x2)))\n\n\ndef make_repetition(key, data_size: int):\n  key1, key2 = jrandom.split(key, 2)\n  mu_true = jrandom.multivariate_normal(key1, jnp.zeros(2), B)\n\n  data = jrandom.multivariate_normal(\n    key2, mu_true, Sigma, shape=(data_size,)\n  )\n\n  y_bar = get_y_bar(data)\n  \n  mu_expected = posterior_mean(\n    data=data,\n    precision_prior=jnp.linalg.inv(B),\n    precision_likelihood=jnp.linalg.inv(Sigma),\n  )\n  return {\n    \"prior\": distance(jnp.zeros(2), mu_true),\n    \"posterior\": distance(mu_expected, mu_true),\n    \"data\": distance(y_bar, mu_true),\n  }\n\nn_reps = 2_000\n\nkey, subkey1, subkey2 = jrandom.split(key, 3)\nreps_small = [make_repetition(k, data_size=2) for k in jrandom.split(subkey1, n_reps)]\n\nreps_large = [make_repetition(k, data_size=10) for k in jrandom.split(subkey2, n_reps)]\n\nfig, axs = plt.subplots(3, 2, sharex=True, sharey=True)\n\nbins = jnp.linspace(0, 5, 15)\n\nax = axs[0, 0]\nax.set_title(\"$N=2$\")\nax.set_xlabel(\"Prior mean\")\nax.hist([r[\"prior\"] for r in reps_small], color=\"white\", density=True, bins=bins)\n\nax = axs[1, 0]\nax.hist([r[\"posterior\"] for r in reps_small], color=\"bisque\", density=True, bins=bins)\nax.set_xlabel(\"Posterior mean\")\n\nax = axs[2, 0]\nax.hist([r[\"data\"] for r in reps_small], color=\"darkorange\", density=True, bins=bins)\nax.set_xlabel(\"Data mean\")\n\n\nax = axs[0, 1]\nax.set_title(\"$N=20$\")\nax.set_xlabel(\"Prior mean\")\nax.hist([r[\"prior\"] for r in reps_large], color=\"white\", density=True, bins=bins)\n\nax = axs[1, 1]\nax.hist([r[\"posterior\"] for r in reps_large], color=\"bisque\", density=True, bins=bins)\nax.set_xlabel(\"Posterior mean\")\n\nax = axs[2, 1]\nax.hist([r[\"data\"] for r in reps_large], color=\"darkorange\", density=True, bins=bins)\nax.set_xlabel(\"Data mean\")\n\n\nfor ax in axs.ravel():\n  ax.spines[[\"top\", \"right\", \"left\"]].set_visible(False)\n\nfig.suptitle(\"Error\")\nfig.tight_layout()\n```\n\n::: {.cell-output .cell-output-display}\n![](estimating-mean-vector_files/figure-html/cell-5-output-1.png){width=662 height=475}\n:::\n:::\n\n\nWe see what expected: for small sample sizes, using plain data results in too large errors (due to large variance).\nA reasonable prior can regularise the posterior mean, so that the error is smaller.\n\n",
    "supporting": [
      "estimating-mean-vector_files"
    ],
    "filters": [],
    "includes": {}
  }
}