{
  "hash": "19da42431094b2aeacbbd5fdb45c3130",
  "result": {
    "markdown": "---\ntitle: From kernel regression to the transformer\ndescription: We'll go from kernel regression to attention.\nauthor: Paweł Czyż\ndate: 1/26/2024\nexecute:\n  freeze: true\nformat:\n  html:\n    code-fold: true\n---\n\nI remember that when we read [*Attention is all you need*](https://arxiv.org/abs/1706.03762) at a journal club back in 2020, I did not really understand what attention was[^1].\n\n[^1]: The authors had to explain self-attention, its multi-head variant, the transformer architecture with encoder and decoder block, and positional encoding. All in a short conference paper, so it may indeed appear quite dense in ideas.\n\nFortunately for me, [*Transformer dissection*](https://arxiv.org/abs/1908.11775) paper and Cosma Shalizi's [post on the topic](http://bactra.org/notebooks/nn-attention-and-transformers.html) appeared, which show the connection between attention and [kernel regression](https://en.wikipedia.org/wiki/Kernel_regression).\nThis point of view was exactly what I needed! I like this so much that when I explain attention to other people, I always start from kernel regression.\n\n## Kernel regression\n\nLet's start with [kernel regression](https://en.wikipedia.org/wiki/Kernel_regression) as independently proposed by Nadaraya and Watson sixty years ago.\nWe will generate some data with [heteroskedastic noise](https://en.wikipedia.org/wiki/Homoscedasticity_and_heteroscedasticity), $y = f(x) + n(x)\\epsilon$ where $\\epsilon \\sim \\mathcal N(0, 1)$, $f(x)$ is the expected value $\\mathbb E[y\\mid x]$ and function $n(x)$ makes the noise heteroskedastic.\n\nWe'll plot the observed data points as well as $f(x) + 2 n(x)$ and $f(x) - 2n(x)$ as is [often done](https://en.wikipedia.org/wiki/68%E2%80%9395%E2%80%9399.7_rule).\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nfrom functools import partial\n\nimport jax\nimport jax.numpy as jnp\nfrom jax import random\n\nimport equinox as eqx\nfrom jaxtyping import Float, Array\n\nimport matplotlib.pyplot as plt\nplt.style.use(\"dark_background\")\n\nVector = Float[Array, \" n_points\"]\n\ndef f(X: Vector) -> Vector:\n    return 0.5 * jnp.sin(X) - 1 * jnp.sin(3 * X) + 0.2 * jnp.square(X)\n\ndef n(X: Vector) -> Vector:\n    return 0.2 + 0.05 * jnp.abs(X)\n\nn_points: int = 150\n\nkey = random.PRNGKey(2024)\nkey, subkey = random.split(key)\nX = jnp.linspace(-3, 3, n_points)\nY = f(X) + n(X) * random.normal(subkey, shape=X.shape)\n\nfig, ax = plt.subplots(figsize=(4, 3), dpi=150)\nX_ax = jnp.linspace(-3, 3, 201)\nax.fill_between(\n    X_ax, f(X_ax)- 2 * n(X_ax), f(X_ax) + 2 * n(X_ax), alpha=0.4, color=\"maroon\"\n)\nax.plot(X_ax, f(X_ax), color=\"maroon\", alpha=0.8)\nax.scatter(X, Y, color=\"white\", s=5, alpha=1.0)\nax.set_xlabel(\"$X$\")\nax.set_ylabel(\"$Y$\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nAn NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=1}\n```\nText(0, 0.5, '$Y$')\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](kernel-regression-transformer_files/figure-html/cell-2-output-3.png){}\n:::\n:::\n\n\nWe will be interested in finding $f(x)$ via the weighted average:\n$$\n\\hat f(x) = \\sum_{i=1}^n y_i\\, w_i(x)\n$$\n\nwhere $w_i(x)$ is the weight of the $i$-th data point used to estimate the value at $x$. To make it a weighted average, we will ensure that $w_1(x) + \\cdots + w_n(x) = 1$. In case where $w_i(x) = 1/n$ we obtain just constant prediction, equal to the sample average over $y_i$.\n\nMore generally, consider a positive function $K\\colon \\mathcal X \\times \\mathcal X \\to \\mathbb R^+$ which measures similarity between two data points: we want $K(x, x')$ to attain the largest possible value and for $x'$ very far from $x$ we want to have $K(x, x')$ to be small.\nFor such a function we can form a set of weights via\n$$\nw_i(x) = \\frac{K(x, x_i)}{\\sum_{j=1}^n K(x, x_j)}.\n$$\n\nLet's restrict our attention for now to Gaussian kernels, $K(x, x'; \\ell) = \\exp \\left(\\left( \\frac{x-x'}{\\ell} \\right)^2 \\right)$ with lengthscale $\\ell$ and visualise the predictions for different lengthscales.\nAs kernels are parameterised functions, we will use [Equinox](https://github.com/patrick-kidger/equinox/): \n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nclass GaussianKernel(eqx.Module):\n    _log_lengthscale: float\n\n    def __init__(self, lengthscale: float) -> None:\n        assert lengthscale > 0, \"Lengthscale should be positive.\"\n        self._log_lengthscale = jnp.log(lengthscale)\n\n    @property\n    def lengthscale(self) -> float:\n        return jnp.exp(self._log_lengthscale)\n\n    def __call__(self, x: float, x_: float) -> float:\n        return jnp.exp(-jnp.square((x-x_) / self.lengthscale))\n\n\n    def predict(self, X_test: Float[Array, \" n_test\"], X_obs: Vector, Y_obs: Vector) -> Float[Array, \" n_test\"]:\n        kernel = self\n        def predict_one(x: float) -> float:\n            ks = jax.vmap(partial(kernel, x))(X_obs)\n            ws = ks / (jnp.sum(ks) + 1e-16)\n            return jnp.sum(Y_obs * ws)\n        return jax.vmap(predict_one)(X_test)    \n\n\nkernels = {lengthscale: GaussianKernel(lengthscale) for lengthscale in [3.0, 0.5, 0.25, 0.05]} \n\n\nfig, axs = plt.subplots(2, 2, figsize=(2*4, 2*3), dpi=150, sharex=True, sharey=True)\n\nfor (lengthscale, k), ax in zip(kernels.items(), axs.ravel()):\n    pred = k.predict(X_ax, X_obs=X, Y_obs=Y)\n    ax.set_title(f\"$\\\\ell = {lengthscale}$\")\n    ax.plot(X_ax, f(X_ax), color=\"maroon\", alpha=0.8)\n    ax.plot(X_ax, pred, color=\"orangered\", alpha=0.8)\n    ax.scatter(X, Y, color=\"white\", s=5, alpha=0.8)\n    ax.set_xlabel(\"$X$\")\n    ax.set_ylabel(\"$Y$\")\n\nfig.tight_layout()\n```\n\n::: {.cell-output .cell-output-display}\n![](kernel-regression-transformer_files/figure-html/cell-3-output-1.png){}\n:::\n:::\n\n\nIt seems that $\\ell=3.0$ results in underfitted, almost constant, predictions, and $\\ell=0.05$ arguably overfits, resulting in predictions changing a bit too quickly.\nGenerally, it seems that $\\ell \\approx 0.25$ is a reasonable choice.\n\n### Masked training\nLet's now think how we could find $\\ell$ algorithmically (and when the true mean curve is not available for comparison!).\n\nFor example, we could use something like the leave-one-out cross-validation:\n\n1. Hold out a data point $(x_i, y_i)$;\n2. Fit the kernel regression with lengthscale $\\ell$ to the data $(x_1, y_1), \\dotsc, (x_{i-1}, y_{i-1}), (x_{i+1}, y_{i+1}), \\dotsc, (x_n, y_n)$;\n3. Predict $y_i$ from $x_i$ given the kernel regression.\n\nLooking at different values $\\ell$ and varying the index $i$ of the hold-out data point may be a reasonable training procedure.\nNote however that if we use standard squared loss, this will have a drawback that points which are further from the mean (due to heteroskedasticity) will be treated similarly to the data points where the noise is small.\nWe could try to reweight them, but we won't do that and implement a vanilla variant.\n\nIn fact, we will try several variants of this approach, allowing to hold out more data points than $1$. In terms of probabilistic interpretation this is even worse: apart from problems with interpreting square loss due to heteroskedasticity, now we are also predicting values at several locations at once, effectively assuming that they are independent, given the observed data. In a way, this is similar to the [BERT training](https://en.wikipedia.org/wiki/BERT_(language_model)). [XLNet](https://arxiv.org/abs/1906.08237) considers different permutations, being closer to an orderless autoregressive model.\nAnyway, BERT had impressive performance, so let's try different variants here:\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nimport optax\n\n\ndef train(\n    key,\n    model: eqx.Module,\n    X: Vector,\n    Y: Vector,\n    learning_rate: float = 0.2,\n    hold_out_size: int = 1,\n    n_steps: int = 100,\n    print_every: int = 100\n) -> eqx.Module:\n    assert n_steps > 1\n    if print_every is None:\n        print_every = n_steps + 100\n    assert print_every > 0\n    assert learning_rate > 0\n\n    assert X.shape[0] == Y.shape[0]\n    n_total = X.shape[0]\n\n    def split_data(key):\n        \"\"\"Splits the data into training and test.\"\"\"\n        indices = random.permutation(key, jnp.arange(n_total))\n        ind_test = indices[:hold_out_size]\n        ind_obs = indices[hold_out_size:]\n\n        return X[ind_obs], Y[ind_obs], X[ind_test], Y[ind_test]\n\n    @jax.jit\n    def step(\n        model: eqx.Module,\n        opt_state,\n        X_obs: Vector,\n        Y_obs: Vector,\n        X_test: Float[Array, \" n_test\"],\n        Y_test: Float[Array, \" n_test\"],\n    ):\n        def loss_fn(model):\n            preds = model.predict(\n                X_test=X_test,\n                X_obs=X_obs,\n                Y_obs=Y_obs,\n            )\n            return jnp.mean(jnp.square(preds - Y_test))\n\n        loss, grads = jax.value_and_grad(loss_fn)(model)\n        updates, opt_state = optimizer.update(grads, opt_state, model)\n        model = optax.apply_updates(model, updates)\n        return model, opt_state, loss\n\n    optimizer = optax.adam(learning_rate=learning_rate)\n    opt_state = optimizer.init(model)\n\n    losses = []\n\n    for n_step in range(1, n_steps + 1):\n        key, subkey = random.split(key)\n        \n        X_obs, Y_obs, X_test, Y_test = split_data(subkey)\n\n        model, opt_state, loss = step(\n            model,\n            opt_state=opt_state,\n            X_obs=X_obs,\n            Y_obs=Y_obs,\n            X_test=X_test,\n            Y_test=Y_test\n        )\n\n        losses.append(loss)\n\n        if n_step % print_every == 0:\n            avg_loss = jnp.mean(jnp.asarray(losses[-20:]))\n            print(f\"Step: {n_step}\")\n            print(f\"Loss: {avg_loss:.2f}\")\n            print(\"-\" * 14)\n\n    return model\n\n\nfig, axs = plt.subplots(2, 2, figsize=(2*4, 2*3), dpi=150, sharex=True, sharey=True)\n\nfor holdout, ax in zip([1, 10, n_points // 2, int(0.8 * n_points)], axs.ravel()):\n    key, subkey = random.split(key)\n    \n    model = train(\n        key=subkey,\n        model=GaussianKernel(lengthscale=1.0),\n        X=X,\n        Y=Y,\n        print_every=None,\n        hold_out_size=holdout,\n        n_steps=100,\n    )\n    pred = model.predict(X_ax, X_obs=X, Y_obs=Y)\n    ax.set_title(f\"Hold-out={holdout}, $\\ell$={model.lengthscale:.2f}\")\n    ax.plot(X_ax, f(X_ax), color=\"maroon\", alpha=0.8)\n    ax.plot(X_ax, pred, color=\"orangered\", alpha=0.8)\n    ax.scatter(X, Y, color=\"white\", s=5, alpha=0.8)\n    ax.set_xlabel(\"$X$\")\n    ax.set_ylabel(\"$Y$\")\n\nfig.tight_layout()\n```\n\n::: {.cell-output .cell-output-display}\n![](kernel-regression-transformer_files/figure-html/cell-4-output-1.png){}\n:::\n:::\n\n\nHey, this worked pretty well!\n\n### Multi-headed kernel regression\nAt this point we'll introduce yet another modification; later we'll see that it's analogous to multi-head attention.\nConsider a model with $H$ \"heads\". Each head will be a kernel with a potentially different lengthscale $\\ell_h$.\nIn this manner, we will allow different heads to capture information at a different lengthscale.\nFinally, we will combine the predictions using auxiliary parameters $u_1, \\dotsc, u_H$:\n$$\n\\hat f(x) = \\sum_{h=1}^H u_h\\, \\hat f_h(x) = \\sum_{h=1}^H u_h\\, \\sum_{i=1}^n y_i \\frac{ K(x, x_i; \\ell_h) }{ \\sum_{j=1}^n K(x, x_j; \\ell_h) }.\n$$\n\nLet's implement it quickly in Equinox:\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\nclass MultiheadGaussianKernel(eqx.Module):\n    kernels: list[GaussianKernel]\n    weights: jax.Array\n\n    def __init__(self, n_heads: int) -> None:\n        assert n_heads > 0\n\n        self.weights = jnp.full(shape=(n_heads,), fill_value=1 / n_heads)\n        self.kernels = [\n            GaussianKernel(lengthscale=l)\n            for l in jnp.linspace(0.1, 3, n_heads)\n        ]\n\n    @property\n    def lengthscale(self) -> list[float]:\n        return [k.lengthscale for k in self.kernels]\n\n    def predict(self, X_test: Float[Array, \" n_test\"], X_obs: Vector, Y_obs: Vector) -> Float[Array, \" n_test\"]:\n        # Shape (kernels, n_test)\n        preds = jnp.stack([k.predict(X_test=X_test, X_obs=X_obs, Y_obs=Y_obs) for k in self.kernels])\n        return jnp.einsum(\"kn,k->n\", preds, self.weights)\n\nfig, axs = plt.subplots(2, 2, figsize=(2*4, 2*3), dpi=150, sharex=True, sharey=True)\n\nfor n_heads, ax in zip([1, 2, 4, 8], axs.ravel()):\n    key, subkey = random.split(key)\n    \n    model = train(\n        key=subkey,\n        model=MultiheadGaussianKernel(n_heads=n_heads),\n        X=X,\n        Y=Y,\n        print_every=None,\n        hold_out_size=1,\n        n_steps=1_000,\n    )\n    pred = model.predict(X_ax, X_obs=X, Y_obs=Y)\n    ax.set_title(f\"Heads={n_heads}\") # $\\ell$={model.lengthscale:.2f}\")\n    ax.plot(X_ax, f(X_ax), color=\"maroon\", alpha=0.8)\n    ax.plot(X_ax, pred, color=\"orangered\", alpha=0.8)\n    ax.scatter(X, Y, color=\"white\", s=5, alpha=0.8)\n    ax.set_xlabel(\"$X$\")\n    ax.set_ylabel(\"$Y$\")\n\n    u_h_str = \", \".join([f\"{w:.2f}\" for w in model.weights])\n    l_h_str = \", \".join([f\"{k.lengthscale:.2f}\" for k in model.kernels])\n\n    print(f\"Number of heads: {n_heads}\")\n    print(f\"  Combination:  {u_h_str}\")\n    print(f\"  Lengthscales: {l_h_str}\")\n\nfig.tight_layout()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nNumber of heads: 1\n  Combination:  1.09\n  Lengthscales: 0.16\nNumber of heads: 2\n  Combination:  0.99, -0.10\n  Lengthscales: 0.08, 0.02\nNumber of heads: 4\n  Combination:  1.09, -0.36, -0.24, 0.10\n  Lengthscales: 0.16, 2.95, 2.18, 11.60\nNumber of heads: 8\n  Combination:  0.90, 0.02, 0.86, 0.95, -0.45, -0.53, -0.51, -0.22\n  Lengthscales: 0.08, 9.49, 20.43, 25.16, 210.44, 33.52, 34.75, 82.24\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](kernel-regression-transformer_files/figure-html/cell-5-output-2.png){}\n:::\n:::\n\n\nWe see that coefficients $u_h$ are not constrained to be positive and they do not have to sum up to 1: we allow an arbitrary linear combination of predictions, rather than a weighted sum.\nNote also that many heads allow for larger flexibility, although on such a small data set this can arguably result in some amount of overfitting. \n\n## Attention\n\nRecall the equation\n$$\n\\hat f(x) = \\sum_{i=1}^n y_i\\, \\frac{K(x, x_i; \\theta)}{ \\sum_{j=1}^n K(x, x_j; \\theta)},\n$$\nwhere there kernel $K$ is now parameterised by $\\theta$.\nAs we want the kernel to give positive values, let's write\n$$\nK(x, x'; \\theta) = \\exp s_\\theta(x, x')\n$$\nfor some function $s_\\theta$.\nHence, we can write\n$$\n\\hat f(x) = \\sum_{i=1}^n y_i \\, \\mathrm{softmax}( s_\\theta(x, x_j)_{j = 1, \\dotsc, n} ).\n$$\nThe usual approach is to use $\\theta = (W^{(q)}, W^{(k)})$ for matrices mapping from $\\mathcal X$ to some space $\\mathbb R^{d_\\text{qk}}$ and use a scalar product\n$$\ns_\\theta(x, x') = \\frac{\\left\\langle W^{(q)}x, W^{(k)}x'\\right\\rangle}{\\sqrt{d_\\text{qk}}} = \\frac{ x^T \\left(W^{(q)}\\right)^T W^{(k)}x'}{\\sqrt{d_\\text{qk}}},\n$$\nwhere the denominator takes various forms and is usually used to ensure that the values are properly normalized and the gradients can propagate through the softmax layer well.\n\nNow consider another modification. We will write $y_i = W^{(v)}x_i$ for some matrix $W^{(v)}$ mapping from $\\mathcal X$ to some space $\\mathbb R^{d_\\text{v}}$.\n(One can think that it's a restriction when it comes to the regression (as we are not using values $y_i$ as provided), but it's not really a big issue: it just suffices to relabel as \"point $x_i$\" a tuple $(x_i, y_i)$ and redefine the introduced parameter matrices, so that they first project on the required component.) \n\nIn this case, we obtain a function\n$$\nx\\mapsto \\sum_{i=1}^n W^{(v)}x_i \\, \\mathrm{softmax}\\left(  \\frac{ x^T \\left(W^{(q)}\\right)^T W^{(k)}x_i}{\\sqrt{d_\\text{qk}}}  \\right).\n$$\n\nIf we apply this formula to each $x$ from the sequence $(x_1, \\dotsc, x_n) \\in \\mathcal X^n$, we obtain a new sequence $(x_1', \\dotsc, x'_n) \\in \\left(\\mathbb R^{d_\\text{v}}\\right)^n$. This is exactly the self-attention layer used in transformers.\nHow to obtain multi-head attention? Similarly as in [multi-head kernel regression](#multi-headed-kernel-regression), we will introduce $H$ different \"heads\" with individual parameters $W^{(k)}_h, W^{(q)}_h, W^{(v)}_h$.\nHence, for each data point $x$ in the original sequence, we have $H$ vectors in $\\mathbb R^{d_\\text{v}}$ given by\n$$\nx\\mapsto \\sum_{i=1}^n W^{(v)}_hx_i \\, \\mathrm{softmax}\\left(  \\frac{ x^T \\left(W^{(q)}_h\\right)^T W^{(k)}_h x_i}{\\sqrt{d_\\text{qk}}}  \\right) \\in \\mathbb R^{d_\\text{v}}.\n$$\n\nIf we want to obtain a mapping into some vector space $\\mathcal Y$, we can now introduce matrices $U_h\\colon \\mathbb R^{d_\\text{v}}\\to \\mathcal Y$, so that in the end we have\n$$\nx\\mapsto \\sum_{h=1}^H U_h \\sum_{i=1}^n W^{(v)}_hx_i \\, \\mathrm{softmax}\\left(  \\frac{ x^T \\left(W^{(q)}_h\\right)^T W^{(k)}_h x_i}{\\sqrt{d_\\text{qk}}}  \\right) \\in \\mathcal Y.\n$$\n\nTo summarize, multi-head attention maps a sequence $(x_1, \\dotsc, x_n)\\in \\mathcal X^n$ to a sequence in $\\mathcal Y^n$ and is parameterised by $H$ tuples of matrices $(W^{(q)}_h, W^{(k)}_h, W^{(v)}_h, U_h)$, where index $h$ corresponds to the attention head.\n\nConveniently, Equinox [implements multi-head attention](https://docs.kidger.site/equinox/api/nn/attention/), from which I took dimension annotations.\n\n",
    "supporting": [
      "kernel-regression-transformer_files"
    ],
    "filters": [],
    "includes": {}
  }
}