{
  "hash": "8acac17602f22e738741db8f1a217697",
  "result": {
    "markdown": "---\ntitle: On mixtures and admixtures\ndescription: A quick look at the differences between mixture and admixture models.\nauthor: Paweł Czyż\ndate: 4/14/2024\nexecute:\n  freeze: true\nbibliography: references.bib\nformat:\n  html:\n    code-fold: true\n---\n\nConsider a binary genome vector $Y_{n\\bullet} = (Y_{n1}, \\dotsc, Y_{nG})$ representing at which loci a mutation has appeared.\nOne of the simplest models is to assume that mutations appear independently, with $\\theta_g = P(Y_{ng} = 1)$ representing the probability of mutation occurring.\nIn other words,\n$$\\begin{align*}\nP(Y_{n\\bullet} = y_{n\\bullet} \\mid \\theta_\\bullet) &= \\prod_{g=1}^G P(Y_{ng}=y_{ng} \\mid \\theta_g ) \\\\\n&= \\prod_{g=1}^G \\theta_g^{Y_{ng}}(1-\\theta_g)^{1-Y_{ng}}.\n\\end{align*}\n$$\n\nConsider the case where we observe $N$ exchangeable genomes.\nWe will assume that they are conditionally independent given the model parameters:\n$$\nP(Y_{*\\bullet} = y_{*\\bullet}\\mid \\theta_\\bullet) = \\prod_{n=1}^N P(Y_{n\\bullet}=y_{n\\bullet}\\mid \\theta_\\bullet).\n$$\n\nThere's an obligatory probabilistic graphical model representing our assumptions:\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport daft\nimport matplotlib.pyplot as plt\nplt.style.use(\"dark_background\")\n\nclass MyPGM:\n  def __init__(self, dpi: int = 200) -> None:\n    assert dpi > 1\n    self.dpi = dpi\n    self.pgm = daft.PGM(dpi=dpi)\n\n  def add_node(self, id: str, name: str, x: float, y: float, observed: bool = False):\n    if observed:\n      params={\"facecolor\": \"grey\"}\n    else:\n      params={\"edgecolor\": \"w\"}\n    self.pgm.add_node(id, name, x, y, plot_params=params)\n\n  def add_edge(self, start: str, end: str):\n    self.pgm.add_edge(start, end, plot_params={\"edgecolor\": \"w\", \"facecolor\": \"w\"})\n\n  def add_plate(\n    self,\n    coords,\n    label: str,\n    shift: float = 0.0,\n    label_offset: tuple[float, float] = (0.02, 0.02),\n  ):\n    \"\"\"\n    Args:\n      coords: [x_left, y_bottom, x_length, y_length]\n      label: label\n      shift: vertical shift\n    \"\"\"\n    label_offset = (label_offset[0] * self.dpi, label_offset[1] * self.dpi)\n    self.pgm.add_plate(coords, label=label, shift=shift, rect_params={\"edgecolor\": \"w\"}, label_offset=label_offset)\n\n  def plot(self):\n    self.pgm.render()\n    plt.show()\n\npgm = MyPGM()\n\npgm.add_node(\"D\", \"$\\\\mathcal{D}$\", 0, 1)\npgm.add_node(\"theta\", r\"$\\theta_g$\", 2, 1)\npgm.add_node(\"Y\", r\"$Y_{ng}$\", 4, 1, observed=True)\n\npgm.add_edge(\"D\", \"theta\")\npgm.add_edge(\"theta\", \"Y\")\n\npgm.add_plate([1.5, 0.5, 3, 1.5], label=r\"$g = 1, \\ldots, G$\", shift=-0.1)\npgm.add_plate([2.7, 0.55, 1.7, 1], label=r\"$n=1, \\ldots, N$\")\n\npgm.plot()\n```\n\n::: {.cell-output .cell-output-display}\n![](mixtures-and-admixtures_files/figure-html/cell-2-output-1.png){}\n:::\n:::\n\n\nwhere $\\mathcal D$ is the prior over the $\\theta_\\bullet$ vector, i.e., $\\theta_g \\mid \\mathcal D \\sim \\mathcal D$ and $\\mathcal D$ is supported on the interval $(0, 1)$.\nThe simplest choice is to fix $\\mathcal D = \\mathrm{Uniform}(0, 1)$, but this may be not flexible enough.\nNamely, different draws from the posterior on $\\theta$ may look quite different than draws from $\\mathcal D$.\nAnd this discrepancy would be easy to observe once $G$ is large.\n\nHence, the simplest improvement to this model is to make $\\mathcal D$ more flexible, i.e., treating it as a random probability measure with some prior on it.\nThe simplest possibility is to use $\\mathcal D = \\mathrm{Beta}(\\alpha, \\beta)$, where $\\alpha$ and $\\beta$ are given some gamma (say) priors, but more flexible models are possible (such as mixtures of beta distribution or a [Dirichlet process](dirichlet-process.qmd)).\n\nThis model has enough flexibility to model marginal the mutation occurrence probabilities $P(Y_{n g} = 1)$: given enough samples $N$, the posterior on $\\theta_g$ should concentrate around the average $N^{-1}\\sum_{n=1}^N Y_{ng}$ (and, provided that $\\mathcal D$ is flexible enough, it may concentrate near the distribution of these averages).\n\nGreat: this model is flexible enough to describe well the mutation probabilities.\nHowever, it's quite rigid when it comes to mutation exclusivity and cooccurrence:\n$$\nP(Y_{n1}=1, Y_{n2} = 1) = P(Y_{n1} = 1) \\cdot P(Y_{n2} = 1) = \\theta_{1}\\theta_{2}.\n$$\n\nWe generally expect that mutations in some genes could lead to [synthetic lethality](https://en.wikipedia.org/wiki/Synthetic_lethality), so they would be exclusive.\nAlso, the genes are ordered in chromosomes and [copy number aberrations](https://doi.org/10.1038/s41467-020-17967-y) can lead to mutations being simultaneously observed in several genes (e.g., if they are in the fragment of the chromosome which is frequently lost).\n\nThe discrepancies between this \"independent mutations\" model and real data is generally easy to observe by plotting the correlation matrix between different genes.\nI also like looking at the the empirical distribution of the observed number of mutations in one sample, i.e., $\\sum_{g=1}^G Y_{ng}$, as the model predictis a [Poisson binomial distribution](https://en.wikipedia.org/wiki/Poisson_binomial_distribution), while real data may show very different behaviour.\n\n## Mixture models\n\nThe model above is not flexible enough to model co-occurrences and exclusivity between different mutations.\n\nImagine that we sequence $G=3$ genes which lie very close together.\n\nIf there is no large deletion in this chromosome, they are independently mutated with probabilities $1\\%$, $5\\%$ and $10\\%$, respectively.\nHowever, if there is a large deletion, we will notice that they are all gone.\nHence, it would make sense to consider two different \"populations\" with mutation probabilities $\\theta_{1\\bullet} = (1\\%, 5\\%, 10\\%)$ and $\\theta_{2\\bullet} = (100\\%, 100\\%, 100\\%)$, where the \"population\" describes whether such a large deletion had place.\nMore generally, if we consider $K$ \"populations\", we can introduce sample-specific variables $Z_n \\in \\{1, 2, \\dotsc, K\\}$\nand the distributions\n$$\nP(Y_{n\\bullet} \\mid Z_n, \\{\\theta_{k\\bullet}\\}_{k=1, \\dotsc, K}) = P(Y_{n\\bullet} \\mid \\theta_{Z_n\\bullet}).\n$$\n\nWe will draw this model as\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\npgm = MyPGM()\n\npgm.add_node(\"pi\", r\"$\\pi$\", 0, 0)\npgm.add_node(\"Z\", \"$Z_n$\", 1, 0)\npgm.add_node(\"Y\", r\"$Y_{n\\bullet}$\", 2, 0, observed=True)\n\npgm.add_node(\"D\", r\"$\\mathcal{D}$\", 1, 1)\npgm.add_node(\"theta\", r\"$\\theta_{k\\bullet}$\", 2, 1)\n\npgm.add_edge(\"pi\", \"Z\")\npgm.add_edge(\"Z\", \"Y\")\n\npgm.add_edge(\"D\", \"theta\")\npgm.add_edge(\"theta\", \"Y\")\n\npgm.add_plate([0.5, -0.5, 2, 1], label=r\"$n=1, \\ldots, N$\", shift=-0.1)\npgm.add_plate([1.5, 0.6, 1, 1], label=r\"$k=1, \\ldots, K$\", label_offset=(0.01, 0.2))\n\npgm.plot()\n```\n\n::: {.cell-output .cell-output-display}\n![](mixtures-and-admixtures_files/figure-html/cell-3-output-1.png){}\n:::\n:::\n\n\nwhere $\\pi\\in \\Delta^{K-1}$ is proportion vector over $K$ \"populations\".\n\nDue to the conditional independence structure in this model, we can also integrate out the $Z_n$ variables to get\n$$\nP(Y_n \\mid \\{\\theta_{k\\bullet}\\}_{k=1, \\dotsc, K}, \\pi) = \\sum_{k=1}^K \\pi_k \\, P(Y_{n\\bullet} \\mid \\theta_{k\\bullet} )\n$$\n\nwhich graphically corresponds to\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\npgm = MyPGM()\n\npgm.add_node(\"pi\", r\"$\\pi$\", 0, 0)\npgm.add_node(\"Y\", r\"$Y_{n\\bullet}$\", 2, 0, observed=True)\n\npgm.add_node(\"D\", r\"$\\mathcal{D}$\", 1, 1)\npgm.add_node(\"theta\", r\"$\\theta_{k\\bullet}$\", 2, 1)\n\npgm.add_edge(\"pi\", \"Y\")\n\npgm.add_edge(\"D\", \"theta\")\npgm.add_edge(\"theta\", \"Y\")\n\npgm.add_plate([0.5, -0.5, 2, 1], label=r\"$n=1, \\ldots, N$\", shift=-0.1)\npgm.add_plate([1.5, 0.6, 1, 1], label=r\"$k=1, \\ldots, K$\", label_offset=(0.01, 0.2))\n\npgm.plot()\n```\n\n::: {.cell-output .cell-output-display}\n![](mixtures-and-admixtures_files/figure-html/cell-4-output-1.png){}\n:::\n:::\n\n\nThis integrated out representation is quite convenient for inference (see [Stan User's guide](https://mc-stan.org/docs/stan-users-guide/finite-mixtures.html)).\n\n## How expressive are finite mixtures?\n\nIn principle, the mixtures can be used to model an arbitrary distribution over binary vectors: consider $K=2^G$ and each $\\theta_{k\\bullet}$ vector to represent a different string $0$ and $1$ digits.\nHence, we see that the \"populations\" do *not* necessarily have biological meaning.\n(See also Chapter 22 of [Bayesian Data Analysis](http://www.stat.columbia.edu/~gelman/book/) or this [Cosma Shalizi's blog post](http://bactra.org/weblog/523.html) explaining the overinterpretation issues in factor analysis).\n\nAnother issue with using $K=2^G$ classes is that $2^G$ is usually *much* larger than $N$, so that finding a suitable set of parameters $\\{\\theta_{k\\bullet}\\}$ may be tricky.\nWe will generally prefer a smaller number of components, although using a [Dirichlet process mixture model](dirichlet-process.qmd) is possible (which is quite funny, because in these models $K=\\infty$ and there may be more occupied components than $2^G$, which still result in the distribution which could be modelled with $2^G$ clusters).\n\nThe mixtures (with a smaller number of components than $2^G$) are generally very popular, even if it is not always immediately clear that a model is a finite mixture: for example, [this model](https://doi.org/10.1371/journal.pcbi.1003503) can be modelled as a mixture with $K = G+1$ components for an appropriate prior constraining the $\\theta$ parameters.\n\nAnother example of such model are [Bayesian pyramids](https://doi.org/10.1093/jrsssb/qkad010): imagine that we sequence genes 1, 2, 3 on one chromosome and genes 4, 5, 6 on another chromosome, then we may consider the following idealised model employing $K=4$ \"populations\":\n\n1. $Z_n=1$ means that neither chromosome is lost. Mutations in the genes arise independently.\n2. $Z_n=2$ means that only the first chromosome is lost, i.e., genes 1, 2, 3 are mutated. The mutations at loci 4, 5, 6 can arise independently.\n3. $Z_n=3$ means that the second chromosome is lost, i.e., genes 4, 5, 6 are mutated. The mutations at loci 1, 2, 3 can arise independently.\n4. $Z_n=4$ means that both chromosomes are lost and we observe mutations in all six genes.\n\nThis model with four populations works.\nHowever, some of the parameters are tied together, as they correspond to the effects of deletion of two different chromosomes and using this structure may be important for the scalability and speed of posterior shrinkage: if we consider 10 different chromosomes, we don't need to model $2^10$ independent clusters.\nMore on this parameter constraints is in Section 3.1 of the original paper.\n(And, if you are interested in the biological applications to tumor genotypes, I'll be showing a poster at RECOMB 2024 on this topic! You can also take a look at the [Jnotype](https://github.com/cbg-ethz/jnotype) package).\n\n## Admixture models\n\nAbove we discussed that mixture models can be very expressive when they have many components (so sometimes the parameters are shared between different components).\n\nLet's think about an admixture model, which can be treated as a simplified version of the [STRUCTURE](https://web.stanford.edu/group/pritchardlab/structure.html) model.\nWe again assume that we have some probability vectors $\\theta_{k\\bullet}$ for $k=1, \\dotsc, K$.\nHowever, in this case we will call them \"topics\" or distinct mutational mechanisms which operate in the following way.\nFor each gene $Y_{ng}$ there is a mechanism $T_{ng} \\in \\{1, \\dotsc, K\\}$ which is used to generate the mutation:\n$$\n  P(Y_{ng} =1 \\mid T_{ng}, \\{\\theta_{k\\bullet}\\}_{k=1, \\dotsc, K}) = \\theta_{T_{ng}g}.\n$$\n\nThe mechanisms corresponding to different genes are drawn independently within each sample, i.e., we have a sample-specific proportion vector $\\pi_n\\in \\Delta^{K-1}$ which is used to sample the topics:\n$$\n  T_{ng} \\mid \\pi_n \\sim \\mathrm{Categorical}(\\pi_n).\n$$\n\nLet's draw the dependencies in this model:\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\npgm = MyPGM()\n\npgm.add_node(\"G\", r\"$\\mathcal{G}$\", 0, 1)\n\npgm.add_node(\"pi\", r\"$\\pi_n$\", 0, 0)\npgm.add_node(\"T\", \"$T_{ng}$\", 1, 0)\npgm.add_node(\"Y\", r\"$Y_{ng}$\", 2, 0, observed=True)\n\npgm.add_node(\"D\", r\"$\\mathcal{D}$\", 1, 1)\npgm.add_node(\"theta\", r\"$\\theta_{k\\bullet}$\", 2, 1)\n\npgm.add_edge(\"G\", \"pi\")\npgm.add_edge(\"pi\", \"T\")\npgm.add_edge(\"T\", \"Y\")\n\npgm.add_edge(\"D\", \"theta\")\npgm.add_edge(\"theta\", \"Y\")\n\npgm.add_plate([-0.5, -0.5, 3, 1], label=r\"$n=1, \\ldots, N$\", shift=-0.1)\npgm.add_plate([1.5, 0.6, 1, 1], label=r\"$k=1, \\ldots, K$\", label_offset=(0.01, 0.2))\n\npgm.add_plate([0.55, -0.45, 1.85, 0.8], label=r\"$g=1, \\ldots, G$\", label_offset=(0.25, 0.01))\n\n\npgm.plot()\n```\n\n::: {.cell-output .cell-output-display}\n![](mixtures-and-admixtures_files/figure-html/cell-5-output-1.png){}\n:::\n:::\n\n\nSimilarly as in the mixture models, we can integrate out the latent variables:\n$$\\begin{align*}\n  P(Y_{n\\bullet} \\mid \\pi_n, \\{\\theta_{k\\bullet}\\}_{k=1, \\dotsc, K}) &= \\sum_{T_{n\\bullet}} P(Y_{n\\bullet} \\mid T_{n\\bullet}, \\{\\theta_{k\\bullet}\\}_{k=1, \\dotsc, K}) P(T_{n\\bullet} \\mid \\pi_n ) \\\\\n  &= \\sum_{T_{n\\bullet}} \\prod_{g=1}^G  P( Y_{ng} \\mid \\theta_{T_{ng}g}) P(T_{ng} \\mid \\pi_n) \\\\\n  &= \\prod_{g=1}^G \\left( \\sum_{k=1}^K P(Y_{ng}\\mid \\theta_{kg})\\, \\pi_{nk} \\right)\n\\end{align*}\n$$\n\nWe see that if the proportions vector $\\pi_n$ becomes [one-hot vectors](https://en.wikipedia.org/wiki/One-hot), then the admixture model reduces to the mixture model we have seen above.\nHowever, this model is more flexible.\n\nLet's draw the version with local variables integrated out:\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\npgm = MyPGM()\n\npgm.add_node(\"G\", r\"$\\mathcal{G}$\", 0, 1)\n\npgm.add_node(\"pi\", r\"$\\pi_n$\", 0, 0)\npgm.add_node(\"Y\", r\"$Y_{ng}$\", 2, 0, observed=True)\n\npgm.add_node(\"D\", r\"$\\mathcal{D}$\", 1, 1)\npgm.add_node(\"theta\", r\"$\\theta_{k\\bullet}$\", 2, 1)\n\npgm.add_edge(\"G\", \"pi\")\npgm.add_edge(\"pi\", \"Y\")\n\npgm.add_edge(\"D\", \"theta\")\npgm.add_edge(\"theta\", \"Y\")\n\npgm.add_plate([-0.5, -0.5, 3, 1], label=r\"$n=1, \\ldots, N$\", shift=-0.1)\npgm.add_plate([1.5, 0.6, 1, 1], label=r\"$k=1, \\ldots, K$\", label_offset=(0.01, 0.2))\n\npgm.add_plate([0.55, -0.45, 1.85, 0.8], label=r\"$g=1, \\ldots, G$\", label_offset=(0.25, 0.01))\n\n\npgm.plot()\n```\n\n::: {.cell-output .cell-output-display}\n![](mixtures-and-admixtures_files/figure-html/cell-6-output-1.png){}\n:::\n:::\n\n\nThe issue in this model is that we have $O(KG)$ parameters in the $\\theta$ matrix (unless some parameter sharing is used) and additionally we have $O(NK)$ parameters of the proportion vectors.\nHence, the inference in this model may be trickier to apply.\n\n\n## How do the samples look like?\n\nLet's take $G=10$ and $K=3$ and simulate some $\\theta$ matrix:\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\nimport numpy as np\nimport seaborn as sns\n\nG = 10\nK = 3\n\nrng = np.random.default_rng(2024)\ntheta = np.zeros((K, G))\nfor k in range(K):\n  theta[k, :] = rng.beta(1, 5 + k**2, size=(G,))\n  theta[k, min(3*k, G):min(3*k+3,10)] = 0.9\n# rng.uniform(size=(K, G))\n\ntheta[0, :] = np.sort(theta[0, :])\ntheta[-1, :] = np.sort(theta[1, :])[::-1]\n\nfig, ax = plt.subplots(figsize=(4, 2), dpi=250)\nsns.heatmap(theta, vmin=0, vmax=1, cmap=\"Greys_r\", ax=ax, xticklabels=False, yticklabels=False, square=True, cbar=False)\nax.set_xlabel(\"Genes\")\nax.set_ylabel(\"Populations\")\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](mixtures-and-admixtures_files/figure-html/cell-7-output-1.png){}\n:::\n:::\n\n\nLet's sample 50 vectors from each component:\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\nz = np.asarray(sum(([k] * 50 for k in range(K)), []))\nprobs = theta[z]\nY = rng.binomial(1, probs)\n\nfig, ax = plt.subplots(figsize=(5, 2), dpi=250)\nsns.heatmap(Y.T, vmin=0, vmax=1, cmap=\"Greys_r\", ax=ax, square=False, xticklabels=False, yticklabels=False, cbar=False)\n\nax.set_xlabel(\"Samples\")\nax.set_ylabel(\"Genes\")\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](mixtures-and-admixtures_files/figure-html/cell-8-output-1.png){}\n:::\n:::\n\n\nOn the other hand, if we sample from an admixture model, where $\\pi_n \\sim \\mathrm{Dirichlet}(\\alpha, \\alpha, \\alpha)$ for different values of $\\alpha$, we will get the following proportion vectors:\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\nN = 150\n\nalphas = [0.001, 0.1, 10.0]\nfig, axs = plt.subplots(3, 1, figsize=(7, 6), dpi=250, sharex=True, sharey=True)\n\npis = np.zeros((len(alphas), N, K))\n\nfor i, (alpha, ax) in enumerate(zip(alphas, axs)):\n  pi = rng.dirichlet(alpha * np.ones(K), size=N)\n  index = np.argsort( np.einsum(\"nk,k->n\", pi, np.arange(K)))\n  pi = pi[index, :]\n  pis[i, ...] = pi\n\n  x_axis = np.arange(0.5, N + 0.5)\n\n  prev = 0.0\n  for k in range(K):\n    ax.fill_between(x_axis, prev, prev + pi[:, k], color=f\"C{k+4}\")\n    prev = prev + pi[:, k]\n\n  ax.spines[[\"top\", \"right\"]].set_visible(False)\n  ax.set_xlabel(\"Samples\")\n  ax.set_ylabel(\"Proportions\")\n  ax.set_xticks([])\n  ax.set_title(f\"$\\\\alpha$: {alpha:.3f}\")\n\nfig.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](mixtures-and-admixtures_files/figure-html/cell-9-output-1.png){}\n:::\n:::\n\n\nNote that for $\\alpha \\approx 0$ the proportion vectors are close to one-hot, so that the admixture reduces to the mixture above. \n\nWe can generate the following samples corresponding to proportion vectors above as following:\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\nfig, axs = plt.subplots(3, 1, figsize=(7, 6), dpi=250, sharex=True, sharey=True)\n\nfor alpha, pi, ax in zip(alphas, pis, axs):\n  Y = np.zeros((N, G), dtype=int)\n\n  for n in range(N):\n    T_ = rng.multinomial(1, pi[n], size=G)\n    T = T_ @ np.arange(K)\n    probs = theta[T, np.arange(G)]\n    Y[n, :] = rng.binomial(1, probs)\n\n  sns.heatmap(Y.T, vmin=0, vmax=1, cmap=\"Greys_r\", ax=ax, square=False, xticklabels=False, yticklabels=False, cbar=False)\n  ax.set_xlabel(\"Samples\")\n  ax.set_ylabel(\"Genes\")\n  ax.set_title(f\"Sparsity: {alpha:.3f}\")\n\nfig.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](mixtures-and-admixtures_files/figure-html/cell-10-output-1.png){}\n:::\n:::\n\n\n## Links\n\n- We discussed mixture models in the [Dirichlet process](dirichlet-process.qmd).\n- There is an excellent introduction to mixture and admixture models in Jeff Miller's [Bayesian methodology in biostatistics](http://jwmi.github.io/BMB/index.html) course (see lectures 9–12).\n- Nicola Roberts used hierarchical Dirichlet processes (which are also an admixture model, although of a bit different kind) to study mutational patterns [during her PhD](https://doi.org/10.17863/CAM.22674).\n- Barbara Engelhardt and Matthew Stephens wrote a [nice paper](https://doi.org/10.1371/journal.pgen.1001117) interpreting admixture models as matrix factorizations. Of course, mixture models (being a special case of an admixture) also fit in this framework.\n- Quantification is naturally related to admixture models. We discussed it [here](em-gibbs-quantification.qmd) and [there](../publications/bayesian-quantification.qmd).\n\n",
    "supporting": [
      "mixtures-and-admixtures_files"
    ],
    "filters": [],
    "includes": {}
  }
}