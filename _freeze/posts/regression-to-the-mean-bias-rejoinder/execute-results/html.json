{
  "hash": "0de2df756c5e9abffe4e1b998dad4e88",
  "result": {
    "markdown": "---\ntitle: Regression to the mean and biased predictions\ndescription: We take a closer look at an example where adding some bias can help with making predictions.\nauthor: Paweł Czyż\ndate: 2/4/2024\nexecute:\n  freeze: true\nformat:\n  html:\n    code-fold: true\n---\n\nIn September 2008 [*Bayesian Analysis*, vol. 3, issue 3](https://projecteuclid.org/journals/bayesian-analysis/volume-3/issue-3) featured a wonderful discussion between five statistics superstars: [Andrew Gelman](http://www.stat.columbia.edu/~gelman/), [José M. Bernardo](https://en.wikipedia.org/wiki/Jos%C3%A9-Miguel_Bernardo), [Joseph B. Kadane](https://en.wikipedia.org/wiki/Joseph_Born_Kadane), [Stephen Senn](http://www.senns.uk/) and [Larry Wasserman](https://www.stat.cmu.edu/~larry/).\n\nThe discussion is a great read on foundations of Bayesian statistics (and it's open access!), but we will not summarise it today. Instead, let's focus on an example from Andrew Gelman's [*Rejoinder*](https://projecteuclid.org/journals/bayesian-analysis/volume-3/issue-3/Rejoinder/10.1214/08-BA318REJ.full) on regression to the mean and unbiased predictions.\n\n## The example\n\nAndrew Gelman considers a problem in which one tries to estimate the height of adult daughter, $Y$, from the height of her mother, $X$.\nConsider an artificial scenario, where we have millions of data points, which we can use to estimate the joint probability distribution $(X, Y)$ and it turns out to be bivariate normal[^1] of the following form:\n$$\n\\begin{pmatrix} X\\\\Y \\end{pmatrix} \\sim \\mathcal N\\left(\\begin{pmatrix}\\mu\\\\\\mu\\end{pmatrix}, \\sigma^2\\begin{pmatrix}  1 & 0.5 \\\\ 0.5 & 1\\end{pmatrix}  \\right)\n$$\nwith known $\\mu$ and $\\sigma$, say, $\\mu=160$ and $\\sigma=10$ in centimeters.\n\n[^1]: Yes, height can't be negative. But let's ignore that: it's a toy, but still informative, problem.\n\nThe marginal distributions on both $X$ and $Y$ are the same: $\\mathcal N(\\mu, \\sigma^2)$. \n\nLet's plot all of them:\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plt\nplt.style.use(\"dark_background\")\n\ndef generate_data(\n    rng,\n    mu: float,\n    sigma: float,\n    n_points: int,\n) -> np.ndarray:\n    var = np.square(sigma)\n    return rng.multivariate_normal(\n        mean=(mu, mu),\n        cov=var * np.asarray([\n            [1.0, 0.5],\n            [0.5, 1.0],\n        ]),\n        size=n_points,\n    )\n\n\nrng = np.random.default_rng(42)\n\nmu = 160\nsigma = 10\n\ndata = generate_data(rng, mu=mu, sigma=sigma, n_points=5_000)\n\nfig, axs = plt.subplots(1, 2, figsize=(4.5, 2.2), dpi=170, sharex=True)\n\nax = axs[0]\nax.scatter(data[:, 0], data[:, 1], c=\"C0\", s=1, alpha=0.1, rasterized=True)\nax.set_title(\"Joint\")\nax.set_xlabel(\"$X$\")\nax.set_ylabel(\"$Y$\")\n\nlim = (mu - 4*sigma, mu + 4*sigma)\n\nax.set_xlim(*lim)\nax.set_ylim(*lim)\n\nax = axs[1]\nax.hist(data[:, 0], color=\"C1\", bins=np.linspace(*lim, 21), rasterized=True, density=True)\nax.set_title(\"Marginal\")\nax.set_xlim(*lim)\nax.set_xlabel(\"$X$\")\nax.set_ylabel(\"PDF\")\n\nfor ax in axs:\n  ax.spines[['top', 'right']].set_visible(False)\n\nfig.tight_layout()\n```\n\n::: {.cell-output .cell-output-display}\n![](regression-to-the-mean-bias-rejoinder_files/figure-html/cell-2-output-1.png){}\n:::\n:::\n\n\nThe [conditional distributions](https://en.wikipedia.org/wiki/Multivariate_normal_distribution#Bivariate_case_2) are also normal and take very similar form:\n$$\nY\\mid X=x \\sim \\mathcal N\\big( \\mu + 0.5(x-\\mu), 0.75\\sigma^2\\big),\n$$\nand\n$$\nX\\mid Y=y \\sim \\mathcal N\\big( \\mu + 0.5(y-\\mu), 0.75\\sigma^2\\big).\n$$\n\nLet's plot the conditional distribution in the fist panel. We will plot conditional distributions using mean $\\mathbb E[Y\\mid X=x]$ and the intervals representing one, two, and three standard deviations from it. \n\nThen, in the second panel we will overlay it with points and the line $y=x$ (dashed red line).\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nfig, axs = plt.subplots(1, 2, figsize=(4.5, 2.2), dpi=170, sharex=True, sharey=True)\n\nx_ax = np.linspace(*lim, 51)\nmean_pred = mu + 0.5 * (x_ax-mu)\n\ndef plot_conditional(ax):\n    ax.set_xlim(*lim)\n    ax.set_ylim(*lim)\n\n    for n_sigma in [1, 2, 3]:\n        band = n_sigma * np.sqrt(0.75) * sigma\n        ax.fill_between(\n            x_ax,\n            mean_pred - band,\n            mean_pred + band,\n            alpha=0.1,\n            color=\"yellow\",\n            edgecolor=None,\n        )\n\n    ax.plot(\n        x_ax,\n        mean_pred,\n        c=\"white\",\n    )\n\nax = axs[0]\nplot_conditional(ax)\nax.set_xlabel(\"$X$\")\nax.set_ylabel(r\"$Y\\mid X$\")\n\nax = axs[1]\nplot_conditional(ax)\nax.set_xlabel(\"$X$\")\nax.set_ylabel(\"$Y$\")\nax.scatter(data[:, 0], data[:, 1], c=\"C0\", s=1, alpha=0.2, rasterized=True)\nax.plot(x_ax, x_ax, c=\"maroon\", linestyle=\"--\")\n\nfor ax in axs:\n  ax.spines[['top', 'right']].set_visible(False)\n\nfig.tight_layout()\n```\n\n::: {.cell-output .cell-output-display}\n![](regression-to-the-mean-bias-rejoinder_files/figure-html/cell-3-output-1.png){}\n:::\n:::\n\n\nThe $y=x$ has greater slope than $\\mathbb E[Y\\mid X=x]$ (namely, 1 is greater than 0.5), which is the usual [regression to the mean](https://en.wikipedia.org/wiki/Regression_toward_the_mean).\n\n## Biased and unbiased estimators\n\nWe have seen that we could do probabilistic prediction, returning the whole conditional distribution $P(Y\\mid X=x)$.\nImagine however that a single point is required as the answer. We can take $\\mathbb E[Y\\mid X=x]$ as one estimator (solid line in the previous plot).\nMore generally, for every number $f\\in \\mathbb R$ define an estimator\n$$\n\\hat Y_f = \\mu + f\\cdot (X-\\mu).\n$$\n\nWe have the following:\n\n- For $f=0$, we have $\\hat Y_0 = \\mu$ is constantly predicting the mean. \n- For $f=0.5$, $\\hat Y_{0.5} = 0.5(X + \\mu)$ is the regression to the mean we have seen above.\n- For $f=1$, $\\hat Y_1 = X$ returns the input, which we also have seen above.\n\nLet's take a look at the bias and variance[^2] of these estimators.\n\n[^2]: It would be a wasted opportunity to not mention [this wonderful joke](https://youtu.be/ZmbrsbYwRWw?feature=shared&t=349). The whole lecture is a true gem.\n\nWe have \n$$\n(X\\mid Y=y) \\sim \\mathcal N\\big(\\mu + 0.5(y-\\mu), 0.75\\sigma^2\\big)\n$$\nand $\\hat Y_f = f\\cdot X + \\mu(1-f)$, meaning that\n$$\n\\mathbb{V}[ \\hat Y_f \\mid Y=y ] = f^2 \\cdot \\mathbb{V}[X \\mid Y=y] = 0.75 f^2\\sigma^2\n$$\nand\n$$\n\\begin{align*}\\mathbb E[ \\hat Y_f \\mid Y=y ] &= \\mu(1-f) + f\\cdot \\mathbb E[X \\mid Y=y ]\\\\ &= \\mu(1-f) + f\\cdot ( \\mu + 0.5(y-\\mu) ) \\\\ &=\\mu-\\mu f + f\\mu + 0.5 f\\cdot (y-\\mu) \\\\\n&= \\mu + 0.5f\\cdot (y-\\mu) \\\\ &= 0.5 f\\cdot y +\\mu(1-0.5f)\\end{align*}\n$$\n\nHence, for $f=0.5$ we have\n$$\n\\mathbb E[\\hat Y_{0.5}\\mid Y=y] = 0.25y + 0.75\\mu,\n$$\nwhich is biased towards the mean.\n\nTo have an unbiased estimate, consider $f=2$:\n$$\n\\mathbb E[\\hat Y_2\\mid Y=y] = y,\n$$\nwhich however has the form $(\\hat Y_2 \\mid X=x) = \\mu + 2(x-\\mu)$, which amplifies the measured distance from the mean!\n\n### Visualisations\n\nLet's spend a minute designing the plots and then visualise the estimators.\n\nThe raw data are visualised by plotting $Y$ and $X$.\nWe can add the lines $\\hat Y_f\\mid X=x$ to them, to add some information on how $\\hat Y_f$ (which is a deterministic function of $X$) varies, and what the predictions are.\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\ndef yhat(x: np.ndarray, f: float) -> np.ndarray:\n    return mu + f * (x - mu)\n\nfig, ax = plt.subplots(figsize=(3, 3), dpi=150)\n\n\nax.scatter(data[:, 0], data[:, 1], c=\"C0\", s=1, alpha=0.2, rasterized=True)\n\nfs = [0, 0.5, 1, 2]\ncolors = [\"orange\", \"white\", \"maroon\", \"purple\"]\n\nfor f, col in zip(fs, colors):\n    ax.plot(\n        x_ax,\n        yhat(x_ax, f),\n        color=col,\n        label=f\"$f=${f:.1f}\",\n    )\n\nax.set_xlabel(\"$X$\")\nax.set_ylabel(\"$Y$\")\nax.set_xlim(*lim)\nax.set_ylim(*lim)\nax.spines[['top', 'right']].set_visible(False)\nax.legend(frameon=False)\nfig.tight_layout()\n```\n\n::: {.cell-output .cell-output-display}\n![](regression-to-the-mean-bias-rejoinder_files/figure-html/cell-4-output-1.png){}\n:::\n:::\n\n\nThen, we can also look at the plot of $\\hat Y_f$ and $Y$. This will be a good illustration showing what bias and variance of these estimators actually mean. We will add a dashed \"diagonal\" line, $\\hat y_f = y$, to each of these plots.\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\nfig, axs = plt.subplots(1, len(fs), figsize=(2 * len(fs), 2), dpi=150, sharex=True, sharey=True)\n\nfor f, ax in zip(fs, axs):\n    x, y = data[:, 0], data[:, 1]\n    y_ = yhat(x, f)\n    ax.scatter(y, y_, c=\"C2\", s=1, alpha=0.2, rasterized=True)\n    ax.plot(\n        x_ax, x_ax\n    )\n    ax.set_xlim(*lim)\n    ax.set_ylim(np.min(y_), np.max(y_))\n    ax.set_title(f\"$f=${f:.1f}\")\n    ax.set_xlabel(\"$Y$\")\n    ax.set_ylabel(f\"$\\hat Y_f$\")\n\nfor ax in axs:\n    ax.spines[['top', 'right']].set_visible(False)\nfig.tight_layout()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n/tmp/ipykernel_43846/2941644956.py:11: UserWarning: Attempting to set identical low and high ylims makes transformation singular; automatically expanding.\n  ax.set_ylim(np.min(y_), np.max(y_))\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](regression-to-the-mean-bias-rejoinder_files/figure-html/cell-5-output-2.png){}\n:::\n:::\n\n\nThe bias can be seen in the following manner: for each value $Y$, the values of $\\hat Y$ corresponding to that $Y$ should be distributed in such a way that the mean lies on the line.\nThis actually will be easier to see once we plot the difference $\\hat Y_f - \\hat Y$ and $Y$.\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\nfig, axs = plt.subplots(1, len(fs), figsize=(2 * len(fs), 2), dpi=150, sharex=True, sharey=True)\n\nfor f, ax in zip(fs, axs):\n    x, y = data[:, 0], data[:, 1]\n    y_ = yhat(x, f)\n    ax.scatter(y, y_ - y, c=\"C2\", s=1, alpha=0.2, rasterized=True)\n    ax.plot(\n        x_ax, np.zeros_like(x_ax)\n    )\n    ax.set_xlim(*lim)\n    ax.set_title(f\"$f=${f:.1f}\")\n    ax.set_xlabel(\"$Y$\")\n    ax.set_ylabel(f\"$\\hat Y_f - Y$\")\n\nfor ax in axs:\n    ax.spines[['top', 'right']].set_visible(False)\nfig.tight_layout()\n```\n\n::: {.cell-output .cell-output-display}\n![](regression-to-the-mean-bias-rejoinder_files/figure-html/cell-6-output-1.png){}\n:::\n:::\n\n\nWe also see what how variance manifests: for $Y\\approx 160$, we have quite a range of corresponding $\\hat Y_2$.\nThe estimator $\\hat Y_{0.5}$ has clear bias for $Y$ far from the mean, but for most of the data points (which lie close the mean in this case) the prediction is reasonable. \n\nIn fact, let's plot $\\mathbb E[|\\hat Y_f -y | \\mid y]$ and  $\\sqrt{\\mathbb E[(\\hat Y_f-y)^2\\mid y]}$ as a function of $y$:\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\nn_y_hat_samples = 10_000\ny_ax = np.linspace(mu - 4 * sigma, mu + 4 * sigma, 51)\n\n# Shape (n_y, n_y_hat_samples)\nx_samples = mu + 0.5 * (y_ax[:, None] - mu) + rng.normal(loc=0, scale=np.sqrt(0.75) * sigma, size=(y_ax.shape[0], n_y_hat_samples))\n\nfig, axs = plt.subplots(1, 2, figsize=(2.5*2+1, 2.5), dpi=150, sharex=True, sharey=True)\n\nax = axs[0]\nfor f in fs:\n    preds = yhat(x_samples, f)\n    loss = np.mean(np.abs(preds - y_ax[:, None]), axis=1)\n    ax.plot(y_ax, loss, label=f\"$f=${f:.1f}\")\n\n# ax.legend(frameon=False)\nax.set_xlabel(\"$y$\")\nax.set_ylabel(r\"$E[|\\hat Y_f-y| \\mid y ]$\")\n\nax = axs[1]\nfor f in fs:\n    preds = yhat(x_ax, f)\n    loss = np.sqrt(np.mean(np.square(preds - y_ax[:, None]), axis=1))\n    ax.plot(y_ax, loss, label=f\"$f=${f:.1f}\")\n\nax.legend(frameon=False, bbox_to_anchor=(1.05, 1.0))\nax.set_xlabel(\"$y$\")\nax.set_ylabel(r\"$\\sqrt{E[(\\hat Y_f-y)^2 \\mid y ]}$\")\n\nfor ax in axs:\n    ax.spines[['top', 'right']].set_visible(False)\nfig.tight_layout()\n```\n\n::: {.cell-output .cell-output-display}\n![](regression-to-the-mean-bias-rejoinder_files/figure-html/cell-7-output-1.png){}\n:::\n:::\n\n\n### Twisting the problem\n\nThe above plots condition on unobserved parameter $y$.\nLet's think what happens when we observe the value $X$ and we want to know how far our estimate $\\hat Y_f$ is from the unobserved value $Y$.\nWe can do a variant of one plots we have seen previously, where we put $X$ on the horizontal axis and $\\hat Y_f-Y$ on the vertical one:\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\nfig, axs = plt.subplots(1, len(fs), figsize=(2 * len(fs), 2), dpi=150, sharex=True, sharey=True)\n\nfor f, ax in zip(fs, axs):\n    x, y = data[:, 0], data[:, 1]\n    y_ = yhat(x, f)\n    ax.scatter(x, y_ - y, c=\"C2\", s=1, alpha=0.2, rasterized=True)\n    ax.plot(\n        x_ax, np.zeros_like(x_ax)\n    )\n    ax.set_xlim(*lim)\n    ax.set_title(f\"$f=${f:.1f}\")\n    ax.set_xlabel(\"$X$\")\n    ax.set_ylabel(f\"$\\hat Y_f - Y$\")\n\nfor ax in axs:\n    ax.spines[['top', 'right']].set_visible(False)\nfig.tight_layout()\n```\n\n::: {.cell-output .cell-output-display}\n![](regression-to-the-mean-bias-rejoinder_files/figure-html/cell-8-output-1.png){}\n:::\n:::\n\n\nWe see that for all values of $X$, the regression to the mean estimator, $\\hat Y_{0.5}$, works well.\nLet's also take a look at the following losses, measuring how wrong our predictions will be on average for a given value of $X$:\n$$\\begin{align*}\n\\ell_1(x) &= \\mathbb E\\left[\\left| Y_f - Y \\right| \\mid X=x \\right], \\\\\n\\ell_2(x) &= \\sqrt{\\mathbb E\\left[\\left( \\hat Y_f - Y \\right)^2 \\mid X=x \\right]}.\n\\end{align*}\n$$\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\nn_y_samples = 10_000\n\n# Shape (n_x, n_y_samples)\ny_samples = mu + 0.5 * (x_ax[:, None] - mu) + rng.normal(loc=0, scale=np.sqrt(0.75) * sigma, size=(x_ax.shape[0], n_y_samples))\n\nfig, axs = plt.subplots(1, 2, figsize=(2.5*2 + 1, 2.5), dpi=150, sharex=True, sharey=True)\n\nax = axs[0]\nfor f in fs:\n    preds = yhat(x_ax, f)\n    loss = np.mean(np.abs(y_samples - preds[:, None]), axis=1)\n    ax.plot(x_ax, loss, label=f\"$f=${f:.1f}\")\n\nax.set_xlabel(\"$x$\")\nax.set_ylabel(r\"$\\ell_1$\")\n\nax = axs[1]\nfor f in fs:\n    preds = yhat(x_ax, f)\n    loss = np.sqrt(np.mean(np.square(y_samples - preds[:, None]), axis=1))\n    ax.plot(x_ax, loss, label=f\"$f=${f:.1f}\")\n\nax.set_xlabel(\"$x$\")\nax.set_ylabel(r\"$\\ell_2$\")\n\nax.legend(frameon=False, bbox_to_anchor=(1.05, 1.0))\n\nfor ax in axs:\n    ax.spines[['top', 'right']].set_visible(False)\nfig.tight_layout()\n```\n\n::: {.cell-output .cell-output-display}\n![](regression-to-the-mean-bias-rejoinder_files/figure-html/cell-9-output-1.png){}\n:::\n:::\n\n\nEven for values of $X$ quite far from the mean, [some bias helps](https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff#Bias%E2%80%93variance_decomposition_of_mean_squared_error)! Overall, we see that regression to the mean is a sensible strategy in this case.\n\nAlso, using $f=1$ (i.e., $\\hat Y_f=X$) performs as well as $f=0$ (constant prediction $\\mu$).\nMore precisely, observe that\n$$\n\\left(Y - \\hat Y_0 \\mid X=x\\right) = (Y-\\mu \\mid X=x) \\sim \\mathcal N\\left( 0.5(x-\\mu), 0.75 \\sigma^2\\right)\n$$\nand\n$$\n\\left(Y - \\hat Y_1 \\mid X=x\\right) = (Y-X \\mid X=x) \\sim \\mathcal N\\left( 0.5(\\mu - x) , 0.75\\sigma^2 \\right)\n$$\n\nand the absolute value (or squaring) makes the losses exactly equal.\nThis is also visible in the plot representing $X$ and $Y_f - Y$.\n\nAs a final note, one can notice that for every $x$, both $\\ell_1$ and $\\ell_2$ are minimised for $f=0.5$ using the following argument: $f=0.5$ predicts the mean of the conditional distribution $Y\\mid X=x$.\nThe sum of squares (in this case we have actually expectation, but let's not worry about averaging) is minimised for the mean.\nSimilarly, median optimises the sum of absolute deviations, and each of the normal distributions representing $Y-\\hat Y_f \\mid X=x$ has mean equal to median.\n\n## Digression: simple linear regression and PCA\n\nThis section perhaps may be distracting and shouldn't really be a part of this post, but I couldn't resist: I still very much like this digression and I'm grateful for the opportunity to figure it out together with David.\n\n### Simple linear regression\n\nWhat would happen if we fitted [simple linear regression](https://en.wikipedia.org/wiki/Simple_linear_regression), $y=a+bx$?\nAs we assume that we have a very large sample size, sample covariance is pretty much the same as the population covariance. Hence, the slope is given by\n$$\nb = \\mathrm{Cov}(X, Y)/\\mathbb{V}[X] = 0.5\\sigma^2/\\sigma^2 = 0.5\n$$\n\nand the intercept is\n$$\n    a = \\mathbb E[Y] - b\\cdot \\mathbb E[X] = \\mu - 0.5\\mu = 0.5\\mu,\n$$\nso that the line is $y = 0.5(\\mu + x)$ and corresponds to the regression to the mean estimator $\\hat Y_{0.5}$.\nIt shouldn't be surprising: simple linear regression minimises the overall squared error.\nFor each value $x$ of $X$ we actually know that it should be $\\mathbb E[Y\\mid X=x]$, which is exactly $\\hat Y_{0.5}$.\n\n### Principal component analysis\n\nWhat is the [first principal component](https://en.wikipedia.org/wiki/Principal_component_analysis)?\nFor PCA we center the data, so that the principal component will be passing through $(\\mu, \\mu)$.\nTo find the slope, we need to find the eigenvector corresponding to the largest value of the covariance matrix of the data.\nWe don't really need to worry about the positive $\\sigma^2$ factor, so let's find the eigenvector of the matrix\n$$\n\\begin{pmatrix}\n    1 & 0.5\\\\\n    0.5 & 1\n\\end{pmatrix}.\n$$\n\nThe largest eigenvalue is $1.5$ with an eigenvector $(1, 1)$ (and the other eigenvalue is $0.5$ with eigenvector, of course orthogonal, $(-1, 1)$).\nHence, the line describing the principal component is given by $(x-\\mu, y-\\mu) = t (1, 1)$, where $t\\in \\mathbb R$, which is the same line as $y=x$.\nWe see that this is essentially the $\\hat Y_1$ estimator.\n\n",
    "supporting": [
      "regression-to-the-mean-bias-rejoinder_files"
    ],
    "filters": [],
    "includes": {}
  }
}