{
  "hash": "72887cd80e2a7b25cf5e2b848721b3b8",
  "result": {
    "markdown": "---\ntitle: Histograms or kernel density estimators?\ndescription: We take a look at summarizing samples with histograms and kernel density estimators.\nauthor: Paweł Czyż\ndate: 8/1/2023\ncategories:\n  - Visualisation\nexecute:\n  freeze: true\nformat:\n  html:\n    code-fold: true\n---\n\nI have recently seen [Michael Betancourt's talk](https://www.youtube.com/live/2hzoMqX1Q-k?feature=share&t=439) in which he explains why kernel density estimators can be misleading when visualising samples and points to his wonderful [case study](https://betanalpha.github.io/assets/case_studies/sampling.html#3_Approximating_Probability_Distributions_with_Samples) which includes comparison between histograms and kernel density estimators, as well as many other things.\n\nI recommend reading this case study in depth; in this blog post we will only try to reproduce the example with kernel density estimators in Python.\n\n## Problem setup\n\nWe will start with a Gaussian mixture with two components and draw the exact [probability density function](https://en.wikipedia.org/wiki/Probability_density_function) (PDF) as well as a histogram with a very large sample size.\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\nimport numpy as np \nfrom scipy import stats\n\nplt.style.use(\"dark_background\")\n\nclass GaussianMixture:\n  def __init__(self, proportions, mus, sigmas) -> None:\n    proportions = np.asarray(proportions)\n    self.proportions = proportions / proportions.sum()\n    assert np.min(self.proportions) > 0\n\n    self.mus = np.asarray(mus)\n    self.sigmas = np.asarray(sigmas)\n\n    n = len(self.proportions)\n    self.n_classes = n\n    assert self.proportions.shape == (n,)\n    assert self.mus.shape == (n,)\n    assert self.sigmas.shape == (n,)\n\n  def sample(self, rng, n: int) -> np.ndarray:\n    z = rng.choice(\n      self.n_classes,\n      p=self.proportions,\n      replace=True,\n      size=n,\n    )\n    return self.mus[z] + self.sigmas[z] * rng.normal(size=n)\n\n  def pdf(self, x):\n    ret = 0\n    for k in range(self.n_classes):\n      ret += self.proportions[k] * stats.norm.pdf(x, loc=self.mus[k], scale=self.sigmas[k])\n    return ret\n\nmixture = GaussianMixture(\n  proportions=[2, 1],\n  mus=[-2, 2],\n  sigmas=[1, 1],\n)\n\nrng = np.random.default_rng(32)\n\nlarge_data = mixture.sample(rng, 100_000)\n\nx_axis = np.linspace(np.min(large_data), np.max(large_data), 101)\npdf_values = mixture.pdf(x_axis)\n\nfig, ax = plt.subplots(figsize=(3, 2), dpi=100)\n\nax.hist(large_data, bins=150, density=True, histtype=\"stepfilled\", alpha=0.5, color=\"C0\")\nax.plot(x_axis, pdf_values, c=\"C2\", linestyle=\"--\")\n\nax.set_title(\"Probability density function\\nand histogram with large sample size\")\n```\n\n::: {.cell-output .cell-output-display execution_count=1}\n```\nText(0.5, 1.0, 'Probability density function\\nand histogram with large sample size')\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](histograms-vs-density-estimation_files/figure-html/cell-2-output-2.png){}\n:::\n:::\n\n\nGreat, histogram with large sample size agreed well with the exact PDF!\n\n## Plain old histograms\n\nLet's now move to a more challenging problem: we have only a moderate sample size available, say 100 points.\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\ndata = mixture.sample(rng, 100)\n\nfig, axs = plt.subplots(5, 1, figsize=(3.2, 3*5), dpi=100)\nbin_sizes = (3, 5, 10, 20, 50)\n\nfor bins, ax in zip(bin_sizes, axs):\n  ax.hist(data, bins=bins, density=True, histtype=\"stepfilled\", alpha=0.5, color=\"C0\")\n  ax.plot(x_axis, pdf_values, c=\"C2\", linestyle=\"--\")\n\n  ax.set_title(f\"{bins} bins\")\n\nfig.tight_layout()\n```\n\n::: {.cell-output .cell-output-display}\n![](histograms-vs-density-estimation_files/figure-html/cell-3-output-1.png){}\n:::\n:::\n\n\nWe see that too few bins (three, but nobody will actually choose this number for 100 data points) we don't see two modes and that for more than 20 and 50 bins the histogram looks quite noisy.\nBoth 5 and 10 bins would make a sensible choice in this problem.\n\n## Kernel density estimators\n\nNow it's the time for kernel density estimators. We will use several kernel families and several different bandwidths:\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nfrom sklearn.neighbors import KernelDensity\n\n\nkernels = [\"gaussian\", \"tophat\", \"cosine\"]\nbandwidths = [0.1, 1.0, 3.0, \"scott\", \"silverman\"]\n\nfig, axs = plt.subplots(\n  len(kernels),\n  len(bandwidths),\n  figsize=(12, 8),\n  dpi=130,\n)\n\nfor i, kernel in enumerate(kernels):\n  axs[i, 0].set_ylabel(f\"Kernel: {kernel}\")\n  for j, bandwidth in enumerate(bandwidths):\n    ax = axs[i, j]\n\n    kde = KernelDensity(bandwidth=bandwidth, kernel=kernel)\n    kde.fit(data[:, None])\n\n    kde_pdf = np.exp(kde.score_samples(x_axis[:, None]))\n\n    ax.plot(x_axis, pdf_values, c=\"C2\", linestyle=\"--\")\n    ax.fill_between(x_axis, 0.0, kde_pdf, color=\"C0\", alpha=0.5)\n\n\nfor j, bandwidth in enumerate(bandwidths):\n  axs[0, j].set_title(f\"Bandwidth: {bandwidth}\")\n\nfig.tight_layout()\n```\n\n::: {.cell-output .cell-output-display}\n![](histograms-vs-density-estimation_files/figure-html/cell-4-output-1.png){}\n:::\n:::\n\n\nI see the point now! Apart from the small bandwidth case (0.1 and sometimes Silverman) the issues with KDE plots are hard to diagnose.\nMoreover, conclusions from different plots are different: is the distribution multimodal? If so, how many modes are there? What are the \"probability masses\" of each modes? Observing only one of these plots can lead to wrong conclusions.\n\n## Links\n\n  - [What's wrong with a kernel density](https://statmodeling.stat.columbia.edu/2009/11/25/whats_wrong_wit/): a blog post by Andrew Gelman, explaining why he prefers histograms over kernel density plots.\n  - [Michael Betancourt's case study](https://betanalpha.github.io/assets/case_studies/sampling.html#3_Approximating_Probability_Distributions_with_Samples), which also discusses histograms with error bars.\n\n",
    "supporting": [
      "histograms-vs-density-estimation_files"
    ],
    "filters": [],
    "includes": {}
  }
}