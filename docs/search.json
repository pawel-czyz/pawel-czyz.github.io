[
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "Publications",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Author\n        \n         \n          Publication\n        \n         \n          Year\n        \n     \n  \n    \n      \n      \n    \n\n\n  \n    On the properties and estimation of pointwise mutual information profiles\n    P. Czyż, F. Grabowski, J.E. Vogt, N. Beerenwinkel, A. Marx\n    Transactions on Machine Learning Research\n    (2025)\n    \n      Details\n    \n    \n       Publication\n    \n    \n       Code\n    \n  \n  \n    Bayesian modeling of mutual exclusivity in cancer mutations\n    P. Czyż, N. Beerenwinkel\n    Preprint\n    (2024)\n    \n      Details\n    \n    \n       Preprint\n    \n    \n       Code\n    \n  \n  \n    Bayesian quantification with black-box estimators\n    A. Ziegler, P. Czyż\n    Transactions on Machine Learning Research\n    (2024)\n    \n      Details\n    \n    \n       Publication\n    \n    \n       Code\n    \n  \n  \n    Beyond normal: on the evaluation of mutual information estimators\n    P. Czyż, F. Grabowski, J.E. Vogt, N. Beerenwinkel, A. Marx\n    Conference on Neural Information Processing Systems (NeurIPS)\n    (2023)\n    \n      Details\n    \n    \n       Publication\n    \n    \n       Code\n    \n  \n  \n    Limits to the rate of information transmission through the MAPK pathway\n    F. Grabowski, P. Czyż, M. Kochańczyk, T. Lipniacki\n    Journal of The Royal Society Interface\n    (2019)\n    \n      Details\n    \n    \n       Publication\n    \n    \n       Code\n    \n  \n\n\nNo matching items\n\n\n\nThis page was created using the following template."
  },
  {
    "objectID": "private/prezenty.html",
    "href": "private/prezenty.html",
    "title": "Lista prezentów",
    "section": "",
    "text": "Drogi Święty Mikołaju, bardzo mi miło, że rozważasz podarowanie mi prezentu! Staram się zostać minimalistą (walczącym z książkoholizmem), a to oznacza, że nie lubię posiadać zbyt wielu rzeczy. Jeśli więc nadal rozważasz podarowanie mi czegoś, przygotowałem listę rzeczy, z których jednak będę bardzo zadowolony.\n\nKartka lub list.\nInformacja o datku na dowolnie wybraną fundację charytatywną. Na przykład UNICEF, Krajowy Fundusz na rzecz Dzieci czy Against Malaria, ale istnieje znacznie więcej różnych metod pomagania.\nOłówek.\nCienkopis.\nNaboje atramentowe LAMY T10, najchętniej w kolorze granatowym, czarnym lub zielonym."
  },
  {
    "objectID": "publications/limits-mapk.html",
    "href": "publications/limits-mapk.html",
    "title": "Limits to the rate of information transmission through the MAPK pathway",
    "section": "",
    "text": "Do cells communicate in bits?"
  },
  {
    "objectID": "publications/limits-mapk.html#premise",
    "href": "publications/limits-mapk.html#premise",
    "title": "Limits to the rate of information transmission through the MAPK pathway",
    "section": "",
    "text": "Do cells communicate in bits?"
  },
  {
    "objectID": "publications/limits-mapk.html#abstract",
    "href": "publications/limits-mapk.html#abstract",
    "title": "Limits to the rate of information transmission through the MAPK pathway",
    "section": "Abstract",
    "text": "Abstract\nTwo important signalling pathways of NF-κB and ERK transmit merely 1 bit of information about the level of extracellular stimulation. It is thus unclear how such systems can coordinate complex cell responses to external cues. We analyse information transmission in the MAPK/ERK pathway that converts both constant and pulsatile EGF stimulation into pulses of ERK activity. Based on an experimentally verified computational model, we demonstrate that, when input consists of sequences of EGF pulses, transmitted information increases nearly linearly with time. Thus, pulse-interval transcoding allows more information to be relayed than the amplitude–amplitude transcoding considered previously for the ERK and NF-κB pathways. Moreover, the information channel capacity \\(C\\), or simply bitrate, is not limited by the bandwidth \\(B = 1/\\tau\\), where \\(\\tau\\approx 1\\, h\\) is the relaxation time. Specifically, when the input is provided in the form of sequences of short binary EGF pulses separated by intervals that are multiples of \\(\\tau/n\\) (but not shorter than \\(\\tau\\)), then for \\(n = 2\\), \\(C \\approx 1.39\\) bit/h; and for \\(n = 4\\), \\(C \\approx 1.86\\) bit/h. The capability to respond to random sequences of EGF pulses enables cells to propagate spontaneous ERK activity waves across tissue."
  },
  {
    "objectID": "publications/limits-mapk.html#citation",
    "href": "publications/limits-mapk.html#citation",
    "title": "Limits to the rate of information transmission through the MAPK pathway",
    "section": "Citation",
    "text": "Citation\n@article{Grabowski-2019-systems-biology,\nauthor = {Grabowski, Frederic  and Czyż, Paweł  and Kochańczyk, Marek  and Lipniacki, Tomasz },\ntitle = {Limits to the rate of information transmission through the {MAPK} pathway},\njournal = {Journal of The Royal Society Interface},\nvolume = {16},\nnumber = {152},\npages = {20180792},\nyear = {2019},\ndoi = {10.1098/rsif.2018.0792},\nURL = {https://royalsocietypublishing.org/doi/abs/10.1098/rsif.2018.0792},\neprint = {https://royalsocietypublishing.org/doi/pdf/10.1098/rsif.2018.0792}\n}"
  },
  {
    "objectID": "publications/limits-mapk.html#acknowledgments",
    "href": "publications/limits-mapk.html#acknowledgments",
    "title": "Limits to the rate of information transmission through the MAPK pathway",
    "section": "Acknowledgments",
    "text": "Acknowledgments\nMany, many thanks to Tomek Lipniacki, Marek Kochańczyk, Karolina Tudelska and, of course, Frederic Grabowski! I think this project was very important for my future development: I learned how much effort it takes to publish a paper, that biology is a wonderful subject to work on, and what mutual information is."
  },
  {
    "objectID": "publications/limits-mapk.html#behind-the-scenes",
    "href": "publications/limits-mapk.html#behind-the-scenes",
    "title": "Limits to the rate of information transmission through the MAPK pathway",
    "section": "Behind the scenes",
    "text": "Behind the scenes\nThis project is partially described in a blog post."
  },
  {
    "objectID": "publications/pmi-profiles.html",
    "href": "publications/pmi-profiles.html",
    "title": "On the properties and estimation of pointwise mutual information profiles",
    "section": "",
    "text": "The pointwise mutual information profile, or simply profile, is the distribution of pointwise mutual information for a given pair of random variables. One of its important properties is that its expected value is precisely the mutual information between these random variables. In this paper, we analytically describe the profiles of multivariate normal distributions and introduce a novel family of distributions, Bend and Mix Models, for which the profile can be accurately estimated using Monte Carlo methods. We then show how Bend and Mix Models can be used to study the limitations of existing mutual information estimators, investigate the behavior of neural critics used in variational estimators, and understand the effect of experimental outliers on mutual information estimation. Finally, we show how Bend and Mix Models can be used to obtain model-based Bayesian estimates of mutual information, suitable for problems with available domain expertise in which uncertainty quantification is necessary."
  },
  {
    "objectID": "publications/pmi-profiles.html#abstract",
    "href": "publications/pmi-profiles.html#abstract",
    "title": "On the properties and estimation of pointwise mutual information profiles",
    "section": "",
    "text": "The pointwise mutual information profile, or simply profile, is the distribution of pointwise mutual information for a given pair of random variables. One of its important properties is that its expected value is precisely the mutual information between these random variables. In this paper, we analytically describe the profiles of multivariate normal distributions and introduce a novel family of distributions, Bend and Mix Models, for which the profile can be accurately estimated using Monte Carlo methods. We then show how Bend and Mix Models can be used to study the limitations of existing mutual information estimators, investigate the behavior of neural critics used in variational estimators, and understand the effect of experimental outliers on mutual information estimation. Finally, we show how Bend and Mix Models can be used to obtain model-based Bayesian estimates of mutual information, suitable for problems with available domain expertise in which uncertainty quantification is necessary."
  },
  {
    "objectID": "publications/pmi-profiles.html#links",
    "href": "publications/pmi-profiles.html#links",
    "title": "On the properties and estimation of pointwise mutual information profiles",
    "section": "Links",
    "text": "Links\n\nThis is a direct follow-up to the Beyond normal manuscript."
  },
  {
    "objectID": "publications/pmi-profiles.html#citation",
    "href": "publications/pmi-profiles.html#citation",
    "title": "On the properties and estimation of pointwise mutual information profiles",
    "section": "Citation",
    "text": "Citation\n@misc{pmi-profiles,\n      title={On the Properties and Estimation of Pointwise Mutual Information Profiles}, \n      author={Paweł Czyż and Frederic Grabowski and Julia E. Vogt and Niko Beerenwinkel and Alexander Marx},\n      year={2023},\n      eprint={2310.10240},\n      archivePrefix={arXiv},\n      primaryClass={stat.ML}\n}"
  },
  {
    "objectID": "publications/pmi-profiles.html#behind-the-scenes",
    "href": "publications/pmi-profiles.html#behind-the-scenes",
    "title": "On the properties and estimation of pointwise mutual information profiles",
    "section": "Behind the scenes",
    "text": "Behind the scenes\n\nThe final version has been published in TMLR and we are thankful to the Reviewers and the Action Editor for many great suggestions. Similarly, this paper has benefitted from comments obtained during the reviews at AABI 2024, which was a great event to participate in. Finally, we would like to thank Paweł Nałęcz-Jawecki and Julia Kostin, who carefully read the paper and provided wonderful feedback.\nThe full story of the project has been described on the blog.\nThe first version of this manuscript was titled The mixtures and the critics: on the pointwise mutual information profiles of fine distributions. This is, of course, named after J.R.R. Tolkien’s The monsters and the critics. I was very stubborn on keeping this, but after weeks of discussions I was eventually convinced that the new title is more informative about the content."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome!",
    "section": "",
    "text": "My name is Paweł Czyż and I model biological data using techniques originating in Bayesian statistics and probabilistic machine learning. Currently I am a Doctoral Fellow at the ETH AI Center."
  },
  {
    "objectID": "posts/stirling-probabilistic-proof.html",
    "href": "posts/stirling-probabilistic-proof.html",
    "title": "A probabilistic proof of the Stirling’s formula",
    "section": "",
    "text": "I was looking at arXiv and I noticed a new preprint from Nils Lid Hjort and Emil Aas Stoltenberg, called Probability Proofs for Stirling (and More): the Ubiquitous Role of \\(\\sqrt{2\\pi}\\). I read many papers authored by N.L. Hjort, and they were all great, so I immediately clicked into this one.\nIt was worth it. The paper is beautiful."
  },
  {
    "objectID": "posts/stirling-probabilistic-proof.html#what-is-the-stirlings-formula",
    "href": "posts/stirling-probabilistic-proof.html#what-is-the-stirlings-formula",
    "title": "A probabilistic proof of the Stirling’s formula",
    "section": "What is the Stirling’s formula?",
    "text": "What is the Stirling’s formula?\nIn a statistical physics or an algorithms class, one quickly learns about the Stirling’s approximation \\[\n  \\log(n!) \\approx n \\log n - n,\n\\]\nwhere the approximation error is about \\(O(\\log n)\\). More precisely, one has \\[\n\\lim_{n\\to \\infty} \\frac{n!}{n^{n+1/2} \\exp(-n) } = \\sqrt{2\\pi}.\n\\]\nAs Hjort and Stoltenberg (2024) note, both the \\(\\exp\\) function and the \\(\\sqrt{2\\pi}\\) constant appear also in the definition of the standard normal distribution. There is a standard normal distribution and a limit with \\(n\\to \\infty\\), so one can think whether the central limit theorem could somehow be involved. And it can!\nBelow we will sketch two proofs included in the paper – the original manuscript proves them formally, as well as many related results. There exist also variations on these two proofs in the literature, for example:\n\nR.A. Khan, A probabilistic proof of Stirling’s formula, The American Mathematical Monthly (1974).\nC.S. Wong, A note on the central limit theorem, The American Mathematical Monthly (1977).\nC.R. Blyth and P.K. Pathak, A note on easy proofs of Stirling’s theorem (1986).\nK.L.D. Gunawardena, Stirling’s formula: An application of the central limit theorem, Missouri Journal of Mathematical Sciences (1993)."
  },
  {
    "objectID": "posts/stirling-probabilistic-proof.html#the-general-idea",
    "href": "posts/stirling-probabilistic-proof.html#the-general-idea",
    "title": "A probabilistic proof of the Stirling’s formula",
    "section": "The general idea",
    "text": "The general idea\nLet \\(X_i\\) be independent and identically distributed variables. We assume that the mean \\(\\mu := \\mathbb E[X_i]\\) and the standard deviation \\(\\sigma = \\sqrt{\\mathbb V[X_i]}\\) both exist.\nLet \\(Y_n = \\sum_{i=1}^n X_i\\) be a prefix sum and \\[\nZ_n = \\frac{Y_n - \\mu n}{\\sigma \\sqrt{n}}\n\\] be the standardized version of it. The central limit theorem asserts that \\(Z_n\\) converges in distribution to the standard normal.\nLet \\[\n  L_n = \\mathbb E[ Z_n  \\cdot \\mathbf{1}[Z_n \\ge 0]]\n\\]\nand\n\\[\n  L = \\mathbb E[ Z  \\cdot \\mathbf{1}[Z \\ge 0]] = \\frac{1}{\\sqrt{2\\pi}}.\n\\]\nIn this case, we have \\(L_n \\to L\\) as \\(n\\to \\infty\\)."
  },
  {
    "objectID": "posts/stirling-probabilistic-proof.html#poisson-random-variables",
    "href": "posts/stirling-probabilistic-proof.html#poisson-random-variables",
    "title": "A probabilistic proof of the Stirling’s formula",
    "section": "Poisson random variables",
    "text": "Poisson random variables\nLet \\(X_i\\sim \\mathrm{Poisson}(1)\\), so that \\(\\mathbb P(X_i = k) = \\frac{e^{-1}}{k!}\\) for non-negative integers \\(k\\in \\{0, 1, 2, \\dotsc\\}\\) and \\(\\mu = \\sigma = 1\\). As \\(Y_n = X_1 + \\cdots + X_n\\), we have \\(Y_n\\sim \\mathrm{Poisson}(n)\\) and \\[\n\\mathbb P(Y_n = k) = \\frac{ e^{-n}n^k }{k!}.\n\\]\nWe have therefore \\[\n  L_n = \\sum_{k\\ge n} \\frac{k-n}{\\sqrt{n}} \\mathbb P(Y_n = k) = \\frac{e^{-n} n^{n+\\frac{1}{2}}}{n!},\n\\]\nwhich tends to \\(L = 1/\\sqrt{2\\pi}\\).\nThe last identity can be proven using a telescoping sum expansion in the paper, or by using Mathematica:\np[k_] := (Exp[-n] n^k)/Factorial[k]\nSum[(k - n)/Sqrt[n]  p[k], {k, n, \\[Infinity]}]\nThis is proof is very elegant. It is related to the proof of Wong (1977); Blyth and Pathak (1986) also provide a version of the proof employing \\(\\mathbb E[|Z_n|]\\), rather than \\(\\mathbb E[Z_n \\cdot \\mathbf 1[Z_n\\ge 0]]\\)."
  },
  {
    "objectID": "posts/stirling-probabilistic-proof.html#exponential-random-variables",
    "href": "posts/stirling-probabilistic-proof.html#exponential-random-variables",
    "title": "A probabilistic proof of the Stirling’s formula",
    "section": "Exponential random variables",
    "text": "Exponential random variables\nLet \\(X_i \\sim \\mathrm{Exp}(1) = \\mathrm{Gamma}(1, 1)\\), so that \\(\\mu = \\sigma = 1\\) and \\(Y_n \\sim \\mathrm{Gamma}(n, 1)\\). The probability density function of \\(Y_n\\) is, hence, given by \\[\n  g_n(t) = \\frac{1}{\\Gamma(n)}t^{n-1}e^{-t},\n\\]\nwhere \\(\\Gamma(n) = (n-1)!\\) is the gamma function.\nWe have \\(t g_n(t) = n g_{n+1}(t)\\) and \\[\n  L_n = \\int\\limits_n^\\infty \\frac{t-n}{\\sqrt{n}} g_n(t)\\, \\mathrm{d}t = \\sqrt{n} \\int \\limits_n^\\infty \\left( g_{n+1}(t) - g_n(t) \\right)\\, \\mathrm{d}t.\n\\]\nThe last integral can be calculated in Mathematica:\ng[n_, t_] := 1/Factorial[n - 1] t^(n - 1) Exp[-t];\n\nIntegrate[g[n + 1, t] - g[n, t], {t, n, \\[Infinity]}]\nwhich evaluates to \\(e^nn^n/n!\\), which is exactly what we need.\nThis proof is the most similar to the ones from Gunawardena (1993) and, if one works with \\(|Z_n|\\), to Khan (1974)."
  },
  {
    "objectID": "posts/stirling-probabilistic-proof.html#summary",
    "href": "posts/stirling-probabilistic-proof.html#summary",
    "title": "A probabilistic proof of the Stirling’s formula",
    "section": "Summary",
    "text": "Summary\nThe paper of Hjort and Stoltenberg is great! Apart from full versions of the proof sketches presented above, it also contains several other results and a nice exposition of the history of the related ideas."
  },
  {
    "objectID": "posts/stirling-probabilistic-proof.html#addendum-how-large-is-the-approximation-error",
    "href": "posts/stirling-probabilistic-proof.html#addendum-how-large-is-the-approximation-error",
    "title": "A probabilistic proof of the Stirling’s formula",
    "section": "Addendum: how large is the approximation error?",
    "text": "Addendum: how large is the approximation error?\nWe know to expect \\[\n  n! \\approx \\sqrt{2\\pi n} \\left(\\frac{n}{e}\\right)^n,\n\\]\nbut how large is the approximation error? As explained in a Wikipedia entry, this MathSE answer or the paper A remark on Stirling’s formula published by H. Robbins in 1955, this approximation is really, really good: for \\(n\\ge 1\\) we have\n\\[\n  n! = \\sqrt{2\\pi n} \\left(\\frac{n}{e}\\right)^n \\cdot R_n,\n\\]\nwhere \\(\\exp\\!\\left(\\frac 1{12n+1} \\right) = \\sqrt[12n+1]{e} &lt; R_n &lt; \\sqrt[12n]{e} = \\exp\\!\\left(\\frac 1{12n} \\right)\\).\nWe see that \\(R_n &gt; 1\\), but it cannot be too far: the sequence \\(n \\mapsto \\exp\\!\\left(\\frac 1{12n} \\right)\\) is decreasing and we have \\(R_5 &lt; 1.02\\), \\(R_{10} &lt; 1.01\\) (the relative error is about 1% already here!), \\(R_{100} &lt; 1.001\\), and \\(R_{1000} &lt; 1.0001\\)."
  },
  {
    "objectID": "posts/on-beyond-normal.html",
    "href": "posts/on-beyond-normal.html",
    "title": "The mutual information saga",
    "section": "",
    "text": "Where and when should we start this story? Probably a good origin will be in the Laboratory of Modeling in Biology and Medicine in Warsaw, where Tomasz Lipniacki and Marek Kochańczyk decided to mentor two students who just completed high-school education, namely the younger versions of Frederic and myself.\nInformally speaking, we tried to model the MAPK pathway as a communication channel. Imagine that the cell is given some input \\(x\\in \\mathcal X\\) and we measure the response \\(y\\in \\mathcal Y\\). Once we vary \\(x\\) and we record different values \\(y\\) we may start observing some patterns – perhaps changes with \\(y\\) is somehow associated with changes in \\(x\\). To make this more formal, consider a random variable \\(X\\) representing the given inputs and another random variable \\(Y\\) representing the outputs. The mutual information \\(I(X; Y)\\) measures how dependent these variables are: \\(I(X; Y) = 0\\) if and only if \\(X\\) and \\(Y\\) are independent. Contrary to correlation, mutual information works for any kind of variables (continuous, discrete, of arbitrary dimension…) and can capture nonlinear dependencies.\nI enjoyed my time in the project and the provided mentorship very much! We wrote a paper, and – perhaps more importantly – I learned that information theory and biology are great fields to study!\nMany years have passed and I thought that perhaps it’s the time to become a mentor on my own. Fortunately, the very first Master’s student I supervised, Anej, was so bright and motivated that he wrote a great Master’s thesis despite having such an unexperienced supervisor as me! Anej was working on representation learning and extensively used mutual information estimators. But I had not done my homework: when I wrote the project proposal, I happily assumed that mutual information estimators have to work, if the space \\(\\mathcal X\\times \\mathcal Y\\) is of moderate dimension and the number of points is quite large. I was wrong. Different mutual information estimators seemed to give very different estimates and that was concerning."
  },
  {
    "objectID": "posts/on-beyond-normal.html#beyond-normal",
    "href": "posts/on-beyond-normal.html#beyond-normal",
    "title": "The mutual information saga",
    "section": "Beyond normal",
    "text": "Beyond normal\nI did the only rational thing in this situation: I ran screaming for help to two mutual information experts, Frederic and Alex. We started thinking:\n\nHow do we really know when mutual information estimators work? As mutual information is analytically known only for the simplest distributions, the estimators are evaluated usually on “simple” low-dimensional distributions (or moderate-dimensional multivariate normal distributions).\nIs it possible to construct more expressive distributions with known ground-truth mutual information?\nHow invariant are the estimators to diffeomorphisms? Namely, if \\(f\\) and \\(g\\) are diffeomorphisms, then \\(I(X; Y) = I(f(X); g(Y))\\). Do the numerical estimates have the same property?\n\nThe 1st and 2nd question are related. But so are 2nd and 3rd! Suppose that we can easily sample points \\((x_1, y_1), \\dotsc, (x_n, y_n)\\) from the joint distribution \\(P_{XY}\\). If \\(f\\colon \\mathcal X\\to \\mathcal X\\) and \\(g\\colon \\mathcal Y\\to \\mathcal Y\\) are diffeomorphisms, we can apply them to obtain a sample \\((f(x_1), g(y_1)), \\dotsc, (f(x_n), g(y_n))\\) from \\(P_{f(X)g(Y)}\\), which is a joint distribution between variables \\(f(X)\\) and \\(g(Y)\\). As we apply a diffeomorphism1, the mutual information does not change: \\(I(X; Y) = I(f(X); g(Y))\\).\nFrederic and I started programming2 different distributions and transformations, in the meantime learning , and after five months we had a ready manuscript titled Are mutual information estimators homeomorphism-invariant?, which shows that the 3rd question was the most important one for a lapsed differential geometer who is currently trying to be an imposter in the machine learning world me.\nWell, I was wrong: after the manuscript got rejected from ICML, we realized that the most important aspect of our work was actually using the transformed distributions to study the strengths and limitations of existing estimators3. We improved the experiments in the paper and changed the story to Beyond normal: on the evaluation of the mutual information estimators4, which was accepted to NeurIPS.\nOf course, we were very happy. But there were some important aspects that deserved to be studied a bit more…"
  },
  {
    "objectID": "posts/on-beyond-normal.html#here-comes-the-trouble",
    "href": "posts/on-beyond-normal.html#here-comes-the-trouble",
    "title": "The mutual information saga",
    "section": "Here comes the trouble",
    "text": "Here comes the trouble\n\nReally that expressive?\nOur distributions were only “beyond normal”, rather than “just amazing”: we suspected that one cannot construct all the interesting distributions.\nConsider \\(\\mathcal X = \\mathbb R^m\\), \\(\\mathcal Y = \\mathbb R^n\\) and a random vector \\(Z \\sim \\mathcal N(0, I_{m+n})\\). Normalizing flows guarantee that there exists a diffeomorphism \\(u: \\mathcal X\\times \\mathcal Y \\to \\mathcal X\\times \\mathcal Y\\) such that \\(P_{XY}\\) is well-approximated5 by the distribution of \\(u(Z)\\).\nHowever, the diffeomorphism \\(u\\) does not have to be of the form \\(f\\times g\\) (recall that \\((f\\times g)(x, y) = (f(x), g(y))\\)), which leaves the mutual information invariant. Thinking geometrically, the product group \\(\\mathrm{Diff}(\\mathcal X)\\times \\mathrm{Diff}(\\mathcal Y)\\) is usually a very small subgroup of \\(\\mathrm{Diff}(\\mathcal X\\times \\mathcal Y)\\), the group of all diffeomorphisms of \\(\\mathcal X\\times \\mathcal Y\\).\nWe had this intuition quite early, but we did not have a convincing counterexample that our distributions were not sufficient. However, Frederic started plotting histograms of pointwise mutual information, which let us formalize this intuition.\nConsider the pointwise mutual information: \\[ i_{XY}(x, y) = \\log \\frac{ p_{XY}(x, y) }{ p_X(x)\\, p_Y(y) },\\] where \\(p_{XY}\\) is the PDF of the joint distribution and \\(p_X\\) and \\(p_Y\\) are the PDFs of the marginal distributions. It is easy to prove that if \\(f\\) and \\(g\\) are diffeomorphisms and that \\(x'=f(x)\\) and \\(y'=g(y)\\), then6 \\[i_{XY}(x, y) = i_{f(X)g(Y)}(x', y').\\] From this it is easy to observe that the distribution of the random variable \\(i_{XY}(X; Y)\\) is the same as of \\(i_{f(X)g(Y)}(f(X); g(Y))\\). We termed it the pointwise mutual information profile, although it’s more than likely that people had already studied this before us.\nHence, diffeomorphisms leave invariant not only the mutual information: they leave invariant also the whole pointwise mutual information profile, what limits how expressive the distributions can be; see also this post for more general invariance results. We did not have yet a counterexample, but a strong feeling that it should exist: we just needed to find distributions with different profiles, but the same mutual information.\n\n\nModel-based mutual information estimation\nWe started the project with the idea that if \\((A, B)\\sim \\mathcal N(0, \\Sigma)\\), then \\(I(A; B)\\) is analytically known in terms of the covariance matrix \\(\\Sigma\\) and we can obtain more complicated dependencies between \\(X=f(A)\\) and \\(Y=f(B)\\) without changing the mutual information: \\(I(X; Y) = I(A; B)\\).\nAt the same time we asked the inverse question: if we have \\(X\\) and \\(Y\\), can we perhaps find a covariance matrix \\(\\Sigma\\) and a normalizing flow \\(f\\times g\\) such that \\((X, Y) = (f(A), g(B))\\) and \\((A, B)\\) are distributed according to the multivariate normal distribution with covariance matrix \\(\\Sigma\\)? If \\(f\\) and \\(g\\) are identity functions, this construction corresponds to the assumption that \\((X, Y)\\) are multivariate normal and calculating the mutual information via the estimation of the joint covariance matrix. A particular example of this approach is canonical correlation analysis, which worked remarkably well for multivariate normal distributions, providing more accurate estimates and requiring a lower number of samples available.\nHowever, as discussed above, generally we cannot expect that a normalizing flow of the form \\(f\\times g\\) will transform a distribution to a multivariate normal. So there is some potential for more explicit modelling of the joint distribution \\(P_{XY}\\), but we needed to make sure that it is expressive enough to cover some interesting cases.\n\n\nDo outliers break everything?\nThere’s no real data without real noise and we wanted to have distributions which can be used in practice. One source of noise are outliers, which sometimes can be attributed to errors in data collection or recording (e.g., the equipment was apparently switched off or some piece of experimental setup broke), and are well-known suspects when an estimator behaves badly. In Beyond normal we investigated heavy-tailed distributions (either by applying some transformations to multivariate normal distributions to make the tails heavier, or by using multivariate Student distributions), but we felt that it was not enough."
  },
  {
    "objectID": "posts/on-beyond-normal.html#the-mixtures-and-the-critics",
    "href": "posts/on-beyond-normal.html#the-mixtures-and-the-critics",
    "title": "The mutual information saga",
    "section": "The mixtures (and the critics)",
    "text": "The mixtures (and the critics)\nThe outliers here were the most concerning and I had the feeling that if \\(P_{XY}\\) is the distribution from which we want to sample and \\(P_{\\tilde X \\tilde Y}\\) is the “noise distribution” with \\(I(\\tilde X; \\tilde Y) = 0\\), then we could perhaps calculate the information contained in the mixture distribution: \\[P_{X'Y'} = (1-\\alpha)\\, P_{XY} + \\alpha \\, P_{\\tilde X \\tilde Y},\\] where \\(\\alpha \\ll 1\\) is the fraction of outliers.\nI spent quite some time with a pen and paper trying to calculate \\(I(X'; Y')\\) in terms of \\(I(X; Y)\\), but I could not really derive anything. Even proving the conjectured bound that \\(I(X'; Y')\\) should not exceed \\(I(X; Y)\\) was hard…\nAnd it’s actually good that I didn’t manage to prove it: this conjecture is false. When I asked Frederic about it, he immediately responded with: 1. An example of two distributions such that each of them encodes 0 bits, but their mixture encodes 1 bit. 2. An example of two distributions such that each of them encodes 1 bit, but their mixture encodes 0 bits.\nThis is disturbing. As Larry Wasserman said: I have decided that mixtures, like tequila, are inherently evil and should be avoided at all costs. Fortunately, when I was still struggling with trying to prove it, I recalled Frederic’s histograms, approximating the pointwise mutual information profile. For multivariate normal and Student distributions he sampled a lot of data points \\((x_1, y_1), \\dotsc, (x_n, y_n)\\) and then evaluated the pointwise mutual information \\(i_{XY}(x_i, y_i)\\) – which is easy to evaluate using \\(\\log p_{XY}\\), \\(\\log p_X\\) and \\(\\log p_Y\\) densities – to construct a histogram. The mean of this sample is the estimate of the mutual information \\(I(X; Y) = \\mathbb E_{(x, y)\\sim P_{XY} }[i_{XY}(x; y)]\\).\nThis works for multivariate normal distributions and Student distributions, so why wouldn’t it work for mixture distributions? In the end this is simply a Monte Carlo estimator: we only need to sample a lot of data points (and sampling from a mixture distribution is trivial if one can sample from the component distributions) and evaluate the pointwise mutual information (which can be calculated from the PDFs of the involved variables. The PDF of the mixture distribution can be evaluated using the PDFs of the components).\nHence, although we do not have an exact formula for \\(I(X; Y)\\) where \\(P_{XY}\\) is a mixture of multivariate normal or Student distributions, we have its Monte Carlo approximation. Now we can apply diffeomorphisms to this distribution obtaining more expressive distribution, having e.g., a normalizing flow applied to a Gaussian mixture or a mixture of normalizing flow, or mix all of these again…\nImplementation of a prototype in TensorFlow Probability took one day7 and after a month we had a manuscript ready.\nI very much like this concept because:\n\nThis construction can model outliers as a mixture of different distributions.\nWe can construct a mixture distribution with a different pointwise mutual information profile than the multivariate normal distribution. Due to the fact that Gaussian mixtures are very expressive, we can build a whole new family of distributions!\nWe also do not have to model \\(P_{XY}\\) as a multivariate normal distribution transformed by diffeomorphism \\(f\\times g\\) – we can use Gaussian (or Student) mixtures! We quickly built a prototype of model-based mutual information estimation with mixtures, which can provide uncertainty quantification on mutual information in the usual Bayesian manner.\n\nLet’s finish this post at the place where we started: in Beyond normal we could apply diffeomorphisms to transform continuous \\(X\\) and \\(Y\\) variables. The Monte Carlo estimator from the follow-up manuscript applies in exactly the same manner also to the case when \\(X\\) is discrete variable and \\(Y\\) is continuous, which is exactly the case we investigated many years ago!"
  },
  {
    "objectID": "posts/on-beyond-normal.html#footnotes",
    "href": "posts/on-beyond-normal.html#footnotes",
    "title": "The mutual information saga",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nMore precisely, if all the spaces involved are standard Borel, then \\(f\\colon \\mathcal X\\to \\mathcal X'\\) and \\(g\\colon \\mathcal Y\\to \\mathcal Y'\\) can be continuous injective mappings and e.g., increase the dimensionality of the space. See Theorem 2.1 here, which is a well-known fact, but it still took us quite some time to prove it. M.S. Pinsker’s Information and information stability of random variables and processes proved to be an invaluable resource.↩︎\nAs a byproduct we learned Snakemake, which transformed my approach to data science entirely. But this is a different story.↩︎\nAlso, Frederic figured out that the 3rd question (whether the estimators which are invariant to diffeomorphisms) has a trivial answer. If \\(\\mathcal M\\) is a connected smooth manifold of dimension at least 2, then for any two sets of distinct points \\(\\{a_1, \\dotsc, a_n\\}\\) and \\(\\{b_1, \\dotsc, b_n\\}\\) there exists a diffeomorphism \\(u\\colon \\mathcal M\\to \\mathcal M\\) such that \\(b_i = u(a_i)\\) (the proof of this fact can be e.g., found in P.W. Michor’s and Cornelia Vizman’s \\(n\\)-transitivity of certain diffeomorphisms groups). Hence, if \\(\\mathcal X\\) and \\(\\mathcal Y\\) fulfil the assumptions above, we can move a finite data set as we wish and the only invariant estimator has to return the same answer for any set of input data points.↩︎\nLet me state two obvious facts. First: “beyond” refers to the fact that we can transform multivariate normal distributions to obtain more expressive distributions. Second: although I intended to name the paper after a wonderful musical, Next to Normal, we worried that the title was copyrighted.↩︎\nImportant detail: “well-approximated” using one statistical distance may me “badly approximated” with respect to another one.↩︎\nIt’s tempting to say that pointwise mutual information transforms as a scalar under the transformations from the group \\(\\mathrm{Diff}(\\mathcal X)\\times \\mathrm{Diff}(\\mathcal Y)\\).↩︎\nActually, it took three weeks of me complaining that we couldn’t use multivariate Student distributions in TensorFlow Probability on JAX. Since I had learned that I was wrong, a few hours passed before we had the prototype and a couple of more days before it was refactored into a stable solution.↩︎"
  },
  {
    "objectID": "posts/irrational-pi-and-e.html",
    "href": "posts/irrational-pi-and-e.html",
    "title": "Proofs that pi and e are irrational",
    "section": "",
    "text": "Timothy Chow wrote a great paper A well-motivated proof that pi is irrational, where he explains a proof of irrationality of \\(e\\) (attributed to Fourier) and a proof of irrationality of \\(\\pi\\), which originally appeared in the I. Niven’s brilliant one-page article.\nIn this post I’ll just recall both proofs, so I can remember them: original articles will be more informative."
  },
  {
    "objectID": "posts/irrational-pi-and-e.html#irrationality-of-e",
    "href": "posts/irrational-pi-and-e.html#irrationality-of-e",
    "title": "Proofs that pi and e are irrational",
    "section": "Irrationality of \\(e\\)",
    "text": "Irrationality of \\(e\\)\nSuppose that \\(e=a/b\\) for positive integers \\(a\\) and \\(b\\). We can write \\[\ne = \\sum_{n=0}^{\\infty} \\frac{1}{n!}\n\\]\nand reach a contradiction by studying an integer:\n\\[\n0 &lt; b!\\cdot e - b!\\sum_{n=0}^b \\frac{1}{n!} = \\sum_{k=1}^{\\infty} \\frac{b!}{(b+k)!} &lt; \\sum_{k=1}^{\\infty} \\frac{1}{(b+1)^k} = \\frac{1}{b} \\le 1.\n\\]\nHence, this integer would lie strictly between \\(0\\) and \\(1\\), what is impossible."
  },
  {
    "objectID": "posts/irrational-pi-and-e.html#irrationality-of-pi",
    "href": "posts/irrational-pi-and-e.html#irrationality-of-pi",
    "title": "Proofs that pi and e are irrational",
    "section": "Irrationality of \\(\\pi\\)",
    "text": "Irrationality of \\(\\pi\\)\nSuppose that \\(\\pi=a/b\\) for positive integers \\(a\\) and \\(b\\). Let \\(n\\) be be a positive integer and define \\[\nf(x)=\\frac{x^n(a-bx)^n}{n!}\n\\]\nThe main idea of I. Niven’s proof is to show that the definite integral \\[\nI := \\int \\limits_0^\\pi f(x) \\sin x\\, \\mathrm{d}x\n\\]\nis an integer. But for \\(0 &lt; x &lt; \\pi\\) there are bounds \\[\n0 &lt; f(x) \\sin x &lt; \\frac{\\pi^n\\cdot (a-0)^n}{n!} = \\frac{ (\\pi a)^n }{n!},\n\\]\nso that for sufficiently large \\(n\\) we have \\(0 &lt; I &lt; 1\\).\nHence, the whole difficulty lies in proving that \\(I \\in \\mathbb Z\\).\nDefine \\[\nF(x) = f(x) - f''(x) + f^{(4)}(x) - \\cdots + (-1)^n f^{(2n)}(x),\n\\]\nwhere \\(f^{(k)}\\) is the \\(k\\)-th derivative of \\(f\\). We have\n\\[\\begin{align*}\n\\frac{\\mathrm{d}}{\\mathrm{d} x}\\left( F'(x) \\sin x - F(x) \\cos x \\right) &= F''(x) \\sin x + F'(x) \\cos x - F'(x) \\cos x + F(x) \\sin x\\\\\n&= (F(x) + F''(x)) \\sin x = f(x) \\sin x,\n\\end{align*}\\] so that \\[\n\\int\\limits_0^\\pi f(x) \\sin x\\, \\mathrm{d}x = \\big[F'(x) \\sin x - F(x) \\cos x\\big]_0^\\pi = F(0) + F(\\pi).\n\\]\nWe can prove that \\(F(0) + F(\\pi)\\) is an integer by proving that all derivatives \\(f^{(k)}_n(0)\\) and \\(f^{(k)}_n(\\pi)\\) are integers.\nFirst, note that \\(f_n(x) = f_n(a/b - x) = f_n(\\pi - x)\\), so that \\(f^{(k)}_n(0) = \\pm f^{(k)}_n(\\pi)\\). To prove that \\(f^{(k)}_n(0)\\) is an integer, observe that each summand in \\(f_n\\) has a form \\(u x^{n+r} / n!\\), where \\(u\\) is an integer and \\(r \\ge 0\\). If we differentiate it \\(k\\) times and evaluate it at \\(x=0\\) we will either have \\(0\\) (for \\(k \\neq n+r\\)) or \\(u \\cdot (n+r)! / n!\\) for \\(k=n+r\\), which is an integer."
  },
  {
    "objectID": "posts/irrational-pi-and-e.html#links",
    "href": "posts/irrational-pi-and-e.html#links",
    "title": "Proofs that pi and e are irrational",
    "section": "Links",
    "text": "Links\n\nI. Niven, A simple proof that \\(\\pi\\) is irrational, 1947: a one-page proof that \\(\\pi\\) is irrational.\nKostya_I, Exposition of Niven’s proof, 2023: a short note connecting Niven’s proof to orthogonal polynomials.\nT. Chow, A well-motivated proof that pi is irrational, 2024: proof of irrationality of \\(\\pi\\) and \\(e^r\\) for nonzero rational \\(r\\)."
  },
  {
    "objectID": "posts/irrational-pi-and-e.html#appendix-some-theorems-of-transcendental-number-theory",
    "href": "posts/irrational-pi-and-e.html#appendix-some-theorems-of-transcendental-number-theory",
    "title": "Proofs that pi and e are irrational",
    "section": "Appendix: some theorems of transcendental number theory",
    "text": "Appendix: some theorems of transcendental number theory\nI often forget the names of major results of transcendental number theory. Let’s quickly recall them:\nLindemann–Weierstrass theorem: if \\(\\alpha_1, \\dotsc, \\alpha_n\\) are algebraic numbers linearly independent over \\(\\mathbb Q\\), then \\(e^{\\alpha_1}, \\dotsc, e^{\\alpha_n}\\) are algebraically independent over \\(\\mathbb Q\\).\nGelfond–Schneider theorem: let \\(a \\in \\mathbb C\\setminus \\{0, 1\\}\\) be a complex algebraic number. If \\(b \\in \\mathbb C\\setminus (\\mathbb Q\\times \\{0\\})\\) is an irrational complex algebraic number, then \\(a^b = \\exp(b\\log a)\\) is transendental (independently on the branches of the functions used)."
  },
  {
    "objectID": "posts/blogs-i-read.html",
    "href": "posts/blogs-i-read.html",
    "title": "Blogs I read",
    "section": "",
    "text": "From time to time I have to recall where I saw a great blog post on the topic of my interest. Hence, it’d be good to formalize the list of blogs which I check semi-regularly to learn new things."
  },
  {
    "objectID": "posts/blogs-i-read.html#statistical-modeling-and-machine-learning",
    "href": "posts/blogs-i-read.html#statistical-modeling-and-machine-learning",
    "title": "Blogs I read",
    "section": "Statistical modeling and machine learning",
    "text": "Statistical modeling and machine learning\n\nAndrew Gelman et al.: Statistical Modeling, Causal Inference, and Social Science\nFrank Harrell: Statistical Thinking\nRichard McElreath: Elements of Evolutionary Anthropology\nMichael Betancourt: Case studies\nDan Simpson: Un garçon pas comme les autres (Bayes)\nChristian Robert: Xi’an’s Og\nJohn Cook: Website\nRyan Giordano: Statistician\nStephen Senn: Blog\nDeborah Mayo: Error Statistics Philosophy\nPatrick Kidger: Thoughts\nJakub M. Tomczak: Blog\nShakir Mohamed: The Spectator\nJuan Camilo Orduz: https://juanitorduz.github.io/\nPiotr Migdał: Blog\nDarren Dahly: StatsEpi and Blog\nSimon Dirmeier: Etudes; GitHub repository\nTim Morris: Statistical Methodology Meanderings"
  },
  {
    "objectID": "posts/blogs-i-read.html#computational-biology-and-bioinformatics",
    "href": "posts/blogs-i-read.html#computational-biology-and-bioinformatics",
    "title": "Blogs I read",
    "section": "Computational biology and bioinformatics",
    "text": "Computational biology and bioinformatics\n\nLior Pachter: Bits of DNA"
  },
  {
    "objectID": "posts/blogs-i-read.html#programming",
    "href": "posts/blogs-i-read.html#programming",
    "title": "Blogs I read",
    "section": "Programming",
    "text": "Programming\n\nPaul Shved: Cold Attic\nRagnar Groot Koerkamp: Curious Coding"
  },
  {
    "objectID": "posts/blogs-i-read.html#mathematical-physics",
    "href": "posts/blogs-i-read.html#mathematical-physics",
    "title": "Blogs I read",
    "section": "Mathematical physics",
    "text": "Mathematical physics\n\nJohn Baez: This Week’s Finds\nPeter Woit: Not Even Wrong"
  },
  {
    "objectID": "posts/blogs-i-read.html#comics",
    "href": "posts/blogs-i-read.html#comics",
    "title": "Blogs I read",
    "section": "Comics",
    "text": "Comics\n\nRandall Munroe: XKCD"
  },
  {
    "objectID": "posts/blogs-i-read.html#misc",
    "href": "posts/blogs-i-read.html#misc",
    "title": "Blogs I read",
    "section": "Misc",
    "text": "Misc\nAnd here is a list of blogs from Statistical Modeling, Causal Inference, and Social Science blog: link, which I should probably start reading."
  },
  {
    "objectID": "posts/no-free-lunch-in-research.html",
    "href": "posts/no-free-lunch-in-research.html",
    "title": "The no free lunch theorem for scientific research",
    "section": "",
    "text": "Yesterday David and I went for pizza after work. As typical for our conversations, we spent quite some time discussing applied statistics and machine learning, and reached our usual conclusion that logistic regression is a wonderful model in so many problems.\nHowever, finding logistic regression or other “simple” methods in research papers can be quite hard, as we tend to look for methodological novelty. As Kristin Lennox nicely summarized, “you don’t get a lot of points for doing really good statistics on really important problems, if these statistics were invented in 1950s”. (In particular, Cynthia Rudin investigated how much applied research goes into the most presitigious machine learning conferences).\nThis is one of the reasons for the phenomenon which everybody in the field knows too well: from time to time you take a paper claiming state-of-the-art performance (“They are 0.02% better on CIFAR-10 than others! Let’s apply it to my problem”), and then find out that the method requires heavy hyperparameter tuning and hours of playing with the brittleness that makes the method impossible to use in practice. And, what’s even worse, the performance isn’t that different from a simple baseline.\nSimilarly, there are voices from many statisticians raising the issue that several of the grandiose results, which often involve solving important problems with the state-of-the-art methodology, may be simply invalid.\nTo summarize, a perfect paper should use novel methodology, aim at solving an important problem, and be correct (which should go without saying). The no free lunch of scientific research says that you can pick two out of three, at most.\nThis “theorem” is not very serious and is, of course, not universal – there exist great papers, but achieving all three goals in one manuscript is very hard to execute and they are exceptions, not the standard. Additionally, I don’t want to dichotomise here: methodological novelty, problem importance and correctness have many facets and subtleties (and may also be hard to assess upfront!), so it’s better to think about the level of each of these traits desired in a study.\nAs a first-order approximation, I found myself usually doing research on either novel methodology (illustrated on toy problems, without direct applications) or working on problems, which I find practical and important, but which require standard and well-trusted tools (at least as a starting point and a baseline).\nOn a more positive note, some novel (as of today) methods will have become standard and well-trusted tools in the coming years, with a lot practical impact to come. And practical problems often lead to improvement on existing methods or asking fundamental questions (see Pasteur’s quadrant). And they usually are much harder to solve, than it seems at the beginning! Let’s finish with a quote from Andrew Gelman: “Whenever I have an applied project, I’m always trying to do the stupidest possible thing, that will allow me to get out of the project alive. Unfortunately, the stupidest possible thing, that could possibly work, always seems to be a little more complicated, than the most complicated thing I already know how to do.”"
  },
  {
    "objectID": "posts/discrete-intractable-likelihood.html",
    "href": "posts/discrete-intractable-likelihood.html",
    "title": "Learning models with discrete Fisher divergence",
    "section": "",
    "text": "ISBA 2024 was a phenomenal experience, with a lot of great people, conversations, and presented projects. Summarising it would take a separate post. Or two. Or three.\nIn this one, however, we’ll take a look only at one topic, presented by Takuo Matsubara. I remember that during the talk I was sitting amazed for the whole time: not only was the talk great and engaging, but also the method was particularly elegant. In January, I have been thinking about using the Fisher’s noncentral hypergeometric distribution, but its normalising constant is computationally too expensive. I wish I had known Takuo’s paper back then!\nThe paper (or the preprint) focuses on the following problem: we have a distribution defined on a discrete space \\(\\mathcal Y = \\{0, 1, \\dotsc, K-1\\}^G\\) and its PMF is everywhere positive and known up to the normalizing constant. I.e., we have\n\\[\n  p_\\theta(y) = \\frac{1}{\\mathcal Z(\\theta)} q_\\theta(y),\n\\]\nwhere \\(q_\\theta(y) &gt; 0\\) everywhere and this function is easy to evaluate. However, evaluating \\(\\mathcal Z(\\theta)\\) is prohibitively expensive, as it usually requires \\(O(K^G)\\) evaluations of \\(q_\\theta\\).\nTakuo’s framework helps to do inference in this model without the need to calculate \\(\\mathcal Z(\\theta)\\) at all! In fact, it applies to more general spaces, although in this blog post we restrict our attention to the special case above.\nBefore we discuss this method, let’s quickly summarise how likelihood-based inference works."
  },
  {
    "objectID": "posts/discrete-intractable-likelihood.html#kullbackleibler-divergence",
    "href": "posts/discrete-intractable-likelihood.html#kullbackleibler-divergence",
    "title": "Learning models with discrete Fisher divergence",
    "section": "Kullback–Leibler divergence",
    "text": "Kullback–Leibler divergence\nAs Takuo explained in his presentation, once we observe data points \\(y_1, \\dotsc, y_N\\), we can form the empirical distribution \\(p_\\text{emp} = \\frac{1}{N} \\sum_{n=1}^N \\delta_{y_n}\\) and consider the Kullback–Leibler divergence \\[\n\\mathrm{KL}(p_\\text{emp} \\parallel p_\\theta ) = -\\frac{1}{N} \\sum_{n=1}^N \\log p_\\theta(y_n) + \\frac{1}{N} \\sum_{n=1}^N \\log p_\\text{emp}(y_n) = -\\frac{1}{N} \\sum_{n=1}^N \\log p_\\theta(y_n) - H(p_\\text{emp}).\n\\]\nHence, \\(N \\cdot \\mathrm{KL}(p_\\text{emp} \\parallel p_\\theta)\\) is the negative loglikelihood (up to an additive constant, which depends on the entropy of the empirical data distribution), which can be then optimised in maximum likelihood approaches. A nice property is that \\(\\mathrm{KL}(p_1\\parallel p_2) \\ge 0\\) and becomes \\(0\\) if and only if \\(p_1 = p_2\\). When we have \\(N\\) large enough, so that \\(p_\\text{emp}\\) is close to the data distribution, using maximum likelihood should result in a distribution being “the closest” to the data distribution amongh the family \\(p_\\theta\\). In particular, under no misspecification we should rediscover the data distribution.\nAs Takuo explained, also Bayesian inference also proceeds in this manner: \\[\n  p( \\theta \\mid y_1, \\dotsc, y_N ) \\propto p(\\theta) \\cdot \\exp(-N\\cdot \\mathrm{KL}(p_\\text{emp} \\parallel p_\\theta) ),\n\\]\nwhere the entropy of \\(p_\\text{emp}\\) is effectively hidden in the proportionality constant.\nBefore Takuo’s talk, I haven’t thought about Bayesian inference in terms of the Kullback–Leibler divergence from the empirical data distribution \\(p_\\text{emp}\\) to the model \\(p_\\theta\\): on continuous spaces \\(p_\\text{emp}\\) is atomic, while \\(p_\\theta\\) is (usually) not and \\(\\mathrm{KL}(p_\\text{emp} \\parallel p_\\theta) = +\\infty\\) for all parameters \\(\\theta\\). However, for a discrete space \\(\\mathcal Y\\) both measures are necessarily atomic and this works nicely.\nHowever, maximum likelihood, minimisation of the Kullback–Leibler divergence, and Bayesian inference all rely on having the access to \\(\\log p_\\theta(y) = \\log q_\\theta(y) - \\log \\mathcal Z(\\theta)\\), which is not tractable in our case."
  },
  {
    "objectID": "posts/discrete-intractable-likelihood.html#discrete-fisher-divergence",
    "href": "posts/discrete-intractable-likelihood.html#discrete-fisher-divergence",
    "title": "Learning models with discrete Fisher divergence",
    "section": "Discrete Fisher divergence",
    "text": "Discrete Fisher divergence\nDefine operators on \\(\\mathcal Y\\) incrementing and decrementing a specific position: \\[\n  (\\mathcal I_g y)_h = \\begin{cases}\n    (y_g + 1) \\mod K &\\text{ if } g = h\\\\\n    y_h &\\text{ otherwise}\n  \\end{cases}\n\\] and \\[\n  (\\mathcal D_g y)_h = \\begin{cases}\n    (y_g - 1) \\mod K &\\text{ if } g = h\\\\\n    y_h &\\text{ otherwise}\n  \\end{cases}\n\\]\nwhere the “mod \\(K\\)” means that we “increment” \\(K-1\\) to \\(0\\) (and, conversely, “decrement” \\(0\\) to \\(K-1\\)). When the data are binary, \\(K=2\\), we have \\(\\mathcal I_g = \\mathcal D_g\\) and they reduce to flipping the \\(g\\)-th bit.\nThe discrete Fisher divergence is then given by \\[\n  \\mathrm{DFD}(p_1\\parallel p_2) = \\mathbb E_{y\\sim p_2}\\left[ \\sum_{g=1}^G \\left( \\frac{ p_1( \\mathcal D_gy ) }{ p_1(y) } \\right)^2 -2 \\frac{  p_1(y) }{ p_1(\\mathcal I_g y) }  \\right] + C(p_2),\n\\]\nwhere \\(C(p_2)\\) is a particular expression which does not depend on \\(p_1\\). In the paper you can find the formula for it, as well as the proof that \\(\\mathrm{DFD}(p_1\\parallel p_2)\\ge 0\\) and is zero if and only if \\(p_1 = p_2\\). Hence, a generalised posterior is proposed: \\[\n  p^{\\mathrm{DFD}} \\propto p(\\theta) \\cdot \\exp(-\\tau N\\cdot \\mathrm{DFD}(p_\\theta \\parallel p_\\text{emp} )),\n\\]\nwhere \\(\\tau\\) is the temperature parameter, used in generalised Bayesian inference and as a solution to model misspecification.\nNote that \\[\\begin{align*}\n\\mathrm{DFD}(p_\\theta \\parallel p_\\text{emp}) &= \\frac{1}{N} \\sum_{n=1}^N\\sum_{g=1}^G \\left(\\frac{ p_\\theta(\\mathcal D_g y_n) }{ p_\\theta(y_n) }\\right)^2 - 2 \\frac{p_\\theta(y_n)}{p_\\theta(\\mathcal{I}_{g}y_n)}  + C(p_\\text{emp}) \\\\\n    &= \\frac{1}{N} \\sum_{n=1}^N\\sum_{g=1}^G \\left(\\frac{ q_\\theta(\\mathcal D_g y_n) }{ q_\\theta(y_n) }\\right)^2 - 2 \\frac{q_\\theta(y_n)}{q_\\theta(\\mathcal{I}_{g}y_n)}  + C(p_\\text{emp})\n\\end{align*}\n\\]\nmeaning that it does not need the (intractable) normalising constant \\(\\mathcal Z(\\theta)\\)! This comes at the price of \\(O(NG)\\) evaluations of \\(q_\\theta\\) (rather than \\(O(N)\\) calls to \\(p_\\theta\\) as in the likelihood-based methods) and the fact that we are using now a generalised Bayesian approach, rather than the typical Bayesian one (the usual question is “what is the value of the temperature \\(\\tau\\) one should use?”).\nOverall, I very much like this idea, with potentially large impact on applications: in computational biology we use many discrete models and likelihood based methods could not be used because of intractable normalising constants."
  },
  {
    "objectID": "posts/discrete-intractable-likelihood.html#experiments-on-binary-data",
    "href": "posts/discrete-intractable-likelihood.html#experiments-on-binary-data",
    "title": "Learning models with discrete Fisher divergence",
    "section": "Experiments on binary data",
    "text": "Experiments on binary data\nLet’s consider \\(K=2\\), so that \\(\\mathcal X = \\{0, 1\\}^G\\). If \\(\\mathcal F_g := \\mathcal I_g = \\mathcal D_g\\) is the bitflip operator, the discrete Fisher divergence takes the form\n\\[\n  \\mathrm{DFD}(p_\\theta \\parallel p_\\text{emp})\n    = \\frac{1}{N} \\sum_{n=1}^N\\sum_{g=1}^G \\left(\\frac{ q_\\theta(\\mathcal F_g y_n) }{ q_\\theta(y_n) }\\right)^2 - 2 \\frac{q_\\theta(y_n)}{q_\\theta(\\mathcal{F}_{g}y_n)}  + C(p_\\text{emp}).\n\\]\nI will simulate the data by tossing \\(G\\) independent coins, each with its own bias \\(\\pi_g\\). In this case the likelihood is tractable, as it is just \\[\n  p_\\pi( y ) = \\prod_{g=1}^G \\pi_g^{y_g} (1-\\pi_g)^{1-y_g}.\n\\]\nIt is convenient to write this model in the exponential family form, by reparameterising it into log-odds, \\(\\alpha_g = \\log\\frac{\\pi_g}{1-\\pi_g}\\). Then, we have \\[\n  q_\\alpha(y) = \\exp\\left( \\sum_{g=1}^G \\alpha_g y_g \\right)\n\\]\n\n\nCode\nfrom functools import partial\nfrom typing import Callable\nfrom jaxtyping import Float, Int, Array\n\nimport numpy as np\nimport jax\nimport jax.numpy as jnp\nfrom jax.scipy import optimize\n\nrng = np.random.default_rng(42)\n\nn_samples: int = 100\nn_genes: int = 10\n\ndef logit(p):\n  return np.log(p) - np.log1p(-p)\n\ndef expit(x):\n  return 1.0 / (1 + np.exp(-x))\n\ntrue_bias = np.linspace(0.2, 0.7, n_genes)\ntrue_alpha = logit(true_bias)\n\nY = rng.binomial(1, p=true_bias, size=(n_samples, n_genes))\n\n\nThe simplest option is to calculate the maximum likelihood solution. Let’s do that:\n\n\nCode\ndef print_estimate_summary(alphas, name: str | None = None) -&gt; None:\n  if name is not None:\n    print(f\"----- {name} -----\")\n  print(\"Absolute error on the bias:\")\n  found_bias = expit(alphas)\n  print(true_bias - found_bias)\n\n  print(\"True bias:\")\n  print(true_bias)\n\nmle_bias = np.mean(Y, axis=0)\nmle_alpha = logit(mle_bias)\n\nprint_estimate_summary(mle_alpha, name=\"maximum likelihood\")\n\n\n----- maximum likelihood -----\nAbsolute error on the bias:\n[ 0.04       -0.07444444 -0.03888889  0.04666667 -0.02777778  0.00777778\n  0.01333333 -0.02111111 -0.06555556 -0.03      ]\nTrue bias:\n[0.2        0.25555556 0.31111111 0.36666667 0.42222222 0.47777778\n 0.53333333 0.58888889 0.64444444 0.7       ]\n\n\nAs we have the luxury of doing full Bayesian inference, let’s try it. We will use a hierarchical model, in which the prior on \\(\\alpha\\) is a flexible normal distribution:\n\n\nCode\nimport numpyro\nimport numpyro.distributions as dist\n\n\ndef independent_model(mutations):\n    n_samples, n_genes = mutations.shape\n    \n    mu = numpyro.sample(\"_mu\", dist.Normal(0.0, 5.0))\n    sigma = numpyro.sample(\"_sigma\", dist.HalfCauchy(scale=3.0))\n\n    z =  numpyro.sample('_z', dist.Normal(jnp.zeros(n_genes), 1))\n\n    alpha = numpyro.deterministic(\"alpha\", mu + sigma * z)    \n\n    with numpyro.plate(\"samples\", n_samples, dim=-2):\n        with numpyro.plate(\"genes\", n_genes, dim=-1):\n            numpyro.sample(\"obs\", dist.BernoulliLogits(alpha[None, :]), obs=mutations)\n\nkey = jax.random.PRNGKey(0)\n\nkey, subkey = jax.random.split(key)\n\nposterior = numpyro.infer.MCMC(\n  numpyro.infer.NUTS(independent_model),\n  num_chains=4,\n  num_samples=500,\n  num_warmup=500,\n)\nposterior.run(subkey, mutations=Y)\n\nposterior.print_summary()\n\nalpha_samples = posterior.get_samples()[\"alpha\"]\nbias_samples = expit(alpha_samples)\n\nposterior_mean_alpha = alpha_samples.mean(axis=0)\n\nprint_estimate_summary(posterior_mean_alpha, name=\"posterior mean\")\n\n\n/tmp/ipykernel_60846/1763543017.py:23: UserWarning: There are not enough devices to run parallel chains: expected 4 but got 1. Chains will be drawn sequentially. If you are running MCMC in CPU, consider using `numpyro.set_host_device_count(4)` at the beginning of your program. You can double-check how many devices are available in your system using `jax.local_device_count()`.\n  posterior = numpyro.infer.MCMC(\n  0%|          | 0/1000 [00:00&lt;?, ?it/s]warmup:   0%|          | 1/1000 [00:01&lt;22:10,  1.33s/it, 1 steps of size 2.34e+00. acc. prob=0.00]warmup:  45%|████▍     | 449/1000 [00:01&lt;00:01, 434.47it/s, 15 steps of size 3.03e-01. acc. prob=0.79]sample:  93%|█████████▎| 933/1000 [00:01&lt;00:00, 969.62it/s, 7 steps of size 1.94e-01. acc. prob=0.89] sample: 100%|██████████| 1000/1000 [00:01&lt;00:00, 646.29it/s, 15 steps of size 1.94e-01. acc. prob=0.89]\n  0%|          | 0/1000 [00:00&lt;?, ?it/s]warmup:  49%|████▉     | 494/1000 [00:00&lt;00:00, 4937.19it/s, 15 steps of size 3.51e-01. acc. prob=0.79]sample: 100%|█████████▉| 998/1000 [00:00&lt;00:00, 4996.62it/s, 7 steps of size 2.14e-01. acc. prob=0.91] sample: 100%|██████████| 1000/1000 [00:00&lt;00:00, 4971.57it/s, 15 steps of size 2.14e-01. acc. prob=0.91]\n  0%|          | 0/1000 [00:00&lt;?, ?it/s]warmup:  49%|████▉     | 489/1000 [00:00&lt;00:00, 4886.30it/s, 31 steps of size 1.42e-01. acc. prob=0.79]sample:  98%|█████████▊| 981/1000 [00:00&lt;00:00, 4903.42it/s, 15 steps of size 1.80e-01. acc. prob=0.94]sample: 100%|██████████| 1000/1000 [00:00&lt;00:00, 4884.00it/s, 15 steps of size 1.80e-01. acc. prob=0.94]\n  0%|          | 0/1000 [00:00&lt;?, ?it/s]warmup:  48%|████▊     | 482/1000 [00:00&lt;00:00, 4814.70it/s, 47 steps of size 1.64e-01. acc. prob=0.79]sample:  98%|█████████▊| 979/1000 [00:00&lt;00:00, 4901.74it/s, 15 steps of size 1.66e-01. acc. prob=0.93]sample: 100%|██████████| 1000/1000 [00:00&lt;00:00, 4869.76it/s, 15 steps of size 1.66e-01. acc. prob=0.93]\n\n\n\n                mean       std    median      5.0%     95.0%     n_eff     r_hat\n       _mu     -0.18      0.31     -0.18     -0.67      0.30    319.12      1.01\n    _sigma      0.92      0.29      0.86      0.50      1.34    290.48      1.01\n     _z[0]     -1.59      0.56     -1.55     -2.44     -0.68    428.13      1.01\n     _z[1]     -0.58      0.41     -0.57     -1.21      0.11    439.12      1.01\n     _z[2]     -0.48      0.40     -0.48     -1.12      0.16    452.72      1.01\n     _z[3]     -0.64      0.42     -0.63     -1.37      0.01    472.24      1.00\n     _z[4]     -0.03      0.39     -0.02     -0.65      0.60    450.87      1.01\n     _z[5]      0.07      0.39      0.08     -0.51      0.70    432.61      1.01\n     _z[6]      0.28      0.39      0.29     -0.37      0.90    408.89      1.01\n     _z[7]      0.70      0.42      0.70      0.02      1.39    363.06      1.01\n     _z[8]      1.19      0.49      1.19      0.39      2.00    343.35      1.00\n     _z[9]      1.29      0.49      1.29      0.42      2.07    350.32      1.00\n\nNumber of divergences: 0\n----- posterior mean -----\nAbsolute error on the bias:\n[ 0.02347436 -0.08082746 -0.04482673  0.04107978 -0.02729267  0.00835171\n  0.01834139 -0.01336413 -0.05330887 -0.01561599]\nTrue bias:\n[0.2        0.25555556 0.31111111 0.36666667 0.42222222 0.47777778\n 0.53333333 0.58888889 0.64444444 0.7       ]\n\n\nThis looks like both maximum likelihood and Bayesian inference do reasonable job in this problem. Let’s implement now the discrete Fisher divergence:\n\n\nCode\nDataPoint = Int[Array, \" G\"]\n\n\ndef bitflip(g: int, y: DataPoint) -&gt; DataPoint:\n  return y.at[g].set(1 - y[g])\n\n\ndef dfd_onepoint(\n  log_q: Callable[[DataPoint], float],\n  y: DataPoint,\n) -&gt; float:\n  log_qy: float = log_q(y)\n\n  def log_q_flip_fn(g: int):\n    return log_q(bitflip(g, y))\n\n  log_qflipped = jax.vmap(log_q_flip_fn)(jnp.arange(y.shape[0]))\n\n  log_ratio = log_qflipped - log_qy\n\n  return jnp.sum( jnp.exp(2 * log_ratio) - 2 * jnp.exp(-log_ratio))\n\n\ndef dfd(log_q, ys) -&gt; float:\n  f = partial(dfd_onepoint, log_q)\n\n  return jnp.mean(jax.vmap(f)(ys))\n\n\ndef linear(alpha: Float[Array, \" G\"], y: DataPoint) -&gt; float:\n  return jnp.sum(alpha * y)\n\n@jax.jit\ndef loss(alpha):\n  return dfd(partial(linear, alpha), Y)\n\nresult = optimize.minimize(loss, jnp.zeros(n_genes), method=\"BFGS\")\n\ndfd_alpha = result.x\n\nprint_estimate_summary(dfd_alpha, name=\"DFD\")\n\n\n----- DFD -----\nAbsolute error on the bias:\n[ 0.03998992 -0.07451569 -0.0388778   0.04673009 -0.02779001  0.00777489\n  0.01333484 -0.02109128 -0.06553831 -0.02999656]\nTrue bias:\n[0.2        0.25555556 0.31111111 0.36666667 0.42222222 0.47777778\n 0.53333333 0.58888889 0.64444444 0.7       ]\n\n\nWow, this looks pretty good to me! Let’s now visualise the performance of all methods:\n\n\nCode\nimport matplotlib.pyplot as plt\nplt.style.use(\"dark_background\")\n\nfig, axs = plt.subplots(1, 2, sharex=True, sharey=False, figsize=(4, 2), dpi=300)\nfor ax in axs:\n  ax.spines[[\"top\", \"right\"]].set_visible(False)\n\ncolors = {\n  \"true\": \"white\",\n  \"mle\": \"maroon\",\n  \"posterior_sample\": \"lightgrey\",\n  \"dfd\": \"gold\",\n}\n\nax = axs[0]\nx_axis = np.arange(1, n_genes + 1)\nax.set_xticks(x_axis)\nax.set_title(\"$\\\\alpha$\")\n\nax = axs[1]\nax.set_xticks(x_axis)\nax.set_title(\"$\\\\pi$\")\n\n\ndef plot(alpha_values, color, alpha=1.0, scatter: bool = True):\n  ax = axs[0]\n  ax.plot(x_axis, alpha_values, c=color, alpha=alpha)\n  if scatter:\n    ax.scatter(x_axis, alpha_values, c=color, alpha=alpha)\n\n  ax = axs[1]\n  bias_values = expit(alpha_values)\n  ax.plot(x_axis, bias_values, c=color, alpha=alpha)\n  if scatter:\n    ax.scatter(x_axis, bias_values, c=color, alpha=alpha)\n\n\nfor sample in alpha_samples[::40]:\n  plot(sample, colors[\"posterior_sample\"], alpha=0.1, scatter=False)\n\nplot(true_alpha, colors[\"true\"])\nplot(mle_alpha, colors[\"mle\"])\nplot(dfd_alpha, colors[\"dfd\"])\n\nfig.tight_layout()\n\n\n\n\n\nThis looks pretty good to me! Let’s do one more thing: sample from the DFD posterior:\n\n\nCode\ndef dfd_model(mutations, temperature):\n    n_samples, n_genes = mutations.shape\n    \n    mu = numpyro.sample(\"_mu\", dist.Normal(0.0, 5.0))\n    sigma = numpyro.sample(\"_sigma\", dist.HalfCauchy(scale=3.0))\n\n    z = numpyro.sample('_z', dist.Normal(jnp.zeros(n_genes), 1))\n\n    alpha = numpyro.deterministic(\"alpha\", mu + sigma * z)    \n\n    numpyro.factor(\"dfd\", -temperature * n_samples * loss(alpha))\n\n\ndfd_samples = {}\n\ntemperature_range = [0.01, 0.1, 0.5, 1.0, 2.0]\n\nfor temperature in temperature_range:\n  key, subkey = jax.random.split(key)\n\n  posterior = numpyro.infer.MCMC(\n    numpyro.infer.NUTS(dfd_model),\n    num_chains=4,\n    num_samples=500,\n    num_warmup=500,\n  )\n  \n  posterior.run(\n    subkey,\n    mutations=Y,\n    temperature=temperature,\n  )\n  print(f\"----- Temperature: {temperature:.2f} -----\")\n  posterior.print_summary()\n\n  dfd_samples[temperature] = posterior.get_samples()[\"alpha\"]\n\n\n/tmp/ipykernel_60846/908130843.py:21: UserWarning: There are not enough devices to run parallel chains: expected 4 but got 1. Chains will be drawn sequentially. If you are running MCMC in CPU, consider using `numpyro.set_host_device_count(4)` at the beginning of your program. You can double-check how many devices are available in your system using `jax.local_device_count()`.\n  posterior = numpyro.infer.MCMC(\n  0%|          | 0/1000 [00:00&lt;?, ?it/s]warmup:   0%|          | 1/1000 [00:01&lt;22:38,  1.36s/it, 1 steps of size 2.34e+00. acc. prob=0.00]warmup:  12%|█▏        | 121/1000 [00:01&lt;00:07, 114.70it/s, 31 steps of size 1.92e-01. acc. prob=0.77]warmup:  23%|██▎       | 229/1000 [00:01&lt;00:03, 230.07it/s, 31 steps of size 3.50e-02. acc. prob=0.78]warmup:  34%|███▍      | 345/1000 [00:01&lt;00:01, 365.50it/s, 31 steps of size 2.54e-01. acc. prob=0.78]warmup:  49%|████▊     | 486/1000 [00:01&lt;00:00, 544.55it/s, 31 steps of size 8.26e-02. acc. prob=0.78]sample:  63%|██████▎   | 631/1000 [00:01&lt;00:00, 723.12it/s, 15 steps of size 2.17e-01. acc. prob=0.77]sample:  78%|███████▊  | 783/1000 [00:01&lt;00:00, 897.42it/s, 15 steps of size 2.17e-01. acc. prob=0.76]sample:  94%|█████████▍| 944/1000 [00:02&lt;00:00, 1067.34it/s, 15 steps of size 2.17e-01. acc. prob=0.74]sample: 100%|██████████| 1000/1000 [00:02&lt;00:00, 475.86it/s, 15 steps of size 2.17e-01. acc. prob=0.74]\n  0%|          | 0/1000 [00:00&lt;?, ?it/s]warmup:  14%|█▍        | 138/1000 [00:00&lt;00:00, 1376.21it/s, 31 steps of size 1.25e-01. acc. prob=0.78]warmup:  28%|██▊       | 276/1000 [00:00&lt;00:00, 1326.50it/s, 7 steps of size 1.89e-01. acc. prob=0.78] warmup:  44%|████▍     | 439/1000 [00:00&lt;00:00, 1459.82it/s, 7 steps of size 1.84e-01. acc. prob=0.79]sample:  61%|██████    | 606/1000 [00:00&lt;00:00, 1539.66it/s, 15 steps of size 3.01e-01. acc. prob=0.89]sample:  78%|███████▊  | 776/1000 [00:00&lt;00:00, 1596.10it/s, 15 steps of size 3.01e-01. acc. prob=0.83]sample:  94%|█████████▍| 938/1000 [00:00&lt;00:00, 1601.95it/s, 15 steps of size 3.01e-01. acc. prob=0.84]sample: 100%|██████████| 1000/1000 [00:00&lt;00:00, 1551.24it/s, 15 steps of size 3.01e-01. acc. prob=0.84]\n  0%|          | 0/1000 [00:00&lt;?, ?it/s]warmup:  11%|█         | 109/1000 [00:00&lt;00:00, 1078.77it/s, 31 steps of size 4.75e-01. acc. prob=0.78]warmup:  26%|██▌       | 257/1000 [00:00&lt;00:00, 1312.59it/s, 15 steps of size 6.13e-02. acc. prob=0.78]warmup:  41%|████      | 407/1000 [00:00&lt;00:00, 1395.57it/s, 7 steps of size 5.14e-01. acc. prob=0.79] sample:  56%|█████▋    | 563/1000 [00:00&lt;00:00, 1458.26it/s, 15 steps of size 2.28e-01. acc. prob=0.78]sample:  73%|███████▎  | 727/1000 [00:00&lt;00:00, 1521.32it/s, 15 steps of size 2.28e-01. acc. prob=0.78]sample:  89%|████████▉ | 891/1000 [00:00&lt;00:00, 1559.12it/s, 15 steps of size 2.28e-01. acc. prob=0.76]sample: 100%|██████████| 1000/1000 [00:00&lt;00:00, 1489.34it/s, 15 steps of size 2.28e-01. acc. prob=0.77]\n  0%|          | 0/1000 [00:00&lt;?, ?it/s]warmup:  11%|█         | 107/1000 [00:00&lt;00:00, 1066.82it/s, 15 steps of size 4.23e-01. acc. prob=0.77]warmup:  21%|██▏       | 214/1000 [00:00&lt;00:00, 1067.49it/s, 15 steps of size 6.69e-02. acc. prob=0.78]warmup:  32%|███▎      | 325/1000 [00:00&lt;00:00, 1077.00it/s, 63 steps of size 1.11e-01. acc. prob=0.78]warmup:  46%|████▌     | 457/1000 [00:00&lt;00:00, 1171.20it/s, 15 steps of size 4.02e-01. acc. prob=0.79]sample:  57%|█████▊    | 575/1000 [00:00&lt;00:00, 1060.12it/s, 31 steps of size 1.64e-01. acc. prob=0.92]sample:  68%|██████▊   | 683/1000 [00:00&lt;00:00, 1043.34it/s, 31 steps of size 1.64e-01. acc. prob=0.93]sample:  79%|███████▉  | 791/1000 [00:00&lt;00:00, 1054.47it/s, 15 steps of size 1.64e-01. acc. prob=0.92]sample:  90%|█████████ | 903/1000 [00:00&lt;00:00, 1073.16it/s, 31 steps of size 1.64e-01. acc. prob=0.91]sample: 100%|██████████| 1000/1000 [00:00&lt;00:00, 1078.38it/s, 31 steps of size 1.64e-01. acc. prob=0.91]\n/tmp/ipykernel_60846/908130843.py:21: UserWarning: There are not enough devices to run parallel chains: expected 4 but got 1. Chains will be drawn sequentially. If you are running MCMC in CPU, consider using `numpyro.set_host_device_count(4)` at the beginning of your program. You can double-check how many devices are available in your system using `jax.local_device_count()`.\n  posterior = numpyro.infer.MCMC(\n  0%|          | 0/1000 [00:00&lt;?, ?it/s]warmup:   0%|          | 1/1000 [00:01&lt;24:04,  1.45s/it, 1 steps of size 2.34e+00. acc. prob=0.00]warmup:  10%|█         | 105/1000 [00:01&lt;00:09, 94.11it/s, 15 steps of size 3.41e-01. acc. prob=0.77]warmup:  18%|█▊        | 185/1000 [00:01&lt;00:04, 174.29it/s, 31 steps of size 1.46e-01. acc. prob=0.78]warmup:  28%|██▊       | 275/1000 [00:01&lt;00:02, 275.43it/s, 63 steps of size 6.17e-02. acc. prob=0.78]warmup:  36%|███▌      | 357/1000 [00:01&lt;00:01, 365.77it/s, 31 steps of size 2.04e-01. acc. prob=0.78]warmup:  46%|████▋     | 465/1000 [00:01&lt;00:01, 502.31it/s, 15 steps of size 4.07e-02. acc. prob=0.78]sample:  57%|█████▋    | 570/1000 [00:02&lt;00:00, 619.46it/s, 15 steps of size 1.29e-01. acc. prob=0.91]sample:  69%|██████▉   | 691/1000 [00:02&lt;00:00, 757.85it/s, 31 steps of size 1.29e-01. acc. prob=0.90]sample:  81%|████████  | 810/1000 [00:02&lt;00:00, 866.06it/s, 15 steps of size 1.29e-01. acc. prob=0.89]sample:  93%|█████████▎| 934/1000 [00:02&lt;00:00, 963.63it/s, 15 steps of size 1.29e-01. acc. prob=0.89]sample: 100%|██████████| 1000/1000 [00:02&lt;00:00, 415.10it/s, 15 steps of size 1.29e-01. acc. prob=0.89]\n  0%|          | 0/1000 [00:00&lt;?, ?it/s]warmup:   8%|▊         | 81/1000 [00:00&lt;00:01, 805.87it/s, 15 steps of size 1.37e-02. acc. prob=0.76]warmup:  17%|█▋        | 168/1000 [00:00&lt;00:00, 838.95it/s, 31 steps of size 7.85e-02. acc. prob=0.77]warmup:  25%|██▌       | 252/1000 [00:00&lt;00:00, 818.74it/s, 2 steps of size 2.54e-01. acc. prob=0.78] warmup:  35%|███▌      | 351/1000 [00:00&lt;00:00, 882.08it/s, 31 steps of size 1.21e-01. acc. prob=0.78]warmup:  46%|████▌     | 460/1000 [00:00&lt;00:00, 954.33it/s, 15 steps of size 3.02e-01. acc. prob=0.79]sample:  56%|█████▌    | 561/1000 [00:00&lt;00:00, 969.55it/s, 63 steps of size 1.16e-01. acc. prob=0.94]sample:  66%|██████▋   | 663/1000 [00:00&lt;00:00, 985.18it/s, 47 steps of size 1.16e-01. acc. prob=0.95]sample:  76%|███████▌  | 762/1000 [00:00&lt;00:00, 986.43it/s, 31 steps of size 1.16e-01. acc. prob=0.95]sample:  87%|████████▋ | 868/1000 [00:00&lt;00:00, 1008.45it/s, 15 steps of size 1.16e-01. acc. prob=0.94]sample:  98%|█████████▊| 979/1000 [00:01&lt;00:00, 1036.30it/s, 63 steps of size 1.16e-01. acc. prob=0.94]sample: 100%|██████████| 1000/1000 [00:01&lt;00:00, 968.19it/s, 15 steps of size 1.16e-01. acc. prob=0.94]\n  0%|          | 0/1000 [00:00&lt;?, ?it/s]warmup:   9%|▉         | 88/1000 [00:00&lt;00:01, 876.77it/s, 31 steps of size 2.28e-02. acc. prob=0.76]warmup:  18%|█▊        | 176/1000 [00:00&lt;00:00, 873.79it/s, 15 steps of size 1.80e-01. acc. prob=0.78]warmup:  26%|██▋       | 264/1000 [00:00&lt;00:00, 857.51it/s, 63 steps of size 4.83e-02. acc. prob=0.78]warmup:  36%|███▌      | 357/1000 [00:00&lt;00:00, 883.21it/s, 31 steps of size 1.13e-01. acc. prob=0.78]warmup:  45%|████▍     | 446/1000 [00:00&lt;00:00, 861.09it/s, 31 steps of size 1.80e-01. acc. prob=0.79]sample:  54%|█████▎    | 535/1000 [00:00&lt;00:00, 869.41it/s, 15 steps of size 1.32e-01. acc. prob=0.95]sample:  63%|██████▎   | 628/1000 [00:00&lt;00:00, 888.53it/s, 15 steps of size 1.32e-01. acc. prob=0.94]sample:  74%|███████▎  | 737/1000 [00:00&lt;00:00, 947.07it/s, 63 steps of size 1.32e-01. acc. prob=0.92]sample:  85%|████████▌ | 853/1000 [00:00&lt;00:00, 1011.16it/s, 63 steps of size 1.32e-01. acc. prob=0.93]sample:  96%|█████████▌| 962/1000 [00:01&lt;00:00, 1033.78it/s, 15 steps of size 1.32e-01. acc. prob=0.93]sample: 100%|██████████| 1000/1000 [00:01&lt;00:00, 952.63it/s, 31 steps of size 1.32e-01. acc. prob=0.93]\n  0%|          | 0/1000 [00:00&lt;?, ?it/s]warmup:   9%|▊         | 86/1000 [00:00&lt;00:01, 852.33it/s, 127 steps of size 3.91e-02. acc. prob=0.77]warmup:  17%|█▋        | 172/1000 [00:00&lt;00:00, 843.41it/s, 63 steps of size 1.43e-01. acc. prob=0.78]warmup:  28%|██▊       | 278/1000 [00:00&lt;00:00, 940.86it/s, 127 steps of size 8.49e-02. acc. prob=0.78]warmup:  42%|████▎     | 425/1000 [00:00&lt;00:00, 1145.04it/s, 23 steps of size 1.69e-01. acc. prob=0.79]sample:  54%|█████▍    | 540/1000 [00:00&lt;00:00, 1096.10it/s, 15 steps of size 1.14e-01. acc. prob=0.96]sample:  65%|██████▌   | 651/1000 [00:00&lt;00:00, 983.26it/s, 15 steps of size 1.14e-01. acc. prob=0.96] sample:  75%|███████▌  | 752/1000 [00:00&lt;00:00, 957.32it/s, 15 steps of size 1.14e-01. acc. prob=0.96]sample:  85%|████████▍ | 849/1000 [00:00&lt;00:00, 909.34it/s, 31 steps of size 1.14e-01. acc. prob=0.96]sample:  94%|█████████▍| 941/1000 [00:00&lt;00:00, 888.96it/s, 31 steps of size 1.14e-01. acc. prob=0.96]sample: 100%|██████████| 1000/1000 [00:01&lt;00:00, 943.11it/s, 15 steps of size 1.14e-01. acc. prob=0.96]\n/tmp/ipykernel_60846/908130843.py:21: UserWarning: There are not enough devices to run parallel chains: expected 4 but got 1. Chains will be drawn sequentially. If you are running MCMC in CPU, consider using `numpyro.set_host_device_count(4)` at the beginning of your program. You can double-check how many devices are available in your system using `jax.local_device_count()`.\n  posterior = numpyro.infer.MCMC(\n  0%|          | 0/1000 [00:00&lt;?, ?it/s]warmup:   0%|          | 1/1000 [00:01&lt;22:51,  1.37s/it, 1 steps of size 2.34e+00. acc. prob=0.00]warmup:   7%|▋         | 66/1000 [00:01&lt;00:15, 61.80it/s, 79 steps of size 2.44e-02. acc. prob=0.76]warmup:  13%|█▎        | 132/1000 [00:01&lt;00:06, 132.57it/s, 79 steps of size 1.10e-01. acc. prob=0.77]warmup:  19%|█▊        | 187/1000 [00:01&lt;00:04, 192.42it/s, 191 steps of size 6.23e-02. acc. prob=0.77]warmup:  26%|██▌       | 262/1000 [00:01&lt;00:02, 286.73it/s, 255 steps of size 1.26e-02. acc. prob=0.78]warmup:  32%|███▏      | 322/1000 [00:01&lt;00:01, 346.03it/s, 31 steps of size 9.07e-02. acc. prob=0.78] warmup:  39%|███▉      | 393/1000 [00:01&lt;00:01, 424.76it/s, 15 steps of size 1.07e-01. acc. prob=0.78]warmup:  46%|████▌     | 456/1000 [00:02&lt;00:01, 466.23it/s, 95 steps of size 3.43e-02. acc. prob=0.78]sample:  52%|█████▏    | 518/1000 [00:02&lt;00:00, 499.05it/s, 15 steps of size 6.86e-02. acc. prob=0.93]sample:  60%|█████▉    | 595/1000 [00:02&lt;00:00, 569.38it/s, 15 steps of size 6.86e-02. acc. prob=0.93]sample:  67%|██████▋   | 671/1000 [00:02&lt;00:00, 620.11it/s, 79 steps of size 6.86e-02. acc. prob=0.92]sample:  75%|███████▌  | 753/1000 [00:02&lt;00:00, 674.96it/s, 15 steps of size 6.86e-02. acc. prob=0.92]sample:  83%|████████▎ | 834/1000 [00:02&lt;00:00, 709.85it/s, 63 steps of size 6.86e-02. acc. prob=0.92]sample:  91%|█████████ | 910/1000 [00:02&lt;00:00, 723.79it/s, 95 steps of size 6.86e-02. acc. prob=0.92]sample:  99%|█████████▊| 986/1000 [00:02&lt;00:00, 713.96it/s, 63 steps of size 6.86e-02. acc. prob=0.92]sample: 100%|██████████| 1000/1000 [00:02&lt;00:00, 354.69it/s, 15 steps of size 6.86e-02. acc. prob=0.92]\n  0%|          | 0/1000 [00:00&lt;?, ?it/s]warmup:   7%|▋         | 70/1000 [00:00&lt;00:01, 697.31it/s, 15 steps of size 2.99e-02. acc. prob=0.76]warmup:  14%|█▍        | 140/1000 [00:00&lt;00:01, 655.18it/s, 95 steps of size 6.36e-02. acc. prob=0.77]warmup:  21%|██        | 206/1000 [00:00&lt;00:01, 633.97it/s, 31 steps of size 9.03e-02. acc. prob=0.78]warmup:  28%|██▊       | 275/1000 [00:00&lt;00:01, 652.89it/s, 31 steps of size 7.94e-02. acc. prob=0.78]warmup:  34%|███▍      | 343/1000 [00:00&lt;00:00, 659.87it/s, 63 steps of size 4.77e-02. acc. prob=0.78]warmup:  41%|████      | 410/1000 [00:00&lt;00:00, 652.81it/s, 31 steps of size 4.90e-02. acc. prob=0.78]warmup:  48%|████▊     | 480/1000 [00:00&lt;00:00, 661.88it/s, 127 steps of size 1.30e-01. acc. prob=0.78]sample:  55%|█████▍    | 547/1000 [00:00&lt;00:00, 648.31it/s, 23 steps of size 6.96e-02. acc. prob=0.95] sample:  61%|██████    | 612/1000 [00:00&lt;00:00, 630.96it/s, 31 steps of size 6.96e-02. acc. prob=0.94]sample:  68%|██████▊   | 676/1000 [00:01&lt;00:00, 621.71it/s, 63 steps of size 6.96e-02. acc. prob=0.94]sample:  74%|███████▍  | 745/1000 [00:01&lt;00:00, 638.28it/s, 63 steps of size 6.96e-02. acc. prob=0.94]sample:  81%|████████  | 809/1000 [00:01&lt;00:00, 626.69it/s, 111 steps of size 6.96e-02. acc. prob=0.93]sample:  87%|████████▋ | 872/1000 [00:01&lt;00:00, 600.58it/s, 15 steps of size 6.96e-02. acc. prob=0.93] sample:  94%|█████████▍| 938/1000 [00:01&lt;00:00, 615.82it/s, 47 steps of size 6.96e-02. acc. prob=0.93]sample: 100%|██████████| 1000/1000 [00:01&lt;00:00, 635.73it/s, 15 steps of size 6.96e-02. acc. prob=0.93]\n  0%|          | 0/1000 [00:00&lt;?, ?it/s]warmup:   8%|▊         | 78/1000 [00:00&lt;00:01, 778.00it/s, 15 steps of size 1.88e-02. acc. prob=0.76]warmup:  16%|█▌        | 156/1000 [00:00&lt;00:01, 671.59it/s, 159 steps of size 4.04e-02. acc. prob=0.77]warmup:  22%|██▎       | 225/1000 [00:00&lt;00:01, 666.63it/s, 15 steps of size 1.91e-01. acc. prob=0.78] warmup:  29%|██▉       | 293/1000 [00:00&lt;00:01, 633.64it/s, 15 steps of size 1.07e-01. acc. prob=0.78]warmup:  36%|███▋      | 364/1000 [00:00&lt;00:00, 658.81it/s, 15 steps of size 5.66e-02. acc. prob=0.78]warmup:  45%|████▌     | 454/1000 [00:00&lt;00:00, 733.90it/s, 191 steps of size 2.03e-02. acc. prob=0.78]sample:  53%|█████▎    | 529/1000 [00:00&lt;00:00, 671.25it/s, 47 steps of size 6.27e-02. acc. prob=0.93] sample:  61%|██████    | 608/1000 [00:00&lt;00:00, 703.53it/s, 31 steps of size 6.27e-02. acc. prob=0.91]sample:  68%|██████▊   | 685/1000 [00:00&lt;00:00, 722.10it/s, 15 steps of size 6.27e-02. acc. prob=0.91]sample:  77%|███████▋  | 769/1000 [00:01&lt;00:00, 754.85it/s, 47 steps of size 6.27e-02. acc. prob=0.91]sample:  86%|████████▌ | 856/1000 [00:01&lt;00:00, 788.83it/s, 15 steps of size 6.27e-02. acc. prob=0.92]sample:  94%|█████████▎| 936/1000 [00:01&lt;00:00, 784.19it/s, 15 steps of size 6.27e-02. acc. prob=0.92]sample: 100%|██████████| 1000/1000 [00:01&lt;00:00, 725.87it/s, 47 steps of size 6.27e-02. acc. prob=0.91]\n  0%|          | 0/1000 [00:00&lt;?, ?it/s]warmup:   6%|▋         | 64/1000 [00:00&lt;00:01, 638.15it/s, 7 steps of size 7.81e-03. acc. prob=0.75]warmup:  13%|█▎        | 131/1000 [00:00&lt;00:01, 654.81it/s, 63 steps of size 8.02e-02. acc. prob=0.77]warmup:  20%|██        | 201/1000 [00:00&lt;00:01, 652.50it/s, 383 steps of size 5.09e-02. acc. prob=0.78]warmup:  30%|██▉       | 295/1000 [00:00&lt;00:00, 755.78it/s, 255 steps of size 3.45e-02. acc. prob=0.78]warmup:  37%|███▋      | 371/1000 [00:00&lt;00:00, 754.53it/s, 63 steps of size 5.71e-02. acc. prob=0.78] warmup:  46%|████▌     | 455/1000 [00:00&lt;00:00, 771.29it/s, 255 steps of size 2.88e-02. acc. prob=0.78]sample:  53%|█████▎    | 533/1000 [00:00&lt;00:00, 744.85it/s, 47 steps of size 5.87e-02. acc. prob=0.92] sample:  62%|██████▏   | 619/1000 [00:00&lt;00:00, 777.03it/s, 111 steps of size 5.87e-02. acc. prob=0.92]sample:  70%|██████▉   | 697/1000 [00:00&lt;00:00, 757.16it/s, 15 steps of size 5.87e-02. acc. prob=0.93] sample:  78%|███████▊  | 785/1000 [00:01&lt;00:00, 791.90it/s, 63 steps of size 5.87e-02. acc. prob=0.91]sample:  86%|████████▋ | 865/1000 [00:01&lt;00:00, 788.40it/s, 31 steps of size 5.87e-02. acc. prob=0.92]sample:  95%|█████████▌| 953/1000 [00:01&lt;00:00, 811.67it/s, 63 steps of size 5.87e-02. acc. prob=0.92]sample: 100%|██████████| 1000/1000 [00:01&lt;00:00, 766.68it/s, 95 steps of size 5.87e-02. acc. prob=0.92]\n/tmp/ipykernel_60846/908130843.py:21: UserWarning: There are not enough devices to run parallel chains: expected 4 but got 1. Chains will be drawn sequentially. If you are running MCMC in CPU, consider using `numpyro.set_host_device_count(4)` at the beginning of your program. You can double-check how many devices are available in your system using `jax.local_device_count()`.\n  posterior = numpyro.infer.MCMC(\n  0%|          | 0/1000 [00:00&lt;?, ?it/s]warmup:   0%|          | 1/1000 [00:01&lt;24:31,  1.47s/it, 1 steps of size 2.34e+00. acc. prob=0.00]warmup:   5%|▍         | 49/1000 [00:01&lt;00:22, 43.00it/s, 63 steps of size 1.83e-02. acc. prob=0.75]warmup:  11%|█         | 108/1000 [00:01&lt;00:08, 104.09it/s, 111 steps of size 1.33e-01. acc. prob=0.77]warmup:  16%|█▌        | 158/1000 [00:01&lt;00:05, 158.20it/s, 63 steps of size 2.03e-02. acc. prob=0.77] warmup:  21%|██▏       | 214/1000 [00:01&lt;00:03, 224.04it/s, 7 steps of size 5.26e-02. acc. prob=0.78] warmup:  28%|██▊       | 276/1000 [00:01&lt;00:02, 296.36it/s, 255 steps of size 6.84e-02. acc. prob=0.78]warmup:  34%|███▍      | 338/1000 [00:02&lt;00:01, 364.10it/s, 135 steps of size 8.97e-02. acc. prob=0.78]warmup:  40%|████      | 400/1000 [00:02&lt;00:01, 421.03it/s, 95 steps of size 6.67e-02. acc. prob=0.78] warmup:  46%|████▋     | 463/1000 [00:02&lt;00:01, 466.42it/s, 511 steps of size 3.62e-02. acc. prob=0.78]sample:  52%|█████▏    | 521/1000 [00:02&lt;00:00, 493.23it/s, 15 steps of size 4.22e-02. acc. prob=0.90] sample:  58%|█████▊    | 584/1000 [00:02&lt;00:00, 527.39it/s, 63 steps of size 4.22e-02. acc. prob=0.93]sample:  64%|██████▍   | 643/1000 [00:02&lt;00:00, 506.63it/s, 63 steps of size 4.22e-02. acc. prob=0.94]sample:  70%|███████   | 705/1000 [00:02&lt;00:00, 536.83it/s, 15 steps of size 4.22e-02. acc. prob=0.95]sample:  76%|███████▋  | 765/1000 [00:02&lt;00:00, 553.06it/s, 175 steps of size 4.22e-02. acc. prob=0.94]sample:  83%|████████▎ | 832/1000 [00:02&lt;00:00, 582.79it/s, 87 steps of size 4.22e-02. acc. prob=0.95] sample:  89%|████████▉ | 893/1000 [00:03&lt;00:00, 564.51it/s, 15 steps of size 4.22e-02. acc. prob=0.95]sample:  95%|█████████▌| 951/1000 [00:03&lt;00:00, 566.00it/s, 79 steps of size 4.22e-02. acc. prob=0.95]sample: 100%|██████████| 1000/1000 [00:03&lt;00:00, 310.19it/s, 31 steps of size 4.22e-02. acc. prob=0.95]\n  0%|          | 0/1000 [00:00&lt;?, ?it/s]warmup:  10%|▉         | 99/1000 [00:00&lt;00:00, 980.29it/s, 63 steps of size 2.15e-02. acc. prob=0.77]warmup:  20%|█▉        | 198/1000 [00:00&lt;00:01, 681.27it/s, 79 steps of size 8.71e-02. acc. prob=0.78]warmup:  27%|██▋       | 272/1000 [00:00&lt;00:01, 570.17it/s, 7 steps of size 2.75e-02. acc. prob=0.78] warmup:  34%|███▍      | 339/1000 [00:00&lt;00:01, 599.04it/s, 39 steps of size 6.02e-02. acc. prob=0.78]warmup:  42%|████▏     | 424/1000 [00:00&lt;00:00, 672.02it/s, 95 steps of size 1.16e-01. acc. prob=0.78]warmup:  50%|████▉     | 495/1000 [00:00&lt;00:00, 606.26it/s, 15 steps of size 2.15e-02. acc. prob=0.78]sample:  56%|█████▌    | 559/1000 [00:00&lt;00:00, 603.29it/s, 79 steps of size 3.82e-02. acc. prob=0.93]sample:  62%|██████▏   | 622/1000 [00:00&lt;00:00, 608.28it/s, 15 steps of size 3.82e-02. acc. prob=0.94]sample:  68%|██████▊   | 685/1000 [00:01&lt;00:00, 608.70it/s, 31 steps of size 3.82e-02. acc. prob=0.94]sample:  75%|███████▍  | 747/1000 [00:01&lt;00:00, 605.91it/s, 15 steps of size 3.82e-02. acc. prob=0.93]sample:  81%|████████  | 809/1000 [00:01&lt;00:00, 608.14it/s, 15 steps of size 3.82e-02. acc. prob=0.94]sample:  87%|████████▋ | 872/1000 [00:01&lt;00:00, 614.24it/s, 95 steps of size 3.82e-02. acc. prob=0.94]sample:  93%|█████████▎| 934/1000 [00:01&lt;00:00, 606.82it/s, 63 steps of size 3.82e-02. acc. prob=0.94]sample: 100%|█████████▉| 996/1000 [00:01&lt;00:00, 609.79it/s, 31 steps of size 3.82e-02. acc. prob=0.94]sample: 100%|██████████| 1000/1000 [00:01&lt;00:00, 619.40it/s, 31 steps of size 3.82e-02. acc. prob=0.94]\n  0%|          | 0/1000 [00:00&lt;?, ?it/s]warmup:   9%|▉         | 92/1000 [00:00&lt;00:00, 914.80it/s, 31 steps of size 1.27e-02. acc. prob=0.76]warmup:  18%|█▊        | 184/1000 [00:00&lt;00:01, 538.61it/s, 63 steps of size 1.11e-01. acc. prob=0.78]warmup:  25%|██▍       | 247/1000 [00:00&lt;00:01, 402.53it/s, 63 steps of size 4.24e-02. acc. prob=0.78]warmup:  30%|███       | 305/1000 [00:00&lt;00:01, 446.32it/s, 15 steps of size 1.24e-01. acc. prob=0.78]warmup:  37%|███▋      | 368/1000 [00:00&lt;00:01, 492.28it/s, 103 steps of size 9.20e-02. acc. prob=0.78]warmup:  43%|████▎     | 433/1000 [00:00&lt;00:01, 533.02it/s, 47 steps of size 7.91e-02. acc. prob=0.79] warmup:  50%|████▉     | 499/1000 [00:00&lt;00:00, 566.63it/s, 191 steps of size 2.37e-02. acc. prob=0.78]sample:  56%|█████▌    | 562/1000 [00:01&lt;00:00, 578.15it/s, 167 steps of size 4.39e-02. acc. prob=0.93]sample:  62%|██████▏   | 624/1000 [00:01&lt;00:00, 587.64it/s, 63 steps of size 4.39e-02. acc. prob=0.92] sample:  70%|██████▉   | 698/1000 [00:01&lt;00:00, 631.73it/s, 31 steps of size 4.39e-02. acc. prob=0.91]sample:  76%|███████▋  | 763/1000 [00:01&lt;00:00, 598.29it/s, 95 steps of size 4.39e-02. acc. prob=0.92]sample:  83%|████████▎ | 831/1000 [00:01&lt;00:00, 616.18it/s, 127 steps of size 4.39e-02. acc. prob=0.92]sample:  91%|█████████ | 911/1000 [00:01&lt;00:00, 667.86it/s, 15 steps of size 4.39e-02. acc. prob=0.92] sample:  98%|█████████▊| 979/1000 [00:01&lt;00:00, 643.35it/s, 95 steps of size 4.39e-02. acc. prob=0.92]sample: 100%|██████████| 1000/1000 [00:01&lt;00:00, 579.95it/s, 15 steps of size 4.39e-02. acc. prob=0.92]\n  0%|          | 0/1000 [00:00&lt;?, ?it/s]warmup:   6%|▌         | 58/1000 [00:00&lt;00:01, 577.58it/s, 15 steps of size 2.84e-02. acc. prob=0.75]warmup:  13%|█▎        | 133/1000 [00:00&lt;00:01, 678.18it/s, 31 steps of size 1.48e-01. acc. prob=0.77]warmup:  20%|██        | 201/1000 [00:00&lt;00:01, 524.95it/s, 31 steps of size 1.07e-01. acc. prob=0.78]warmup:  26%|██▋       | 265/1000 [00:00&lt;00:01, 560.97it/s, 191 steps of size 3.58e-02. acc. prob=0.78]warmup:  34%|███▎      | 335/1000 [00:00&lt;00:01, 603.47it/s, 159 steps of size 8.17e-02. acc. prob=0.78]warmup:  40%|███▉      | 398/1000 [00:00&lt;00:00, 607.00it/s, 135 steps of size 1.05e-01. acc. prob=0.78]warmup:  46%|████▌     | 461/1000 [00:00&lt;00:00, 586.03it/s, 127 steps of size 1.43e-02. acc. prob=0.78]sample:  52%|█████▏    | 523/1000 [00:00&lt;00:00, 591.42it/s, 103 steps of size 3.85e-02. acc. prob=0.96]sample:  60%|█████▉    | 595/1000 [00:00&lt;00:00, 627.54it/s, 31 steps of size 3.85e-02. acc. prob=0.95] sample:  66%|██████▌   | 659/1000 [00:01&lt;00:00, 601.07it/s, 15 steps of size 3.85e-02. acc. prob=0.95]sample:  72%|███████▏  | 720/1000 [00:01&lt;00:00, 586.66it/s, 47 steps of size 3.85e-02. acc. prob=0.94]sample:  78%|███████▊  | 780/1000 [00:01&lt;00:00, 578.14it/s, 47 steps of size 3.85e-02. acc. prob=0.94]sample:  84%|████████▍ | 843/1000 [00:01&lt;00:00, 591.36it/s, 95 steps of size 3.85e-02. acc. prob=0.94]sample:  90%|█████████ | 903/1000 [00:01&lt;00:00, 581.78it/s, 31 steps of size 3.85e-02. acc. prob=0.94]sample:  97%|█████████▋| 971/1000 [00:01&lt;00:00, 608.61it/s, 31 steps of size 3.85e-02. acc. prob=0.94]sample: 100%|██████████| 1000/1000 [00:01&lt;00:00, 595.93it/s, 31 steps of size 3.85e-02. acc. prob=0.94]\n/tmp/ipykernel_60846/908130843.py:21: UserWarning: There are not enough devices to run parallel chains: expected 4 but got 1. Chains will be drawn sequentially. If you are running MCMC in CPU, consider using `numpyro.set_host_device_count(4)` at the beginning of your program. You can double-check how many devices are available in your system using `jax.local_device_count()`.\n  posterior = numpyro.infer.MCMC(\n  0%|          | 0/1000 [00:00&lt;?, ?it/s]warmup:   0%|          | 1/1000 [00:01&lt;22:11,  1.33s/it, 1 steps of size 2.34e+00. acc. prob=0.00]warmup:   6%|▋         | 64/1000 [00:01&lt;00:15, 61.23it/s, 319 steps of size 9.04e-03. acc. prob=0.75]warmup:  11%|█         | 111/1000 [00:01&lt;00:08, 110.52it/s, 63 steps of size 3.10e-02. acc. prob=0.76]warmup:  16%|█▋        | 164/1000 [00:01&lt;00:04, 169.33it/s, 447 steps of size 1.95e-02. acc. prob=0.77]warmup:  22%|██▏       | 217/1000 [00:01&lt;00:03, 230.57it/s, 47 steps of size 4.15e-02. acc. prob=0.78] warmup:  28%|██▊       | 276/1000 [00:01&lt;00:02, 299.04it/s, 383 steps of size 2.14e-02. acc. prob=0.78]warmup:  33%|███▎      | 329/1000 [00:01&lt;00:01, 346.68it/s, 447 steps of size 2.53e-02. acc. prob=0.78]warmup:  39%|███▉      | 391/1000 [00:02&lt;00:01, 408.89it/s, 111 steps of size 6.06e-02. acc. prob=0.78]warmup:  46%|████▌     | 455/1000 [00:02&lt;00:01, 465.08it/s, 159 steps of size 2.42e-02. acc. prob=0.78]sample:  51%|█████     | 512/1000 [00:02&lt;00:01, 430.42it/s, 31 steps of size 3.50e-02. acc. prob=0.95] sample:  56%|█████▋    | 563/1000 [00:02&lt;00:01, 413.99it/s, 319 steps of size 3.50e-02. acc. prob=0.95]sample:  61%|██████    | 610/1000 [00:02&lt;00:00, 423.45it/s, 127 steps of size 3.50e-02. acc. prob=0.95]sample:  66%|██████▌   | 662/1000 [00:02&lt;00:00, 442.35it/s, 191 steps of size 3.50e-02. acc. prob=0.94]sample:  71%|███████   | 711/1000 [00:02&lt;00:00, 453.16it/s, 63 steps of size 3.50e-02. acc. prob=0.94] sample:  76%|███████▋  | 763/1000 [00:02&lt;00:00, 465.17it/s, 191 steps of size 3.50e-02. acc. prob=0.93]sample:  81%|████████  | 812/1000 [00:02&lt;00:00, 465.18it/s, 15 steps of size 3.50e-02. acc. prob=0.93] sample:  87%|████████▋ | 870/1000 [00:03&lt;00:00, 494.00it/s, 159 steps of size 3.50e-02. acc. prob=0.93]sample:  92%|█████████▏| 921/1000 [00:03&lt;00:00, 477.23it/s, 15 steps of size 3.50e-02. acc. prob=0.93] sample:  97%|█████████▋| 970/1000 [00:03&lt;00:00, 454.22it/s, 15 steps of size 3.50e-02. acc. prob=0.93]sample: 100%|██████████| 1000/1000 [00:03&lt;00:00, 297.69it/s, 15 steps of size 3.50e-02. acc. prob=0.93]\n  0%|          | 0/1000 [00:00&lt;?, ?it/s]warmup:   6%|▌         | 61/1000 [00:00&lt;00:01, 605.09it/s, 63 steps of size 6.04e-03. acc. prob=0.74]warmup:  12%|█▏        | 122/1000 [00:00&lt;00:01, 584.52it/s, 54 steps of size 1.01e-02. acc. prob=0.76]warmup:  18%|█▊        | 181/1000 [00:00&lt;00:01, 576.41it/s, 63 steps of size 5.78e-02. acc. prob=0.77]warmup:  24%|██▍       | 239/1000 [00:00&lt;00:01, 535.90it/s, 31 steps of size 1.15e-01. acc. prob=0.78]warmup:  29%|██▉       | 293/1000 [00:00&lt;00:01, 442.11it/s, 15 steps of size 1.02e-01. acc. prob=0.78]warmup:  34%|███▍      | 340/1000 [00:00&lt;00:01, 449.55it/s, 127 steps of size 7.71e-02. acc. prob=0.78]warmup:  40%|████      | 401/1000 [00:00&lt;00:01, 487.35it/s, 255 steps of size 4.94e-02. acc. prob=0.78]warmup:  45%|████▌     | 454/1000 [00:00&lt;00:01, 474.94it/s, 767 steps of size 8.95e-03. acc. prob=0.78]sample:  50%|█████     | 503/1000 [00:01&lt;00:01, 439.51it/s, 175 steps of size 2.65e-02. acc. prob=0.96]sample:  55%|█████▍    | 548/1000 [00:01&lt;00:01, 432.44it/s, 175 steps of size 2.65e-02. acc. prob=0.95]sample:  59%|█████▉    | 592/1000 [00:01&lt;00:01, 403.54it/s, 31 steps of size 2.65e-02. acc. prob=0.96] sample:  63%|██████▎   | 634/1000 [00:01&lt;00:00, 402.29it/s, 255 steps of size 2.65e-02. acc. prob=0.95]sample:  68%|██████▊   | 678/1000 [00:01&lt;00:00, 412.09it/s, 255 steps of size 2.65e-02. acc. prob=0.95]sample:  72%|███████▏  | 723/1000 [00:01&lt;00:00, 415.83it/s, 287 steps of size 2.65e-02. acc. prob=0.95]sample:  76%|███████▋  | 765/1000 [00:01&lt;00:00, 383.89it/s, 15 steps of size 2.65e-02. acc. prob=0.95] sample:  81%|████████  | 811/1000 [00:01&lt;00:00, 402.33it/s, 95 steps of size 2.65e-02. acc. prob=0.95]sample:  85%|████████▌ | 852/1000 [00:01&lt;00:00, 388.70it/s, 95 steps of size 2.65e-02. acc. prob=0.95]sample:  91%|█████████ | 910/1000 [00:02&lt;00:00, 438.71it/s, 127 steps of size 2.65e-02. acc. prob=0.96]sample:  96%|█████████▌| 955/1000 [00:02&lt;00:00, 409.88it/s, 255 steps of size 2.65e-02. acc. prob=0.96]sample: 100%|█████████▉| 997/1000 [00:02&lt;00:00, 398.76it/s, 15 steps of size 2.65e-02. acc. prob=0.95] sample: 100%|██████████| 1000/1000 [00:02&lt;00:00, 433.97it/s, 127 steps of size 2.65e-02. acc. prob=0.95]\n  0%|          | 0/1000 [00:00&lt;?, ?it/s]warmup:   5%|▍         | 49/1000 [00:00&lt;00:01, 488.03it/s, 39 steps of size 1.41e-02. acc. prob=0.74]warmup:  11%|█         | 110/1000 [00:00&lt;00:01, 554.03it/s, 111 steps of size 8.65e-02. acc. prob=0.77]warmup:  17%|█▋        | 166/1000 [00:00&lt;00:01, 531.65it/s, 31 steps of size 4.23e-02. acc. prob=0.77] warmup:  22%|██▏       | 220/1000 [00:00&lt;00:01, 469.94it/s, 31 steps of size 3.82e-02. acc. prob=0.78]warmup:  27%|██▋       | 270/1000 [00:00&lt;00:01, 470.52it/s, 191 steps of size 1.21e-02. acc. prob=0.78]warmup:  32%|███▏      | 318/1000 [00:00&lt;00:01, 424.56it/s, 239 steps of size 2.51e-02. acc. prob=0.78]warmup:  38%|███▊      | 378/1000 [00:00&lt;00:01, 470.23it/s, 95 steps of size 3.77e-02. acc. prob=0.78] warmup:  43%|████▎     | 427/1000 [00:00&lt;00:01, 465.21it/s, 47 steps of size 2.71e-02. acc. prob=0.78]warmup:  48%|████▊     | 475/1000 [00:01&lt;00:01, 414.12it/s, 191 steps of size 2.49e-02. acc. prob=0.78]sample:  52%|█████▏    | 520/1000 [00:01&lt;00:01, 421.12it/s, 199 steps of size 3.39e-02. acc. prob=0.91]sample:  56%|█████▋    | 564/1000 [00:01&lt;00:01, 421.10it/s, 15 steps of size 3.39e-02. acc. prob=0.93] sample:  61%|██████    | 610/1000 [00:01&lt;00:00, 430.16it/s, 95 steps of size 3.39e-02. acc. prob=0.93]sample:  65%|██████▌   | 654/1000 [00:01&lt;00:00, 402.25it/s, 79 steps of size 3.39e-02. acc. prob=0.93]sample:  72%|███████▏  | 716/1000 [00:01&lt;00:00, 461.58it/s, 31 steps of size 3.39e-02. acc. prob=0.93]sample:  76%|███████▋  | 764/1000 [00:01&lt;00:00, 465.96it/s, 95 steps of size 3.39e-02. acc. prob=0.92]sample:  81%|████████  | 812/1000 [00:01&lt;00:00, 452.43it/s, 175 steps of size 3.39e-02. acc. prob=0.93]sample:  86%|████████▌ | 861/1000 [00:01&lt;00:00, 462.43it/s, 15 steps of size 3.39e-02. acc. prob=0.92] sample:  91%|█████████ | 908/1000 [00:02&lt;00:00, 439.42it/s, 15 steps of size 3.39e-02. acc. prob=0.92]sample:  95%|█████████▌| 953/1000 [00:02&lt;00:00, 438.54it/s, 31 steps of size 3.39e-02. acc. prob=0.92]sample: 100%|██████████| 1000/1000 [00:02&lt;00:00, 453.73it/s, 15 steps of size 3.39e-02. acc. prob=0.92]\n  0%|          | 0/1000 [00:00&lt;?, ?it/s]warmup:   7%|▋         | 72/1000 [00:00&lt;00:01, 678.61it/s, 255 steps of size 1.16e-02. acc. prob=0.75]warmup:  14%|█▍        | 140/1000 [00:00&lt;00:01, 454.79it/s, 39 steps of size 7.82e-03. acc. prob=0.77]warmup:  19%|█▉        | 190/1000 [00:00&lt;00:02, 352.63it/s, 159 steps of size 3.29e-02. acc. prob=0.77]warmup:  24%|██▎       | 236/1000 [00:00&lt;00:02, 381.03it/s, 79 steps of size 6.74e-02. acc. prob=0.78] warmup:  28%|██▊       | 278/1000 [00:00&lt;00:02, 344.27it/s, 15 steps of size 6.07e-02. acc. prob=0.78]warmup:  32%|███▎      | 325/1000 [00:00&lt;00:01, 375.88it/s, 45 steps of size 2.14e-02. acc. prob=0.78]warmup:  37%|███▋      | 374/1000 [00:00&lt;00:01, 405.79it/s, 159 steps of size 4.15e-02. acc. prob=0.78]warmup:  43%|████▎     | 427/1000 [00:01&lt;00:01, 440.22it/s, 7 steps of size 3.09e-02. acc. prob=0.78]  warmup:  47%|████▋     | 473/1000 [00:01&lt;00:01, 396.00it/s, 15 steps of size 5.15e-02. acc. prob=0.78]sample:  52%|█████▏    | 520/1000 [00:01&lt;00:01, 415.52it/s, 63 steps of size 3.29e-02. acc. prob=0.91]sample:  56%|█████▋    | 564/1000 [00:01&lt;00:01, 418.76it/s, 15 steps of size 3.29e-02. acc. prob=0.92]sample:  62%|██████▏   | 623/1000 [00:01&lt;00:00, 462.00it/s, 159 steps of size 3.29e-02. acc. prob=0.91]sample:  67%|██████▋   | 671/1000 [00:01&lt;00:00, 425.96it/s, 15 steps of size 3.29e-02. acc. prob=0.91] sample:  72%|███████▏  | 715/1000 [00:01&lt;00:00, 410.81it/s, 7 steps of size 3.29e-02. acc. prob=0.91] sample:  76%|███████▌  | 757/1000 [00:01&lt;00:00, 392.43it/s, 15 steps of size 3.29e-02. acc. prob=0.91]sample:  81%|████████  | 808/1000 [00:01&lt;00:00, 423.23it/s, 31 steps of size 3.29e-02. acc. prob=0.92]sample:  85%|████████▌ | 852/1000 [00:02&lt;00:00, 409.48it/s, 95 steps of size 3.29e-02. acc. prob=0.92]sample:  90%|████████▉ | 897/1000 [00:02&lt;00:00, 416.68it/s, 143 steps of size 3.29e-02. acc. prob=0.92]sample:  94%|█████████▍| 944/1000 [00:02&lt;00:00, 430.65it/s, 255 steps of size 3.29e-02. acc. prob=0.91]sample:  99%|█████████▉| 988/1000 [00:02&lt;00:00, 406.61it/s, 111 steps of size 3.29e-02. acc. prob=0.91]sample: 100%|██████████| 1000/1000 [00:02&lt;00:00, 412.09it/s, 31 steps of size 3.29e-02. acc. prob=0.91]\n\n\n----- Temperature: 0.01 -----\n\n                mean       std    median      5.0%     95.0%     n_eff     r_hat\n       _mu     -0.13      0.26     -0.14     -0.52      0.31    727.16      1.00\n    _sigma      0.43      0.29      0.39      0.00      0.82    547.15      1.00\n     _z[0]     -0.85      0.99     -0.92     -2.65      0.62   1237.86      1.00\n     _z[1]     -0.30      0.82     -0.32     -1.66      1.03   1398.45      1.00\n     _z[2]     -0.26      0.84     -0.28     -1.67      1.07   1504.57      1.00\n     _z[3]     -0.33      0.87     -0.34     -1.72      1.10   1825.18      1.00\n     _z[4]     -0.02      0.85     -0.02     -1.50      1.29   1658.80      1.00\n     _z[5]      0.01      0.84      0.01     -1.28      1.36   1918.23      1.00\n     _z[6]      0.14      0.81      0.13     -1.14      1.43   1666.27      1.00\n     _z[7]      0.30      0.79      0.32     -1.08      1.52   1669.95      1.00\n     _z[8]      0.56      0.85      0.57     -0.82      1.99   1633.24      1.00\n     _z[9]      0.57      0.86      0.59     -0.86      1.94   1463.64      1.00\n\nNumber of divergences: 21\n----- Temperature: 0.10 -----\n\n                mean       std    median      5.0%     95.0%     n_eff     r_hat\n       _mu     -0.14      0.33     -0.13     -0.61      0.40    228.95      1.00\n    _sigma      0.91      0.28      0.86      0.52      1.29    226.41      1.03\n     _z[0]     -1.75      0.58     -1.72     -2.65     -0.76    228.53      1.02\n     _z[1]     -0.60      0.43     -0.59     -1.32      0.05    280.44      1.01\n     _z[2]     -0.50      0.42     -0.50     -1.17      0.23    356.88      1.00\n     _z[3]     -0.65      0.42     -0.65     -1.29      0.07    299.21      1.01\n     _z[4]     -0.05      0.41     -0.06     -0.75      0.59    342.85      1.00\n     _z[5]      0.03      0.40      0.04     -0.60      0.68    315.56      1.00\n     _z[6]      0.23      0.41      0.22     -0.42      0.88    315.50      1.00\n     _z[7]      0.62      0.43      0.62     -0.13      1.26    330.74      1.01\n     _z[8]      1.13      0.47      1.14      0.34      1.85    310.08      1.01\n     _z[9]      1.25      0.49      1.24      0.45      2.03    286.92      1.02\n\nNumber of divergences: 0\n----- Temperature: 0.50 -----\n\n                mean       std    median      5.0%     95.0%     n_eff     r_hat\n       _mu     -0.13      0.27     -0.14     -0.55      0.33    193.62      1.03\n    _sigma      0.92      0.25      0.87      0.54      1.26    216.76      1.00\n     _z[0]     -1.76      0.51     -1.73     -2.68     -1.00    258.98      1.01\n     _z[1]     -0.64      0.33     -0.64     -1.21     -0.10    278.92      1.02\n     _z[2]     -0.54      0.32     -0.54     -1.06     -0.01    272.43      1.02\n     _z[3]     -0.70      0.33     -0.70     -1.21     -0.09    275.79      1.02\n     _z[4]     -0.06      0.30     -0.07     -0.53      0.43    239.25      1.02\n     _z[5]      0.03      0.31      0.03     -0.46      0.53    240.01      1.02\n     _z[6]      0.25      0.32      0.24     -0.28      0.79    220.40      1.02\n     _z[7]      0.67      0.36      0.67      0.06      1.23    193.99      1.02\n     _z[8]      1.19      0.43      1.19      0.48      1.89    181.31      1.01\n     _z[9]      1.31      0.46      1.31      0.51      2.01    181.15      1.01\n\nNumber of divergences: 1\n----- Temperature: 1.00 -----\n\n                mean       std    median      5.0%     95.0%     n_eff     r_hat\n       _mu     -0.19      0.27     -0.17     -0.56      0.29    176.79      1.01\n    _sigma      0.92      0.24      0.87      0.57      1.26    215.09      1.01\n     _z[0]     -1.70      0.51     -1.70     -2.63     -0.93    211.78      1.00\n     _z[1]     -0.60      0.33     -0.60     -1.20     -0.12    197.15      1.00\n     _z[2]     -0.50      0.32     -0.51     -1.01      0.02    200.85      1.00\n     _z[3]     -0.65      0.34     -0.66     -1.17     -0.05    203.18      1.00\n     _z[4]     -0.02      0.28     -0.03     -0.45      0.50    188.09      1.01\n     _z[5]      0.07      0.28      0.06     -0.36      0.59    189.28      1.01\n     _z[6]      0.30      0.28      0.28     -0.17      0.78    195.43      1.01\n     _z[7]      0.72      0.31      0.70      0.17      1.21    177.20      1.02\n     _z[8]      1.24      0.37      1.22      0.61      1.84    186.58      1.02\n     _z[9]      1.35      0.39      1.33      0.67      1.97    187.23      1.02\n\nNumber of divergences: 0\n----- Temperature: 2.00 -----\n\n                mean       std    median      5.0%     95.0%     n_eff     r_hat\n       _mu     -0.21      0.33     -0.20     -0.72      0.28    173.57      1.01\n    _sigma      0.93      0.25      0.89      0.57      1.30    182.60      1.02\n     _z[0]     -1.66      0.54     -1.62     -2.54     -0.85    153.43      1.04\n     _z[1]     -0.57      0.37     -0.57     -1.12      0.03    158.81      1.03\n     _z[2]     -0.47      0.35     -0.47     -1.01      0.10    161.59      1.03\n     _z[3]     -0.62      0.37     -0.63     -1.20     -0.03    158.92      1.03\n     _z[4]      0.01      0.33      0.00     -0.55      0.58    176.08      1.02\n     _z[5]      0.10      0.33      0.09     -0.42      0.70    186.36      1.02\n     _z[6]      0.33      0.34      0.32     -0.26      0.87    191.94      1.01\n     _z[7]      0.75      0.38      0.74      0.08      1.32    200.20      1.01\n     _z[8]      1.26      0.45      1.26      0.49      1.96    200.88      1.00\n     _z[9]      1.37      0.47      1.37      0.57      2.10    199.99      1.00\n\nNumber of divergences: 1\n\n\nWe can visualise the samples and compare them with the usual Bayesian approach:\n\n\nCode\nfig, axs = plt.subplots(n_genes, len(temperature_range) + 1)\n\nfor i in range(n_genes):\n  for j in range(len(temperature_range) + 1):\n    samples = alpha_samples if j == len(temperature_range) else  dfd_samples[temperature_range[j]]\n\n    color = colors[\"posterior_sample\"] if j == len(temperature_range) else colors[\"dfd\"]\n    if j == len(temperature_range):\n      c_alpha = 1.0\n    else:\n      c_alpha = 0.3 + 0.7 * j / len(temperature_range)\n\n    bias_samples = expit(samples)\n\n    ax = axs[i, j]\n    ax.hist(bias_samples[:, i], bins=np.linspace(0, 1, 20), density=True, alpha=c_alpha, color=color)\n\n    ax.set_yticks([]) \n    ax.set_xticks([])\n    ax.spines[[\"top\", \"right\"]].set_visible(False)\n\n    ax.axvline(true_bias[i], color=colors[\"true\"], linestyle=\"--\"  )\n\nfor i, ax in enumerate(axs[:, 0]):\n  ax.set_ylabel(f\"{i}\")\n\nfor j, ax in enumerate(axs[0, :]):\n  name = \"Bayes\" if j == len(temperature_range) else  f\"$T$={temperature_range[j]:.2f}\"\n  ax.set_title(name)\n\nfor ax in axs[-1, :]:\n  ax.set_xticks([0, 0.5, 1])\n\n\n\n\n\nIf the temperature is too low, the prior seems to have too large influence. In this case \\(T = 0.1\\) seems to be the most reasonable, yielding reasonable uncertainty quantification."
  },
  {
    "objectID": "posts/discrete-intractable-likelihood.html#summary",
    "href": "posts/discrete-intractable-likelihood.html#summary",
    "title": "Learning models with discrete Fisher divergence",
    "section": "Summary",
    "text": "Summary\nOverall, I have to say that I very much like the discrete Fisher divergence idea and I congratulate the authors on this article!\nIn the manuscript there are some experiments with the Ising model – I will need to try this method on the related high-dimensional problems, as (on the toy problem above) it showed some very promising performance. This method is also easy to implement and fast to run."
  },
  {
    "objectID": "posts/mixtures-and-admixtures.html",
    "href": "posts/mixtures-and-admixtures.html",
    "title": "On mixtures and admixtures",
    "section": "",
    "text": "Consider a binary genome vector \\(Y_{n\\bullet} = (Y_{n1}, \\dotsc, Y_{nG})\\) representing at which loci a mutation has appeared. One of the simplest models is to assume that mutations appear independently, with \\(\\theta_g = P(Y_{ng} = 1)\\) representing the probability of mutation occurring. In other words, \\[\\begin{align*}\nP(Y_{n\\bullet} = y_{n\\bullet} \\mid \\theta_\\bullet) &= \\prod_{g=1}^G P(Y_{ng}=y_{ng} \\mid \\theta_g ) \\\\\n&= \\prod_{g=1}^G \\theta_g^{Y_{ng}}(1-\\theta_g)^{1-Y_{ng}}.\n\\end{align*}\n\\]\nConsider the case where we observe \\(N\\) exchangeable genomes. We will assume that they are conditionally independent given the model parameters: \\[\nP(Y_{*\\bullet} = y_{*\\bullet}\\mid \\theta_\\bullet) = \\prod_{n=1}^N P(Y_{n\\bullet}=y_{n\\bullet}\\mid \\theta_\\bullet).\n\\]\nThere’s an obligatory probabilistic graphical model representing our assumptions:\nCode\nimport daft\nimport matplotlib.pyplot as plt\nplt.style.use(\"dark_background\")\n\nclass MyPGM:\n  def __init__(self, dpi: int = 200) -&gt; None:\n    assert dpi &gt; 1\n    self.dpi = dpi\n    self.pgm = daft.PGM(dpi=dpi)\n\n  def add_node(self, id: str, name: str, x: float, y: float, observed: bool = False):\n    if observed:\n      params={\"facecolor\": \"grey\"}\n    else:\n      params={\"edgecolor\": \"w\"}\n    self.pgm.add_node(id, name, x, y, plot_params=params)\n\n  def add_edge(self, start: str, end: str):\n    self.pgm.add_edge(start, end, plot_params={\"edgecolor\": \"w\", \"facecolor\": \"w\"})\n\n  def add_plate(\n    self,\n    coords,\n    label: str,\n    shift: float = 0.0,\n    label_offset: tuple[float, float] = (0.02, 0.02),\n  ):\n    \"\"\"\n    Args:\n      coords: [x_left, y_bottom, x_length, y_length]\n      label: label\n      shift: vertical shift\n    \"\"\"\n    label_offset = (label_offset[0] * self.dpi, label_offset[1] * self.dpi)\n    self.pgm.add_plate(coords, label=label, shift=shift, rect_params={\"edgecolor\": \"w\"}, label_offset=label_offset)\n\n  def plot(self):\n    self.pgm.render()\n    plt.show()\n\npgm = MyPGM()\n\npgm.add_node(\"D\", \"$\\\\mathcal{D}$\", 0, 1)\npgm.add_node(\"theta\", r\"$\\theta_g$\", 2, 1)\npgm.add_node(\"Y\", r\"$Y_{ng}$\", 4, 1, observed=True)\n\npgm.add_edge(\"D\", \"theta\")\npgm.add_edge(\"theta\", \"Y\")\n\npgm.add_plate([1.5, 0.5, 3, 1.5], label=r\"$g = 1, \\ldots, G$\", shift=-0.1)\npgm.add_plate([2.7, 0.55, 1.7, 1], label=r\"$n=1, \\ldots, N$\")\n\npgm.plot()\nwhere \\(\\mathcal D\\) is the prior over the \\(\\theta_\\bullet\\) vector, i.e., \\(\\theta_g \\mid \\mathcal D \\sim \\mathcal D\\) and \\(\\mathcal D\\) is supported on the interval \\((0, 1)\\). The simplest choice is to fix \\(\\mathcal D = \\mathrm{Uniform}(0, 1)\\), but this may be not flexible enough. Namely, different draws from the posterior on \\(\\theta\\) may look quite different than draws from \\(\\mathcal D\\). And this discrepancy would be easy to observe once \\(G\\) is large.\nHence, the simplest improvement to this model is to make \\(\\mathcal D\\) more flexible, i.e., treating it as a random probability measure with some prior on it. The simplest possibility is to use \\(\\mathcal D = \\mathrm{Beta}(\\alpha, \\beta)\\), where \\(\\alpha\\) and \\(\\beta\\) are given some gamma (say) priors, but more flexible models are possible (such as mixtures of beta distribution or a Dirichlet process).\nThis model has enough flexibility to model marginal the mutation occurrence probabilities \\(P(Y_{n g} = 1)\\): given enough samples \\(N\\), the posterior on \\(\\theta_g\\) should concentrate around the average \\(N^{-1}\\sum_{n=1}^N Y_{ng}\\) (and, provided that \\(\\mathcal D\\) is flexible enough, it may concentrate near the distribution of these averages).\nGreat: this model is flexible enough to describe well the mutation probabilities. However, it’s quite rigid when it comes to mutation exclusivity and cooccurrence: \\[\nP(Y_{n1}=1, Y_{n2} = 1) = P(Y_{n1} = 1) \\cdot P(Y_{n2} = 1) = \\theta_{1}\\theta_{2}.\n\\]\nWe generally expect that mutations in some genes could lead to synthetic lethality, so they would be exclusive. Also, the genes are ordered in chromosomes and copy number aberrations can lead to mutations being simultaneously observed in several genes (e.g., if they are in the fragment of the chromosome which is frequently lost).\nThe discrepancies between this “independent mutations” model and real data is generally easy to observe by plotting the correlation matrix between different genes. I also like looking at the the empirical distribution of the observed number of mutations in one sample, i.e., \\(\\sum_{g=1}^G Y_{ng}\\), as the model predictis a Poisson binomial distribution, while real data may show very different behaviour."
  },
  {
    "objectID": "posts/mixtures-and-admixtures.html#mixture-models",
    "href": "posts/mixtures-and-admixtures.html#mixture-models",
    "title": "On mixtures and admixtures",
    "section": "Mixture models",
    "text": "Mixture models\nThe model above is not flexible enough to model co-occurrences and exclusivity between different mutations.\nImagine that we sequence \\(G=3\\) genes which lie very close together.\nIf there is no large deletion in this chromosome, they are independently mutated with probabilities \\(1\\%\\), \\(5\\%\\) and \\(10\\%\\), respectively. However, if there is a large deletion, we will notice that they are all gone. Hence, it would make sense to consider two different “populations” with mutation probabilities \\(\\theta_{1\\bullet} = (1\\%, 5\\%, 10\\%)\\) and \\(\\theta_{2\\bullet} = (100\\%, 100\\%, 100\\%)\\), where the “population” describes whether such a large deletion had place. More generally, if we consider \\(K\\) “populations”, we can introduce sample-specific variables \\(Z_n \\in \\{1, 2, \\dotsc, K\\}\\) and the distributions \\[\nP(Y_{n\\bullet} \\mid Z_n, \\{\\theta_{k\\bullet}\\}_{k=1, \\dotsc, K}) = P(Y_{n\\bullet} \\mid \\theta_{Z_n\\bullet}).\n\\]\nWe will draw this model as\n\n\nCode\npgm = MyPGM()\n\npgm.add_node(\"pi\", r\"$\\pi$\", 0, 0)\npgm.add_node(\"Z\", \"$Z_n$\", 1, 0)\npgm.add_node(\"Y\", r\"$Y_{n\\bullet}$\", 2, 0, observed=True)\n\npgm.add_node(\"D\", r\"$\\mathcal{D}$\", 1, 1)\npgm.add_node(\"theta\", r\"$\\theta_{k\\bullet}$\", 2, 1)\n\npgm.add_edge(\"pi\", \"Z\")\npgm.add_edge(\"Z\", \"Y\")\n\npgm.add_edge(\"D\", \"theta\")\npgm.add_edge(\"theta\", \"Y\")\n\npgm.add_plate([0.5, -0.5, 2, 1], label=r\"$n=1, \\ldots, N$\", shift=-0.1)\npgm.add_plate([1.5, 0.6, 1, 1], label=r\"$k=1, \\ldots, K$\", label_offset=(0.01, 0.2))\n\npgm.plot()\n\n\n\n\n\nwhere \\(\\pi\\in \\Delta^{K-1}\\) is proportion vector over \\(K\\) “populations”.\nDue to the conditional independence structure in this model, we can also integrate out the \\(Z_n\\) variables to get \\[\nP(Y_n \\mid \\{\\theta_{k\\bullet}\\}_{k=1, \\dotsc, K}, \\pi) = \\sum_{k=1}^K \\pi_k \\, P(Y_{n\\bullet} \\mid \\theta_{k\\bullet} )\n\\]\nwhich graphically corresponds to\n\n\nCode\npgm = MyPGM()\n\npgm.add_node(\"pi\", r\"$\\pi$\", 0, 0)\npgm.add_node(\"Y\", r\"$Y_{n\\bullet}$\", 2, 0, observed=True)\n\npgm.add_node(\"D\", r\"$\\mathcal{D}$\", 1, 1)\npgm.add_node(\"theta\", r\"$\\theta_{k\\bullet}$\", 2, 1)\n\npgm.add_edge(\"pi\", \"Y\")\n\npgm.add_edge(\"D\", \"theta\")\npgm.add_edge(\"theta\", \"Y\")\n\npgm.add_plate([0.5, -0.5, 2, 1], label=r\"$n=1, \\ldots, N$\", shift=-0.1)\npgm.add_plate([1.5, 0.6, 1, 1], label=r\"$k=1, \\ldots, K$\", label_offset=(0.01, 0.2))\n\npgm.plot()\n\n\n\n\n\nThis integrated out representation is quite convenient for inference (see Stan User’s guide)."
  },
  {
    "objectID": "posts/mixtures-and-admixtures.html#how-expressive-are-finite-mixtures",
    "href": "posts/mixtures-and-admixtures.html#how-expressive-are-finite-mixtures",
    "title": "On mixtures and admixtures",
    "section": "How expressive are finite mixtures?",
    "text": "How expressive are finite mixtures?\nIn principle, the mixtures can be used to model an arbitrary distribution over binary vectors: consider \\(K=2^G\\) and each \\(\\theta_{k\\bullet}\\) vector to represent a different string \\(0\\) and \\(1\\) digits. Hence, we see that the “populations” do not necessarily have biological meaning. (See also Chapter 22 of Bayesian Data Analysis or this Cosma Shalizi’s blog post explaining the overinterpretation issues in factor analysis).\nAnother issue with using \\(K=2^G\\) classes is that \\(2^G\\) is usually much larger than \\(N\\), so that finding a suitable set of parameters \\(\\{\\theta_{k\\bullet}\\}\\) may be tricky. We will generally prefer a smaller number of components, although using a Dirichlet process mixture model is possible (which is quite funny, because in these models \\(K=\\infty\\) and there may be more occupied components than \\(2^G\\), which still result in the distribution which could be modelled with \\(2^G\\) clusters).\nThe mixtures (with a smaller number of components than \\(2^G\\)) are generally very popular, even if it is not always immediately clear that a model is a finite mixture: for example, this model can be modelled as a mixture with \\(K = G+1\\) components for an appropriate prior constraining the \\(\\theta\\) parameters.\nAnother example of such model are Bayesian pyramids: imagine that we sequence genes 1, 2, 3 on one chromosome and genes 4, 5, 6 on another chromosome, then we may consider the following idealised model employing \\(K=4\\) “populations”:\n\n\\(Z_n=1\\) means that neither chromosome is lost. Mutations in the genes arise independently.\n\\(Z_n=2\\) means that only the first chromosome is lost, i.e., genes 1, 2, 3 are mutated. The mutations at loci 4, 5, 6 can arise independently.\n\\(Z_n=3\\) means that the second chromosome is lost, i.e., genes 4, 5, 6 are mutated. The mutations at loci 1, 2, 3 can arise independently.\n\\(Z_n=4\\) means that both chromosomes are lost and we observe mutations in all six genes.\n\nThis model with four populations works. However, some of the parameters are tied together, as they correspond to the effects of deletion of two different chromosomes and using this structure may be important for the scalability and speed of posterior shrinkage: if we consider 10 different chromosomes, we don’t need to model \\(2^10\\) independent clusters. More on this parameter constraints is in Section 3.1 of the original paper. (And, if you are interested in the biological applications to tumor genotypes, I’ll be showing a poster at RECOMB 2024 on this topic! You can also take a look at the Jnotype package)."
  },
  {
    "objectID": "posts/mixtures-and-admixtures.html#admixture-models",
    "href": "posts/mixtures-and-admixtures.html#admixture-models",
    "title": "On mixtures and admixtures",
    "section": "Admixture models",
    "text": "Admixture models\nAbove we discussed that mixture models can be very expressive when they have many components (so sometimes the parameters are shared between different components).\nLet’s think about an admixture model, which can be treated as a simplified version of the STRUCTURE model. We again assume that we have some probability vectors \\(\\theta_{k\\bullet}\\) for \\(k=1, \\dotsc, K\\). However, in this case we will call them “topics” or distinct mutational mechanisms which operate in the following way. For each gene \\(Y_{ng}\\) there is a mechanism \\(T_{ng} \\in \\{1, \\dotsc, K\\}\\) which is used to generate the mutation: \\[\n  P(Y_{ng} =1 \\mid T_{ng}, \\{\\theta_{k\\bullet}\\}_{k=1, \\dotsc, K}) = \\theta_{T_{ng}g}.\n\\]\nThe mechanisms corresponding to different genes are drawn independently within each sample, i.e., we have a sample-specific proportion vector \\(\\pi_n\\in \\Delta^{K-1}\\) which is used to sample the topics: \\[\n  T_{ng} \\mid \\pi_n \\sim \\mathrm{Categorical}(\\pi_n).\n\\]\nLet’s draw the dependencies in this model:\n\n\nCode\npgm = MyPGM()\n\npgm.add_node(\"G\", r\"$\\mathcal{G}$\", 0, 1)\n\npgm.add_node(\"pi\", r\"$\\pi_n$\", 0, 0)\npgm.add_node(\"T\", \"$T_{ng}$\", 1, 0)\npgm.add_node(\"Y\", r\"$Y_{ng}$\", 2, 0, observed=True)\n\npgm.add_node(\"D\", r\"$\\mathcal{D}$\", 1, 1)\npgm.add_node(\"theta\", r\"$\\theta_{k\\bullet}$\", 2, 1)\n\npgm.add_edge(\"G\", \"pi\")\npgm.add_edge(\"pi\", \"T\")\npgm.add_edge(\"T\", \"Y\")\n\npgm.add_edge(\"D\", \"theta\")\npgm.add_edge(\"theta\", \"Y\")\n\npgm.add_plate([-0.5, -0.5, 3, 1], label=r\"$n=1, \\ldots, N$\", shift=-0.1)\npgm.add_plate([1.5, 0.6, 1, 1], label=r\"$k=1, \\ldots, K$\", label_offset=(0.01, 0.2))\n\npgm.add_plate([0.55, -0.45, 1.85, 0.8], label=r\"$g=1, \\ldots, G$\", label_offset=(0.25, 0.01))\n\n\npgm.plot()\n\n\n\n\n\nSimilarly as in the mixture models, we can integrate out the latent variables: \\[\\begin{align*}\n  P(Y_{n\\bullet} \\mid \\pi_n, \\{\\theta_{k\\bullet}\\}_{k=1, \\dotsc, K}) &= \\sum_{T_{n\\bullet}} P(Y_{n\\bullet} \\mid T_{n\\bullet}, \\{\\theta_{k\\bullet}\\}_{k=1, \\dotsc, K}) P(T_{n\\bullet} \\mid \\pi_n ) \\\\\n  &= \\sum_{T_{n\\bullet}} \\prod_{g=1}^G  P( Y_{ng} \\mid \\theta_{T_{ng}g}) P(T_{ng} \\mid \\pi_n) \\\\\n  &= \\prod_{g=1}^G \\left( \\sum_{k=1}^K P(Y_{ng}\\mid \\theta_{kg})\\, \\pi_{nk} \\right)\n\\end{align*}\n\\]\nWe see that if the proportions vector \\(\\pi_n\\) becomes one-hot vectors, then the admixture model reduces to the mixture model we have seen above. However, this model is more flexible.\nLet’s draw the version with local variables integrated out:\n\n\nCode\npgm = MyPGM()\n\npgm.add_node(\"G\", r\"$\\mathcal{G}$\", 0, 1)\n\npgm.add_node(\"pi\", r\"$\\pi_n$\", 0, 0)\npgm.add_node(\"Y\", r\"$Y_{ng}$\", 2, 0, observed=True)\n\npgm.add_node(\"D\", r\"$\\mathcal{D}$\", 1, 1)\npgm.add_node(\"theta\", r\"$\\theta_{k\\bullet}$\", 2, 1)\n\npgm.add_edge(\"G\", \"pi\")\npgm.add_edge(\"pi\", \"Y\")\n\npgm.add_edge(\"D\", \"theta\")\npgm.add_edge(\"theta\", \"Y\")\n\npgm.add_plate([-0.5, -0.5, 3, 1], label=r\"$n=1, \\ldots, N$\", shift=-0.1)\npgm.add_plate([1.5, 0.6, 1, 1], label=r\"$k=1, \\ldots, K$\", label_offset=(0.01, 0.2))\n\npgm.add_plate([0.55, -0.45, 1.85, 0.8], label=r\"$g=1, \\ldots, G$\", label_offset=(0.25, 0.01))\n\n\npgm.plot()\n\n\n\n\n\nThe issue in this model is that we have \\(O(KG)\\) parameters in the \\(\\theta\\) matrix (unless some parameter sharing is used) and additionally we have \\(O(NK)\\) parameters of the proportion vectors. Hence, the inference in this model may be trickier to apply."
  },
  {
    "objectID": "posts/mixtures-and-admixtures.html#how-do-the-samples-look-like",
    "href": "posts/mixtures-and-admixtures.html#how-do-the-samples-look-like",
    "title": "On mixtures and admixtures",
    "section": "How do the samples look like?",
    "text": "How do the samples look like?\nLet’s take \\(G=10\\) and \\(K=3\\) and simulate some \\(\\theta\\) matrix:\n\n\nCode\nimport numpy as np\nimport seaborn as sns\n\nG = 10\nK = 3\n\nrng = np.random.default_rng(2024)\ntheta = np.zeros((K, G))\nfor k in range(K):\n  theta[k, :] = rng.beta(1, 5 + k**2, size=(G,))\n  theta[k, min(3*k, G):min(3*k+3,10)] = 0.9\n# rng.uniform(size=(K, G))\n\ntheta[0, :] = np.sort(theta[0, :])\ntheta[-1, :] = np.sort(theta[1, :])[::-1]\n\nfig, ax = plt.subplots(figsize=(4, 2), dpi=250)\nsns.heatmap(theta, vmin=0, vmax=1, cmap=\"Greys_r\", ax=ax, xticklabels=False, yticklabels=False, square=True, cbar=False)\nax.set_xlabel(\"Genes\")\nax.set_ylabel(\"Populations\")\nplt.show()\n\n\n\n\n\nLet’s sample 50 vectors from each component:\n\n\nCode\nz = np.asarray(sum(([k] * 50 for k in range(K)), []))\nprobs = theta[z]\nY = rng.binomial(1, probs)\n\nfig, ax = plt.subplots(figsize=(5, 2), dpi=250)\nsns.heatmap(Y.T, vmin=0, vmax=1, cmap=\"Greys_r\", ax=ax, square=False, xticklabels=False, yticklabels=False, cbar=False)\n\nax.set_xlabel(\"Samples\")\nax.set_ylabel(\"Genes\")\nplt.show()\n\n\n\n\n\nOn the other hand, if we sample from an admixture model, where \\(\\pi_n \\sim \\mathrm{Dirichlet}(\\alpha, \\alpha, \\alpha)\\) for different values of \\(\\alpha\\), we will get the following proportion vectors:\n\n\nCode\nN = 150\n\nalphas = [0.001, 0.1, 10.0]\nfig, axs = plt.subplots(3, 1, figsize=(7, 6), dpi=250, sharex=True, sharey=True)\n\npis = np.zeros((len(alphas), N, K))\n\nfor i, (alpha, ax) in enumerate(zip(alphas, axs)):\n  pi = rng.dirichlet(alpha * np.ones(K), size=N)\n  index = np.argsort( np.einsum(\"nk,k-&gt;n\", pi, np.arange(K)))\n  pi = pi[index, :]\n  pis[i, ...] = pi\n\n  x_axis = np.arange(0.5, N + 0.5)\n\n  prev = 0.0\n  for k in range(K):\n    ax.fill_between(x_axis, prev, prev + pi[:, k], color=f\"C{k+4}\")\n    prev = prev + pi[:, k]\n\n  ax.spines[[\"top\", \"right\"]].set_visible(False)\n  ax.set_xlabel(\"Samples\")\n  ax.set_ylabel(\"Proportions\")\n  ax.set_xticks([])\n  ax.set_title(f\"$\\\\alpha$: {alpha:.3f}\")\n\nfig.tight_layout()\nplt.show()\n\n\n\n\n\nNote that for \\(\\alpha \\approx 0\\) the proportion vectors are close to one-hot, so that the admixture reduces to the mixture above.\nWe can generate the following samples corresponding to proportion vectors above as following:\n\n\nCode\nfig, axs = plt.subplots(3, 1, figsize=(7, 6), dpi=250, sharex=True, sharey=True)\n\nfor alpha, pi, ax in zip(alphas, pis, axs):\n  Y = np.zeros((N, G), dtype=int)\n\n  for n in range(N):\n    T_ = rng.multinomial(1, pi[n], size=G)\n    T = T_ @ np.arange(K)\n    probs = theta[T, np.arange(G)]\n    Y[n, :] = rng.binomial(1, probs)\n\n  sns.heatmap(Y.T, vmin=0, vmax=1, cmap=\"Greys_r\", ax=ax, square=False, xticklabels=False, yticklabels=False, cbar=False)\n  ax.set_xlabel(\"Samples\")\n  ax.set_ylabel(\"Genes\")\n  ax.set_title(f\"Sparsity: {alpha:.3f}\")\n\nfig.tight_layout()\nplt.show()"
  },
  {
    "objectID": "posts/mixtures-and-admixtures.html#links",
    "href": "posts/mixtures-and-admixtures.html#links",
    "title": "On mixtures and admixtures",
    "section": "Links",
    "text": "Links\n\nWe discussed mixture models in the Dirichlet process.\nThere is an excellent introduction to mixture and admixture models in Jeff Miller’s Bayesian methodology in biostatistics course (see lectures 9–12).\nNicola Roberts used hierarchical Dirichlet processes (which are also an admixture model, although of a bit different kind) to study mutational patterns during her PhD.\nBarbara Engelhardt and Matthew Stephens wrote a nice paper interpreting admixture models as matrix factorizations. Of course, mixture models (being a special case of an admixture) also fit in this framework.\nQuantification is naturally related to admixture models. We discussed it here and there."
  },
  {
    "objectID": "posts/almost-binomial-markov-chain.html",
    "href": "posts/almost-binomial-markov-chain.html",
    "title": "An almost binomial Markov chain",
    "section": "",
    "text": "Recall that \\(Y\\sim \\mathrm{Bernoulli}(p)\\) if \\(Y\\) can attain values from the set \\(\\{0, 1\\}\\) with probability \\(P(Y=1) = p\\) and \\(P(Y=0) = 1-p\\). It’s easy to see that:\n\nFor every \\(k\\ge 1\\) the random variables \\(Y^k\\) and \\(Y\\) are equal.\nThe expected value is \\(\\mathbb E[Y] = p\\).\nThe variance is \\(\\mathbb{V}[Y]=\\mathbb E[Y^2]-\\mathbb E[Y]^2 = p-p^2 = p(1-p).\\) From AM-GM we see that \\(\\mathbb{V}[Y] \\le (1/2)^2=1/4\\).\n\nNow, if we consider independent and identically distributed variables \\(Y_1\\sim \\mathrm{Bernoulli}(p)\\), \\(Y_2\\sim \\mathrm{Bernoulli}(p)\\), …, \\(Y_n\\sim \\mathrm{Bernoulli}(p)\\), we can define a new variable \\(N_n = Y_1 + \\cdots + Y_n\\) and an average \\[ \\bar Y^{(n)} = \\frac{N_n}{n}.\\]\nThe random variable \\(N_n\\) is distributed according to the binomial distribution and it’s easy to calculate the mean \\(\\mathbb E[N_n] = np\\) and variance \\(\\mathbb V[N_n] = np(1-p)\\). Consequently: \\(\\mathbb E[ \\bar Y^{(n)} ] = p\\) and \\(\\mathbb V[\\bar Y^{(n)}] = np(1-p)/n^2 = p(1-p)/n \\le 1/4n\\).\nHence, we see that if we want to estimate \\(p\\), then \\(\\bar Y^{(n)}\\) is a reasonable estimator to use, and we can control its variance by choosing an appropriate \\(n\\).\nFor very large \\(n\\), recall that the strong law of large numbers guarantees that \\[\nP\\left( \\lim\\limits_{n\\to \\infty} \\bar Y^{(n)} = p \\right) = 1.\n\\]"
  },
  {
    "objectID": "posts/almost-binomial-markov-chain.html#a-bernoulli-random-variable",
    "href": "posts/almost-binomial-markov-chain.html#a-bernoulli-random-variable",
    "title": "An almost binomial Markov chain",
    "section": "",
    "text": "Recall that \\(Y\\sim \\mathrm{Bernoulli}(p)\\) if \\(Y\\) can attain values from the set \\(\\{0, 1\\}\\) with probability \\(P(Y=1) = p\\) and \\(P(Y=0) = 1-p\\). It’s easy to see that:\n\nFor every \\(k\\ge 1\\) the random variables \\(Y^k\\) and \\(Y\\) are equal.\nThe expected value is \\(\\mathbb E[Y] = p\\).\nThe variance is \\(\\mathbb{V}[Y]=\\mathbb E[Y^2]-\\mathbb E[Y]^2 = p-p^2 = p(1-p).\\) From AM-GM we see that \\(\\mathbb{V}[Y] \\le (1/2)^2=1/4\\).\n\nNow, if we consider independent and identically distributed variables \\(Y_1\\sim \\mathrm{Bernoulli}(p)\\), \\(Y_2\\sim \\mathrm{Bernoulli}(p)\\), …, \\(Y_n\\sim \\mathrm{Bernoulli}(p)\\), we can define a new variable \\(N_n = Y_1 + \\cdots + Y_n\\) and an average \\[ \\bar Y^{(n)} = \\frac{N_n}{n}.\\]\nThe random variable \\(N_n\\) is distributed according to the binomial distribution and it’s easy to calculate the mean \\(\\mathbb E[N_n] = np\\) and variance \\(\\mathbb V[N_n] = np(1-p)\\). Consequently: \\(\\mathbb E[ \\bar Y^{(n)} ] = p\\) and \\(\\mathbb V[\\bar Y^{(n)}] = np(1-p)/n^2 = p(1-p)/n \\le 1/4n\\).\nHence, we see that if we want to estimate \\(p\\), then \\(\\bar Y^{(n)}\\) is a reasonable estimator to use, and we can control its variance by choosing an appropriate \\(n\\).\nFor very large \\(n\\), recall that the strong law of large numbers guarantees that \\[\nP\\left( \\lim\\limits_{n\\to \\infty} \\bar Y^{(n)} = p \\right) = 1.\n\\]"
  },
  {
    "objectID": "posts/almost-binomial-markov-chain.html#a-bit-lazy-coin-tossing",
    "href": "posts/almost-binomial-markov-chain.html#a-bit-lazy-coin-tossing",
    "title": "An almost binomial Markov chain",
    "section": "A bit lazy coin tossing",
    "text": "A bit lazy coin tossing\nAbove we defined a sequence of independent Bernoulli variables. Let’s introduce some dependence between them: define \\[\nY_1\\sim \\mathrm{Bernoulli}(p)\n\\] and, for \\(n\\ge 1\\), \\[\nY_{n+1}\\mid Y_n \\sim w\\,\\delta_{Y_n} +(1-w)\\, \\mathrm{Bernoulli}(p).\n\\]\nHence, to draw \\(Y_1\\) we simply toss a coin, but to draw \\(Y_2\\) we can be lazy with probability \\(w\\) and use the sampled value \\(Y_1\\) or, with probability \\(1-w\\), actually do the hard work of tossing a coin again.\nLet’s think about the marginal distributions, i.e., we observe only the \\(n\\)th coin toss. As \\(Y_n\\) takes values in \\(\\{0, 1\\}\\), it has to be distributed according to some Bernoulli distribution.\nOf course, we have \\(\\mathbb E[Y_1] = p\\), but what is \\(\\mathbb E[Y_2]\\)? Using the law of total expectation we have \\[\n\\mathbb E[Y_2] = \\mathbb E[ \\mathbb E[Y_2\\mid Y_1] ] = \\mathbb E[ w Y_1 + (1-w)p ] = p.\n\\]\nInteresting! Even if we have large \\(w\\), e.g., \\(w=0.9\\), we will still see \\(Y_2=1\\) with the original probability \\(p\\). More generally, we can prove by induction that \\(\\mathbb E[Y_n] = p\\) for all \\(n\\ge 1\\).\nTo calculate the variance, we could try the law of total variance, but there is a simpler way: from the above observations we see that all the variables are distributed as \\(Y_n\\sim \\mathrm{Bernoulli}(p)\\) (so they are identically distributed, but not independent for \\(w &gt; 0\\)) and the variance has to be \\(\\mathbb V[Y_n] = p(1-p)\\).\nLet’s now introduce variables \\(N_n = Y_1 + \\cdots + Y_n\\) and \\(\\bar Y^{(n)}=N_n/n\\). As expectation is a linear operator, we know that \\(\\mathbb E[N_n] = np\\) and \\(\\mathbb E[\\bar Y^{(n)}]=p\\), but how exactly are these variables distributed? Or, at least, can we say anything about their variance?\nIt’s instructive to see what happens for \\(w=1\\): intuitively, we only tossed the coin once, and then just “copied” the result \\(n\\) times, so the sample size used to estimate \\(\\bar Y^{(n)}\\) is still one.\nMore formally, with probability 1 we have \\(Y_1 = Y_2 = \\cdots = Y_n\\), so that \\(N_n = nY_1\\) and \\(\\mathbb V[N_n] = n^2p(1-p)\\). Then, also with probability 1, we also have \\(\\bar Y^{(n)}=Y_1\\) and \\(\\mathbb V[\\bar Y^{(n)}]=p(1-p)\\).\nMore generally, we have \\[\n\\mathbb V[N_n] = \\sum_{i=1}^n \\mathbb V[Y_i] + \\sum_{i\\neq j} \\mathrm{cov}[Y_i, Y_j]\n\\]\nand we can suspect that the covariance terms will be non-negative, usually incurring larger variance than a corresponding binomial distribution (obtained from independent draws). Let’s prove that.\n\nMarkov chain\nWe will be interested in covariance terms \\[\\begin{align*}\n\\mathrm{cov}(Y_i, Y_{i+k}) &= \\mathbb E[Y_i\\cdot Y_{i+k}] - \\mathbb E[Y_i]\\cdot \\mathbb E[Y_{i+k}] \\\\\n&= P(Y_i=1, Y_{i+k}=1)-p^2 \\\\\n&= P(Y_i=1)P( Y_{i+k}=1\\mid Y_i=1) -p^2 \\\\\n&= p\\cdot P(Y_{i+k}=1 \\mid Y_i=1) - p^2.\n\\end{align*}\n\\]\nTo calculate the probability \\(P(Y_{i+k}=1\\mid Y_i=1)\\) we need an observation: the sampling procedure defines a Markov chain with the transition matrix \\[\nT = \\begin{pmatrix}\n    P(0\\to 0) & P(0 \\to 1)\\\\\n    P(1\\to 0) & P(1\\to 1)\n\\end{pmatrix}\n= \\begin{pmatrix}\n    w+(1-w)(1-p) & p(1-w)\\\\\n    (1-w)(1-p) & w + p(1-w)\n\\end{pmatrix}.\n\\]\nBy induction and a handy identity \\((1-x)(1+x+\\cdots + x^{k-1}) = 1-x^{k}\\) one can prove that \\[\nT^k = \\begin{pmatrix}\n    1-p(1-w^k) & p(1-w^k)\\\\\n    (1-p)(1-w^k) & p+w^k(1-p),\n\\end{pmatrix}\n\\] from which we can conveniently read \\[\nP(Y_{i+k}=1\\mid Y_i=1) = p+w^k(1-p)\n\\] and \\[\\mathrm{cov}(Y_i, Y_{i+k}) = w^k\\cdot p(1-p).\\]\nGreat, these terms are always non-negative! Let’s do a quick check: for \\(w=0\\) the covariance terms vanish, resulting in \\(\\mathbb V[N_n]=np(1-p) + 0\\) and for \\(w=1\\) we have \\(\\mathbb V[N_n] = np(1-p) + n(n-1)p(1-p)=n^2p(1-p)\\).\nFor \\(w\\neq 1\\), we can use the same identity as before to get \\[\\begin{align*}\n    \\mathbb V[N_n] &= p(1-p)\\cdot \\left(n+\\sum_{i=1}^n \\sum_{k=1}^{n-i} w^k \\right) \\\\\n    &= p(1-p)\\left( n+ \\frac{2 w \\left(w^n-n w+n-1\\right)}{(w-1)^2} \\right)\n\\end{align*}\n\\]\nLet’s numerically check whether this formula seems correct:\n\n\nCode\nfrom functools import partial\n\nimport jax\nimport jax.numpy as jnp\nfrom jax import random, lax\n\n@partial(jax.jit, static_argnames=[\"n\"])\ndef simulate_markov_chain(key, n: int, p: float, w: float) -&gt; jnp.ndarray:\n    keys = random.split(key, n)\n\n    def step(i, y):\n        key_w, key_p = random.split(keys[i])\n        y_prev = y[i-1]\n        mixture_sample = random.bernoulli(key_w, w)\n        y_next = jnp.where(mixture_sample, y_prev, random.bernoulli(key_p, p))\n        y = y.at[i].set(y_next)\n        return y\n    \n    y_init = jnp.zeros(n, dtype=jnp.int32)\n    y_init = y_init.at[0].set(random.bernoulli(keys[0], p))\n\n    y_final = lax.fori_loop(1, n, step, y_init)\n    return y_final\n\ndef simulate_correlated_binomial(key, n: int, p: float, w: float) -&gt; int: \n    return simulate_markov_chain(key=key, n=n, p=p, w=w).sum()\n\n@partial(jax.jit, static_argnames=[\"n\", \"n_samples\"])\ndef sample_correlated_binomial(key, n: int, p: float, w: float, n_samples: int = 1_000_000) -&gt; jnp.ndarray:\n    keys = random.split(key, n_samples)\n    return jax.vmap(partial(simulate_correlated_binomial, n=n, p=p, w=w))(keys)\n\ndef variance_correlated_binomial(n: int, p: float, w: float) -&gt; float:\n    factor = n**2\n    if w &lt; 1.0:\n        factor = n + ( 2 * w * (-1 + n - n * w + w**n)) / (-1 + w)**2\n    return p*(1-p) * factor\n\nkey = random.PRNGKey(2024-1-19)\n\ntest_cases = [\n    (10, 0.5, 0.5),\n    (10, 0.3, 0.8),\n    (10, 0.2, 0.1),\n    (5, 0.4, 0.3),\n    (20, 0.8, 0.7),\n]\n\nfor n, p, w in test_cases:\n    key, subkey = random.split(key)\n    approx = jnp.var(sample_correlated_binomial(subkey, n, p, w))\n    exact = variance_correlated_binomial(n, p, w)\n\n    print(f\"Variance (appr.): {approx:.2f}\")\n    print(f\"Variance (exact): {exact:.2f}\")\n    print(\"-\"*23)\n\n\nAn NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n\n\nVariance (appr.): 6.50\nVariance (exact): 6.50\n-----------------------\nVariance (appr.): 11.40\nVariance (exact): 11.40\n-----------------------\nVariance (appr.): 1.92\nVariance (exact): 1.92\n-----------------------\nVariance (appr.): 1.93\nVariance (exact): 1.94\n-----------------------\nVariance (appr.): 15.66\nVariance (exact): 15.65\n-----------------------"
  },
  {
    "objectID": "posts/almost-binomial-markov-chain.html#markov-chain-monte-carlo",
    "href": "posts/almost-binomial-markov-chain.html#markov-chain-monte-carlo",
    "title": "An almost binomial Markov chain",
    "section": "Markov chain Monte Carlo",
    "text": "Markov chain Monte Carlo\nRecall that when the samples are independent, we can estimate \\(p\\) via \\(\\bar Y^{(n)}\\) which is an unbiased estimator, i.e., \\(\\mathbb E[\\bar Y^{(n)}] = p\\) and its variance is \\(\\mathbb V[\\bar Y^{(n)}]=p(1-p)/n\\le 1/4n\\).\nWhen we passed to a Markov chain introducing parameter \\(w\\), we also found that \\(\\mathbb E[\\bar Y^{(n)}]=p\\). Moreover, for \\(w &lt; 1\\) (i.e., there is some genuine sampling, rather than just copying the first result) the variance of \\(N_n\\) also grows as \\(\\mathcal O(n + w^n)=\\mathcal O(n)\\), so that \\(\\mathbb V[\\bar Y^{(n)}] =\\mathcal O(1/n)\\), so that for a large \\(n\\) the estimator \\(\\bar Y^{(n)}\\) can be a reliable estimator for \\(p\\). However, note that in the variance there’s a term \\(1/(1-w)^2\\), so that for \\(w\\) close to \\(1\\) one may have to use very, very large \\(n\\) to make sure that the variance is small enough.\nThis Markov chain is, in fact, connected to Markov chain Monte Carlo samplers, used to sample from a given distribution.\nThe Markov chain \\(Y_2, Y_3, \\dotsc\\) has transition matrix \\(T\\) and initial distribution of \\(Y_1\\) (namely \\(\\mathrm{Bernoulli}(p)\\)). For \\(0 &lt; p &lt; 1\\) and \\(w &lt; 1\\) the transition matrix has positive entries, which implies that this Markov chain is ergodic (both irreducibility and aperiodicity are trivially satisfied in this case. More generally, quasi-positivity, i.e., \\(T^k\\) has positive entries for some \\(k\\ge 1\\), is equivalent to ergodicity).\nWe can deduce two things. First, there is a unique stationary distribution of this Markov chain. It can be found by solving the equations for the eigenvector \\(T^{t}\\pi=\\pi\\); in this case \\(\\pi=(1-p, p)\\) (what a surprise!), meaning that the stationary distribution is \\(\\mathrm{Bernoulli}(p)\\).\nSecondly, we can use the ergodic theorem. The ergodic theorem states that in this case, for every function1 \\(f\\colon \\{0, 1\\}\\to \\mathbb R\\) we have \\[\nP\\left(\\lim\\limits_{n\\to\\infty} \\frac{1}{n}\\sum_{i=1}^n f(Y_i) = \\mathbb E[f] \\right) = 1\n\\] where the expectation \\(\\mathbb E[f]\\) is taken with respect to \\(\\pi\\).\nNote that for \\(f(x) = x\\) we see that with probability \\(1\\) we have \\(\\lim\\limits_{n\\to \\infty} \\bar Y^{(n)} = p\\).\nPerhaps it’s worth commenting on why the stationary distribution is \\(\\mathrm{Bernoulli}(p)\\). Consider any distribution \\(\\mathcal D\\) and a Markov chain \\[\nY_{n+1} \\mid Y_n \\sim w\\, \\delta_{Y_{n}} + (1-w)\\, \\mathcal D\n\\]\nfor \\(w &lt; 1\\). Intuitively, this Markov chain will either jump to a new location with the right probability, or stay at a current point by some additional time. This additional time depends only on \\(w\\), so that on average, at each point we spend the same time. Hence, it should not affect time averages over very long sequences. (However, as we have seen, large \\(w\\) may imply large autocorrelation in the Markov chain and the chain would have to be extremely long to yield an acceptable variance).\nI think it should not be hard to formalize and prove the above observation, but that’s not for today. This review could be useful for investigating this further."
  },
  {
    "objectID": "posts/almost-binomial-markov-chain.html#how-does-it-differ-from-beta-binomial",
    "href": "posts/almost-binomial-markov-chain.html#how-does-it-differ-from-beta-binomial",
    "title": "An almost binomial Markov chain",
    "section": "How does it differ from beta-binomial?",
    "text": "How does it differ from beta-binomial?\nRecall that a beta-binomial distribution generates samples as follows:\n\nDraw \\(b\\sim \\mathrm{Beta}(\\alpha, \\beta)\\);\nThen, draw \\(M \\mid b \\sim \\mathrm{Binomial}(n, b)\\).\n\nFirst, a random coin is selected from a set of coins with different biases, and then it is tossed \\(n\\) times. This distribution has two degrees of freedom: \\(\\alpha\\) and \\(\\beta\\), which allows for more flexible control over both the mean and the variance. The mean is given by \\[\n\\mathbb E[M] = n\\frac{\\alpha}{\\alpha + \\beta},\n\\]\nso if we write \\(p = \\alpha/(\\alpha + \\beta)\\), we match the mean of a “corresponding” binomial distribution. The variance is given by \\[\n\\mathbb V[M] = np(1-p)\\left(1 + \\frac{n-1}{\\alpha + \\beta + 1} \\right),\n\\]\nso for \\(n \\ge 2\\) we see a larger variance than for a binomial distribution with the corresponding mean.\nWe see that this variance is quadratic in \\(n\\), which is different from the formula for the variance of the almost binomial Markov chain. Nevertheless, we can ask whether beta-binomial can be a good approximation to the distribution studied before.\nThis intuition may be formalized in many ways, such as the minimization of statistical discrepancy measures, including total variation, various Wasserstain distances, or \\(f\\)-divergences. Instead, we will simply match mean and variance.\nOf course, we will take \\(p=\\alpha/(\\alpha + \\beta)\\). Now we need \\(\\mathbb V[M] = V\\). The solution is then given by \\[\\begin{align*}\n\\alpha &= pR,\\\\\n\\beta &= (1-p)R,\\\\\nR &= \\frac{n^2 p(1-p)-V}{V - n p(1-p)}.\n\\end{align*}\n\\]\nNow it’s coding time! We could use TensorFlow Probability on JAX to sample from beta-binomial distribution, but we will use core JAX.\n\n\nCode\nimport matplotlib.pyplot as plt \nplt.style.use(\"dark_background\")\n\ndef find_alpha_beta(n: int, p: float, variance: float) -&gt; tuple[float, float]:\n    num = n**2 * p * (1-p) - variance\n    den = variance - n * p * (1-p)\n    r = num / den\n\n    if r &lt;= 0 or p &lt;= 0 or p &gt;= 1:\n        raise ValueError(\"Input results in non-positive alpha or beta\")\n\n    return p*r, (1-p) * r\n\n@partial(jax.jit, static_argnames=[\"n\"])\ndef _sample_beta_binomial(key, n: int, alpha: float, beta: float) -&gt; int:\n    key_p, key_b = random.split(key)\n    p = random.beta(key_p, a=alpha, b=beta)\n    ber = random.bernoulli(key_b, p=p, shape=(n,))\n    return jnp.sum(ber)\n\n@partial(jax.jit, static_argnames=[\"n\", \"n_samples\"])\ndef sample_beta_binomial(key, n: int, alpha: float, beta: float, n_samples: int = 1_000_000) -&gt; jnp.ndarray:\n    keys = random.split(key, n_samples)\n    return jax.vmap(partial(_sample_beta_binomial, n=n, alpha=alpha, beta=beta))(keys)\n\n\ndef plot_compare(key, ax: plt.Axes, n: int, p: float, w: float, n_samples: int = 1_000_000, n_bins: int | None = None) -&gt; None:\n    variance = variance_correlated_binomial(n=n, p=p, w=w)\n    alpha, beta = find_alpha_beta(n=n, p=p, variance=variance)\n\n    key1, key2 = random.split(key)\n    sample_corr = sample_correlated_binomial(key1, n=n, p=p, w=w, n_samples=n_samples)\n    sample_betabin = sample_beta_binomial(key2, n=n, alpha=alpha, beta=beta, n_samples=n_samples)\n\n    if n_bins is None:\n        bins = jnp.arange(-0.5, n + 1.5, 1)\n    else:\n        bins = jnp.linspace(-0.1, n + 0.1, n_bins)\n\n    ax.hist(\n        sample_corr, bins=bins, density=True, rasterized=True,\n        color=\"yellow\",\n        label=\"Markov chain\",\n        histtype=\"step\",\n    )\n    ax.hist(\n        sample_betabin, bins=bins, density=True, rasterized=True,\n        color=\"orange\",\n        label=\"Beta-binomial\",\n        histtype=\"step\",\n        linestyle=\"--\"\n    )\n    ax.spines[[\"top\", \"right\"]].set_visible(False)\n    ax.set_xlabel(\"Number of heads\")\n    ax.set_ylabel(\"Probability\")\n\n\nfig, _axs = plt.subplots(2, 2, dpi=250)\naxs = _axs.ravel()\n\nkey, *keys = random.split(key, 1 + len(axs))\n\nparams = [\n    # (n, p, w, n_bins)\n    (10, 0.5, 0.9, None),\n    (10, 0.3, 0.2, None),\n    (100, 0.7, 0.98, 41),\n    (100, 0.3, 0.6, 41),\n]\nassert len(params) == len(axs)\n\nfor key, ax, (n, p, w, n_bins) in zip(keys, axs, params):\n    plot_compare(\n        key=key,\n        ax=ax,\n        n=n,\n        p=p,\n        w=w,\n        n_samples=1_000_000,\n        n_bins=n_bins,\n)\naxs[0].legend(frameon=False)\nfig.tight_layout()"
  },
  {
    "objectID": "posts/almost-binomial-markov-chain.html#footnotes",
    "href": "posts/almost-binomial-markov-chain.html#footnotes",
    "title": "An almost binomial Markov chain",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nUsually one has to add that the function is bounded. But we are working with a finite domain \\(\\{0, 1\\}\\), so literally every function is bounded.↩︎"
  },
  {
    "objectID": "posts/scrna-seq-literature.html",
    "href": "posts/scrna-seq-literature.html",
    "title": "A very subjective single-cell transcriptomics reading list",
    "section": "",
    "text": "I do not think that single-cell transcriptomics (scRNA-seq) is easy data to model. The signal-to-noise ratio is low. Batch effects are everywhere. Over 1700 different methods exist, but understanding when a particular method should (or should not) be used is hard to navigate.\nBelow I assembled a subjective reading list. It consists of resources which I wish I had known when I was working on the topic – perhaps it will turn out to be useful for somebody. If you think I have missed something important (and I most definitely have), let me know!"
  },
  {
    "objectID": "posts/scrna-seq-literature.html#eleven-grand-challenges",
    "href": "posts/scrna-seq-literature.html#eleven-grand-challenges",
    "title": "A very subjective single-cell transcriptomics reading list",
    "section": "Eleven grand challenges",
    "text": "Eleven grand challenges\nOne of my favourite papers is the review by D. Lähnemann, et al. Eleven grand challenges in single-cell data science.\nI do not know how many times I read this paper, but whenever I open it, I see something new. I think it is an excellent starting point, showing the complexity of the problem."
  },
  {
    "objectID": "posts/scrna-seq-literature.html#how-to-model-the-data",
    "href": "posts/scrna-seq-literature.html#how-to-model-the-data",
    "title": "A very subjective single-cell transcriptomics reading list",
    "section": "How to model the data?",
    "text": "How to model the data?\nRafael Irizarry gave a wonderful talk Statistical challenges in single cell RNA-seq technologies on modelling scRNA-seq data. Watching it should be complemented by reading Feature selection and dimension reduction for single-cell RNA-Seq based on a multinomial model by F.W. Townes et al. I very much like the explicit focus on modelling and model validation. This is important.\nA great review of statistical models for high-dimensional data is in Kevin Murphy’s Probabilistic Machine Learning book (vol. 2, chapter 28). I think reading this chapter (and then re-reading it. And again, but this time reading also the references) is a great method to build some intuition about good modeling strategies, which can work on a particular data set.\nI always appreciate when people see the models as building blocks and can adjust or expand them as needed for a particular problems. These methods are closely connected and understanding underlying assumptions and mathematical structure is important to develop principled model expansions.\nTo give another source for learning about different models, I very much enjoyed reading the material from Jeff Miller’s Bayesian Methodology in Biostatistics course."
  },
  {
    "objectID": "posts/scrna-seq-literature.html#the-double-dipping-problem",
    "href": "posts/scrna-seq-literature.html#the-double-dipping-problem",
    "title": "A very subjective single-cell transcriptomics reading list",
    "section": "The double dipping problem",
    "text": "The double dipping problem\nThe double dipping problem is a common mistakes is single-cell analyses, resulting in arguably false discoveries. There is an excellent talk from Daniela Witten on the topic. It can be complemented by reading many associated papers."
  },
  {
    "objectID": "posts/scrna-seq-literature.html#beware-when-interpreting-clusters-and-latent-factors",
    "href": "posts/scrna-seq-literature.html#beware-when-interpreting-clusters-and-latent-factors",
    "title": "A very subjective single-cell transcriptomics reading list",
    "section": "Beware when interpreting clusters and latent factors",
    "text": "Beware when interpreting clusters and latent factors\nLatent variable models are great to look at the data. However, they are also easy to overinterpret and can lead to wrong conclusions. After all, exploratory data analysis is about looking at the data from many perspectives and building some understanding, perhaps formalised in terms of related scientific hypotheses. We should remember that many patterns found can be just artifacts of the model or noise in the data.\nI think that this topic deserves a separate blog post. For now I’ll just link to several excellent blog posts, written by Cosma Shalizi, Frank E. Harrell, Darren Dahly. For a longer read, there is this excellent article by D.J. Lawson et al. A tutorial on how not to over-interpret STRUCTURE and ADMIXTURE bar plots."
  },
  {
    "objectID": "posts/scrna-seq-literature.html#simulating-the-data",
    "href": "posts/scrna-seq-literature.html#simulating-the-data",
    "title": "A very subjective single-cell transcriptomics reading list",
    "section": "Simulating the data",
    "text": "Simulating the data\nTo understand how a particular statistical procedure performs, it is good to have some data sets for which the ground-truth answers are known. One way of obtaining such data is through simulation.\nEven though I think this is a great way to develop intuition about particular problems, it is important to remember that scRNA-seq are very high-dimensional and the available simulators are not perfect, as shown in the benchmark by H.L. Crowell et al., The shaky foundations of simulating single-cell RNA sequencing data."
  },
  {
    "objectID": "posts/scrna-seq-literature.html#statistics-is-hard",
    "href": "posts/scrna-seq-literature.html#statistics-is-hard",
    "title": "A very subjective single-cell transcriptomics reading list",
    "section": "Statistics is hard",
    "text": "Statistics is hard\nFinally, a periodic reminder which I get from data, whenever I analyse them. I think I’ll mention these two posts from Frank E. Harrell on problems encountered in data analysis."
  },
  {
    "objectID": "posts/determinant-multilinear.html",
    "href": "posts/determinant-multilinear.html",
    "title": "Matrix determinant from the exterior algebra viewpoint",
    "section": "",
    "text": "In every linear algebra course matrix determinant is a must. Often it is introduced in the following form:\nThe definition above has a lot of advantages, but it also has an important drawback — the “why” of this construction is hidden and appears only later in a long list of its properties.\nWe’ll take an alternative viewpoint, which I have learned from Darling (1994, chap. 1), and is based around the exterior algebra."
  },
  {
    "objectID": "posts/determinant-multilinear.html#motivational-examples",
    "href": "posts/determinant-multilinear.html#motivational-examples",
    "title": "Matrix determinant from the exterior algebra viewpoint",
    "section": "Motivational examples",
    "text": "Motivational examples\nConsider \\(V=\\mathbb R^3\\). For vectors \\(v\\) and \\(w\\) we can define their vector product \\(v\\times w\\) with the following properties:\n\nBilinearity: \\((\\lambda v+v')\\times w = \\lambda (v\\times w) + v'\\times w\\) and \\(v\\times (\\lambda w+w') = \\lambda (v\\times w) + v\\times w'\\).\nAntisymmetry: \\(v\\times w = -w\\times v\\).\n\nGeometrically we can think of it as of a signed area of the parallelepiped spanned by \\(v\\) and \\(w\\).\nFor three vectors \\(v, w, u\\) we can form signed volume: \\[\\langle v, w, u\\rangle = v\\cdot (w\\times u),\\] which has similar properties:\n\nTrilinearity: \\(\\langle \\lambda v+v', w, u \\rangle = \\lambda \\langle v, w, u \\rangle + \\langle v', w, u\\rangle\\) (and similarly in \\(w\\) and \\(u\\) arguments).\nAntisymmetry: when we swap any two arguments the sign changes, e.g., \\(\\langle v, w, u\\rangle = -\\langle w, v, u\\rangle = \\langle w, u, v\\rangle = -\\langle u, w, v\\rangle\\).\n\nExterior algebra will be a generalisation of the above construction beyond the three-dimensional space \\(V=\\mathbb R^3\\)."
  },
  {
    "objectID": "posts/determinant-multilinear.html#exterior-algebra",
    "href": "posts/determinant-multilinear.html#exterior-algebra",
    "title": "Matrix determinant from the exterior algebra viewpoint",
    "section": "Exterior algebra",
    "text": "Exterior algebra\nLet’s start with the natural definition:\n\nDefinition 2 (Antisymmetric multilinear function) Let \\(V\\) and \\(U\\) be vector spaces and \\(f\\colon V\\times V \\times \\cdots \\times V \\to U\\) be a function. We will say that it is multilinear if for all \\(i = 1, 2, \\dotsc, n\\) it holds that \\[\nf(v_1, v_2, \\dotsc, \\lambda v_i + v_i', v_{i+1}, \\dotsc, v_n) = \\lambda f(v_1, \\dotsc, v_i, \\dotsc, v_n) + f(v_1, \\dotsc, v_i', \\dotsc, v_n).\n\\] We will say that it is antisymmetric if it changes the sign whenever we swap any two arguments: \\[\nf(v_1, \\dotsc, v_i, \\dotsc, v_j, \\dotsc, v_n) = -f(v_1, \\dotsc, v_j, \\dotsc, v_i, \\dotsc, v_n).\n\\]\n\nAs we have seen above both \\((v, w)\\mapsto v\\times w\\) and \\((v, w, u)\\mapsto v\\cdot (w\\times u)\\) are antisymmetric multilinear functions.\nNote that for every \\(\\sigma\\in S_n\\) it holds that \\[\nf(v_1, \\dotsc, v_n) = \\mathrm{sgn}\\,\\sigma \\, f(v_{\\sigma(1)}, \\dotsc, v_{\\sigma(n)})\n\\] as \\(\\mathrm{sgn}\\,\\sigma\\) counts transpositions modulo 2.\n\nExercise 1 Let \\(f\\colon V\\times V\\to U\\) be multilinear. Show that the following are equivalent:\n\n\\(f\\) is antisymmetric, i.e., \\(f(v, w) = -f(w, v)\\) for every \\(v, w \\in V\\).\n\\(f\\) is alternating, i.e., \\(f(v, v) = 0\\) for every \\(v\\in V\\).\n\nGeneralise to multilinear mappings \\(f\\colon V\\times V \\times \\cdots\\times V\\to U\\).\n\n\n\n\n\n\nHint\n\n\n\n\n\nExpand \\(f(v+w, v+w)\\) using multilinearity.\n\n\n\n\nNow we are ready to construct (a particular) exterior algebra.\n\nDefinition 3 (Second exterior power) Let \\(V\\) be a vector space. Its second exterior power \\(\\bigwedge^2 V\\) we be the vector space of expressions \\[\n\\lambda_1 v_1\\wedge w_1 + \\cdots + \\lambda_n v_n\\wedge w_n\n\\] with the following rules:\n\nThe wedge \\(\\wedge\\) operator is bilinear, i.e., \\((\\lambda v+v')\\wedge w = \\lambda v\\wedge w + v'\\wedge w\\) and \\(v\\wedge (\\lambda w+w') = \\lambda v\\wedge w + v\\wedge w'\\).\n\\(\\wedge\\) is antisymmetric, i.e., \\(v\\wedge w = -w\\wedge v\\) (or, equivalently, \\(v\\wedge v=0\\)).\nIf \\(e_1, \\dotsc, e_n\\) is a basis of \\(V\\), then \\[\\begin{align*}\n    &e_1\\wedge e_2, e_1\\wedge e_3, \\dotsc, e_1\\wedge e_n, \\\\\n    &e_2\\wedge e_3, \\dotsc, e_2\\wedge e_n\\\\\n    &\\qquad\\vdots\\\\\n    &e_{n-1}\\wedge e_n\n    \\end{align*}\n    \\] is a basis of \\(\\bigwedge^2 V\\).\n\n\nNote that \\(v\\wedge w\\) has the interpretation of a signed area of the parallelepiped spanned by \\(v\\) and \\(w\\). Such parallelepipeds can be formally added and there is a resemblance between the wedge product and the vector product in \\(\\mathbb R^3\\).\nWe just need to prove that such a space actually exists (this construction can be skipped at the first reading): similarly to the tensor space, build the free vector space on the set \\(V\\times V\\). Now quotient it by expressions like \\((v, v)\\), \\((\\lambda v, w) - (v, \\lambda w)\\), \\((v+v', w) - (v, w) - (v', w)\\) and \\((v, w+w') - (v, w) - (v, w')\\).\nThen define \\(v\\wedge w\\) to be the equivalence class \\([(v, w)]\\).\n\n\n\n\n\n\nNote\n\n\n\n\n\nIf we had introduced the determinant by other means, we could construct the exterior algebra \\(\\bigwedge^k V\\) also as the space of antisymmetric multilinear functions \\(V^*\\times V^*\\to \\mathbb R\\) (where \\(V^*\\) is the dual space) by\n\\[\n(v\\wedge w)(\\alpha, \\beta) := \\det \\begin{pmatrix}  \\alpha(v_1) & \\alpha(v_2) \\\\ \\beta(v_1) & \\beta(v_2) \\end{pmatrix}\n\\]\n\n\n\nAnalogously we can construct:\n\nDefinition 4 (Exterior power) Let \\(V\\) be a vector space. We define \\(\\bigwedge^0 V = \\mathbb R\\), \\(\\bigwedge^1 V = V\\) and for \\(k\\ge 2\\) its \\(k\\)th exterior power \\(\\bigwedge^k V\\) as the vector space of expressions \\[\n\\lambda_1 a_1\\wedge a_2\\wedge \\cdots\\wedge a_k + \\cdots + \\lambda_n v_1\\wedge v_2 \\wedge \\cdots\\wedge v_k\n\\] such that the wedge operator \\(\\wedge\\) is multilinear and antisymmetric (alternating) and that if \\(e_1, \\dotsc, e_n\\) is a basis of \\(V\\), then the set \\[\n\\{ e_{i_1}\\wedge e_{i_2}\\wedge \\cdots \\wedge e_{i_k}\\mid i_1 &lt; i_2 &lt; \\cdots &lt; i_k \\}\n\\]\nis a basis of \\(\\bigwedge^k V\\).\n\n\nExercise 2 Show that if \\(\\dim V = n\\), then \\(\\dim \\bigwedge^k V = \\binom{n}{k}\\). (And that in particular for \\(k &gt; n\\) we have \\(\\bigwedge^k V = 0\\), the trivial vector space).\n\nThe introduced space can be used to convert between antisymmetric multilinear and linear functions by the means of the universal property:\n\nTheorem 1 (Universal property) Let \\(f\\colon V\\times V \\cdots\\times V\\to U\\) be an antisymmetric multilinear function. Then, there exists a unique linear mapping \\(\\tilde f\\colon \\bigwedge^k V\\to U\\) such that for every set of vectors \\(v_1, \\dotsc, v_k\\) \\[\nf(v_1, \\dotsc, v_k) = \\tilde f(v_1\\wedge \\dotsc \\wedge v_k).\n\\]\n\n\nProof. (Can be skipped at the first reading.)\nAs \\(f\\) is multlilinear, its values are determined by the values on the tuples \\((e_{i_1}, \\dotsc, e_{i_k})\\), where \\(\\{e_1, \\dotsc, e_n\\}\\) is a basis of \\(V\\).\nWe can use antisymmetry to show that by “sorting out” the elements such that \\(i_1 \\le i_2\\cdots \\le i_k\\) and defining \\(\\tilde f(e_{i_1} \\wedge \\dotsc, \\wedge e_{i_k}) = f(e_{i_1}, \\dotsc, e_{i_k})\\) we obtain a well-defined mapping. Linearity is easy to proof.\nNow the uniqueness is proven by observing that antisymmetry and multilinearity uniquely prescribe the values at the basis elements of \\(\\bigwedge^k V\\).\n\nIts importance is the following: to show that a linear map \\(\\bigwedge^k V\\to U\\) is well-defined, one can construct a multilinear antisymmetric map \\(V\\times V\\times \\cdots \\times V\\to U\\)."
  },
  {
    "objectID": "posts/determinant-multilinear.html#determinants",
    "href": "posts/determinant-multilinear.html#determinants",
    "title": "Matrix determinant from the exterior algebra viewpoint",
    "section": "Determinants",
    "text": "Determinants\nFinally, we can define the determinant. Note that if \\(\\dim V = n\\), then \\(\\dim \\bigwedge^n V = 1\\).\n\nDefinition 5 (Determinant) Let \\(n=\\dim V\\) and \\(A\\colon V\\to V\\) be a linear mapping. We consider the mapping \\[\n(v_1, \\dotsc, v_n) \\mapsto (Av_1) \\wedge \\cdots \\wedge (Av_n).\n\\]\nAs it is antisymmetric and multilinear, we know that it induces a unique linear mapping \\(\\bigwedge^n V\\to \\bigwedge^n V\\).\nBecause \\(\\bigwedge^n V\\) is one-dimensional, this mapping must be multiplication by a number. Namely, we define the determinant \\(\\det A\\) to be the number such that for every set of vectors \\(v_1, \\dotsc, v_n\\) \\[\nAv_1 \\wedge \\cdots \\wedge Av_n = \\det A\\, (v_1\\wedge \\cdots \\wedge v_n).\n\\]\n\nIn other words, determinant measures the volume stretch of the parallelepiped spanned by the vectors after they are transformed by the mapping.\nI like this geometric intuition, especially that it is clear that determinant depends only on the linear map, rather than a particular matrix representation — it is independent on the chosen basis.\nWe can now show a number of lemmata.\n\nProposition 1 If \\(\\mathrm{id}_V\\colon V\\to V\\) is the identity mapping, then \\(\\det \\mathrm{id}_V = 1\\).\n\n\nProof. Obvious from the definition! Similarly, it’s clear that \\(\\det \\left(\\lambda\\cdot \\mathrm{id}_V\\right) = \\lambda^{\\dim V}\\).\n\n\nProposition 2 For every two mappings \\(A, B\\colon V\\to V\\) it holds that \\(\\det (B\\circ A) = \\det B\\cdot \\det A\\).\n\n\nProof. For every set of vectors we have \\[\n\\begin{align*}\n\\det (B\\circ A) \\, v_1\\wedge \\cdots \\wedge v_n &= (BAv_1) \\wedge \\cdots \\wedge (BAv_n) \\\\\n&= B(Av_1) \\wedge \\cdots \\wedge B(Av_n) \\\\\n&= \\det B \\, (Av_1) \\wedge \\cdots \\wedge (Av_n) \\\\\n&= \\det B\\cdot \\det A\\, v_1\\wedge \\cdots\\wedge v_n.\n\\end{align*}\n\\]\n\n\nProposition 3 (Only invertible matrices have non-zero determinants) A mapping is an isomorphism if and only if it has non-zero determinant.\n\n\nProof. If the mapping is invertible, then \\(A\\circ A^{-1} = \\mathrm{id}\\) and we have \\(\\det A \\cdot \\det A^{-1} = 1\\), so its determinant must be non-zero.\nNow assume that the mapping is non-invertible. This means that there exists a non-zero vector \\(k\\in \\ker A\\) such that \\(Ak=0\\). Let’s complete \\(k\\) to a basis \\(k, e_1, \\dotsc, e_{n-1}\\). Then \\[\n\\det A\\, k\\wedge e_1\\wedge \\cdots\\wedge e_{n-1} = (Ak) \\wedge \\cdots \\wedge (Ae_{n-1}) = 0,\n\\] which means that \\(\\det A=0\\) as \\(\\{k\\wedge e_1\\wedge \\dotsc \\wedge e_{n-1}\\}\\) is a basis of \\(\\bigwedge^n V\\).\n\nLet’s now connect the usual definition of the determinant to the one coming from exterior algebra:\n\nProposition 4 (Recovering the standard expression) Let \\(e_1, \\dotsc, e_n\\) be a basis of \\(V\\) and \\((A^{i}_j)\\) be the matrix of coordinates, i.e., \\[\nAe_k = \\sum_i A^{i}_k e_i.\n\\] Then the determinant \\(\\det A\\) can be calculated as \\[\n\\det A = \\sum_{\\sigma\\in S_n} \\mathrm{sgn}\\,\\sigma \\, A^{\\sigma(1)}_1 A^{\\sigma(2)}_2 \\dotsc A^{\\sigma(n)}_n.\n\\]\n\n\nProof. Observe that \\[\\begin{align*}\n\\det A e_1 \\wedge \\cdots \\wedge e_n &= Ae_1 \\wedge \\cdots \\wedge Ae_n\\\\\n&= \\left( \\sum_{i_1} A^{i_1}_1 e_{i_1} \\right) \\wedge \\cdots \\wedge \\left( \\sum_{i_n} A^{i_n}_n e_{i_n} \\right)\\\\\n&= \\sum_{i_1, \\dotsc, i_n} A^{i_1}A^{i_2}\\cdots A^{i_n} \\, e_{i_1} \\wedge \\cdots \\wedge e_{i_n}.\n\\end{align*}\\]\nNow we see that repeated indices give zero contribution to this sum, so we can only consider the indices which are permutations of \\(1, 2, \\dotsc, n\\). We also see that \\(e_{i_1} \\wedge \\cdots \\wedge e_{i_n}\\) can be then written as \\(\\pm 1\\, e_1\\wedge \\dotsc \\wedge e_n\\), where the sign is the number of required transpositions, that is the sign of the permutation. This ends the proof.\n\nGoing just a bit further into exterior algebra we can also show that matrix transposition does not change the determinant.\nTo represent matrix transposition, we will use the dual mapping: if \\(A\\colon V\\to V\\) there is the dual mapping \\(A^*\\colon V^*\\to V^*\\), given as \\[\n  (A^*\\omega)(v) := \\omega(Av).\n\\]\nWe can therefore build the \\(n\\)th exterior power of \\(V^*\\): \\(\\bigwedge^n (V^*)\\) and consider the determinant \\(\\det A^*\\).\nWe will formally show that\n\nProposition 5 (Determinant of the transpose) Let \\(A\\colon V\\to V\\) be a linear map and \\(A^*\\colon V^*\\to V^*\\) be its dual. Then \\[\n\\det A^* = \\det A.\n\\]\n\n\nProof. To do this we will need an isomorphism \\[\n\\iota \\colon {\\bigwedge}^n (V^*) \\to \\left({\\bigwedge}^n V\\right)^*\n\\] given on basis elements by \\[\n\\iota( \\omega^1 \\wedge \\cdots \\wedge \\omega^n ) (v_1\\wedge \\cdots \\wedge v_n) = \\det \\big(\\omega^i(v_j) \\big)_{i, j = 1, \\cdots, n},\n\\] where on the right side we use any already known formula for the determinant. It is easy to show that this mapping is well-defined and linear, as it descends from a multilinear alternating mapping.\nHaving this, the proof becomes straightforward calculation: \\[\n\\begin{align*}\n  \\det A^* \\iota\\left(  \\omega^1\\wedge \\cdots\\wedge \\omega^n  \\right)(v_1\\wedge \\cdots\\wedge v_n ) &=\n  \\iota\\bigg( \\det A^* \\, \\omega^1\\wedge \\cdots\\wedge \\omega^n  \\bigg)(v_1\\wedge \\cdots\\wedge v_n ) \\\\\n  &=\\iota\\bigg( A^*\\omega^1 \\wedge \\cdots\\wedge A^*\\omega^n  \\bigg )(v_1\\wedge \\cdots\\wedge v_n) \\\\\n  &= \\det \\bigg((A^*\\omega^i)(v_j)\\bigg) = \\det \\bigg( \\omega^i(Av_j ) \\bigg) \\\\\n  &= \\iota\\left(\\omega^1\\wedge \\cdots\\wedge \\omega^n \\right)(Av_1\\wedge\\cdots\\wedge Av_n) \\\\\n  &= \\iota\\left(\\omega^1\\wedge \\cdots\\wedge \\omega^n \\right)(\\det A\\, v_1\\wedge\\cdots\\wedge v_n) \\\\\n  &= \\det A~ \\iota\\left(\\omega^1\\wedge \\cdots\\wedge \\omega^n\\right)(v_1\\wedge\\cdots\\wedge v_n)\n\\end{align*}\n\\]\n\nEstablishing such isomorphisms is quite a nice technique, which also can be used to prove\n\nProposition 6 (Determinant of a block-diagonal matrix) Let \\(A\\colon V\\to V\\) and \\(B\\colon W\\to W\\) be two linear mappings and \\(A\\oplus B\\colon V\\oplus W\\to V\\oplus W\\) be the mapping given by \\[\n(A\\oplus B)(v, w) = (Av, Bw).\n\\]\nThen \\(\\det (A\\oplus B) = \\det A\\cdot \\det B\\).\n\n\nProof. We will use this approach: there exists an isomorphism \\[\n{\\bigwedge}^p (V\\oplus W) \\simeq \\bigoplus_k {\\bigwedge}^k V \\otimes {\\bigwedge}^{p-k} W,\n\\] so if we take \\(n=\\dim V\\) and \\(m=\\dim W\\) and note that \\(\\bigwedge^{p} V = 0\\) for \\(p &gt; n\\) (and similarly for \\(W\\)) we have \\[\n\\iota\\colon {\\bigwedge}^{n+m} (V\\oplus W) \\simeq {\\bigwedge}^n V\\otimes {\\bigwedge}^m W.\n\\] If \\(i\\colon V\\to V\\oplus W\\) and \\(j\\colon W\\to V\\oplus W\\) are the two “canonical” inclusions, this isomorphism is given as \\[\n\\iota\\big( iv_1 \\wedge \\cdots \\wedge iv_n\\wedge jw_1 \\wedge \\cdots \\wedge jw_m \\big) = (v_1\\wedge \\cdots\\wedge v_n) \\otimes (w_1\\wedge\\cdots\\wedge w_m).\n\\] Now we calculate: \\[\\begin{align*}\n(A\\oplus B)( iv_1 \\wedge \\cdots \\wedge iv_n\\wedge jw_1 \\wedge \\cdots \\wedge jw_m ) &=\niAv_1 \\wedge \\cdots \\wedge iAv_n \\wedge jBw_1\\wedge\\cdots\\wedge jBw_m \\\\\n&= \\iota^{-1}\\big( Av_1\\wedge \\cdots\\wedge Av_n \\otimes Bw_1\\wedge\\cdots\\wedge Bw_m   \\big) \\\\\n&= \\iota^{-1}\\big(\\det A\\cdot \\det B\\, v_1\\wedge \\cdots \\wedge v_n \\otimes w_1\\wedge\\cdots\\wedge w_m) \\\\\n&= \\det A\\cdot \\det B \\, \\iota^{-1}\\big( v_1\\wedge \\cdots \\wedge v_n \\otimes w_1\\wedge\\cdots\\wedge w_m \\big)\\\\\n&= \\det A\\cdot \\det B \\, iv_1\\wedge \\cdots \\wedge iv_n \\wedge jw_1\\wedge\\cdots\\wedge jw_m.\n\\end{align*}\n\\]\n\n\nProposition 7 (Determinant of an upper-triangular matrix) Let \\(A\\colon V\\to V\\) be a linear mapping and \\(e_1, \\dotsc, e_n\\) be a basis of \\(V\\) such that matrix \\((A^i_j)\\) is upper-triangular, that is \\[\n\\begin{align*}\n  Ae_1 &= A^1_1 e_1\\\\\n  Ae_2 &= A^1_2 e_1 + A^2_2 e_2\\\\\n  &\\vdots\\\\\n  Ae_n &= A^1_n e_1 + A^2_ne_2 + \\dotsc + A^n_n e_n\n\\end{align*}\n\\] Then \\[\n\\det A = \\prod_{i=1}^n A^i_i.\n\\]\n\nOnce proven, this result can also be used for lower-triangular matrices due to Proposition 5.\n\nProof. Recall that whenever there is \\(i_j=i_k\\), then \\(e_{i_1}\\wedge \\cdots\\wedge e_{i_n} = 0\\). Hence, there is only one term that may be non-zero: \\[\nAe_1\\wedge Ae_2 \\wedge \\cdots \\wedge Ae_n = A^1_1 e_1 \\wedge \\cdots \\wedge A^n_n e_n = \\prod_{i=1}^n A^i_i\\, e_1\\wedge \\cdots\\wedge e_n.\n\\]"
  },
  {
    "objectID": "posts/determinant-multilinear.html#acknowledgements",
    "href": "posts/determinant-multilinear.html#acknowledgements",
    "title": "Matrix determinant from the exterior algebra viewpoint",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nI would like to thank Adam Klukowski for helpful editing suggestions."
  },
  {
    "objectID": "posts/clustering-exponential-family-softmax.html",
    "href": "posts/clustering-exponential-family-softmax.html",
    "title": "Softmax, mixtures and exponential families",
    "section": "",
    "text": "Recently, Carl kindly explained his variational classification paper to me. In particular, he recalled the following sentence: “the softmax layer can be interpreted as applying Bayes’ rule (…), assuming that the variables follow exponential family class-conditional distributions”.\nI very much like this observation (as well as the paper), but I did not understand at all why this was true: isn’t that too powerful? Let’s try to rewrite it, so I understand it better: consider a space of features \\(\\mathcal X\\) and a space of labels \\(\\mathcal Y = \\{1, 2, \\dotsc, L\\}\\).\nWe want the conditional distributions \\(P(X\\mid Y=y)\\) to have PDFs with respect to some nice reference measure \\(\\mu\\) on \\(\\mathcal X\\) and we will assume that these PDFs are positive everywhere. For example, (non-singular) multivariate normal and Student distributions have this property on \\(\\mathbb R^n\\) (but truncated normal distributions generally do not).\nThen, we can write \\[\np_{X\\mid Y}(x\\mid y) = \\exp(\\log p_{X\\mid Y}(x\\mid y)) = f(x)\\cdot \\exp\\!\\big( \\langle \\eta_y, T(x) \\rangle  \\big),\n\\]\nwhere \\(T(x) = \\big(\\log p_{X\\mid Y}(x\\mid y) \\big)_{y=1,\\dotsc, L} \\in \\mathbb R^L\\) is called the sufficient statistic; \\(\\eta_y \\in \\mathbb R^L\\) are the \\(y\\)-th standard basis vectors in \\(\\mathbb R^L\\) (i.e., the one-hot encoding) forming the natural parameters, and \\(f(x)=1\\) is there just to make the formula look more familiar: it turns out that if the conditional distributions are fully supported, then they have to form1 an exponential family!\nSo, in a way, whenever we have positive densities, we need to have an exponential family. We can transpose this statement using the quote from the beginning: whenever we have positive probabilities, we need to have softmax! Namely, \\[\\begin{align*}\n  p_{Y\\mid X}(y\\mid x) &= \\frac{ p_{X\\mid Y}(x\\mid y)\\, p_Y(y) }{ \\sum_{y'} p_{X\\mid Y}(x\\mid y')\\, p_Y(y') } \\\\\n  &= \\mathrm{softmax}( \\log p_{X\\mid Y}(x\\mid \\bullet) + \\log p_Y(\\bullet) ).\n\\end{align*}\n\\]"
  },
  {
    "objectID": "posts/clustering-exponential-family-softmax.html#footnotes",
    "href": "posts/clustering-exponential-family-softmax.html#footnotes",
    "title": "Softmax, mixtures and exponential families",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIt may seem that we are missing the log-partition function, \\(A_y\\), but this is indeed the case: \\[\\begin{align*}\nA_y &= \\log \\int_{\\mathcal X} f(x) \\exp(\\langle \\eta_y, T(x)\\rangle) \\, \\mathrm{d}\\mu(x) \\\\\n&= \\log \\int_{\\mathcal X} p_{X\\mid Y}(x\\mid y) \\, \\mathrm{d} \\mu(x) \\\\\n&= \\log 1 = 0.\n\\end{align*}\n\\]↩︎"
  },
  {
    "objectID": "posts/kernel-regression-transformer.html",
    "href": "posts/kernel-regression-transformer.html",
    "title": "From kernel regression to the transformer",
    "section": "",
    "text": "I remember that when we read Attention is all you need at a journal club back in 2020, I did not really understand what attention was1.\nFortunately for me, Transformer dissection paper and Cosma Shalizi’s post on the topic appeared, which show the connection between attention and kernel regression. This point of view was exactly what I needed! I like this so much that when I explain attention to other people, I always start from kernel regression."
  },
  {
    "objectID": "posts/kernel-regression-transformer.html#kernel-regression",
    "href": "posts/kernel-regression-transformer.html#kernel-regression",
    "title": "From kernel regression to the transformer",
    "section": "Kernel regression",
    "text": "Kernel regression\nLet’s start with kernel regression as independently proposed by Nadaraya and Watson sixty years ago. We will generate some data with heteroskedastic noise, \\(y = f(x) + n(x)\\epsilon\\) where \\(\\epsilon \\sim \\mathcal N(0, 1)\\), \\(f(x)\\) is the expected value \\(\\mathbb E[y\\mid x]\\) and function \\(n(x)\\) makes the noise heteroskedastic.\nWe’ll plot the observed data points as well as \\(f(x) + 2 n(x)\\) and \\(f(x) - 2n(x)\\) as is often done.\n\n\nCode\nfrom functools import partial\n\nimport jax\nimport jax.numpy as jnp\nfrom jax import random\n\nimport equinox as eqx\nfrom jaxtyping import Float, Array\n\nimport matplotlib.pyplot as plt\nplt.style.use(\"dark_background\")\n\nVector = Float[Array, \" n_points\"]\n\ndef f(X: Vector) -&gt; Vector:\n    return 0.5 * jnp.sin(X) - 1 * jnp.sin(3 * X) + 0.2 * jnp.square(X)\n\ndef n(X: Vector) -&gt; Vector:\n    return 0.2 + 0.05 * jnp.abs(X)\n\nn_points: int = 150\n\nkey = random.PRNGKey(2024)\nkey, subkey = random.split(key)\nX = jnp.linspace(-3, 3, n_points)\nY = f(X) + n(X) * random.normal(subkey, shape=X.shape)\n\nfig, ax = plt.subplots(figsize=(4, 3), dpi=150)\nX_ax = jnp.linspace(-3, 3, 201)\nax.fill_between(\n    X_ax, f(X_ax)- 2 * n(X_ax), f(X_ax) + 2 * n(X_ax), alpha=0.4, color=\"maroon\"\n)\nax.plot(X_ax, f(X_ax), color=\"maroon\", alpha=0.8)\nax.scatter(X, Y, color=\"white\", s=5, alpha=1.0)\nax.set_xlabel(\"$X$\")\nax.set_ylabel(\"$Y$\")\n\n\nAn NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n\n\nText(0, 0.5, '$Y$')\n\n\n\n\n\nWe will be interested in finding \\(f(x)\\) via the weighted average: \\[\n\\hat f(x) = \\sum_{i=1}^n y_i\\, w_i(x)\n\\]\nwhere \\(w_i(x)\\) is the weight of the \\(i\\)-th data point used to estimate the value at \\(x\\). To make it a weighted average, we will ensure that \\(w_1(x) + \\cdots + w_n(x) = 1\\). In case where \\(w_i(x) = 1/n\\) we obtain just constant prediction, equal to the sample average over \\(y_i\\).\nMore generally, consider a positive function \\(K\\colon \\mathcal X \\times \\mathcal X \\to \\mathbb R^+\\) which measures similarity between two data points: we want \\(K(x, x')\\) to attain the largest possible value and for \\(x'\\) very far from \\(x\\) we want to have \\(K(x, x')\\) to be small. For such a function we can form a set of weights via \\[\nw_i(x) = \\frac{K(x, x_i)}{\\sum_{j=1}^n K(x, x_j)}.\n\\]\nLet’s restrict our attention for now to Gaussian kernels, \\(K(x, x'; \\ell) = \\exp \\left(\\left( \\frac{x-x'}{\\ell} \\right)^2 \\right)\\) with lengthscale \\(\\ell\\) and visualise the predictions for different lengthscales. As kernels are parameterised functions, we will use Equinox:\n\n\nCode\nclass GaussianKernel(eqx.Module):\n    _log_lengthscale: float\n\n    def __init__(self, lengthscale: float) -&gt; None:\n        assert lengthscale &gt; 0, \"Lengthscale should be positive.\"\n        self._log_lengthscale = jnp.log(lengthscale)\n\n    @property\n    def lengthscale(self) -&gt; float:\n        return jnp.exp(self._log_lengthscale)\n\n    def __call__(self, x: float, x_: float) -&gt; float:\n        return jnp.exp(-jnp.square((x-x_) / self.lengthscale))\n\n\n    def predict(self, X_test: Float[Array, \" n_test\"], X_obs: Vector, Y_obs: Vector) -&gt; Float[Array, \" n_test\"]:\n        kernel = self\n        def predict_one(x: float) -&gt; float:\n            ks = jax.vmap(partial(kernel, x))(X_obs)\n            ws = ks / (jnp.sum(ks) + 1e-16)\n            return jnp.sum(Y_obs * ws)\n        return jax.vmap(predict_one)(X_test)    \n\n\nkernels = {lengthscale: GaussianKernel(lengthscale) for lengthscale in [3.0, 0.5, 0.25, 0.05]} \n\n\nfig, axs = plt.subplots(2, 2, figsize=(2*4, 2*3), dpi=150, sharex=True, sharey=True)\n\nfor (lengthscale, k), ax in zip(kernels.items(), axs.ravel()):\n    pred = k.predict(X_ax, X_obs=X, Y_obs=Y)\n    ax.set_title(f\"$\\\\ell = {lengthscale}$\")\n    ax.plot(X_ax, f(X_ax), color=\"maroon\", alpha=0.8)\n    ax.plot(X_ax, pred, color=\"orangered\", alpha=0.8)\n    ax.scatter(X, Y, color=\"white\", s=5, alpha=0.8)\n    ax.set_xlabel(\"$X$\")\n    ax.set_ylabel(\"$Y$\")\n\nfig.tight_layout()\n\n\n\n\n\nIt seems that \\(\\ell=3.0\\) results in underfitted, almost constant, predictions, and \\(\\ell=0.05\\) arguably overfits, resulting in predictions changing a bit too quickly. Generally, it seems that \\(\\ell \\approx 0.25\\) is a reasonable choice.\n\nMasked training\nLet’s now think how we could find \\(\\ell\\) algorithmically (and when the true mean curve is not available for comparison!).\nFor example, we could use something like the leave-one-out cross-validation:\n\nHold out a data point \\((x_i, y_i)\\);\nFit the kernel regression with lengthscale \\(\\ell\\) to the data \\((x_1, y_1), \\dotsc, (x_{i-1}, y_{i-1}), (x_{i+1}, y_{i+1}), \\dotsc, (x_n, y_n)\\);\nPredict \\(y_i\\) from \\(x_i\\) given the kernel regression.\n\nLooking at different values \\(\\ell\\) and varying the index \\(i\\) of the hold-out data point may be a reasonable training procedure. Note however that if we use standard squared loss, this will have a drawback that points which are further from the mean (due to heteroskedasticity) will be treated similarly to the data points where the noise is small. We could try to reweight them, but we won’t do that and implement a vanilla variant.\nIn fact, we will try several variants of this approach, allowing to hold out more data points than \\(1\\). In terms of probabilistic interpretation this is even worse: apart from problems with interpreting square loss due to heteroskedasticity, now we are also predicting values at several locations at once, effectively assuming that they are independent, given the observed data. In a way, this is similar to the BERT training. XLNet considers different permutations, being closer to an orderless autoregressive model. Anyway, BERT had impressive performance, so let’s try different variants here:\n\n\nCode\nimport optax\n\n\ndef train(\n    key,\n    model: eqx.Module,\n    X: Vector,\n    Y: Vector,\n    learning_rate: float = 0.2,\n    hold_out_size: int = 1,\n    n_steps: int = 100,\n    print_every: int = 100\n) -&gt; eqx.Module:\n    assert n_steps &gt; 1\n    if print_every is None:\n        print_every = n_steps + 100\n    assert print_every &gt; 0\n    assert learning_rate &gt; 0\n\n    assert X.shape[0] == Y.shape[0]\n    n_total = X.shape[0]\n\n    def split_data(key):\n        \"\"\"Splits the data into training and test.\"\"\"\n        indices = random.permutation(key, jnp.arange(n_total))\n        ind_test = indices[:hold_out_size]\n        ind_obs = indices[hold_out_size:]\n\n        return X[ind_obs], Y[ind_obs], X[ind_test], Y[ind_test]\n\n    @jax.jit\n    def step(\n        model: eqx.Module,\n        opt_state,\n        X_obs: Vector,\n        Y_obs: Vector,\n        X_test: Float[Array, \" n_test\"],\n        Y_test: Float[Array, \" n_test\"],\n    ):\n        def loss_fn(model):\n            preds = model.predict(\n                X_test=X_test,\n                X_obs=X_obs,\n                Y_obs=Y_obs,\n            )\n            return jnp.mean(jnp.square(preds - Y_test))\n\n        loss, grads = jax.value_and_grad(loss_fn)(model)\n        updates, opt_state = optimizer.update(grads, opt_state, model)\n        model = optax.apply_updates(model, updates)\n        return model, opt_state, loss\n\n    optimizer = optax.adam(learning_rate=learning_rate)\n    opt_state = optimizer.init(model)\n\n    losses = []\n\n    for n_step in range(1, n_steps + 1):\n        key, subkey = random.split(key)\n        \n        X_obs, Y_obs, X_test, Y_test = split_data(subkey)\n\n        model, opt_state, loss = step(\n            model,\n            opt_state=opt_state,\n            X_obs=X_obs,\n            Y_obs=Y_obs,\n            X_test=X_test,\n            Y_test=Y_test\n        )\n\n        losses.append(loss)\n\n        if n_step % print_every == 0:\n            avg_loss = jnp.mean(jnp.asarray(losses[-20:]))\n            print(f\"Step: {n_step}\")\n            print(f\"Loss: {avg_loss:.2f}\")\n            print(\"-\" * 14)\n\n    return model\n\n\nfig, axs = plt.subplots(2, 2, figsize=(2*4, 2*3), dpi=150, sharex=True, sharey=True)\n\nfor holdout, ax in zip([1, 10, n_points // 2, int(0.8 * n_points)], axs.ravel()):\n    key, subkey = random.split(key)\n    \n    model = train(\n        key=subkey,\n        model=GaussianKernel(lengthscale=1.0),\n        X=X,\n        Y=Y,\n        print_every=None,\n        hold_out_size=holdout,\n        n_steps=100,\n    )\n    pred = model.predict(X_ax, X_obs=X, Y_obs=Y)\n    ax.set_title(f\"Hold-out={holdout}, $\\ell$={model.lengthscale:.2f}\")\n    ax.plot(X_ax, f(X_ax), color=\"maroon\", alpha=0.8)\n    ax.plot(X_ax, pred, color=\"orangered\", alpha=0.8)\n    ax.scatter(X, Y, color=\"white\", s=5, alpha=0.8)\n    ax.set_xlabel(\"$X$\")\n    ax.set_ylabel(\"$Y$\")\n\nfig.tight_layout()\n\n\n\n\n\nHey, this worked pretty well!\n\n\nMulti-headed kernel regression\nAt this point we’ll introduce yet another modification; later we’ll see that it’s analogous to multi-head attention. Consider a model with \\(H\\) “heads”. Each head will be a kernel with a potentially different lengthscale \\(\\ell_h\\). In this manner, we will allow different heads to capture information at a different lengthscale. Finally, we will combine the predictions using auxiliary parameters \\(u_1, \\dotsc, u_H\\): \\[\n\\hat f(x) = \\sum_{h=1}^H u_h\\, \\hat f_h(x) = \\sum_{h=1}^H u_h\\, \\sum_{i=1}^n y_i \\frac{ K(x, x_i; \\ell_h) }{ \\sum_{j=1}^n K(x, x_j; \\ell_h) }.\n\\]\nLet’s implement it quickly in Equinox:\n\n\nCode\nclass MultiheadGaussianKernel(eqx.Module):\n    kernels: list[GaussianKernel]\n    weights: jax.Array\n\n    def __init__(self, n_heads: int) -&gt; None:\n        assert n_heads &gt; 0\n\n        self.weights = jnp.full(shape=(n_heads,), fill_value=1 / n_heads)\n        self.kernels = [\n            GaussianKernel(lengthscale=l)\n            for l in jnp.linspace(0.1, 3, n_heads)\n        ]\n\n    @property\n    def lengthscale(self) -&gt; list[float]:\n        return [k.lengthscale for k in self.kernels]\n\n    def predict(self, X_test: Float[Array, \" n_test\"], X_obs: Vector, Y_obs: Vector) -&gt; Float[Array, \" n_test\"]:\n        # Shape (kernels, n_test)\n        preds = jnp.stack([k.predict(X_test=X_test, X_obs=X_obs, Y_obs=Y_obs) for k in self.kernels])\n        return jnp.einsum(\"kn,k-&gt;n\", preds, self.weights)\n\nfig, axs = plt.subplots(2, 2, figsize=(2*4, 2*3), dpi=150, sharex=True, sharey=True)\n\nfor n_heads, ax in zip([1, 2, 4, 8], axs.ravel()):\n    key, subkey = random.split(key)\n    \n    model = train(\n        key=subkey,\n        model=MultiheadGaussianKernel(n_heads=n_heads),\n        X=X,\n        Y=Y,\n        print_every=None,\n        hold_out_size=1,\n        n_steps=1_000,\n    )\n    pred = model.predict(X_ax, X_obs=X, Y_obs=Y)\n    ax.set_title(f\"Heads={n_heads}\") # $\\ell$={model.lengthscale:.2f}\")\n    ax.plot(X_ax, f(X_ax), color=\"maroon\", alpha=0.8)\n    ax.plot(X_ax, pred, color=\"orangered\", alpha=0.8)\n    ax.scatter(X, Y, color=\"white\", s=5, alpha=0.8)\n    ax.set_xlabel(\"$X$\")\n    ax.set_ylabel(\"$Y$\")\n\n    u_h_str = \", \".join([f\"{w:.2f}\" for w in model.weights])\n    l_h_str = \", \".join([f\"{k.lengthscale:.2f}\" for k in model.kernels])\n\n    print(f\"Number of heads: {n_heads}\")\n    print(f\"  Combination:  {u_h_str}\")\n    print(f\"  Lengthscales: {l_h_str}\")\n\nfig.tight_layout()\n\n\nNumber of heads: 1\n  Combination:  1.09\n  Lengthscales: 0.16\nNumber of heads: 2\n  Combination:  0.99, -0.10\n  Lengthscales: 0.08, 0.02\nNumber of heads: 4\n  Combination:  1.09, -0.36, -0.24, 0.10\n  Lengthscales: 0.16, 2.95, 2.18, 11.60\nNumber of heads: 8\n  Combination:  0.90, 0.02, 0.86, 0.95, -0.45, -0.53, -0.51, -0.22\n  Lengthscales: 0.08, 9.49, 20.43, 25.16, 210.44, 33.52, 34.75, 82.24\n\n\n\n\n\nWe see that coefficients \\(u_h\\) are not constrained to be positive and they do not have to sum up to 1: we allow an arbitrary linear combination of predictions, rather than a weighted sum. Note also that many heads allow for larger flexibility, although on such a small data set this can arguably result in some amount of overfitting."
  },
  {
    "objectID": "posts/kernel-regression-transformer.html#attention",
    "href": "posts/kernel-regression-transformer.html#attention",
    "title": "From kernel regression to the transformer",
    "section": "Attention",
    "text": "Attention\nRecall the equation \\[\n\\hat f(x) = \\sum_{i=1}^n y_i\\, \\frac{K(x, x_i; \\theta)}{ \\sum_{j=1}^n K(x, x_j; \\theta)},\n\\] where there kernel \\(K\\) is now parameterised by \\(\\theta\\). As we want the kernel to give positive values, let’s write \\[\nK(x, x'; \\theta) = \\exp s_\\theta(x, x')\n\\] for some function \\(s_\\theta\\). Hence, we can write \\[\n\\hat f(x) = \\sum_{i=1}^n y_i \\, \\mathrm{softmax}( s_\\theta(x, x_j)_{j = 1, \\dotsc, n} ).\n\\] The usual approach is to use \\(\\theta = (W^{(q)}, W^{(k)})\\) for matrices mapping from \\(\\mathcal X\\) to some space \\(\\mathbb R^{d_\\text{qk}}\\) and use a scalar product \\[\ns_\\theta(x, x') = \\frac{\\left\\langle W^{(q)}x, W^{(k)}x'\\right\\rangle}{\\sqrt{d_\\text{qk}}} = \\frac{ x^T \\left(W^{(q)}\\right)^T W^{(k)}x'}{\\sqrt{d_\\text{qk}}},\n\\] where the denominator takes various forms and is usually used to ensure that the values are properly normalized and the gradients can propagate through the softmax layer well.\nNow consider another modification. We will write \\(y_i = W^{(v)}x_i\\) for some matrix \\(W^{(v)}\\) mapping from \\(\\mathcal X\\) to some space \\(\\mathbb R^{d_\\text{v}}\\). (One can think that it’s a restriction when it comes to the regression (as we are not using values \\(y_i\\) as provided), but it’s not really a big issue: it just suffices to relabel as “point \\(x_i\\)” a tuple \\((x_i, y_i)\\) and redefine the introduced parameter matrices, so that they first project on the required component.)\nIn this case, we obtain a function \\[\nx\\mapsto \\sum_{i=1}^n W^{(v)}x_i \\, \\mathrm{softmax}\\left(  \\frac{ x^T \\left(W^{(q)}\\right)^T W^{(k)}x_i}{\\sqrt{d_\\text{qk}}}  \\right).\n\\]\nIf we apply this formula to each \\(x\\) from the sequence \\((x_1, \\dotsc, x_n) \\in \\mathcal X^n\\), we obtain a new sequence \\((x_1', \\dotsc, x'_n) \\in \\left(\\mathbb R^{d_\\text{v}}\\right)^n\\). This is exactly the self-attention layer used in transformers. How to obtain multi-head attention? Similarly as in multi-head kernel regression, we will introduce \\(H\\) different “heads” with individual parameters \\(W^{(k)}_h, W^{(q)}_h, W^{(v)}_h\\). Hence, for each data point \\(x\\) in the original sequence, we have \\(H\\) vectors in \\(\\mathbb R^{d_\\text{v}}\\) given by \\[\nx\\mapsto \\sum_{i=1}^n W^{(v)}_hx_i \\, \\mathrm{softmax}\\left(  \\frac{ x^T \\left(W^{(q)}_h\\right)^T W^{(k)}_h x_i}{\\sqrt{d_\\text{qk}}}  \\right) \\in \\mathbb R^{d_\\text{v}}.\n\\]\nIf we want to obtain a mapping into some vector space \\(\\mathcal Y\\), we can now introduce matrices \\(U_h\\colon \\mathbb R^{d_\\text{v}}\\to \\mathcal Y\\), so that in the end we have \\[\nx\\mapsto \\sum_{h=1}^H U_h \\sum_{i=1}^n W^{(v)}_hx_i \\, \\mathrm{softmax}\\left(  \\frac{ x^T \\left(W^{(q)}_h\\right)^T W^{(k)}_h x_i}{\\sqrt{d_\\text{qk}}}  \\right) \\in \\mathcal Y.\n\\]\nTo summarize, multi-head attention maps a sequence \\((x_1, \\dotsc, x_n)\\in \\mathcal X^n\\) to a sequence in \\(\\mathcal Y^n\\) and is parameterised by \\(H\\) tuples of matrices \\((W^{(q)}_h, W^{(k)}_h, W^{(v)}_h, U_h)\\), where index \\(h\\) corresponds to the attention head.\nConveniently, Equinox implements multi-head attention, from which I took dimension annotations."
  },
  {
    "objectID": "posts/kernel-regression-transformer.html#footnotes",
    "href": "posts/kernel-regression-transformer.html#footnotes",
    "title": "From kernel regression to the transformer",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe authors had to explain self-attention, its multi-head variant, the transformer architecture with encoder and decoder block, and positional encoding. All in a short conference paper, so it may indeed appear quite dense in ideas.↩︎"
  },
  {
    "objectID": "posts/board-games-monte-carlo.html",
    "href": "posts/board-games-monte-carlo.html",
    "title": "Understanding board games with Monte Carlo simulations",
    "section": "",
    "text": "I like playing board games, but I never remember the probabilities of different interesting events. Let’s code a very simple Monte Carlo simulation to evaluate probabilities used in them, so I can revisit to this website and use it to (maybe eventually) win."
  },
  {
    "objectID": "posts/board-games-monte-carlo.html#fight-or-flight",
    "href": "posts/board-games-monte-carlo.html#fight-or-flight",
    "title": "Understanding board games with Monte Carlo simulations",
    "section": "Fight or flight?",
    "text": "Fight or flight?\nIn the rare days when I find time to play Runebound, I find myself in situations fighting monsters and trying to decide whether I should try to fight them or escape. I know a monster’s strength (high), I know my strength (low), but I don’t know how likely it is that the difference can be compensated by throwing two ten-sided dice.\nLet’s estimate the chances of getting at least \\(X\\) points due to the dice throw.\n\n\nCode\nimport numpy as np\n\nn_simulations: int = 100_000\ndice: int = 10\n\nrng = np.random.default_rng(42)\noccurrences = np.zeros(2 * dice + 1, dtype=float)\n\nthrows = rng.integers(1, dice, endpoint=True, size=(n_simulations, 2))\ntotal = throws.sum(axis=1)\n\nfor t in total:\n    occurrences[:t+1] += 1\n\noccurrences /= n_simulations\n\nfor i, p in enumerate(occurrences):\n    if i &lt; 1:\n        continue\n    print(f\"{i}: {100*p:.1f}%\")\n\n\n1: 100.0%\n2: 100.0%\n3: 99.0%\n4: 97.0%\n5: 94.0%\n6: 90.1%\n7: 85.2%\n8: 79.2%\n9: 72.2%\n10: 64.1%\n11: 55.1%\n12: 45.2%\n13: 36.0%\n14: 28.0%\n15: 21.1%\n16: 15.1%\n17: 10.0%\n18: 6.0%\n19: 3.0%\n20: 1.0%\n\n\nIn this case it’s also very easy to actually calculate the probabilities without Monte Carlo simulation:\n\n\nCode\nprobabilities = np.zeros(2*dice + 1, dtype=float)\n\nfor result1 in range(1, dice + 1):\n    for result2 in range(1, dice + 1):\n        total = result1 + result2\n        probabilities[:total + 1] += 1/dice**2\n\nfor i, p in enumerate(occurrences):\n    if i &lt; 1:\n        continue\n    print(f\"{i}: {100*p:.1f}%\")\n\n\n1: 100.0%\n2: 100.0%\n3: 99.0%\n4: 97.0%\n5: 94.0%\n6: 90.1%\n7: 85.2%\n8: 79.2%\n9: 72.2%\n10: 64.1%\n11: 55.1%\n12: 45.2%\n13: 36.0%\n14: 28.0%\n15: 21.1%\n16: 15.1%\n17: 10.0%\n18: 6.0%\n19: 3.0%\n20: 1.0%\n\n\nThe exact solution requires \\(O(K^2)\\) operations, where one uses two dice with \\(K\\) sides1. For a larger number of dice this solution may not be as tractable, so Monte Carlo approximations may shine."
  },
  {
    "objectID": "posts/board-games-monte-carlo.html#where-should-my-cheese-be",
    "href": "posts/board-games-monte-carlo.html#where-should-my-cheese-be",
    "title": "Understanding board games with Monte Carlo simulations",
    "section": "Where should my cheese be?",
    "text": "Where should my cheese be?\nIn Cashflow one way to win the end-game is to quickly get to the tile with a cheese-shaped token. As this token can be placed in advance, I was wondering what the optimal location of it should be.\nIf I put the token on the first tile, I need to throw exactly one in my first throw or I will need to travel across the whole board to close the loop and have another chance (or try to win the game in another way).\nLet’s use Monte Carlo simulation to estimate where I should put the token so I can win in at most five moves:\n\n\nCode\nimport numpy as np \n\nN_SIMULATIONS: int = 100_000\nN_THROWS: int = 5\nDICE: int = 6  # Number of sides on the dice\nrng = np.random.default_rng(101)\n\nvisitations = np.zeros(N_THROWS * DICE + 1)\n\nfor simulation in range(N_SIMULATIONS):\n    position = 0\n    for throw_index in range(N_THROWS):\n        result = rng.integers(1, DICE, endpoint=True)\n        position += result\n        visitations[position] += 1\n\nfor i in range(N_THROWS * DICE + 1):\n    percentage = 100 * visitations[i] / N_SIMULATIONS\n    print(f\"{i}: {percentage:.1f}\")\n\n\n0: 0.0\n1: 16.5\n2: 19.3\n3: 22.8\n4: 26.4\n5: 30.8\n6: 36.2\n7: 25.2\n8: 26.8\n9: 28.1\n10: 28.6\n11: 28.4\n12: 27.9\n13: 25.8\n14: 25.1\n15: 24.0\n16: 21.8\n17: 19.6\n18: 16.5\n19: 13.9\n20: 11.2\n21: 8.5\n22: 6.2\n23: 4.3\n24: 2.7\n25: 1.6\n26: 0.9\n27: 0.5\n28: 0.2\n29: 0.1\n30: 0.0\n\n\nAgain, we could do this in the exact fashion — for example, for 30 we know that the probability is exactly \\(6^{-5}\\approx 0.013\\%\\), but it’s quite clear that the sixth tile gives decent chances of winning in the first few moves."
  },
  {
    "objectID": "posts/board-games-monte-carlo.html#footnotes",
    "href": "posts/board-games-monte-carlo.html#footnotes",
    "title": "Understanding board games with Monte Carlo simulations",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe implemented solution works in \\(O(K^3)\\) due to the probabilities[:total + 1] operation. If the performance did really matter here, we could store the occurrences and then calculate cumulative sums only once in the end.↩︎"
  },
  {
    "objectID": "posts/invariance-mi-pmi-profile.html",
    "href": "posts/invariance-mi-pmi-profile.html",
    "title": "Invariance of the mutual information and the profile",
    "section": "",
    "text": "In our recent manuscript, we study the pointwise mutual information profile, which generalizes the notion of mutual information between two random variables to higher moments. In this paper, we assume that all the spaces and functions involved have sufficient regularity conditions (e.g., smoothness is our typical assumption) and prove that the pointwise mutual information profile doesn’t change when either variable is reparameterized by a diffeomorphism.\nThese assumptions are convenient, allowing for very short proofs for many interesting distributions, but they are also too strong. In this post, we’ll pay the price of doing some measure theory to make the results more general and show what is really the point there. Update: A summary of this post has been to our manuscript as Appendix A.5."
  },
  {
    "objectID": "posts/invariance-mi-pmi-profile.html#kl-divergence",
    "href": "posts/invariance-mi-pmi-profile.html#kl-divergence",
    "title": "Invariance of the mutual information and the profile",
    "section": "KL divergence",
    "text": "KL divergence\nLet \\(P\\) and \\(Q\\) be two probability distributions on a standard Borel space \\(\\mathcal X\\). In fact, the most interesting case for us is when \\(P=P_{XY}\\) is a joint probability distribution of random variables \\(X\\) and \\(Y\\), and \\(Q=P_X\\otimes P_Y\\) is the product of marginal distributions, but some extra generality won’t harm us.\nWe will assume that \\(P\\ll Q\\), so that we have a well-defined Radon–Nikodym derivative \\(f = \\mathrm{d}P/\\mathrm{d}Q \\colon \\mathcal X\\to [0, \\infty)\\), which is a measurable function. Note that it is defined only up to a \\(Q\\)-null set (and, as \\(P\\ll Q\\), every \\(Q\\)-null set is also a \\(P\\)-null set). By appropriately extending the logarithm function, one can define a measurable function \\(\\log f\\colon \\mathcal{S}\\to [-\\infty, \\infty) = \\mathbb R \\cup \\{-\\infty\\}\\), which appears in the well-known definition of the Kullback–Leibler divergence: \\[\n\\mathrm{KL}(P\\parallel Q) = \\int f\\, \\log f\\, \\mathrm{d}Q = \\int \\log f\\, \\mathrm{d}P.\n\\]\nIt is easy to show that this definition does not depend on the version of \\(f\\) used."
  },
  {
    "objectID": "posts/invariance-mi-pmi-profile.html#pmi-profile",
    "href": "posts/invariance-mi-pmi-profile.html#pmi-profile",
    "title": "Invariance of the mutual information and the profile",
    "section": "PMI profile",
    "text": "PMI profile\nHowever, we will be more interested in the “histogram of \\(\\log f\\) values”. By the pointwise mutual information profile (perhaps we should call it pointwise log-density ratio profile, but let’s use the former name) we will understand the pushforward distribution \\[\n  \\mathrm{Prof}_{P\\parallel Q} := (\\log f)_\\sharp P,\n\\] which (a) seems to be defined on \\([-\\infty, \\infty) = \\mathbb R \\cup \\{-\\infty\\}\\) and (b) doesn’t have to actually exist, as \\(\\log f\\) is defined only up to a \\(Q\\)-null set!\nIn fact neither of these issues is serious. To show that, let’s write \\(\\mathrm{Prof}\\) for the profile and note that \\[\n\\begin{align*}\n\\mathrm{Prof}(-\\{\\infty\\}) &= P( \\{s\\in \\mathcal S\\mid \\log f(s) = -\\infty\\}) \\\\\n&= P(\\{s\\in \\mathcal S\\mid f(s) = 0\\}) = 0,\n\\end{align*}\n\\] because we can write \\(P(E) = \\int_E f\\, \\mathrm{d}Q\\).\nThe proof that the profile doesn’t really depend on which version of \\(f\\) we use is also easy: if \\(g = f\\) up to a \\(Q\\)-null set, we have for any Borel subset \\(B\\in [-\\infty, \\infty)\\) the equality \\(\\mathbf{1}_B(\\log g) = \\mathbf{1}_B(\\log f)\\) up to a \\(P\\)-null set (remember, we have \\(P \\ll Q\\)) and the measure assigned to it, \\(\\mathrm{Prof}(B) = \\int \\mathbf{1}_B( \\log f ) \\, \\mathrm{d}P\\), is the same.\n\nInvariance of the profile\nLet’s now prove the invariance of the profile: we want to show that under reasonable assumptions on \\(i\\colon \\mathcal X\\to \\mathcal X'\\) there exists a profile of the push-forward distributions \\(\\mathrm{Prof}_{i_\\sharp P\\parallel i_\\sharp Q}\\) and that, in fact, it is the same as the original profile \\(\\mathrm{Prof}_{P\\parallel Q}\\).\n\n\n\n\n\n\nLemma\n\n\n\nLet \\(i\\colon \\mathcal X\\to \\mathcal X'\\) be a measurable mapping between standard Borel spaces such that there exists a measurable left inverse \\(a\\colon \\mathcal X' \\to \\mathcal X\\). If \\(P\\) and \\(Q\\) are two probability distributions on \\(\\mathcal X\\) such that \\(P \\ll Q\\) and \\(f = \\mathrm{d}P/\\mathrm{d}Q\\) is the Radon–Nikodym derivative, then \\(i_\\sharp P \\ll i_\\sharp Q\\) and the Radon–Nikodym derivative is given by \\(\\mathrm{d} i_\\sharp P / \\mathrm{d}i_\\sharp Q = f\\circ a\\).\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\n\nTo prove that \\(f\\circ a\\colon \\mathcal X'\\to [0, \\infty)\\) is the Radon–Nikodym derivative we need to prove that \\[\n  i_\\sharp P(B) = \\int_B f\\circ a \\, \\mathrm{d}i_\\sharp Q\n\\] for every Borel subset \\(B\\subseteq \\mathcal X'\\). This is an easy corollary of the change of variables formula:\n\\[\n\\begin{align*}\n  \\int_B f\\circ a \\, \\mathrm{d}i_\\sharp Q &= \\int_{i^{-1}(B)} f\\circ a\\circ i\\, \\mathrm{d}Q \\\\\n  &= \\int_{i^{-1}(B)} f\\, \\mathrm{d}Q \\\\\n  &= \\int_{i^{-1}(B)} \\mathrm{d}P \\\\\n  &= P(i^{-1}(B)) = i_\\sharp P(B).\n\\end{align*}\n\\]\nThere’s also another proof at MathStackExchange, which looks interesting (I’m not however sure why approximation by simple functions is needed).\n\n\n\nGreat! Now we know that the profile \\(\\mathrm{Prof}_{i_\\sharp P \\parallel i_\\sharp Q }\\) is indeed well-defined. The proof of the invariance is then very simple:\n\n\n\n\n\n\nTheorem\n\n\n\nLet \\(i\\colon \\mathcal X\\to \\mathcal X'\\) be a measurable mapping between standard Borel spaces with a measurable left inverse. If \\(P\\) and \\(Q\\) are two probability distributions on \\(\\mathcal X\\) such that \\(P \\ll Q\\), then \\(\\mathrm{Prof}_{i_\\sharp P \\parallel i_\\sharp Q } = \\mathrm{Prof}_{P \\parallel Q }\\).\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nFrom the lemma above we know that \\(i_\\sharp P \\ll i_\\sharp Q\\), so that the profile is well-defined: \\[\n  \\left(\\log\\frac{\\mathrm{d} i_\\sharp P }{\\mathrm{d} i_\\sharp Q}\\right)_\\sharp (i_\\sharp P).\n\\]\nNow let’s take any measurable left inverse \\(a\\) of \\(i\\) and write \\[\n  \\log\\frac{\\mathrm{d} i_\\sharp P }{\\mathrm{d} i_\\sharp Q} = \\log f\\circ a,\n\\] where \\(f=\\mathrm{d}P/\\mathrm{d}Q\\).\nWe now have\n\\[\n  (\\log f \\circ a)_\\sharp \\, i_\\sharp P = (\\log f\\circ a\\circ i)_\\sharp P = (\\log f)_\\sharp P.\n\\]\n\n\n\n\n\nHow often do measurable left inverses exist?\nAbove we assumed the existence of a measurable left inverse, while it’s common to see an assumption of using a continuous injective mapping (which is a very convenient criterion as it’s easy to verify). Fortunately, as we work with standard Borel spaces, we can use the following result:\n\n\n\n\n\n\nLemma\n\n\n\nLet \\(i\\colon \\mathcal S\\to \\mathcal S'\\) be a continuous injective mapping between two standard Borel spaces. Then, \\(i\\) admits a measurable left inverse.\n\n\n\n\n\n\n\n\nProof\n\n\n\n\n\nLet’s choose an arbitrary point \\(x_0\\in \\mathcal X\\) and define a function \\(a\\colon \\mathcal X'\\to \\mathcal X\\) in the following manner: \\[\na(y) = \\begin{cases}\n  x_0 &\\text{ if } y\\notin i(\\mathcal X)\\\\\n  x   &\\text{ if } x \\text{ is the (unique) point such that } i(x) = y\n\\end{cases}\n\\]\nThis function is well-defined due to the fact that \\(i\\) is injective and it is a left inverse: \\(a(i(x)) = x\\) for all \\(x\\in \\mathcal X\\). Now we need to prove that it is, indeed, measurable. Take any Borel set \\(B\\subseteq \\mathcal X\\) and consider its preimage \\(a^{-1}(B) = \\{y \\in \\mathcal X' \\mid a(y) \\in B \\}\\). If \\(x_0\\notin B\\), we have \\(a^{-1}(B) = i(B)\\), which is Borel by Lusin–Suslin theorem. If \\(x_0\\in B\\), we can write \\[\n\\begin{align*}\na^{-1}(B) &=  a^{-1}( B\\setminus \\{x_0\\} ) \\cup a^{-1}(\\{x_0\\}) \\\\\n          &= i(B\\setminus \\{x_0\\}) \\cup \\{ i(x_0) \\} \\cup ( \\mathcal X' \\setminus i(\\mathcal X)),\n\\end{align*}\n\\]\nwhich is Borel as a finite union of Borel sets.\n\n\n\n\n\n\n\n\n\nHistorical note\n\n\n\n\n\nThis result allows one to prove that mutual information is invariant under continuous injective mappings using the data processing inequality (see Theorems 2.15 and 3.2d in Polyanskiy and Wu (2022)).\nWe prove it in the Beyond normal paper in rather a complicated manner, employing the interpretation of mutual information as a supremum over mutual information induced by finite partitions (see e.g., Sec. 4.2 and 4.5–4.6 of Polyanskiy and Wu (2022) or Chapters 1 and 2 of Pinsker and Feinstein (1964)). As Reviewer FjRx had a very good intuition that the data processing inequality is a key result to be applied here, I wish I had known this lemma back then!"
  },
  {
    "objectID": "posts/invariance-mi-pmi-profile.html#what-are-the-profiles-good-for",
    "href": "posts/invariance-mi-pmi-profile.html#what-are-the-profiles-good-for",
    "title": "Invariance of the mutual information and the profile",
    "section": "What are the profiles good for?",
    "text": "What are the profiles good for?\nIn our manuscript we studied the PMI profiles for the following reasons:\n\nAs they are invariant to continuous injective mappings, it turns out that our Beyond normal paper had only a few “really different” distributions.\nThe PMI profiles seem to be related to the estimation of mutual information using variational losses.\nWhen we were trying to understand the mutual information in the Student distribution, we decided to use a Monte Carlo estimator of mutual information, essentially constructing the PMI profiles as a byproduct. This idea then turned out to be an interesting one for building distributions for which analytical expressions for ground-truth MI are not available, but can be obtained via Monte Carlo approximations.\nWe felt that if the variance of the PMI profile is large, the mutual information (being the mean) may be hard to estimate, on the basis of the Monte Carlo standard error. However, I don’t have a good intuition whether it is true or not.\n\nOverall, are they a useful concept? I’m not so sure: let’s give it some time and see what the community decides! I would like to see the PMI profiles appearing in more contexts, but perhaps looking at just the first moment (the mutual information) is enough for all the purposes.\nI also think that it may be possible to generalize the PMI profiles to the \\(f\\)-divergence setting. There exist variational lower bounds, which are related to \\(f\\)-GANs, but I can’t say yet whether introducing an “\\(f\\)-divergence” profile would yield any practical benefits. (Or even how to define it: in the end, many pushforwards could be defined, but it does not necessarily mean that they have to be good objects to study!)"
  },
  {
    "objectID": "posts/estimating-mean-vector.html",
    "href": "posts/estimating-mean-vector.html",
    "title": "Estimating the mean vector",
    "section": "",
    "text": "I recently ended up building another Gibbs sampler1. I had \\(N\\) vectors \\((Y_n)\\) such that each vector \\(Y_n = (Y_{n1}, \\dotsc, Y_{nG})\\) was assumed to come from the multivariate normal distribution:\n\\[\nY_n\\mid \\mu \\sim \\mathcal N(\\mu, \\Sigma),\n\\]\nwhere \\(\\Sigma\\) is a known \\(G\\times G\\) covariance matrix and \\(\\mu \\sim \\mathcal N(0, B)\\) is the unknown population mean, given a multivariate normal prior. In this case, it is important that we know \\(\\Sigma\\) and that \\(B\\) is a fixed matrix, which was not necessarily build using \\(\\Sigma\\): the Wikipedia derivation for Bayesian multivariate linear regression (which is a more general case) uses a different prior. I searched the internet for some time and I found a nice project, The Book of Statistical Proofs, but I still could not find the derivation adressing the simple case above.\nLet’s quickly derive it. Define \\(\\nu(x) = \\exp(-x/2)\\), which has two key properties. First, \\(\\nu(x)\\cdot \\nu(y) = \\nu(x + y)\\). Second, \\[\\begin{align*}\n  \\mathcal N(x\\mid m, V) &\\propto \\nu\\big( (x-m)^T V^{-1}(x-m) \\big)\\\\\n  &\\propto \\nu( x^TV^{-1}x - 2m^TV^{-1}x),\n\\end{align*}\n\\]\nwhich shows us how to recognise the mean and the covariance matrix of a multivariate normal distribution.\nLet’s define \\(\\bar Y = N^{-1}\\sum_{n=1}^N Y_n\\) to be the mean vector and \\(V = (B^{-1} + N\\Sigma^{-1})^{-1}\\) to be an auxiliary matrix. (We see that \\(V^{-1}\\) looks like sum of precision matrices, so may turn out to be some precision matrix!). The posterior on \\(\\mu\\) is given by \\[\\begin{align*}\n  p\\big(\\mu \\mid (Y_n), \\Sigma, B\\big) &\\propto  \\mathcal N( \\mu\\mid 0, B) \\cdot \\prod_{n=1}^N \\mathcal N(Y_n\\mid \\mu, \\Sigma) \\\\\n  &\\propto \\nu( \\mu^T B^{-1}\\mu )\\cdot \\nu\\left( \\sum_{n=1}^N (Y_n - \\mu)^T \\Sigma^{-1} (Y_n - \\mu)  \\right) \\\\\n  &\\propto \\nu\\left(\n    \\mu^T \\left(B^{-1} + N \\Sigma^{-1}\\right)\\mu - 2 N \\bar Y^T \\Sigma^{-1} \\mu\n    \\right) \\\\\n  & \\propto \\nu\\left(\n    \\mu^T V^{-1} \\mu - 2 N \\bar Y^T \\Sigma^{-1} (V V^{-1}) \\mu\n  \\right) \\\\\n  & \\propto \\nu\\left(\n    \\mu^T V^{-1} \\mu - 2\\left(N \\bar Y^T \\Sigma^{-1} V\\right) V^{-1} \\mu\n  \\right).\n\\end{align*}\n\\]\nLet’s define \\(m^T = N\\bar Y^T \\Sigma^{-1} V\\), so that \\(m = N \\cdot V \\Sigma^{-1} \\bar Y\\). In turn, we have \\(p\\big(\\mu \\mid (Y_n), \\Sigma, B\\big) = \\mathcal N(\\mu \\mid m, V)\\).\nIt looks a bit surprising that we have \\(m\\) being proportional to \\(N\\): we would expect that for \\(N\\gg 1\\) we would have \\(m\\approx \\bar Y\\). However, this is fine as for \\(N\\gg 1\\) we have \\(V \\approx N^{-1}\\Sigma\\) and \\(m\\approx \\bar Y\\). For a small sample size, however, the prior regularises the estimate.\nLet’s implement these equations in JAX:\nCode\nfrom typing import Callable\n\nimport jax\nimport jax.numpy as jnp\nimport jax.random as jrandom\n\nimport blackjax\nfrom jaxtyping import Float, Array\n\n\ndef normal_logp(\n  x: Float[Array, \" G\"],\n  mean: Float[Array, \" G\"],\n  precision: Float[Array, \"G G\"],\n) -&gt; Float[Array, \"\"]:\n  y = x - mean\n  return -0.5 * jnp.einsum(\"g,gh,h-&gt;\", y, precision, y)\n\n\ndef logposterior_fn(\n  data: Float[Array, \"N G\"],\n  precision_prior: Float[Array, \"G G\"],\n  precision_likelihood: Float[Array, \"G G\"],\n) -&gt; Callable[[Float[Array, \" G\"]], Float[Array, \"\"]]:\n  def fn(mu: Float[Array, \" G\"]) -&gt; Float[Array, \"\"]:\n    logprior = normal_logp(\n      x=mu,\n      mean=jnp.zeros_like(mu),\n      precision=precision_prior,\n    )\n    loglike = jnp.sum(\n      jax.vmap(\n        normal_logp,\n        in_axes=(0, None, None),)(\n          data,\n          mu,\n          precision_likelihood,\n        )\n    )\n    return logprior + loglike\n  \n  return fn\n\n\ndef get_y_bar(data: Float[Array, \"N G\"]) -&gt; Float[Array, \" G\"]:\n  return jnp.mean(data, axis=0)\n\n\ndef posterior_precision(\n  data: Float[Array, \"N G\"],\n  precision_prior: Float[Array, \"G G\"],\n  precision_likelihood: Float[Array, \"G G\"],\n):\n  N = data.shape[0]\n  return precision_prior + N * precision_likelihood\n\n\ndef posterior_mean(\n  data: Float[Array, \"N G\"],\n  precision_prior: Float[Array, \"G G\"],\n  precision_likelihood: Float[Array, \"G G\"],\n):\n  N = data.shape[0]\n  posterior_cov = jnp.linalg.inv(\n    posterior_precision(\n      data=data,\n      precision_prior=precision_prior,\n      precision_likelihood=precision_likelihood,\n    )\n  )\n  return (N * posterior_cov) @ precision_likelihood  @  get_y_bar(data)\n\n\ndef posterior_sample(\n  key,\n  data: Float[Array, \"N G\"],\n  precision_prior: Float[Array, \"G G\"],\n  precision_likelihood: Float[Array, \"G G\"],\n  size: int = 1_000,\n):\n  N = data.shape[0]\n\n  m = posterior_mean(\n    data=data,\n    precision_prior=precision_prior,\n    precision_likelihood=precision_likelihood,\n  )\n  V = jnp.linalg.inv(posterior_precision(\n    data=data,\n    precision_prior=precision_prior,\n    precision_likelihood=precision_likelihood,\n  ))\n\n  return jrandom.multivariate_normal(\n    key, mean=m, cov=V, shape=(size,)\n  )\nWe start by generating some data points:\nCode\nn_samples = 4_000\ndata_size: int = 3\n\ncorr = 0.95\nSigma = jnp.asarray([\n  [1.0, 2 * corr],\n  [2 * corr, 2.0**2 * 1.0],\n])\n\nB = 1.0**2 * jnp.eye(2)\n\nmu = jnp.asarray([0.0, 1.5])\n\nkey = jrandom.PRNGKey(42)\nkey, subkey = jrandom.split(key)\n\ndata = jrandom.multivariate_normal(key, mu, Sigma, shape=(data_size,))\nNow let’s do inference in three different ways:\nAdditionally, we will plot a sample from the prior. On top of that we plot three points: the ground-truth vector \\(\\mu^*\\), data mean \\(\\bar Y\\), and the plotted (prior or an appropriate posterior) distribution mean[^2].\nCode\nimport matplotlib.pyplot as plt\nplt.style.use(\"dark_background\")\n\n\n# Sample from the prior\nkey, subkey = jrandom.split(key)\nprior = jrandom.multivariate_normal(\n  subkey,\n  mean=jnp.zeros(2),\n  cov=B,\n  shape=(n_samples,)\n)\n\n# Sample from the posterior using analytic formula\nkey, subkey = jrandom.split(key)\nposterior = posterior_sample(\n  subkey,\n  data=data,\n  precision_prior=jnp.linalg.inv(B),\n  precision_likelihood=jnp.linalg.inv(Sigma),\n  size=n_samples,\n)\n\n\n# Sample from the posterior using BlackJAX\nlogdensity_fn = logposterior_fn(\n  data=data,\n  precision_prior=jnp.linalg.inv(B),\n  precision_likelihood=jnp.linalg.inv(Sigma),\n)\n\nnuts = blackjax.nuts(\n  logdensity_fn,\n  1e-2,\n  jnp.ones(2),\n)\n\nn_warmup = 2_000\n\nstate = nuts.init(jnp.zeros_like(mu))\nstep_fn = jax.jit(nuts.step)\n\nkey, subkey = jrandom.split(key)\nfor i in range(n_warmup):\n    nuts_key = jrandom.fold_in(subkey, i)\n    state, _ = step_fn(nuts_key, state)\n\nposterior_blackjax = []\nkey, subkey = jrandom.split(key)\nfor i in range(n_samples):\n    nuts_key = jrandom.fold_in(subkey, i)\n    state, _ = step_fn(nuts_key, state)\n    posterior_blackjax.append(state.position)\n\nposterior_blackjax = jnp.asarray(posterior_blackjax)\n\n# Assume that errors are uncorrelated and use analytic formula\nkey, subkey = jrandom.split(key)\nposterior_ind = posterior_sample(\n  subkey,\n  data=data,\n  precision_prior=jnp.linalg.inv(B),\n  precision_likelihood=jnp.diag(1.0 / jnp.diagonal(Sigma)),\n  size=5_000,\n)\n\n\ndef _annotate(ax, x, y, marker, color, label=None):\n  ax.scatter([x], [y], s=6**2, c=color, marker=marker, label=label)\n\ndef annotate_axis(ax):\n  _annotate(ax, mu[0], mu[1], marker=\"x\", color=\"r\", label=\"$\\\\mu^*$\")\n  _annotate(ax, data.mean(axis=0)[0], data.mean(axis=0)[1], marker=\"+\", color=\"yellow\", label=\"$\\\\bar Y$\")\n\n\nfig, axs = plt.subplots(2, 2, sharex=True, sharey=True, dpi=200)\n\nax = axs[0, 0]\nax.set_title(\"Prior\")\nax.scatter(prior[:, 0], prior[:, 1], s=1, c=\"lightblue\", alpha=0.3)\n_annotate(ax, mu[0], mu[1], marker=\"x\", color=\"r\")\n_annotate(ax, 0.0, 0.0, marker=\"*\", color=\"salmon\")\n\nax = axs[0, 1]\nax.set_title(\"Posterior (uncorrelated $\\\\Sigma$)\")\nax.scatter(posterior_ind[:, 0], posterior_ind[:, 1], s=1, c=\"blue\", alpha=0.3)\nax.scatter([mu[0]], [mu[1]], s=10, c=\"red\", marker=\"x\")\nannotate_axis(ax)\n_annotate(ax, posterior_ind[:, 0].mean(), posterior_ind[:, 1].mean(), marker=\"*\", color=\"salmon\")\n\n\nax = axs[1, 0]\nax.set_title(\"Posterior (analytic)\")\nax.scatter(posterior[:, 0], posterior[:, 1], s=1, c=\"blue\", alpha=0.3)\nax.scatter([mu[0]], [mu[1]], s=10, c=\"red\", marker=\"x\")\nannotate_axis(ax)\n_annotate(ax, posterior[:, 0].mean(), posterior[:, 1].mean(), marker=\"*\", color=\"salmon\")\n\nax = axs[1, 1]\nax.set_title(\"Posterior (BlackJAX)\")\nax.scatter(posterior_blackjax[:, 0], posterior_blackjax[:, 1], s=1, c=\"blue\", alpha=0.3)\nax.scatter([mu[0]], [mu[1]], s=10, c=\"red\", marker=\"x\")\nannotate_axis(ax)\n_annotate(ax, posterior_blackjax[:, 0].mean(), posterior_blackjax[:, 1].mean(), marker=\"*\", color=\"salmon\", label=\"Mean\")\nax.legend(frameon=False)\n\n\nfor ax in axs.ravel():\n  ax.set_xlabel(\"$\\\\mu_1$\")\n  ax.set_ylabel(\"$\\\\mu_2$\")\n  ax.spines[[\"top\", \"right\"]].set_visible(False)\n\nfig.tight_layout()\nLooks like BlackJAX and analytic formula give the same posterior, so perhaps there is no mistake in the algebra. We also see that using a proper \\(\\Sigma\\) should help us estimate the mean vector better and that using the prior should regularise the inference.\nLet’s do several repetitions of this experiment and evaluate the distance from the point estimate to the ground-truth value:\nCode\ndef distance(x1, x2):\n  return jnp.sqrt(jnp.sum(jnp.square(x1 - x2)))\n\n\ndef make_repetition(key, data_size: int):\n  key1, key2 = jrandom.split(key, 2)\n  mu_true = jrandom.multivariate_normal(key1, jnp.zeros(2), B)\n\n  data = jrandom.multivariate_normal(\n    key2, mu_true, Sigma, shape=(data_size,)\n  )\n\n  y_bar = get_y_bar(data)\n  \n  mu_expected = posterior_mean(\n    data=data,\n    precision_prior=jnp.linalg.inv(B),\n    precision_likelihood=jnp.linalg.inv(Sigma),\n  )\n\n  mu_diagonal = posterior_mean(\n    data=data,\n    precision_prior=jnp.linalg.inv(B),\n    precision_likelihood=jnp.diag(1.0 / jnp.diagonal(Sigma)),\n  )\n\n  return {\n    \"prior\": distance(jnp.zeros(2), mu_true),\n    \"posterior\": distance(mu_expected, mu_true),\n    \"data\": distance(y_bar, mu_true),\n    \"diagonal\": distance(mu_diagonal, mu_true),\n  }\n\nn_reps = 2_000\n\ndef make_plots(key, axs, data_size: int):\n  reps = [make_repetition(k, data_size=data_size) for k in jrandom.split(key, n_reps)]\n\n  bins = jnp.linspace(0, 4, 20)\n\n  def plot(ax, name, color):\n    ax.hist(\n      [r[name] for r in reps],\n      color=color,\n      density=True,\n      bins=bins,\n    )\n\n  ax = axs[0]\n  ax.set_title(f\"$N={data_size}$\")\n  ax.set_xlabel(\"Prior mean\")\n  plot(ax, \"prior\", \"white\")\n\n  ax = axs[1]\n  ax.set_xlabel(\"Posterior mean\")\n  plot(ax, \"posterior\", \"bisque\")\n\n  ax = axs[2]\n  ax.set_xlabel(\"Data mean\")\n  plot(ax, \"data\", \"darkorange\")\n\n  ax = axs[3]\n  ax.set_xlabel(\"Diagonal model\")\n  plot(ax, \"diagonal\", \"purple\")\n\n\nfig, axs = plt.subplots(4, 4, sharex=True, sharey=\"row\")\n\nfor i, size in enumerate([2, 10, 50, 250]):\n  key, subkey = jrandom.split(key)\n  make_plots(\n    subkey,\n    axs=axs[:, i],\n    data_size=size,\n  )\n\nfor ax in axs.ravel():\n  ax.spines[[\"top\", \"right\", \"left\"]].set_visible(False)\n  ax.set_yticks([])\n\nfig.suptitle(\"Error\")\nfig.tight_layout()\nWe see what should expected:\nAdditionally, we see that a model assuming diagonal \\(\\Sigma\\) (i.e., ignoring the correlations) also has performance quite similar to the true one.\nThis “performance looks similar” can actually be somewhat misleading: each of this distributions has quite large variance, so minor differences can be unobserved.\nLet’s now repeat this experiment, but this time plotting the difference between distances, so that we can see any difference better. Namely, for the method \\(M\\) and and the \\(s\\)-th simulation, write \\(d^{(M)}_s\\) for the obtained distance. Now, instead of plotting the data sets \\(\\{ d^{(M_1)}_{1}, \\dotsc, d^{(M_1)}_S\\}\\) and \\(\\{ d^{(M_2)}_{1}, \\dotsc, d^{(M_2)}_S\\}\\), we can plot the differences \\(\\{ d^{(M_2)}_{1} - d^{(M_1)}_{1}, \\dotsc, d^{(M_2)}_{S} - d^{(M_1)}_{S} \\}\\).\nLet’s use the posterior mean in the right model (potentially the best solution) as the baseline and compare it with three other models. In each of the plots, the samples on the right of zero, represent positive difference, i.e., the case when the baseline method (in our case the posterior in the right model) was better than the considered alternative. Apart from raw samples, let’s plot the mean of such distribution (and, intuitively, we should expect it to be larger than zero) and report the percentage of samples on the right from zero.\nCode\nn_reps = 3_000\n\ndef compare_with_diagonal(key, data_size: int):\n  key1, key2 = jrandom.split(key, 2)\n  mu_true = jrandom.multivariate_normal(key1, jnp.zeros(2), B)\n\n  data = jrandom.multivariate_normal(\n    key2, mu_true, Sigma, shape=(data_size,)\n  )\n\n  y_bar = get_y_bar(data)\n  \n  mu_posterior = posterior_mean(\n    data=data,\n    precision_prior=jnp.linalg.inv(B),\n    precision_likelihood=jnp.linalg.inv(Sigma),\n  )\n  mu_diagonal = posterior_mean(\n    data=data,\n    precision_prior=jnp.linalg.inv(B),\n    precision_likelihood=jnp.diag(1.0 / jnp.diagonal(Sigma)),\n  )\n\n  baseline = distance(mu_posterior, mu_true)\n\n  return {\n    \"delta_prior\": distance(jnp.zeros(2), mu_true) - baseline,\n    \"delta_diagonal\": distance(mu_diagonal, mu_true) - baseline,\n    \"delta_data\": distance(y_bar, mu_true) - baseline,\n  }\n\n\ndef make_plots(key, axs, data_size: int):\n  reps = [compare_with_diagonal(k, data_size=data_size) for k in jrandom.split(key, n_reps)]\n\n  bins = jnp.linspace(-2, 2, 20)\n\n  def plot(ax, name, color):\n    samples = jnp.array([r[name] for r in reps])\n    ax.hist(\n      samples,\n      color=color,\n      density=True,\n      bins=bins,\n    )\n    p_worse = float(100 * jnp.mean(samples &gt; 0))\n    ax.axvline(jnp.mean(samples), linestyle=\":\", color=\"salmon\")\n    ax.axvline(0.0, linestyle=\":\", color=\"white\")\n    ax.annotate(f\"{p_worse:.0f}%\", xy=(0.05, 0.5), xycoords='axes fraction')\n\n  ax = axs[0]\n  ax.set_title(f\"$N={data_size}$\")\n  ax.set_xlabel(\"Prior mean\")\n  plot(ax, \"delta_prior\", \"white\")\n\n  ax = axs[1]\n  ax.set_xlabel(\"Data mean\")\n  plot(ax, \"delta_data\", \"darkorange\")\n\n  ax = axs[2]\n  ax.set_xlabel(\"Diagonal model\")\n  plot(ax, \"delta_diagonal\", \"purple\")\n\n\nfig, axs = plt.subplots(3, 4, sharex=True, sharey=\"row\")\n\nfor i, size in enumerate([2, 10, 50, 250]):\n  key, subkey = jrandom.split(key)\n  make_plots(\n    subkey,\n    axs=axs[:, i],\n    data_size=size,\n  )\n\nfor ax in axs.ravel():\n  ax.spines[[\"top\", \"right\", \"left\"]].set_visible(False)\n  ax.set_yticks([])\n\nfig.suptitle(\"Extra error over the baseline\")\nfig.tight_layout()\nAs expected, a well-specified Bayesian model performs the best. However, having “enough” data points one can use the data mean as well (or the misspecified model without off-diagonal terms in the covariance). An interesting question would be to check how this “enough” depends on the dimensionality of the problem."
  },
  {
    "objectID": "posts/estimating-mean-vector.html#footnotes",
    "href": "posts/estimating-mean-vector.html#footnotes",
    "title": "Estimating the mean vector",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nProbably I shouldn’t have, but I had to use a sparse prior over the space of positive definite matrices and I don’t know how to run Hamiltonian Monte Carlo with these choices…↩︎"
  },
  {
    "objectID": "posts/histograms-vs-density-estimation.html",
    "href": "posts/histograms-vs-density-estimation.html",
    "title": "Histograms or kernel density estimators?",
    "section": "",
    "text": "I have recently seen Michael Betancourt’s talk in which he explains why kernel density estimators can be misleading when visualising samples and points to his wonderful case study which includes comparison between histograms and kernel density estimators, as well as many other things.\nI recommend reading this case study in depth; in this blog post we will only try to reproduce the example with kernel density estimators in Python."
  },
  {
    "objectID": "posts/histograms-vs-density-estimation.html#problem-setup",
    "href": "posts/histograms-vs-density-estimation.html#problem-setup",
    "title": "Histograms or kernel density estimators?",
    "section": "Problem setup",
    "text": "Problem setup\nWe will start with a Gaussian mixture with two components and draw the exact probability density function (PDF) as well as a histogram with a very large sample size.\n\n\nCode\nimport matplotlib.pyplot as plt\nimport numpy as np \nfrom scipy import stats\n\nplt.style.use(\"dark_background\")\n\nclass GaussianMixture:\n  def __init__(self, proportions, mus, sigmas) -&gt; None:\n    proportions = np.asarray(proportions)\n    self.proportions = proportions / proportions.sum()\n    assert np.min(self.proportions) &gt; 0\n\n    self.mus = np.asarray(mus)\n    self.sigmas = np.asarray(sigmas)\n\n    n = len(self.proportions)\n    self.n_classes = n\n    assert self.proportions.shape == (n,)\n    assert self.mus.shape == (n,)\n    assert self.sigmas.shape == (n,)\n\n  def sample(self, rng, n: int) -&gt; np.ndarray:\n    z = rng.choice(\n      self.n_classes,\n      p=self.proportions,\n      replace=True,\n      size=n,\n    )\n    return self.mus[z] + self.sigmas[z] * rng.normal(size=n)\n\n  def pdf(self, x):\n    ret = 0\n    for k in range(self.n_classes):\n      ret += self.proportions[k] * stats.norm.pdf(x, loc=self.mus[k], scale=self.sigmas[k])\n    return ret\n\nmixture = GaussianMixture(\n  proportions=[2, 1],\n  mus=[-2, 2],\n  sigmas=[1, 1],\n)\n\nrng = np.random.default_rng(32)\n\nlarge_data = mixture.sample(rng, 100_000)\n\nx_axis = np.linspace(np.min(large_data), np.max(large_data), 101)\npdf_values = mixture.pdf(x_axis)\n\nfig, ax = plt.subplots(figsize=(3, 2), dpi=100)\n\nax.hist(large_data, bins=150, density=True, histtype=\"stepfilled\", alpha=0.5, color=\"C0\")\nax.plot(x_axis, pdf_values, c=\"C2\", linestyle=\"--\")\n\nax.set_title(\"Probability density function\\nand histogram with large sample size\")\n\n\nText(0.5, 1.0, 'Probability density function\\nand histogram with large sample size')\n\n\n\n\n\nGreat, histogram with large sample size agreed well with the exact PDF!"
  },
  {
    "objectID": "posts/histograms-vs-density-estimation.html#plain-old-histograms",
    "href": "posts/histograms-vs-density-estimation.html#plain-old-histograms",
    "title": "Histograms or kernel density estimators?",
    "section": "Plain old histograms",
    "text": "Plain old histograms\nLet’s now move to a more challenging problem: we have only a moderate sample size available, say 100 points.\n\n\nCode\ndata = mixture.sample(rng, 100)\n\nfig, axs = plt.subplots(5, 1, figsize=(3.2, 3*5), dpi=100)\nbin_sizes = (3, 5, 10, 20, 50)\n\nfor bins, ax in zip(bin_sizes, axs):\n  ax.hist(data, bins=bins, density=True, histtype=\"stepfilled\", alpha=0.5, color=\"C0\")\n  ax.plot(x_axis, pdf_values, c=\"C2\", linestyle=\"--\")\n\n  ax.set_title(f\"{bins} bins\")\n\nfig.tight_layout()\n\n\n\n\n\nWe see that too few bins (three, but nobody will actually choose this number for 100 data points) we don’t see two modes and that for more than 20 and 50 bins the histogram looks quite noisy. Both 5 and 10 bins would make a sensible choice in this problem."
  },
  {
    "objectID": "posts/histograms-vs-density-estimation.html#kernel-density-estimators",
    "href": "posts/histograms-vs-density-estimation.html#kernel-density-estimators",
    "title": "Histograms or kernel density estimators?",
    "section": "Kernel density estimators",
    "text": "Kernel density estimators\nNow it’s the time for kernel density estimators. We will use several kernel families and several different bandwidths:\n\n\nCode\nfrom sklearn.neighbors import KernelDensity\n\n\nkernels = [\"gaussian\", \"tophat\", \"cosine\"]\nbandwidths = [0.1, 1.0, 3.0, \"scott\", \"silverman\"]\n\nfig, axs = plt.subplots(\n  len(kernels),\n  len(bandwidths),\n  figsize=(12, 8),\n  dpi=130,\n)\n\nfor i, kernel in enumerate(kernels):\n  axs[i, 0].set_ylabel(f\"Kernel: {kernel}\")\n  for j, bandwidth in enumerate(bandwidths):\n    ax = axs[i, j]\n\n    kde = KernelDensity(bandwidth=bandwidth, kernel=kernel)\n    kde.fit(data[:, None])\n\n    kde_pdf = np.exp(kde.score_samples(x_axis[:, None]))\n\n    ax.plot(x_axis, pdf_values, c=\"C2\", linestyle=\"--\")\n    ax.fill_between(x_axis, 0.0, kde_pdf, color=\"C0\", alpha=0.5)\n\n\nfor j, bandwidth in enumerate(bandwidths):\n  axs[0, j].set_title(f\"Bandwidth: {bandwidth}\")\n\nfig.tight_layout()\n\n\n\n\n\nI see the point now! Apart from the small bandwidth case (0.1 and sometimes Silverman) the issues with KDE plots are hard to diagnose. Moreover, conclusions from different plots are different: is the distribution multimodal? If so, how many modes are there? What are the “probability masses” of each modes? Observing only one of these plots can lead to wrong conclusions."
  },
  {
    "objectID": "posts/histograms-vs-density-estimation.html#links",
    "href": "posts/histograms-vs-density-estimation.html#links",
    "title": "Histograms or kernel density estimators?",
    "section": "Links",
    "text": "Links\n\nWhat’s wrong with a kernel density: a blog post by Andrew Gelman, explaining why he prefers histograms over kernel density plots.\nMichael Betancourt’s case study, which also discusses histograms with error bars."
  },
  {
    "objectID": "posts/distinct-ising-models.html",
    "href": "posts/distinct-ising-models.html",
    "title": "How many distinct Ising models are over there?",
    "section": "",
    "text": "Recall “the” Ising model. We have a graph with \\(G\\) nodes and each node takes a value \\(s_g \\in \\{-1, 1\\}\\). It is parameterised by a symmetric \\(G\\times G\\) matrix \\(\\Omega = (\\Omega_{gg'})\\), representing interactions between different nodes, and an additional vector \\(\\mathbf{\\alpha} = (\\alpha_g)\\). In physics, \\(s_g\\) may be the magnetic moments of different sites in a lattice of magnets: \\(\\Omega\\) describes interactions between different sites (e.g., due to the spatial structure of the lattice it can be constrained) and \\(\\mathbf{\\alpha}\\) describes the interactions with an external magnetic field, providing asymmetry between the states \\(-1\\) and \\(+1\\).\nGiven the configuration vector \\(\\mathbf{s} = (s_g)\\), the energy of the system is given as \\[\n    E(\\mathbf{s}) = \\mathbf{s}^T \\Omega \\mathbf{s} + \\mathbf{\\mathbf{\\alpha}}^T \\mathbf{s} = \\sum_{g} \\alpha_g s_g + \\sum_{g}\\sum_{g'}  \\Omega_{gg'}s_g s_{g'}.\n\\]\nGiven the energy, we will be interested in the probability distribution \\[\n  P(\\mathbf{s}) = \\mathcal Z^{-1} \\exp(-E(\\mathbf{s})),\n\\] where \\(\\mathcal Z\\) is the normalising constant and can depend on the parameters \\(\\mathbf{\\alpha}\\) and \\(\\Omega\\).\nThis is an interesting system: it can be specified by \\(G(G+1)/2 + G = O(G^2)\\) parameters, but it can attribute distinct energies to each of the possible \\(2^G\\) states. It does not have to be the case for every set of parameters, though: for example, if we restrict the parameters, e.g., set \\(\\mathbf{\\alpha} = \\mathbf{0}\\), then the states \\(\\mathbf{s}\\) and \\(\\mathbf{-s}\\) have the same energy and, at most, \\(2^{G-1}\\) distinct energy levels can be represented. Moreover, even if this model attributes \\(2^G\\) distinct energies, it does not need to be a good model for an arbitrary probability distribution over the set \\(\\{-1, 1\\}^G\\), which can require even \\(2^G\\) parameters to specify.\nIf we decided to count different energy functions \\(E\\) as different models, the answer “how many of different Ising models do we have” would be \\(+\\infty\\). However, this is not a very compelling answer and as all of them have the same structure, we will think of them as of one model.\nHowever, not all the parameters are really necessary. Because for every \\(s_g \\in \\{0, 1\\}\\) we have \\(s_g^2 = 1\\), for every state \\(\\mathbf{s}\\) we have a summand \\(\\mathbf{s}^T \\mathrm{diag}(\\Omega) \\mathbf{s} = \\sum_g \\Omega_{gg}\\). This offset does not change energy differences and makes the statistical model unidentifiable. We can therefore drop the diagonal terms (by setting \\(\\Omega_{gg} = 0\\)) and think of the Ising model as of one having \\(G(G-1) / 2 + G = G(G+1)/2\\) parameters. Note that this is not the same as dropping the \\(\\mathbf{\\alpha}\\) terms, which are needed to introduce asymmetry between \\(\\mathbf{s}\\) and \\(-\\mathbf{s}\\) states."
  },
  {
    "objectID": "posts/distinct-ising-models.html#a-different-ising-model",
    "href": "posts/distinct-ising-models.html#a-different-ising-model",
    "title": "How many distinct Ising models are over there?",
    "section": "A “different” Ising model",
    "text": "A “different” Ising model\nNow consider a physical model of interacting magnets, but a statistical model representing mutations. This idea dates back to the 2013 paper of Lingzhou Xue, Hui Zou and Tianxi Cai and the 2001 paper of Jacek Majewski, Hao Li and Jurg Ott, in which they use Ising models described above to model mutations.\nHowever, for me assigning numbers from the set \\(\\{-1, 1\\}\\) to mutation data is not natural: I think of a binary genotype in terms of a vector \\(\\mathbf{y}\\in \\{0, 1\\}^G\\) representing the presence or the absence of a mutation at a particular locus.\nThe energy for that model would be \\[\n  \\tilde E(\\mathbf{y}) = \\sum_g \\pi_g y_g + \\sum_{g'\\neq g} \\Theta_{gg'} y_gy_g',\n\\]\nwhere \\(\\pi_g\\) is then related to the probability of gene mutation in the model where all mutations arise independently (i.e., \\(\\Theta_{gg'} = 0\\) for all \\(g\\neq g'\\). See the model we discussed in this blog post) and \\(\\Theta_{gg'}=\\Theta_{g'g}\\) are used to model some (anti-)correlations between genes \\(g\\) and \\(g'\\). Note that as \\(y_g = y_g^2\\) for \\(y_g \\in \\{0, 1\\}\\) we can write \\(\\Theta_{gg} = \\pi_g\\) and use a single matrix \\(\\Theta\\): \\[\n  \\tilde E(\\mathbf{y}) = \\mathbf{y}^T \\Theta \\mathbf{y} = \\sum_{g}\\sum_{g'} \\Theta_{gg'} y_g y_{g'}.\n\\]\nThis model can also result in \\(2^G\\) different energy levels."
  },
  {
    "objectID": "posts/distinct-ising-models.html#arent-these-the-same-model",
    "href": "posts/distinct-ising-models.html#arent-these-the-same-model",
    "title": "How many distinct Ising models are over there?",
    "section": "Aren’t these the same model?",
    "text": "Aren’t these the same model?\nWe can hope that both models are essentially equivalent, i.e., if we relabel \\(\\mathbf{s}\\) to \\(\\mathbf{y}\\), we can find the parameters \\(\\Theta\\) such that energy in the new model, \\(\\tilde E(\\mathbf{y})\\) will “agree” with the old energy \\(E(\\mathbf{s})\\). (And, that for every state \\(\\mathbf{y}\\) relabelled to \\(\\mathbf{s}\\), we can find parameters \\(\\mathbf{\\alpha}\\) and \\(\\Omega\\) such that the energies agree).\nWe shouldn’t hope for the exact equality of energies (after all recall that (a) zero point energy is not well defined in the first model, as we can always change the diagonal terms of \\(\\Omega\\) shifting all energy levels simultaneously by the same number, (b) only the energy differences matter, as in this model we can observe only frequencies of different states and shifting the energy levels doesn’t change the probability distribution), but the equality of the differences \\(\\tilde E(\\mathbf{y}) - \\tilde E(\\mathbf{y}')\\) and \\(E(\\mathbf s) - E(\\mathbf s')\\) whenever we respectively relabel \\(\\mathbf s\\) and \\(\\mathbf s'\\) to \\(\\mathbf y\\) and \\(\\mathbf y'\\). (And that relabelling from \\(\\mathbf y\\) to \\(\\mathbf s\\) also “works”, in the sense explained above).\nWe can consider two relabelling procedures:\n\\[\n  s_g = -1 \\leftrightarrow y_g= 0,\\quad s_g =1 \\leftrightarrow y_g = 1\n\\] and \\[\n  s_g = -1 \\leftrightarrow y_g = 1, \\quad s_g = 1 \\leftrightarrow y_g = 0.\n\\]\nI focus on the first one as it feels more natural to me. Mathematically, we can write it as \\(y_g = 0.5(s_g + 1)\\) or \\(s_g = 1 - 2y_g\\). The other relabelling can also be introduced by using an “internal” relabelling in either model (see the digression below).\n\n\n\n\n\n\nDigression: internal relabelling\n\n\n\n\n\nNote that for the \\(\\{-1, 1\\}\\) model, the mapping \\(\\mathbf{s}\\mapsto -\\mathbf{s}\\) for all states does not change the energy differences, once \\(\\mathbf{\\alpha}\\) is changed to \\(-\\mathbf{\\alpha}\\). The interaction terms, represented by the \\(\\Omega\\) matrix, do not need to be changed.\nIn the \\(\\{0, 1\\}\\) model, we can check what happens if we apply \\(y_g \\mapsto 1-y_g\\) symmetry and change \\(\\Theta\\) to \\(\\tilde\\Theta\\). We have \\[\\begin{align*}\n\\tilde E(\\mathbf{1} -\\mathbf{y}) &= \\sum_{g}\\sum_{g'}  \\tilde \\Theta_{gg'}(1-y_g)(1-y_{g'}) \\\\\n&= \\sum_{g}\\sum_{g'} \\tilde \\Theta_{gg'} y_{g}y_{g'} + \\sum_{g}\\sum_{g'} \\tilde \\Theta_{gg'} - 2\\sum_{g}\\left(\\sum_{g'} \\Theta_{g'g}  \\right) y_g,\n\\end{align*}\n\\]\nwhich looks a bit different than \\[\n\\tilde E(\\mathbf{y}) = \\sum_{g}\\sum_{g'} \\Theta_{gg'}y_gy_{g'}.\n\\]\nHowever, note that:\n\nThe first term, i.e., interactions, does not need to be changed. We can take \\(\\tilde \\Theta_{gg'} = \\Theta_{gg'}\\) for all \\(g\\neq g'\\).\nThe second term does not depend on \\(\\mathbf{y}\\). It does not affect the energy differences, so that it does not change the probabilities. We can simply ignore it.\nThe last term can be incorporated in the diagonal entries of \\(\\Theta\\), similarly as we did it with the \\(\\mathbf{\\pi}\\) vector before.\n\nHence, both models show similar symmetry. Parameterisation in terms of \\(\\Theta\\) may be more natural for biological applications, but it changes less intuitively under the relabelling symmetry (which in physics is a more natural operation than in biology).\n\n\n\nThe energy is given by \\[\\begin{align*}\n  \\tilde E(\\mathbf{y}) &= \\sum_{g}\\sum_{g'} \\Theta_{gg'} y_g y_g' \\\\\n  &= \\frac{1}{4}\\sum_{g}\\sum_{g'} \\Theta_{gg'} (1 + s_g)(1 + s_{g'}) \\\\\n  &= \\mathrm{const.} + \\sum_{g}\\sum_{g'} \\frac{\\Theta_{gg'}}{4} s_g s_{g'} + \\sum_{g} \\left(\\sum_{g'}\\frac{\\Theta_{gg'}}{2}\\right) s_g,\n\\end{align*}\n\\]\nwhere \\(\\mathrm{const.}\\) does not depend on \\(\\mathbf{y}\\) (or, more appropriate in this context, on \\(\\mathbf{s}\\)), so that it does not change the energy differences. and we can ignore it. From this we can read the expressions for \\(\\Omega\\) and \\(\\mathbf{\\alpha}\\) vectors resulting in the same distribution. I think it is important to notice that, in many cases, \\(\\mathbf{\\alpha}\\neq 0\\). In other words, to model mutational dependencies one has to use the Ising model including interactions with the external field, to introduce possible asymmetry.\nLet’s do this exercise now in the other direction. We have \\(s_g = 1 - 2y_g\\), so \\[\\begin{align*}\n  E(\\mathbf{s}) &= \\sum_g \\alpha_g s_g + \\sum_g \\sum_{g'} \\Omega_{gg'} s_g s_{g'} \\\\\n  &= \\left(\\sum_g \\alpha_g - 2 \\sum_{g} \\alpha_g y_g \\right) + \\sum_{g}\\sum_{g'} 4 \\Omega_{gg'} y_g y_{g'} - \\sum_g \\left(\\sum_{g'} 4\\Omega_{gg'} \\right)y_g.\n\\end{align*}\n\\]\nWe can happily ignore the constant term \\(\\sum_{g}\\alpha_g\\) and define appropriate matrix \\(\\Theta\\) corresponding to this model. Overall, the interaction terms, i.e., \\(\\Omega_{gg'}\\) and \\(\\Theta_{gg'}\\) are equivalent up to scaling.\nThe diagonal terms seem to be more delicate, though. Let’s see what happens when \\(\\mathbf{\\alpha} = 0\\). In this case, the energy is proportional (I dropped the factor of 4) to \\[\n  \\sum_{g\\neq g'} \\Omega_{gg'} y_gy_g' + \\sum_{g}\\left( \\Omega_{gg} - \\sum_{g'} \\Omega_{gg'} \\right)y_g =\n  \\sum_{g\\neq g'} \\Omega_{gg'} y_gy_g' - \\sum_{g}\\left(\\sum_{g'\\neq g} \\Omega_{gg'} \\right)y_g.\n\\]\nNote that the left hand side of the equation looks a bit wrong: it explicitly mentions the diagonal terms \\(\\Omega_{gg}\\), even though they should not affect the energy differences! On the right hand side we see that this dependency is a mirage: the terms \\(\\Omega_{gg}\\) cancel out. In particular, this model has a strong constraint: the baseline occurence probabilities, \\(\\Theta_{gg}\\), depend on the offdiagonal terms!\nI don’t find this constraint natural and I think it’s important to remember about mentioning it as an explicit assumption, when one is fitting a model with \\(\\mathbf{\\alpha} = \\mathbf{0}\\). In this nice paper from 2013, the analysis seems to have been performed with this constraint in mind. I wonder what would have happened if the authors had decided to model mutations without this assumption (I think that the 2001 paper allows for the additional \\(\\mathbf{\\alpha}\\) term). In particular:\n\nWould there be some interaction terms looking different? (These are high-dimensional problems with no known ground-truth, so perhaps a simulation study would be more informative).\nWould the more flexible model fit the data better?\nCan (penalised) pseudolikelihood still provide reliable estimates in this case? As an alternative method of training these models, one could use discrete Fisher divergence, which already appeared on this blog.\nHow does sparsity interact with this constraint? I recall that many sparsity-inducing priors for learning graphical models (which is a very hard tassk) may incur some bias, but I don’t understand it as well as I want.\n\nThese Ising models look very interesting. I should try implementing some of them!"
  },
  {
    "objectID": "posts/em-gibbs-quantification.html",
    "href": "posts/em-gibbs-quantification.html",
    "title": "Expectation-maximization and Gibbs sampling in quantification",
    "section": "",
    "text": "Consider an unlabeled image data set \\(x_1, \\dotsc, x_N\\). We know that each image in this data set corresponds to a unique class (e.g., a cat or a dog) \\(y\\in \\{1, \\dotsc, L\\}\\) and we would like to estimate how many images \\(x_i\\) belong to each class. This problem is known as quantification and there exist numerous approaches to this problem, employing an auxiliary data set. Albert Ziegler and I were interested in additionally quantifying uncertainty1 around such estimates (see Ziegler and Czyż 2023) by building a generative model on summary statistic and performing Bayesian inference.\nWe got a very good question from the reviewer: if we compare our method to point estimates produced by an expectation-maximization algorithm (Saerens, Latinne, and Decaestecker 2001) and we are interested in uncertainty quantification, why don’t we upgrade this method to a Gibbs sampler?\nI like this question, because it’s very natural to ask, yet I overlooked the possibility of doing it. As Richard McElreath explains here, Hamiltonian Markov chain Monte Carlo is usually the preferred way of sampling, but let’s see how exactly the expectation-maximization algorithm works in this case and how to adapt it to a Gibbs sampler."
  },
  {
    "objectID": "posts/em-gibbs-quantification.html#modelling-assumptions",
    "href": "posts/em-gibbs-quantification.html#modelling-assumptions",
    "title": "Expectation-maximization and Gibbs sampling in quantification",
    "section": "Modelling assumptions",
    "text": "Modelling assumptions\nThe model is very similar to the one used in clustering problems: for each object we have an observed random variable \\(X_i\\) (with its realization being the image \\(x_i\\)) and a latent random variable \\(Y_i\\), which is valued in the set of labels \\(\\{1, \\dotsc, L\\}\\).\nAdditionally, there’s a latent vector \\(\\pi = (\\pi_1, \\dotsc, \\pi_L)\\) with non-negative entries, such that \\(\\pi_1 + \\cdots + \\pi_L = 1\\). In other words, vector \\(\\pi\\) is the proportion vector of interest.\nWe can visualise the assumed dependencies in the following graphical model:\n\n\nCode\nimport daft\nimport matplotlib.pyplot as plt\nplt.style.use(\"dark_background\")\n\n# Instantiate a PGM object\npgm = daft.PGM(dpi=200)\n\ndef add_node(id: str, name: str, x: float, y: float, observed: bool = False):\n  if observed:\n    params={\"facecolor\": \"grey\"}\n  else:\n    params={\"edgecolor\": \"w\"}\n  pgm.add_node(id, name, x, y, plot_params=params)\n\ndef add_edge(start: str, end: str):\n  pgm.add_edge(start, end, plot_params={\"edgecolor\": \"w\", \"facecolor\": \"w\"})\n\ndef add_plate(coords, label: str, shift: float):\n  pgm.add_plate(coords, label=label, shift=shift, rect_params={\"edgecolor\": \"w\"})\n\nadd_node(\"pi\", \"$\\\\pi$\", 0, 1)\nadd_node(\"Y_i\", r\"$Y_i$\", 2, 1)\nadd_node(\"X_i\", r\"$X_i$\", 4, 1, observed=True)\n\n# Add edges\nadd_edge(\"pi\", \"Y_i\")\nadd_edge(\"Y_i\", \"X_i\")\n\n# Add a plate\nadd_plate([1.5, 0.5, 3, 1.5], label=r\"$i = 1, \\ldots, N$\", shift=-0.1)\n\n# Render and show the PGM\npgm.render()\nplt.show()\n\n\n\n\n\nAs \\(\\pi\\) is simplex-valued, it’s convenient to model it with a Dirichlet prior. Then, \\(Y_i\\mid \\pi \\sim \\mathrm{Categorical}(\\pi)\\). Finally, we assume that each class \\(y\\) has a corresponding distribution \\(D_y\\) from which the image is sampled. In other words, \\(X_i\\mid Y_i=y \\sim D_y\\).\nIn case we know all distributions \\(D_y\\), this is quite a simple problem: we can marginalise the latent variables \\(Y_i\\) obtaining \\[\nP(\\{X_i=x_i\\} \\mid \\pi) = \\prod_{i=1}^N \\big( \\pi_1 D_1(x_i) + \\cdots + \\pi_L D_L(x_i) \\big)\n\\] which in turn can be used to infer \\(\\pi\\) using Hamiltonian Markov chain Monte Carlo algorithms. In fact, a variant of this approach, employing maximum likelihood estimate, rather than Bayesian inference, was proposed by Peters and Coberly (1976) as early as in 1976!"
  },
  {
    "objectID": "posts/em-gibbs-quantification.html#why-expectation-maximization",
    "href": "posts/em-gibbs-quantification.html#why-expectation-maximization",
    "title": "Expectation-maximization and Gibbs sampling in quantification",
    "section": "Why expectation-maximization?",
    "text": "Why expectation-maximization?\nHowever, learning well-calibrated generative models \\(D_y\\) may be very hard task. Saerens, Latinne, and Decaestecker (2001) instead propose to learn a well-calibrated probabilistic classifier \\(P(Y \\mid X, \\pi^{(0)})\\) on an auxiliary population.\nThe assumption on the auxiliary population is the following: the conditional probability distributions \\(D_y = P(X\\mid Y=y)\\) have to be the same. The only thing that can differ is the proportion vector \\(\\pi_0\\), assumed to be known. This assumption is called prior probability shift or label shift and is rather strong, but also quite hard to avoid: if arbitrary distribution shifts are avoided, it’s not possible to generalize from one distribution to another! Finding suitable ways how to weaken the prior probability shift is therefore an interesting research problem on its own.\nNote that if we have a well-calibrated classifier \\(P(Y\\mid X, \\pi^{(0)})\\), we also have an access to a distribution \\(P(Y\\mid X, \\pi)\\). Namely, note that \\[\\begin{align*}\nP(Y=y\\mid X=x, \\pi) &\\propto P(Y=y, X=x \\mid \\pi) \\\\\n&= P(X=x \\mid Y=y, \\pi) P(Y=y\\mid \\pi) \\\\\n&= P(X=x \\mid Y=y)\\, \\pi_y,\n\\end{align*}\n\\] where the proportionality constant does not depend on \\(y\\). Analogously, \\[\nP(Y=y\\mid X=x, \\pi^{(0)}) \\propto P(X=x\\mid Y=y)\\, \\pi^{(0)}_y,\n\\] where the key observation is that for both distributions we assume that the conditional distribution \\(P(X=x\\mid Y=y)\\) is the same. Now we can take the ratio of both expressions and obtain \\[\nP(Y=y\\mid X=x, \\pi) \\propto P(Y=y\\mid X=x, \\pi^{(0)}) \\frac{ \\pi_y }{\\pi^{(0)}_y},\n\\] where the proportionality does not depend on \\(y\\). Hence, we can calculate unnormalized probabilities in this manner and then normalize them, so that they sum up to \\(1\\).\nTo summarize, we have the access to:\n\nWell-calibrated probability \\(P(Y=y\\mid X=x, \\pi)\\);\nThe prior probability \\(P(\\pi)\\);\nThe probability \\(P(Y_i=y \\mid \\pi) = \\pi_y\\);\n\nand we want to do inference on the posterior \\(P(\\pi \\mid \\{X_i\\})\\)."
  },
  {
    "objectID": "posts/em-gibbs-quantification.html#expectation-maximization",
    "href": "posts/em-gibbs-quantification.html#expectation-maximization",
    "title": "Expectation-maximization and Gibbs sampling in quantification",
    "section": "Expectation-maximization",
    "text": "Expectation-maximization\nExpectation-maximization is an iterative algorithm trying to find a stationary point of the log-posterior \\[\\begin{align*}\n\\log P(\\pi \\mid \\{X_i=x_i\\}) &= P(\\pi) + \\log P(\\{X_i = x_i\\} \\mid \\pi) \\\\\n&= P(\\pi) + \\sum_{i=1}^N \\log P(X_i=x_i\\mid \\pi).\n\\end{align*}\n\\]\nIn particular, by running the optimization procedure several times, we can hope to find the maximum a posteriori estimate (or the maximum likelihood estimate, when the uniform distribution over the simplex is used as \\(P(\\pi)\\)). Interestingly, this optimization procedure will not assume that we can compute \\(\\log P(X_i=x_i\\mid \\pi)\\), using instead quantities available to us.\nAssume that at the current iteration the proportion vector is \\(\\pi^{(t)}\\). Then, \\[\\begin{align*}\n\\log P(X_i = x_i\\mid \\pi) &= \\log \\sum_{y=1}^L P(X_i = x_i, Y_i = y\\mid \\pi) \\\\\n&= \\log \\sum_{y=1}^L P(Y_i=y \\mid \\pi^{(t)}, X_i = x_i ) \\frac{ P(X_i=x_i, Y_i=y \\mid \\pi) }{P(Y_i=y \\mid \\pi^{(t)}, X_i=x_i)} \\\\\n&\\ge \\sum_{y=1}^L P(Y_i=y\\mid \\pi^{(t)}, X_i=x_i) \\log \\frac{P(X_i=x_i, Y_i=y \\mid \\pi)}{P(Y_i=y \\mid \\pi^{(t)}, X_i=x_i)}\n\\end{align*}\n\\]\nwhere the inequality follows from Jensen’s inequality for concave functions2.\nWe can now bound the loglikelihood by \\[\\begin{align*}\n\\log P(\\{X_i = x_i \\}\\mid \\pi) &= \\sum_{i=1}^N \\log P(X_i=x_i\\mid \\pi) \\\\\n&\\ge \\sum_{i=1}^N \\sum_{y=1}^L P(Y_i=y\\mid \\pi^{(t)}, X_i=x_i) \\log \\frac{P(X_i=x_i, Y_i=y \\mid \\pi)}{P(Y_i=y \\mid \\pi^{(t)}, X_i=x_i)}.\n\\end{align*}\n\\]\nNow let \\[\nQ(\\pi, \\pi^{(t)}) = \\log P(\\pi) + \\sum_{i=1}^N \\sum_{y=1}^L P(Y_i=y\\mid \\pi^{(t)}, X_i=x_i) \\log \\frac{P(X_i=x_i, Y_i=y \\mid \\pi)}{P(Y_i=y \\mid \\pi^{(t)}, X_i=x_i)},\n\\] which is a lower bound on the log-posterior. We will define the value \\(\\pi^{(t+1)}\\) by optimizing this lower bound: \\[\n\\pi^{(t+1)} := \\mathrm{argmax}_\\pi Q(\\pi, \\pi^{(t)}).\n\\]\nLet’s define auxiliary quantities \\(\\xi_{iy} = P(Y_i=y \\mid \\pi^{(t)}, X_i=x_i)\\), which can be calculated using the probabilistic classifier, as outlined above. This is called the expectation step (although we are actually calculating just probabilities, rather than more general expectations). In the new notation we have \\[\nQ(\\pi, \\pi^{(t)}) = \\log P(\\pi) + \\sum_{i=1}^N\\sum_{y=1}^L \\left(\\xi_{iy} \\log P(X_i=x_i, Y_i=y\\mid \\pi) - \\xi_{iy} \\log \\xi_{iy}\\right)\n\\]\nThe term \\(\\xi_{iy}\\log \\xi_{iy}\\) does not depend on \\(\\pi\\), so we don’t have to include it in the optimization. Writing \\(\\log P(X_i = x_i, Y_i=y\\mid \\pi) = \\log D_y(x_i) + \\log \\pi_y\\) we see that it suffices to optimize for \\(\\pi\\) the expression \\[\n\\log P(\\pi) + \\sum_{i=1}^N\\sum_{y=1}^L \\xi_{iy}\\left( \\log \\pi_y + \\log D_y(x_i) \\right).\n\\] Even better: not only \\(\\xi_{iy}\\) does not depend on \\(\\pi\\), but also \\(\\log D_y(x_i)\\)! Hence, we can drop from the optimization the terms requiring the generative models and we are left only with the easy to calculate quantities: \\[\n\\log P(\\pi) + \\sum_{i=1}^N\\sum_{y=1}^L \\xi_{iy} \\log \\pi_y.\n\\]\nLet’s use the prior \\(P(\\pi) = \\mathrm{Dirichlet}(\\pi \\mid \\alpha_1, \\dotsc, \\alpha_L)\\), so that \\(\\log P(\\pi) = \\mathrm{const.} + \\sum_{y=1}^L (\\alpha_y-1)\\log \\pi_y\\). Hence, we are interested in optimising \\[\n\\sum_{y=1}^L \\left((\\alpha_y-1) + \\sum_{i=1}^N \\xi_{iy} \\right)\\log \\pi_y.\n\\]\nWrite \\(A_y = \\alpha_y - 1 + \\sum_{i=1}^N\\xi_{iy}\\). We have to optimize the expression \\[\n\\sum_{y=1}^L A_y\\log \\pi_y\n\\] under a constraint \\(\\pi_1 + \\cdots + \\pi_L = 1\\).\nSaerens, Latinne, and Decaestecker (2001) use Lagrange multipliers, but we will use the first \\(L-1\\) coordinates to parameterise the simplex and write \\(\\pi_L = 1 - (\\pi_1 + \\cdots + \\pi_{L-1})\\). In this case, if we differentiate with respect to \\(\\pi_l\\), we obtain \\[\n\\frac{A_l}{\\pi_l} + \\frac{A_L}{\\pi_L} \\cdot (-1) = 0,\n\\]\nwhich in turn gives that \\(\\pi_y = k A_y\\) for some constant \\(k &gt; 0\\). We have \\[\n\\sum_{y=1}^L A_y = \\sum_{y=1}^L \\alpha_y - L + \\sum_{i=1}^N\\sum_{y=1}^L \\xi_{iy} = \\sum_{y=1}^L \\alpha_y - L + N.\n\\] Hence, \\[\n\\pi_y = \\frac{1}{(\\alpha_1 + \\cdots + \\alpha_L) + N - L}\\left( \\alpha_y-1 + \\sum_{i=1}^N \\xi_{iy} \\right),\n\\] which is taken as the next \\(\\pi^{(t+1)}\\).\nAs a minor observation, note that for a uniform prior over the simplex (i.e., all \\(\\alpha_y = 1\\)) we have \\[\n\\pi^{(t+1)}_y = \\frac 1N\\sum_{i=1}^N P(Y_i=y_i \\mid X_i=x_i, \\pi^{(t)} ).\n\\] Once we have converged to a fixed point and we have \\(\\pi^{(t)} = \\pi^{(t+1)}\\), it very much looks like \\[\nP(Y) = \\frac 1N\\sum_{i=1}^N P(Y_i \\mid X_i, \\pi) \\approx \\mathbb E_{X \\sim \\pi_1 D_1 + \\dotsc + \\pi_L D_L}[ P(Y\\mid X) ]\n\\] when \\(N\\) is large."
  },
  {
    "objectID": "posts/em-gibbs-quantification.html#gibbs-sampler",
    "href": "posts/em-gibbs-quantification.html#gibbs-sampler",
    "title": "Expectation-maximization and Gibbs sampling in quantification",
    "section": "Gibbs sampler",
    "text": "Gibbs sampler\nFinally, let’s think how to implement a Gibbs sampler for this problem. Compared to the expectation-maximization this will be easy.\nTo solve the quantification problem we have to sample from the posterior distribution \\(P(\\pi \\mid \\{X_i\\})\\). Instead, let’s sample from a high-dimensional distribution \\(P(\\pi, \\{Y_i\\} \\mid \\{X_i\\})\\) — once we have samples of the form \\((\\pi, \\{Y_i\\})\\) we can simply forget about the \\(Y_i\\) values.\nThis is computationally a harder problem (we have many more variables to sample), however each sampling step will be very convenient. We will alternatively sample from \\[\n\\pi \\sim P(\\pi \\mid \\{X_i, Y_i\\})\n\\] and \\[\n\\{Y_i\\} \\sim P(\\{Y_i\\} \\mid \\{X_i\\}, \\pi).\n\\]\nThe first step is easy: \\(P(\\pi \\mid \\{X_i, Y_i\\}) = P(\\pi\\mid \\{Y_i\\})\\) which (assuming a Dirichlet prior) is a Dirichlet distribution. Namely, if \\(P(\\pi) = \\mathrm{Dirichlet}(\\alpha_1, \\dotsc, \\alpha_L)\\), then \\[\nP(\\pi\\mid \\{Y_i=y_i\\}) = \\mathrm{Dirichlet}\\left( \\alpha_1 + \\sum_{i=1}^N \\mathbf{1}[y_i = 1], \\dotsc, \\alpha_L + \\sum_{i=1}^N \\mathbf{1}[y_i=L] \\right).\n\\]\nLet’s think how to sample \\(\\{Y_i\\} \\sim P(\\{Y_i\\} \\mid \\{X_i\\}, \\pi)\\). This is a high-dimensional distribution, so let’s… use Gibbs sampling. Namely, we can iteratively sample \\[\nY_k \\sim P(Y_k \\mid \\{Y_1, \\dotsc, Y_{k-1}, Y_{k+1}, \\dotsc, Y_L\\}, \\{X_i\\}, \\pi).\n\\]\nThanks to the particular structure of this model, this is equivalent to sampling from \\[\nY_k \\sim P(Y_k \\mid X_k, \\pi) = \\mathrm{Categorical}(\\xi_{k1}, \\dotsc, \\xi_{kL}),\n\\] where \\(\\xi_{ky} = P(Y_k = y\\mid X_k = x_k, \\pi)\\) is obtained by recalibrating the given classifier."
  },
  {
    "objectID": "posts/em-gibbs-quantification.html#summary",
    "href": "posts/em-gibbs-quantification.html#summary",
    "title": "Expectation-maximization and Gibbs sampling in quantification",
    "section": "Summary",
    "text": "Summary\nTo sum up, the reviewer was right: it’s very simple to upgrade the inference scheme in this model from a point estimate to a sample from the posterior!\nI however haven’t run simulations to know how well this sampler works in practice: I expect that this approach could suffer from:\n\nProblems from not-so-well-calibrated probabilistic classifier.\nEach iteration of the algorithm (whether expectation-maximization or a Gibbs sampler) requires passing through all \\(N\\) examples.\nAs there are \\(N\\) latent variables sampled, the convergence may perhaps be slow.\n\nIt’d be interesting to see how problematic these points are in practice (perhaps not at all!)"
  },
  {
    "objectID": "posts/em-gibbs-quantification.html#appendix-numerical-implementation-in-jax",
    "href": "posts/em-gibbs-quantification.html#appendix-numerical-implementation-in-jax",
    "title": "Expectation-maximization and Gibbs sampling in quantification",
    "section": "Appendix: numerical implementation in JAX",
    "text": "Appendix: numerical implementation in JAX\nAs these algorithms are so simple, let’s quickly implement them in JAX. We will consider two Gaussian densities \\(D_1 = \\mathcal N(0, 1^2)\\) and \\(D_2 = \\mathcal N(\\mu, 1^2)\\). Let’s generate some data:\n\n\nCode\nimport jax\nimport jax.numpy as jnp\nfrom jax import random\nfrom jaxtyping import Array, Float, Int\nfrom jax.scipy.special import logsumexp\n\nn_cases: Int[Array, \" classes\"] = jnp.asarray([10, 40], dtype=int)\nmus: Float[Array, \" classes\"] = jnp.asarray([0.0, 1.0])\n\nkey = random.PRNGKey(42)\nkey, *subkeys = random.split(key, len(n_cases) + 1)\n\nxs: Float[Array, \" points\"] = jnp.concatenate(tuple(\n  mu + random.normal(subkey, shape=(n,))\n  for subkey, n, mu in zip(subkeys, n_cases, mus)\n))\n\nn_classes: int = len(n_cases)\nn_points: int = len(xs)\n\n\nAn NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n\n\nNow we need a probabilistic classifier. We will assume that it was calibrated on population with proportion \\(\\pi^{(0)} = (0.4, 0.6)\\).\n\n\nCode\n_normalizer: float = 0.5 * jnp.log(2 * jnp.pi)\n\ndef log_p(x, mu: float) -&gt; float:\n  \"\"\"Log-density N(x | mu, 1^2).\"\"\"\n  return -0.5 * jnp.square(x - mu) - _normalizer\n\n\n# Auxiliary matrix log P(X | Y)\n_log_p_x_y: Float[Array, \"points classes\"] = jnp.stack(tuple(log_p(xs, mu) for mu in mus)).T\nassert _log_p_x_y.shape == (n_points, n_classes), f\"Shape mismatch: {_log_p_x_y.shape}.\"\n\nlog_pi0: Float[Array, \" classes\"] = jnp.log(jnp.asarray([0.4, 0.6]))\n\n# Matrix representing log P(Y | X) for labeled population\nlog_p_y_x: Float[Array, \"points classes\"] = _log_p_x_y + log_pi0[None, :]\n# ... currently it's unnormalized, so we have to normalize it\n\ndef normalize_logprobs(log_ps: Float[Array, \"points classes\"]) -&gt; Float[Array, \"points classes\"]:\n  log_const = logsumexp(log_ps, keepdims=True, axis=-1)\n  return log_ps - log_const\n\nlog_p_y_x = normalize_logprobs(log_p_y_x)\n\n# Let's quickly check if it works\nsums = jnp.sum(jnp.exp(log_p_y_x), axis=1)\nassert sums.shape == (n_points,)\nassert jnp.min(sums) &gt; 0.999, f\"Minimum: {jnp.min(sums)}.\"\nassert jnp.max(sums) &lt; 1.001, f\"Maximum: {jnp.max(sums)}.\"\n\n\n\nExpectation-maximization algorithm\nIt’s time to implement expectation-maximization.\n\n\nCode\ndef expectation_maximization(\n  log_p_y_x: Float[Array, \"points classes\"],\n  log_pi0: Float[Array, \" classes\"],\n  log_start: None | Float[Array, \" classes\"] = None,\n  alpha: Float[Array, \" classes\"] | None = None,\n  n_iterations: int = 10_000,\n) -&gt; Float[Array, \" classes\"]:\n  \"\"\"Runs the expectation-maximization algorithm.\n\n  Args:\n    log_p_y_x: array log P(Y | X) for the calibrated population\n    log_pi0: array log P(Y) for the calibrated population\n    log_start: starting point. If not provided, `log_pi0` will be used\n    alpha: concentration parameters for the Dirichlet prior.\n      If not provided, the uniform prior will be used\n    n_iterations: number of iterations to run the algorithm for\n  \"\"\"\n  if log_start is None:\n    log_start = log_pi0\n  if alpha is None:\n    alpha = jnp.ones_like(log_pi0)\n\n  def iteration(_, log_pi: Float[Array, \" classes\"]) -&gt; Float[Array, \" classes\"]:\n    # Calculate log xi[n, y]\n    log_ps = normalize_logprobs(log_p_y_x + log_pi[None, :] - log_pi0[None, :])\n    # Sum xi[n, y] over n. We use the logsumexp, as we have log xi[n, y]\n    summed = jnp.exp(logsumexp(log_ps, axis=0, keepdims=False))\n    # The term inside the bracket (numerator)\n    numerator = summed + alpha - 1.0\n    # Denominator\n    denominator = jnp.sum(alpha) + log_p_y_x.shape[0] - log_p_y_x.shape[1]\n    return jnp.log(numerator / denominator)\n\n  return jax.lax.fori_loop(\n    0, n_iterations, iteration, log_start\n  )\n\nlog_estimated = expectation_maximization(\n  log_p_y_x=log_p_y_x,\n  log_pi0=log_pi0,\n  n_iterations=1000,\n  # Let's use slight shrinkage towards more uniform solutions\n  alpha=2.0 * jnp.ones_like(log_pi0),\n)\nestimated = jnp.exp(log_estimated)\nprint(f\"Estimated: {estimated}\")\nprint(f\"Actual:    {n_cases / n_cases.sum()}\")\n\n\nEstimated: [0.16425547 0.83574456]\nActual:    [0.2 0.8]\n\n\n\n\nGibbs sampler\nExpectation-maximization returns only a point estimate. We’ll explore the region around the posterior mode with a Gibbs sampler.\n\n\nCode\ndef gibbs_sampler(\n  key: random.PRNGKeyArray,\n  log_p_y_x: Float[Array, \"points classes\"],\n  log_pi0: Float[Array, \" classes\"],\n  log_start: None | Float[Array, \" classes\"] = None,\n  alpha: Float[Array, \" classes\"] | None = None,\n  n_warmup: int = 1_000,\n  n_samples: int = 1_000,\n) -&gt; Float[Array, \"n_samples classes\"]:\n  if log_start is None:\n    log_start = log_pi0\n  if alpha is None:\n    alpha = jnp.ones_like(log_pi0)\n\n  def iteration(\n    log_ps: Float[Array, \" classes\"],\n    key: random.PRNGKeyArray,\n  ) -&gt; tuple[Float[Array, \" classes\"], Float[Array, \" classes\"]]:\n    key, subkey1, subkey2 = random.split(key, 3)\n\n    ys = random.categorical(\n      subkey1,\n      log_ps[None, :] + log_p_y_x - log_pi0[None, :],\n      axis=-1,\n    )\n    counts = jnp.bincount(ys, length=log_pi0.shape[0])\n\n    new_log_pi = jnp.log(\n      random.dirichlet(subkey2, alpha + counts)\n    )\n\n    return new_log_pi, new_log_pi\n\n  _, samples = jax.lax.scan(\n    iteration,\n    log_start,\n    random.split(key, n_warmup + n_samples),\n  )\n  return samples[n_warmup:, :]\n\nkey, subkey = random.split(key)\nsamples = gibbs_sampler(\n  key=subkey,\n  log_p_y_x=log_p_y_x,\n  log_pi0=log_pi0,\n  # Let's use slight shrinkage towards more uniform solutions\n  alpha=2.0 * jnp.ones_like(log_pi0),\n  # Use EM point as a starting point\n  log_start=log_estimated,\n  n_samples=5_000,\n)\nsamples = jnp.exp(samples)\n\nprint(f\"Mean:   {jnp.mean(samples, axis=0)}\")\nprint(f\"Std:    {jnp.std(samples, axis=0)}\")\nprint(f\"Actual: {n_cases / n_cases.sum()}\")\n\n\n/tmp/ipykernel_43705/3942328187.py:2: DeprecationWarning: jax.random.PRNGKeyArray is deprecated. Use jax.Array for annotations, and jax.dtypes.issubdtype(arr.dtype, jax.dtypes.prng_key) for runtime detection of typed prng keys.\n  key: random.PRNGKeyArray,\n/tmp/ipykernel_43705/3942328187.py:17: DeprecationWarning: jax.random.PRNGKeyArray is deprecated. Use jax.Array for annotations, and jax.dtypes.issubdtype(arr.dtype, jax.dtypes.prng_key) for runtime detection of typed prng keys.\n  key: random.PRNGKeyArray,\n\n\nMean:   [0.20171012 0.79828984]\nStd:    [0.09912279 0.09912279]\nActual: [0.2 0.8]\n\n\nLet’s visualise the posterior samples, together with the expectation-maximization solution and the ground truth:\n\n\nCode\nimport matplotlib.pyplot as plt\nplt.style.use(\"dark_background\")\n\nfig, ax = plt.subplots(dpi=150)\n\nbins = jnp.linspace(0, 1, 40)\n\nfor y in range(n_classes):\n  color = f\"C{y+1}\"\n  ax.hist(samples[:, y], bins=bins, density=True, histtype=\"step\", color=color)\n  ax.axvline(n_cases[y] / n_cases.sum(), color=color, linewidth=3)\n  ax.axvline(estimated[y], color=color, linestyle=\"--\")\n\nax.set_title(\"Posterior distribution\")\nax.set_ylabel(\"Posterior density\")\nax.set_xlabel(\"Component value\")\nax.spines[[\"top\", \"right\"]].set_visible(False)\nfig.tight_layout()"
  },
  {
    "objectID": "posts/em-gibbs-quantification.html#footnotes",
    "href": "posts/em-gibbs-quantification.html#footnotes",
    "title": "Expectation-maximization and Gibbs sampling in quantification",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nLet’s call this problem “quantification of uncertainty in quantification problems”.↩︎\nIt’s good to remember: \\(\\log \\mathbb E[A] \\ge \\mathbb E[\\log A]\\).↩︎"
  },
  {
    "objectID": "posts/regression-to-the-mean-bias-rejoinder.html",
    "href": "posts/regression-to-the-mean-bias-rejoinder.html",
    "title": "Regression to the mean and biased predictions",
    "section": "",
    "text": "In September 2008 Bayesian Analysis, vol. 3, issue 3 featured a wonderful discussion between five statistics superstars: Andrew Gelman, José M. Bernardo, Joseph B. Kadane, Stephen Senn and Larry Wasserman.\nThe discussion is a great read on foundations of Bayesian statistics (and it’s open access!), but we will not summarise it today. Instead, let’s focus on an example from Andrew Gelman’s Rejoinder on regression to the mean and unbiased predictions."
  },
  {
    "objectID": "posts/regression-to-the-mean-bias-rejoinder.html#the-example",
    "href": "posts/regression-to-the-mean-bias-rejoinder.html#the-example",
    "title": "Regression to the mean and biased predictions",
    "section": "The example",
    "text": "The example\nAndrew Gelman considers a problem in which one tries to estimate the height of adult daughter, \\(Y\\), from the height of her mother, \\(X\\). Consider an artificial scenario, where we have millions of data points, which we can use to estimate the joint probability distribution \\((X, Y)\\) and it turns out to be bivariate normal1 of the following form: \\[\n\\begin{pmatrix} X\\\\Y \\end{pmatrix} \\sim \\mathcal N\\left(\\begin{pmatrix}\\mu\\\\\\mu\\end{pmatrix}, \\sigma^2\\begin{pmatrix}  1 & 0.5 \\\\ 0.5 & 1\\end{pmatrix}  \\right)\n\\] with known \\(\\mu\\) and \\(\\sigma\\), say, \\(\\mu=160\\) and \\(\\sigma=10\\) in centimeters.\nThe marginal distributions on both \\(X\\) and \\(Y\\) are the same: \\(\\mathcal N(\\mu, \\sigma^2)\\).\nLet’s plot all of them:\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nplt.style.use(\"dark_background\")\n\ndef generate_data(\n    rng,\n    mu: float,\n    sigma: float,\n    n_points: int,\n) -&gt; np.ndarray:\n    var = np.square(sigma)\n    return rng.multivariate_normal(\n        mean=(mu, mu),\n        cov=var * np.asarray([\n            [1.0, 0.5],\n            [0.5, 1.0],\n        ]),\n        size=n_points,\n    )\n\n\nrng = np.random.default_rng(42)\n\nmu = 160\nsigma = 10\n\ndata = generate_data(rng, mu=mu, sigma=sigma, n_points=5_000)\n\nfig, axs = plt.subplots(1, 2, figsize=(4.5, 2.2), dpi=170, sharex=True)\n\nax = axs[0]\nax.scatter(data[:, 0], data[:, 1], c=\"C0\", s=1, alpha=0.1, rasterized=True)\nax.set_title(\"Joint\")\nax.set_xlabel(\"$X$\")\nax.set_ylabel(\"$Y$\")\n\nlim = (mu - 4*sigma, mu + 4*sigma)\n\nax.set_xlim(*lim)\nax.set_ylim(*lim)\n\nax = axs[1]\nax.hist(data[:, 0], color=\"C1\", bins=np.linspace(*lim, 21), rasterized=True, density=True)\nax.set_title(\"Marginal\")\nax.set_xlim(*lim)\nax.set_xlabel(\"$X$\")\nax.set_ylabel(\"PDF\")\n\nfor ax in axs:\n  ax.spines[['top', 'right']].set_visible(False)\n\nfig.tight_layout()\n\n\n\n\n\nThe conditional distributions are also normal and take very similar form: \\[\nY\\mid X=x \\sim \\mathcal N\\big( \\mu + 0.5(x-\\mu), 0.75\\sigma^2\\big),\n\\] and \\[\nX\\mid Y=y \\sim \\mathcal N\\big( \\mu + 0.5(y-\\mu), 0.75\\sigma^2\\big).\n\\]\nLet’s plot the conditional distribution in the fist panel. We will plot conditional distributions using mean \\(\\mathbb E[Y\\mid X=x]\\) and the intervals representing one, two, and three standard deviations from it.\nThen, in the second panel we will overlay it with points and the line \\(y=x\\) (dashed red line).\n\n\nCode\nfig, axs = plt.subplots(1, 2, figsize=(4.5, 2.2), dpi=170, sharex=True, sharey=True)\n\nx_ax = np.linspace(*lim, 51)\nmean_pred = mu + 0.5 * (x_ax-mu)\n\ndef plot_conditional(ax):\n    ax.set_xlim(*lim)\n    ax.set_ylim(*lim)\n\n    for n_sigma in [1, 2, 3]:\n        band = n_sigma * np.sqrt(0.75) * sigma\n        ax.fill_between(\n            x_ax,\n            mean_pred - band,\n            mean_pred + band,\n            alpha=0.1,\n            color=\"yellow\",\n            edgecolor=None,\n        )\n\n    ax.plot(\n        x_ax,\n        mean_pred,\n        c=\"white\",\n    )\n\nax = axs[0]\nplot_conditional(ax)\nax.set_xlabel(\"$X$\")\nax.set_ylabel(r\"$Y\\mid X$\")\n\nax = axs[1]\nplot_conditional(ax)\nax.set_xlabel(\"$X$\")\nax.set_ylabel(\"$Y$\")\nax.scatter(data[:, 0], data[:, 1], c=\"C0\", s=1, alpha=0.2, rasterized=True)\nax.plot(x_ax, x_ax, c=\"maroon\", linestyle=\"--\")\n\nfor ax in axs:\n  ax.spines[['top', 'right']].set_visible(False)\n\nfig.tight_layout()\n\n\n\n\n\nThe \\(y=x\\) has greater slope than \\(\\mathbb E[Y\\mid X=x]\\) (namely, 1 is greater than 0.5), which is the usual regression to the mean."
  },
  {
    "objectID": "posts/regression-to-the-mean-bias-rejoinder.html#biased-and-unbiased-estimators",
    "href": "posts/regression-to-the-mean-bias-rejoinder.html#biased-and-unbiased-estimators",
    "title": "Regression to the mean and biased predictions",
    "section": "Biased and unbiased estimators",
    "text": "Biased and unbiased estimators\nWe have seen that we could do probabilistic prediction, returning the whole conditional distribution \\(P(Y\\mid X=x)\\). Imagine however that a single point is required as the answer. We can take \\(\\mathbb E[Y\\mid X=x]\\) as one estimator (solid line in the previous plot). More generally, for every number \\(f\\in \\mathbb R\\) define an estimator \\[\n\\hat Y_f = \\mu + f\\cdot (X-\\mu).\n\\]\nWe have the following:\n\nFor \\(f=0\\), we have \\(\\hat Y_0 = \\mu\\) is constantly predicting the mean.\nFor \\(f=0.5\\), \\(\\hat Y_{0.5} = 0.5(X + \\mu)\\) is the regression to the mean we have seen above.\nFor \\(f=1\\), \\(\\hat Y_1 = X\\) returns the input, which we also have seen above.\n\nLet’s take a look at the bias and variance2 of these estimators.\nWe have \\[\n(X\\mid Y=y) \\sim \\mathcal N\\big(\\mu + 0.5(y-\\mu), 0.75\\sigma^2\\big)\n\\] and \\(\\hat Y_f = f\\cdot X + \\mu(1-f)\\), meaning that \\[\n\\mathbb{V}[ \\hat Y_f \\mid Y=y ] = f^2 \\cdot \\mathbb{V}[X \\mid Y=y] = 0.75 f^2\\sigma^2\n\\] and \\[\n\\begin{align*}\\mathbb E[ \\hat Y_f \\mid Y=y ] &= \\mu(1-f) + f\\cdot \\mathbb E[X \\mid Y=y ]\\\\ &= \\mu(1-f) + f\\cdot ( \\mu + 0.5(y-\\mu) ) \\\\ &=\\mu-\\mu f + f\\mu + 0.5 f\\cdot (y-\\mu) \\\\\n&= \\mu + 0.5f\\cdot (y-\\mu) \\\\ &= 0.5 f\\cdot y +\\mu(1-0.5f)\\end{align*}\n\\]\nHence, for \\(f=0.5\\) we have \\[\n\\mathbb E[\\hat Y_{0.5}\\mid Y=y] = 0.25y + 0.75\\mu,\n\\] which is biased towards the mean.\nTo have an unbiased estimate, consider \\(f=2\\): \\[\n\\mathbb E[\\hat Y_2\\mid Y=y] = y,\n\\] which however has the form \\((\\hat Y_2 \\mid X=x) = \\mu + 2(x-\\mu)\\), which amplifies the measured distance from the mean!\n\nVisualisations\nLet’s spend a minute designing the plots and then visualise the estimators.\nThe raw data are visualised by plotting \\(Y\\) and \\(X\\). We can add the lines \\(\\hat Y_f\\mid X=x\\) to them, to add some information on how \\(\\hat Y_f\\) (which is a deterministic function of \\(X\\)) varies, and what the predictions are.\n\n\nCode\ndef yhat(x: np.ndarray, f: float) -&gt; np.ndarray:\n    return mu + f * (x - mu)\n\nfig, ax = plt.subplots(figsize=(3, 3), dpi=150)\n\n\nax.scatter(data[:, 0], data[:, 1], c=\"C0\", s=1, alpha=0.2, rasterized=True)\n\nfs = [0, 0.5, 1, 2]\ncolors = [\"orange\", \"white\", \"maroon\", \"purple\"]\n\nfor f, col in zip(fs, colors):\n    ax.plot(\n        x_ax,\n        yhat(x_ax, f),\n        color=col,\n        label=f\"$f=${f:.1f}\",\n    )\n\nax.set_xlabel(\"$X$\")\nax.set_ylabel(\"$Y$\")\nax.set_xlim(*lim)\nax.set_ylim(*lim)\nax.spines[['top', 'right']].set_visible(False)\nax.legend(frameon=False)\nfig.tight_layout()\n\n\n\n\n\nThen, we can also look at the plot of \\(\\hat Y_f\\) and \\(Y\\). This will be a good illustration showing what bias and variance of these estimators actually mean. We will add a dashed “diagonal” line, \\(\\hat y_f = y\\), to each of these plots.\n\n\nCode\nfig, axs = plt.subplots(1, len(fs), figsize=(2 * len(fs), 2), dpi=150, sharex=True, sharey=True)\n\nfor f, ax in zip(fs, axs):\n    x, y = data[:, 0], data[:, 1]\n    y_ = yhat(x, f)\n    ax.scatter(y, y_, c=\"C2\", s=1, alpha=0.2, rasterized=True)\n    ax.plot(\n        x_ax, x_ax\n    )\n    ax.set_xlim(*lim)\n    ax.set_ylim(np.min(y_), np.max(y_))\n    ax.set_title(f\"$f=${f:.1f}\")\n    ax.set_xlabel(\"$Y$\")\n    ax.set_ylabel(f\"$\\hat Y_f$\")\n\nfor ax in axs:\n    ax.spines[['top', 'right']].set_visible(False)\nfig.tight_layout()\n\n\n/tmp/ipykernel_43846/2941644956.py:11: UserWarning: Attempting to set identical low and high ylims makes transformation singular; automatically expanding.\n  ax.set_ylim(np.min(y_), np.max(y_))\n\n\n\n\n\nThe bias can be seen in the following manner: for each value \\(Y\\), the values of \\(\\hat Y\\) corresponding to that \\(Y\\) should be distributed in such a way that the mean lies on the line. This actually will be easier to see once we plot the difference \\(\\hat Y_f - \\hat Y\\) and \\(Y\\).\n\n\nCode\nfig, axs = plt.subplots(1, len(fs), figsize=(2 * len(fs), 2), dpi=150, sharex=True, sharey=True)\n\nfor f, ax in zip(fs, axs):\n    x, y = data[:, 0], data[:, 1]\n    y_ = yhat(x, f)\n    ax.scatter(y, y_ - y, c=\"C2\", s=1, alpha=0.2, rasterized=True)\n    ax.plot(\n        x_ax, np.zeros_like(x_ax)\n    )\n    ax.set_xlim(*lim)\n    ax.set_title(f\"$f=${f:.1f}\")\n    ax.set_xlabel(\"$Y$\")\n    ax.set_ylabel(f\"$\\hat Y_f - Y$\")\n\nfor ax in axs:\n    ax.spines[['top', 'right']].set_visible(False)\nfig.tight_layout()\n\n\n\n\n\nWe also see what how variance manifests: for \\(Y\\approx 160\\), we have quite a range of corresponding \\(\\hat Y_2\\). The estimator \\(\\hat Y_{0.5}\\) has clear bias for \\(Y\\) far from the mean, but for most of the data points (which lie close the mean in this case) the prediction is reasonable.\nIn fact, let’s plot \\(\\mathbb E[|\\hat Y_f -y | \\mid y]\\) and \\(\\sqrt{\\mathbb E[(\\hat Y_f-y)^2\\mid y]}\\) as a function of \\(y\\):\n\n\nCode\nn_y_hat_samples = 10_000\ny_ax = np.linspace(mu - 4 * sigma, mu + 4 * sigma, 51)\n\n# Shape (n_y, n_y_hat_samples)\nx_samples = mu + 0.5 * (y_ax[:, None] - mu) + rng.normal(loc=0, scale=np.sqrt(0.75) * sigma, size=(y_ax.shape[0], n_y_hat_samples))\n\nfig, axs = plt.subplots(1, 2, figsize=(2.5*2+1, 2.5), dpi=150, sharex=True, sharey=True)\n\nax = axs[0]\nfor f in fs:\n    preds = yhat(x_samples, f)\n    loss = np.mean(np.abs(preds - y_ax[:, None]), axis=1)\n    ax.plot(y_ax, loss, label=f\"$f=${f:.1f}\")\n\n# ax.legend(frameon=False)\nax.set_xlabel(\"$y$\")\nax.set_ylabel(r\"$E[|\\hat Y_f-y| \\mid y ]$\")\n\nax = axs[1]\nfor f in fs:\n    preds = yhat(x_ax, f)\n    loss = np.sqrt(np.mean(np.square(preds - y_ax[:, None]), axis=1))\n    ax.plot(y_ax, loss, label=f\"$f=${f:.1f}\")\n\nax.legend(frameon=False, bbox_to_anchor=(1.05, 1.0))\nax.set_xlabel(\"$y$\")\nax.set_ylabel(r\"$\\sqrt{E[(\\hat Y_f-y)^2 \\mid y ]}$\")\n\nfor ax in axs:\n    ax.spines[['top', 'right']].set_visible(False)\nfig.tight_layout()\n\n\n\n\n\n\n\nTwisting the problem\nThe above plots condition on unobserved parameter \\(y\\). Let’s think what happens when we observe the value \\(X\\) and we want to know how far our estimate \\(\\hat Y_f\\) is from the unobserved value \\(Y\\). We can do a variant of one plots we have seen previously, where we put \\(X\\) on the horizontal axis and \\(\\hat Y_f-Y\\) on the vertical one:\n\n\nCode\nfig, axs = plt.subplots(1, len(fs), figsize=(2 * len(fs), 2), dpi=150, sharex=True, sharey=True)\n\nfor f, ax in zip(fs, axs):\n    x, y = data[:, 0], data[:, 1]\n    y_ = yhat(x, f)\n    ax.scatter(x, y_ - y, c=\"C2\", s=1, alpha=0.2, rasterized=True)\n    ax.plot(\n        x_ax, np.zeros_like(x_ax)\n    )\n    ax.set_xlim(*lim)\n    ax.set_title(f\"$f=${f:.1f}\")\n    ax.set_xlabel(\"$X$\")\n    ax.set_ylabel(f\"$\\hat Y_f - Y$\")\n\nfor ax in axs:\n    ax.spines[['top', 'right']].set_visible(False)\nfig.tight_layout()\n\n\n\n\n\nWe see that for all values of \\(X\\), the regression to the mean estimator, \\(\\hat Y_{0.5}\\), works well. Let’s also take a look at the following losses, measuring how wrong our predictions will be on average for a given value of \\(X\\): \\[\\begin{align*}\n\\ell_1(x) &= \\mathbb E\\left[\\left| Y_f - Y \\right| \\mid X=x \\right], \\\\\n\\ell_2(x) &= \\sqrt{\\mathbb E\\left[\\left( \\hat Y_f - Y \\right)^2 \\mid X=x \\right]}.\n\\end{align*}\n\\]\n\n\nCode\nn_y_samples = 10_000\n\n# Shape (n_x, n_y_samples)\ny_samples = mu + 0.5 * (x_ax[:, None] - mu) + rng.normal(loc=0, scale=np.sqrt(0.75) * sigma, size=(x_ax.shape[0], n_y_samples))\n\nfig, axs = plt.subplots(1, 2, figsize=(2.5*2 + 1, 2.5), dpi=150, sharex=True, sharey=True)\n\nax = axs[0]\nfor f in fs:\n    preds = yhat(x_ax, f)\n    loss = np.mean(np.abs(y_samples - preds[:, None]), axis=1)\n    ax.plot(x_ax, loss, label=f\"$f=${f:.1f}\")\n\nax.set_xlabel(\"$x$\")\nax.set_ylabel(r\"$\\ell_1$\")\n\nax = axs[1]\nfor f in fs:\n    preds = yhat(x_ax, f)\n    loss = np.sqrt(np.mean(np.square(y_samples - preds[:, None]), axis=1))\n    ax.plot(x_ax, loss, label=f\"$f=${f:.1f}\")\n\nax.set_xlabel(\"$x$\")\nax.set_ylabel(r\"$\\ell_2$\")\n\nax.legend(frameon=False, bbox_to_anchor=(1.05, 1.0))\n\nfor ax in axs:\n    ax.spines[['top', 'right']].set_visible(False)\nfig.tight_layout()\n\n\n\n\n\nEven for values of \\(X\\) quite far from the mean, some bias helps! Overall, we see that regression to the mean is a sensible strategy in this case.\nAlso, using \\(f=1\\) (i.e., \\(\\hat Y_f=X\\)) performs as well as \\(f=0\\) (constant prediction \\(\\mu\\)). More precisely, observe that \\[\n\\left(Y - \\hat Y_0 \\mid X=x\\right) = (Y-\\mu \\mid X=x) \\sim \\mathcal N\\left( 0.5(x-\\mu), 0.75 \\sigma^2\\right)\n\\] and \\[\n\\left(Y - \\hat Y_1 \\mid X=x\\right) = (Y-X \\mid X=x) \\sim \\mathcal N\\left( 0.5(\\mu - x) , 0.75\\sigma^2 \\right)\n\\]\nand the absolute value (or squaring) makes the losses exactly equal. This is also visible in the plot representing \\(X\\) and \\(Y_f - Y\\).\nAs a final note, one can notice that for every \\(x\\), both \\(\\ell_1\\) and \\(\\ell_2\\) are minimised for \\(f=0.5\\) using the following argument: \\(f=0.5\\) predicts the mean of the conditional distribution \\(Y\\mid X=x\\). The sum of squares (in this case we have actually expectation, but let’s not worry about averaging) is minimised for the mean. Similarly, median optimises the sum of absolute deviations, and each of the normal distributions representing \\(Y-\\hat Y_f \\mid X=x\\) has mean equal to median."
  },
  {
    "objectID": "posts/regression-to-the-mean-bias-rejoinder.html#digression-simple-linear-regression-and-pca",
    "href": "posts/regression-to-the-mean-bias-rejoinder.html#digression-simple-linear-regression-and-pca",
    "title": "Regression to the mean and biased predictions",
    "section": "Digression: simple linear regression and PCA",
    "text": "Digression: simple linear regression and PCA\nThis section perhaps may be distracting and shouldn’t really be a part of this post, but I couldn’t resist: I still very much like this digression and I’m grateful for the opportunity to figure it out together with David.\n\nSimple linear regression\nWhat would happen if we fitted simple linear regression, \\(y=a+bx\\)? As we assume that we have a very large sample size, sample covariance is pretty much the same as the population covariance. Hence, the slope is given by \\[\nb = \\mathrm{Cov}(X, Y)/\\mathbb{V}[X] = 0.5\\sigma^2/\\sigma^2 = 0.5\n\\]\nand the intercept is \\[\n    a = \\mathbb E[Y] - b\\cdot \\mathbb E[X] = \\mu - 0.5\\mu = 0.5\\mu,\n\\] so that the line is \\(y = 0.5(\\mu + x)\\) and corresponds to the regression to the mean estimator \\(\\hat Y_{0.5}\\). It shouldn’t be surprising: simple linear regression minimises the overall squared error. For each value \\(x\\) of \\(X\\) we actually know that it should be \\(\\mathbb E[Y\\mid X=x]\\), which is exactly \\(\\hat Y_{0.5}\\).\n\n\nPrincipal component analysis\nWhat is the first principal component? For PCA we center the data, so that the principal component will be passing through \\((\\mu, \\mu)\\). To find the slope, we need to find the eigenvector corresponding to the largest value of the covariance matrix of the data. We don’t really need to worry about the positive \\(\\sigma^2\\) factor, so let’s find the eigenvector of the matrix \\[\n\\begin{pmatrix}\n    1 & 0.5\\\\\n    0.5 & 1\n\\end{pmatrix}.\n\\]\nThe largest eigenvalue is \\(1.5\\) with an eigenvector \\((1, 1)\\) (and the other eigenvalue is \\(0.5\\) with eigenvector, of course orthogonal, \\((-1, 1)\\)). Hence, the line describing the principal component is given by \\((x-\\mu, y-\\mu) = t (1, 1)\\), where \\(t\\in \\mathbb R\\), which is the same line as \\(y=x\\). We see that this is essentially the \\(\\hat Y_1\\) estimator."
  },
  {
    "objectID": "posts/regression-to-the-mean-bias-rejoinder.html#footnotes",
    "href": "posts/regression-to-the-mean-bias-rejoinder.html#footnotes",
    "title": "Regression to the mean and biased predictions",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nYes, height can’t be negative. But let’s ignore that: it’s a toy, but still informative, problem.↩︎\nIt would be a wasted opportunity to not mention this wonderful joke. The whole lecture is a true gem.↩︎"
  },
  {
    "objectID": "posts/expectations-student-mentorship.html",
    "href": "posts/expectations-student-mentorship.html",
    "title": "Student mentorship: expectations document",
    "section": "",
    "text": "Welcome! This document is supposed to explain my general mentoring style and act as a skeleton around we can build the collaboration and mentorship rules.\nPlease, note:"
  },
  {
    "objectID": "posts/expectations-student-mentorship.html#mission-statement",
    "href": "posts/expectations-student-mentorship.html#mission-statement",
    "title": "Student mentorship: expectations document",
    "section": "Mission statement",
    "text": "Mission statement\nWhen I advise on a project I try to keep the following in mind:\n\nI want you to learn and become a better researcher and engineer at the end of the project.\nIt’s more important that we understand each other and are happy with the mentorship, rather than we get an additional feature."
  },
  {
    "objectID": "posts/expectations-student-mentorship.html#expectations",
    "href": "posts/expectations-student-mentorship.html#expectations",
    "title": "Student mentorship: expectations document",
    "section": "Expectations",
    "text": "Expectations\n\nWhat you can expect from me\n\nI’ll find regular time to meet with you and advise on the steps which may be worth taking. While I will be more “hands-on” and have more precise ideas when you start, I want you to become an independent thinker with a good knowledge on the topic – it’s also likely that you’ll know more about the topic than me by the end of your project!\nI’ll advise you on your code and writing, to make sure that your skills improve.\n\nI’ll keep an open mindset to your comments and suggestions. If you encounter any issues, let me know and we’ll work together on resolving them.\n\n\n\nWhat I’d like to expect from you\n\nHonesty. If something doesn’t work for you (e.g., the expectations and the workload are too high), I said something ridiculously wrong, or the experiments fail, let’s discuss. I’m still learning both how to be a good mentor and a good scientist.\nConforming to use good research and coding practices. We will work on open-source projects and I expect you to write good code (with documentation and tests) and run reproducible experiments. Developing these skills takes time and we will work together to make sure that your research and programming skills are improving.\nTaking the ownership of conforming to the university rules. You should remind me when your thesis is due three months before submitting it, so we can discuss the outline, and send me the first draft three weeks before the deadline, so I can review it.\n\n\n\nConflict resolution\nIn case of a conflict with an academic or a student, please contact me and we will work together to resolve the conflict. If you feel that you do not want me to be involved (e.g., the conflict is between you and me), I encourage you to contact my mentor, Professor Niko Beerenwinkel or ETH’s Ombudspersons.\n\n\nMisc\nI’m not an established researcher in the field (and I don’t have a PhD!). Apart from the fact that I may be wrong in different aspects (happy to learn!), the reference letters written by me are unlikely to be accepted e.g., if you apply for a PhD. If you need a reference letter at the end of the project, I’d suggest to ask Professor Niko Beerenwinkel (and CC me) whether he could provide one."
  },
  {
    "objectID": "posts/expectations-student-mentorship.html#general-research-advice",
    "href": "posts/expectations-student-mentorship.html#general-research-advice",
    "title": "Student mentorship: expectations document",
    "section": "General research advice",
    "text": "General research advice\nAlthough I will supply you with an initial reading list tailored to your project, I’d like to share below some general advice on research, knowledge work, and learning. (Remember – if you see that some of these do not work you, feel free to replace them with better practices. I’d also be grateful if you could share them with me, so I can update this document).\n\nResearch notes and journal\nI’d strongly encourage you to book some time at the start and the end of every working day to work on your research notes and write your observations in a journal.\nThis serves multple purposes: - I believe it will help you to improve your understanding of the domain. - At some point you will need to write your thesis. You will see that it is much easier to edit a series of connected research notes into a first draft, rather than starting with an empty page. - By practicing this over the duration of the project, you will end up with a skill which is useful regardless whether you decide to move into industry or stay in academia.\nTo start writing research notes, read an Andy Matuschak’s note or watch Martin Adams’ video. Popular software includes Obsidian and Zettlr (and you can use them for the journal as well).\nFor your research journal, you may find this blog post useful. Journal can also be helpful to track your feelings and attitude towards the project, so we can adjust the workload or troubleshoot the process – see this post.\n\n\nReading scientific literature\n\nI’d suggest to read this Andy Matuschak’s note and this short P.N. Edwards’ article.\nThis is also a skill which takes time to master, so I’d suggest to practice it regularly and go back and refresh the principles of effective reading.\nYou will see that there is always too much literature to read than the time permits, so it’s critical to think what you want to learn from a paper.\n\nAre there specific questions I want to have the answer to by reading this paper? (It’s always good to read papers with several questions in mind.)\nIs this some maths or statistics which is crucial to deeply understand for the project? If so, several hours may be required and there is nothing to be done.\nIs this a paper which main conclusion can be quickly understood just by looking at one figure and the abstract or conclusions?\nIs this a paper which is potentially useful if problem X arises? If so, it’s probably good to say in a research problem on topic X that this paper may be useful to deeply understand it then.\n\nI like to use Connected Papers to find papers related to the paper of interest. Another strategy is to use Google Scholar to find papers which cited the paper of interest or see what the superstars are doing.\nSpeaking of superstars, Twitter has become a place where new research results are often announced and have short “tl;dr” threads. I would suggest to create a recurrent task (e.g., half an hour every two weeks) and check what the superstars have been doing. (Note that the temptation to procrastinate can be huge. This is why I recommend to set only a specific time to check it.)\n\n\n\nFinding a sustainable working style\n\nAs Bastian Rieck advises, it’s crucial that you find a sustainable workflow. Working on a project over a few months is a long time and “it’s rather a marathon than a sprint”.\nI’m interested in seeing that your expertise grows and that work you produce is of good quality, rather than in counting the hours you put into the work:\n\nIf you think that my expectations are unrealistic, just talk to me – we don’t need to rush and the scope of the thesis can always be adjusted to be more realistic.\nMake sure that you prioritize your mental health and well-being.\nPlease, please, please, no work on weekends and holidays.\n\nYou will see that different people have different working styles. This is fine – they are also working on different projects, have different backgrounds, and have different goals. Don’t compare yourself with them and embrace your way of working as well as theirs.\n\n\n\nProgramming\n\nWe will use Git version control and GitHub. Please, make sure that you have an account and send me your username, so I can add you to the project.\nIf you had not worked with Git before, I recommend (a) Creating a “sandbox” repository and playing with different commands. R. Dudler’s “The simple guide” is a nice way to get started.\nLearning good software practices is like learning a new language – working with a dictionary won’t make one proficient in one day, but using it regularly can help to avoid common errors. I recommend Google Style Guide and (to know what should be avoided) Python anti-patterns.\n\n\n\nMisc\n\nIf you want to become a researcher, R. Hamming’s “You and your research” talk is a classic.\nPatrick Kidger and Andrej Karpathy also wrote on this topic."
  },
  {
    "objectID": "posts/expectations-student-mentorship.html#references",
    "href": "posts/expectations-student-mentorship.html#references",
    "title": "Student mentorship: expectations document",
    "section": "References",
    "text": "References\nI used the following resources to draft the document above. However, all the mistakes (scientific, mentoring, grammar) are to blame on myself.\n\nK.S. Masters and P.K. Kreeger’s “Ten Simple Rules” article\nYinghzhen Li’s blogpost\nThe document issued by Niko Beerenwinkel to his PhD students."
  },
  {
    "objectID": "posts/triangle-distributions.html",
    "href": "posts/triangle-distributions.html",
    "title": "Two distributions on a triangle",
    "section": "",
    "text": "Frederic, Alex and I have been discussing some experiments related to our work on mutual information estimators and Frederic suggested to look at one distribution. I misunderstood what he meant, but this mistake turned out to be quite an interesting object.\nSo let’s take a look at two distributions defined over a triangle \\[T = \\{ (x, y)\\in (0, 1)\\times (0, 1) \\mid y &lt; x \\}\\] and calculate their mutual information."
  },
  {
    "objectID": "posts/triangle-distributions.html#uniform-joint",
    "href": "posts/triangle-distributions.html#uniform-joint",
    "title": "Two distributions on a triangle",
    "section": "Uniform joint",
    "text": "Uniform joint\nConsider a probability distribution with constant probability density function (PDF) of the joint distribution: \\[p_{XY}(x, y) = 2 \\cdot \\mathbf{1}[y&lt;x].\\]\nWe have \\[p_X(x) = \\int\\limits_0^x p_{XY}(x, y)\\, \\mathrm{d}y = 2x\\] and \\[ p_Y(y) = \\int\\limits_0^1 p_{XY}(x, y) \\mathbf{1}[y &lt; x]  \\, \\mathrm{d}x = \\int\\limits_y^1 p_{XY}(x, y) \\, \\mathrm{d}x = 2(1-y).\\]\nHence, pointwise mutual information is given by \\[ i(x, y) = \\log \\frac{ p_{XY}(x, y) }{p_X(x) \\, p_Y(y) } = \\log \\frac{1}{2x(1-y)}\\] and mutual information is\n\\[I(X; Y) = \\int\\limits_0^1 \\mathrm{d}x \\int\\limits_x^1 i(x, y)\\, p_{XY}(x, y) \\mathrm{d}y = 1-\\log 2 \\approx 0.307.\\]\nFinally, let’s visualise this distribution to numerically validate the formulae above:\n\n\nCode\nfrom typing import Protocol\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nplt.style.use(\"dark_background\")\n\n\nclass Distribution(Protocol):\n  def sample(self, rng, n_samples: int) -&gt; np.ndarray:\n    pass\n\n  def p_xy(self, x: np.ndarray, y: np.ndarray) -&gt; np.ndarray:\n    pass\n\n  def p_x(self, x: np.ndarray) -&gt; np.ndarray:\n    pass\n\n  def p_y(self, y: np.ndarray) -&gt; np.ndarray:\n    pass\n\n  def pmi(self, x: np.ndarray, y: np.ndarray) -&gt; np.ndarray:\n    pass\n\n  @property\n  def mi(self) -&gt; float:\n    pass\n\n\nclass UniformJoint(Distribution):\n  def sample(self, rng, n_samples):\n    samples = rng.uniform(low=1e-9, size=(3 * n_samples, 2))\n    samples = np.asarray(list(filter(lambda point: point[1] &lt; point[0], samples)))\n    if len(samples) &lt; n_samples:\n      samples = self.sample(rng, n_samples)\n    \n    assert len(samples) &gt;= n_samples\n    return samples[:n_samples, ...]\n\n  def p_xy(self, x, y):\n    return np.where(y &lt; x, 2.0, 0.0)\n\n  def p_x(self, x):\n    return 2*x\n\n  def p_y(self, y):\n    return 2*(1-y)\n\n  def pmi(self, x, y):\n    return np.where(y &lt; x, -np.log(2*x*(1-y)), np.nan)\n\n  @property\n  def mi(self):\n    return 0.307\n\n\ndef visualise_dist(\n  rng,\n  dist: Distribution,\n  n_samples: int = 15_000,\n) -&gt; plt.Figure:\n  fig, axs = plt.subplots(2, 3, figsize=(3*2.2, 2*2.2))\n\n  samples = dist.sample(rng, n_samples=n_samples)\n\n  t_axis = np.linspace(1e-9, 1 - 1e-9, 51)\n\n  X, Y = np.meshgrid(t_axis, t_axis)\n\n  # Visualise joint probability\n  ax = axs[0, 0]\n  ax.scatter(samples[:, 0], samples[:, 1], rasterized=True, alpha=0.3, s=0.2, marker=\".\")\n  ax.set_xlim(0, 1)\n  ax.set_ylim(0, 1)\n  ax.set_title(\"Samples from $P_{XY}$\")\n  ax.set_xlabel(\"$x$\")\n  ax.set_ylabel(\"$y$\")\n\n  ax = axs[1, 0]\n  ax.imshow(dist.p_xy(X, Y), origin=\"lower\", extent=[0, 1, 0, 1], cmap=\"magma\")\n  ax.set_title(\"PDF $p_{XY}$\")\n  ax.set_xlabel(\"$x$\")\n  ax.set_ylabel(\"$y$\")\n\n  # Visualise marginal distributions\n  ax = axs[0, 1]\n  ax.set_xlim(0, 1)\n  ax.hist(samples[:, 0], bins=np.linspace(0, 1, 51), density=True, alpha=0.2, rasterized=True)\n  ax.plot(t_axis, dist.p_x(t_axis))\n  ax.set_xlabel(\"$x$\")\n  ax.set_title(\"PDF $p_X$\")\n\n  ax = axs[1, 1]\n  ax.set_xlim(0, 1)\n  ax.hist(samples[:, 1], bins=np.linspace(0, 1, 51), density=True, alpha=0.2, rasterized=True)\n  t_axis = np.linspace(0, 1, 51)\n  ax.plot(t_axis, dist.p_y(t_axis))\n  ax.set_xlabel(\"$y$\")\n  ax.set_title(\"PDF $p_Y$\")\n\n  # Visualise PMI\n  ax = axs[0, 2]\n  ax.set_xlim(0, 1)\n  ax.set_ylim(0, 1)\n  ax.imshow(dist.pmi(X, Y), origin=\"lower\", extent=[0, 1, 0, 1], cmap=\"magma\")\n  ax.set_title(\"PMI\")\n  ax.set_xlabel(\"$x$\")\n  ax.set_ylabel(\"$y$\")\n\n  ax = axs[1, 2]\n  pmi_profile = dist.pmi(samples[:, 0], samples[:, 1])\n  mi = np.mean(pmi_profile)\n  ax.set_title(f\"PMI histogram. MI={dist.mi:.2f}\")  \n  ax.axvline(mi, color=\"navy\", linewidth=1)\n  ax.axvline(dist.mi, color=\"salmon\", linewidth=1, linestyle=\"--\")\n  ax.hist(pmi_profile, bins=np.linspace(-2, 5, 21), density=True)\n  ax.set_xlabel(\"PMI value\")\n\n  return fig\n\nrng = np.random.default_rng(42)\ndist = UniformJoint()\n\nfig = visualise_dist(rng, dist)\nfig.tight_layout()"
  },
  {
    "objectID": "posts/triangle-distributions.html#uniform-margin",
    "href": "posts/triangle-distributions.html#uniform-margin",
    "title": "Two distributions on a triangle",
    "section": "Uniform margin",
    "text": "Uniform margin\nThe above distribution is interesting, but when I heard about the distribution over the triangle, I actually had the following generative model in mind: \\[\\begin{align*}\n  X &\\sim \\mathrm{Uniform}(0, 1),\\\\\n  Y \\mid X=x &\\sim \\mathrm{Uniform}(0, x).\n\\end{align*}\\]\nWe have \\(p_X(x) = 1\\) and therefore \\[p_{XY}(x, y) = p_{Y\\mid X}(y\\mid x) = \\frac{1}{x}\\,\\mathbf{1}[y &lt; x].\\]\nAgain, this distribution is defined on the triangle \\(T\\), although now the joint is not uniform.\nWe have \\[ p_Y(y) = \\int\\limits_y^1  \\frac{1}{x} \\, \\mathrm{d}x = -\\log y\\] and \\[i(x, y) = \\log \\frac{1}{-x \\log y} = -\\log \\big(x\\cdot (-\\log y)\\big )\n= - \\left(\\log(x) + \\log(-\\log y) \\right) = -\\log x - \\log(-\\log y).\\] This expression suggests that if \\(p_Y(y)\\) were uniform on \\((0, 1)\\) (but it is not), the pointwise mutual information \\(i(x, Y)\\) would be distributed according to Gumbel distribution.\nThe mutual information \\[\n  I(X; Y) = -\\int\\limits_0^1 \\mathrm{d}y \\int\\limits_y^1 \\frac{ \\log x + \\log(-\\log y)}{x} \\, \\mathrm{d}x = \\frac{1}{2} \\int\\limits_0^1 \\log y \\cdot \\log \\left(y \\log ^2 y\\right) \\, \\mathrm{d}y = \\gamma \\approx 0.577\n\\] is in this case the Euler–Mascheroni constant. I don’t know how to do this integral, but both Mathematica and Wolfram Alpha seem to be quite confident in it.\nPerhaps it shouldn’t be too surprising as \\(\\gamma\\) can appears in expressions involving mean of the Gumbel distribution. However, I’d like to understand this connection better.\nPerhaps another time; let’s finish this post with another visualisation:\n\n\nCode\nclass UniformMargin(Distribution):\n  def sample(self, rng, n_samples: int) -&gt; np.ndarray:\n    x = rng.uniform(size=(n_samples,))\n    y = rng.uniform(high=x)\n    return np.hstack([x.reshape((-1, 1)), y.reshape((-1, 1))])\n\n  def p_xy(self, x: np.ndarray, y: np.ndarray) -&gt; np.ndarray:\n    return np.where(y &lt; x, np.reciprocal(x), np.nan)\n\n  def p_x(self, x: np.ndarray) -&gt; np.ndarray:\n    return np.full_like(x, fill_value=1.0)\n\n  def p_y(self, y: np.ndarray) -&gt; np.ndarray:\n    return -np.log(y)\n\n  def pmi(self, x: np.ndarray, y: np.ndarray) -&gt; np.ndarray:\n    return np.where(y &lt; x, -np.log(-x * np.log(y)), np.nan)\n\n  @property\n  def mi(self):\n    return 0.577\n\n\nrng = np.random.default_rng(42)\ndist = UniformMargin()\n\nfig = visualise_dist(rng, dist)\nfig.tight_layout()\n\n\n/tmp/ipykernel_44155/2727834072.py:14: RuntimeWarning: divide by zero encountered in log\n  return -np.log(y)"
  },
  {
    "objectID": "posts/beta-bernoulli.html",
    "href": "posts/beta-bernoulli.html",
    "title": "Beta-Bernoulli distribution",
    "section": "",
    "text": "I have already demonstrated that I don’t know how to properly toss a coin. Let’s do it again."
  },
  {
    "objectID": "posts/beta-bernoulli.html#famous-bernoulli-and-the-company",
    "href": "posts/beta-bernoulli.html#famous-bernoulli-and-the-company",
    "title": "Beta-Bernoulli distribution",
    "section": "Famous Bernoulli and the company",
    "text": "Famous Bernoulli and the company\n\nCounting the distributions\nBefore we go any further: how many distributions can produce outcomes from the set \\(\\{0, 1\\}\\)?\nIn other words, we have a measurable space \\(\\{0, 1\\}\\) (where all four subsets are measurable) and we would like to know how many probability measures exist on this space. We have \\(P(\\varnothing) = 0\\) and \\(P(\\{0, 1\\}) = 1\\) straight from the definition of a probability measure. As we need to have \\(P(\\{0\\}) + P(\\{1\\}) = 1\\) we see that there is a bijection between these probability measures and numbers from the set \\([0, 1]\\), given by \\(P_b(\\{1\\}) = b\\) for any bias \\(b\\in [0, 1]\\). This distribution is called \\(\\mathrm{Bernoulli}(b)\\) and it’s easy to prove that if \\(X\\sim \\mathrm{Bernoulli}(b)\\), then \\[\n\\mathbb E[X] = P(X=1) = b.\n\\]\nHence, the first moment fully determines any distribution on \\(\\{0, 1\\}\\).\nThe derivation of variance is quite elegant, once one notices that if \\(X\\sim \\mathrm{Bernoulli}(b)\\), then also \\(X^2\\sim \\mathrm{Bernoulli}(b)\\), because it has to be some Bernoulli distribution and \\(P(X^2=1) = P(X=1)\\). Then, we have: \\[\n\\mathbb V[X] = \\mathbb E[X^2] - \\mathbb E[X]^2 = b - b^2 = b(1-b).\n\\]\nBy considering both cases, we see that for every outcome \\(x \\in \\{0, 1\\}\\), the likelihood is given by \\[\n\\mathrm{Bernoulli}(x\\mid b) = b^x(1-b)^{1-x}.\n\\]\n\n\nA few coins\nThis characterization of distributions over \\(\\{0, 1\\}\\) is very powerful.\nConsider the following problem: we have \\(K\\) coins with biases \\(b_1, \\dotsc, b_K\\) and we throw a loaded die which can give \\(K\\) different outcomes, each with probability \\(d_1, \\dotsc, d_K\\) (which, of course, sum up to \\(1\\)) to decide which coin we will toss. What is the outcome of this distribution? It is, of course, a coin toss outcome, which is a number from the set \\(\\{0, 1\\}\\). Hence, this has to be some Bernoulli distribution. Which one? Bernoulli distributions are fully determined by their expectations, and the expectation in this case is given by a weighted average \\(\\bar b = b_1 d_1 + \\cdots + b_K d_K\\). In other words, we can replace a loaded die and \\(K\\) biased coins with just a single biased coin.\nTo have more equations, the first procedure corresponds to \\[\\begin{align*}\nD &\\sim \\mathrm{Categorical}(d_1, \\dotsc, d_K)\\\\\nX \\mid D &\\sim \\mathrm{Bernoulli}(b_D)\n\\end{align*}\n\\]\nand the second one to \\[\nY \\sim \\mathrm{Bernoulli}(\\bar b),\n\\]\nwith \\(\\bar b = b_1d_1 + \\cdots + b_Kd_K\\).\nBoth of these distributions are the same, i.e., \\(\\mathrm{law}\\, X = \\mathrm{law}\\, Y\\).\nIn particular, the likelihood has to be the same, proving an equality \\[\n\\sum_{k=1}^K d_k\\, b_k^x(1-b_k)^{1-x} = \\bar b^x(1-\\bar b)^{1-x}\n\\]\nfor every \\(x\\in \\{0, 1\\}\\).\n\n\nEven more coins\nEven more interesting, consider infinitely many coins with different biases, which are chosen according to a beta distribution. Once the coin is chosen, we toss it: \\[\\begin{align*}\nB &\\sim \\mathrm{Beta}(\\alpha, \\beta)\\\\\nX\\mid B &\\sim \\mathrm{Bernoulli}(B)\n\\end{align*}\n\\]\nThis is a continuous mixture, which we might call \\(\\mathrm{BetaBernoulli}(\\alpha, \\beta)\\)… if it weren’t just a plain Bernoulli distribution with bias \\[\n\\mathbb E[X] = \\mathbb E[\\mathbb E[X\\mid B]] = \\mathbb E[B] = \\frac{\\alpha}{\\alpha + \\beta}.\n\\]\n\n\nNoisy communication channel\nLet’s consider an example involving plain Bernoulli distributions and a noisy communication channel.\nLet \\(X\\sim \\mathrm{Bernoulli}(b)\\) be an input variable. The binary output, \\(Y\\), is a noisy version of \\(X\\), with \\(\\alpha\\) controlling the false positive rate and \\(\\beta\\) the false negative rate: \\[\nP(Y = 1 \\mid X=1) = 1-\\beta, \\quad P(Y=1\\mid X=0) = \\alpha.\n\\]\nWe can write this model as: \\[\\begin{align*}\nX &\\sim \\mathrm{Bernoulli}(b)\\\\\nY \\mid X &\\sim \\mathrm{Bernoulli}( X(1-\\beta) + (1-X) \\alpha)\n\\end{align*}\n\\]\nIn fact, we have already seen this example: we can treat \\(X\\) as a special case of a loaded dice, indexing a finite mixture with just two components. Hence, the marginal distribution of \\(Y\\) is \\[\nY \\sim \\mathrm{Bernoulli}(b(1-\\beta) + (1-b) \\alpha).\n\\]\n\n\nTossing multiple coins\nWe know that a single coin toss characterises all probability distributions on the set \\(\\{0, 1\\}\\). However, once we consider \\(N\\ge 2\\) coin tosses, yielding outcomes in the set \\(\\{0, 1, 2, \\dotsc, N\\}\\), many different distributions will appear.\nWe mentioned some of these distributions in this post, but just for completeness, at the end there is a list of standard distributions.\nSimilarly, if one is interested in modeling binary vectors, which are from the set \\(\\{0, 1\\}\\times \\{0, 1\\} \\cdots \\{0, 1\\}\\), many different distributions will appear. Let’s analyse an example below."
  },
  {
    "objectID": "posts/beta-bernoulli.html#two-deceptively-similar-distributions",
    "href": "posts/beta-bernoulli.html#two-deceptively-similar-distributions",
    "title": "Beta-Bernoulli distribution",
    "section": "Two deceptively similar distributions",
    "text": "Two deceptively similar distributions\n\nDenoising problem\nWe have a fixed bit \\(T \\in \\{0, 1\\}\\) and we observe its noisy measurements, with false negative rate \\(\\beta\\) and false positive rate \\(\\alpha\\) (recall this section). We will write \\(c_0 = \\alpha\\) and \\(c_1 = 1-\\beta\\) and put some prior on \\(T\\): \\[\\begin{align*}\n  \\theta &\\sim \\mathrm{Beta}(\\alpha, \\beta)\\\\\n  T\\mid \\theta &\\sim \\mathrm{Bernoulli}(\\theta)\\\\\n  X_n\\mid T &\\sim \\mathrm{Bernoulli}( c_0(1-T) + c_1T) \\text{ for } n = 1, \\cdots, N\n\\end{align*}\n\\]\nLet’s use shorthand notation \\(\\mathbf{X} = (X_1, \\dotsc, X_N)\\) and note that the likelihood is: \\[\\begin{align*}\n  P(\\mathbf{X} \\mid T) &= \\prod_{n=1}^N P(X_n \\mid b(T) ) \\\\\n  &= \\prod_{n=1}^N b(T) ^{X_n} \\left(1-b(T)\\right)^{1-X_n} \\\\\n  &= b(T)^S \\left(1-b(T)\\right)^{N-S}\n\\end{align*}\n\\]\nwhere \\(b(T) = c_0(1-T) + c_1T\\) and \\(S = X_1 + \\cdots + X_N\\).\nAfter reading the discussion above, there is a natural question: why is \\(\\theta\\) introduced at all? For a simple denoising question, i.e., finding \\(P(T\\mid \\mathbf{X}) \\propto P(\\mathbf{X} \\mid T) P(T)\\), this parameter is not needed at all: \\(P(T)\\) is just a Bernoulli variable, with bias parameter \\(\\bar \\theta = \\alpha/(\\alpha + \\beta)\\). Then, \\[\n  P(T=1\\mid \\mathbf{X}) = \\frac{ c_1^S (1-c_1)^{N-S} \\cdot \\bar \\theta }{ c_1^S(1-c_1)^{N-S} \\cdot \\bar \\theta + c_0^S(1-c_0)^{N-S}\\cdot (1-\\bar \\theta) }.\n\\]\nLet’s quickly implement this formula and see how the posterior looks if we start with an unbiased coin (i.e., \\(\\bar \\theta=0.5\\)), observe \\(S = N - 1\\) successes, and we vary the noise level \\(\\alpha = \\beta = c_0 = 1 - c_1\\):\n\n\nCode\nfrom typing import Callable, NamedTuple\n\nimport jax\nimport jax.numpy as jnp\n\nimport numpyro\nimport numpyro.distributions as dist\nimport matplotlib.pyplot as plt\nplt.style.use(\"dark_background\")\n\n\ndef _binomial_loglikelihood(n: int, s: int, bias: float) -&gt; float:\n  return s * jnp.log(bias) + (n - s) * jnp.log1p(-bias)\n\n\nclass LikelihoodArgs(NamedTuple):\n  n: int  # Number of throws\n  s: int  # Number of successes, 0 &lt;= s &lt;= n\n  c0: float  # Probability of observing success if T = 0 (false positive rate) \n  c1: float  # Probability of observing success if T = 1 (true positive rate)\n\n\nclass BetaPriorArgs(NamedTuple):\n  alpha: float\n  beta: float\n\n  @property\n  def mean(self) -&gt; float:\n    return self.alpha / (self.alpha + self.beta)\n\n\ndef posterior_t(\n  bias: float,\n  like: LikelihoodArgs,\n) -&gt; float:\n  \"\"\"Calculates P(T | data),\n  where the prior is given by T ~ Bernoulli(bias).\"\"\"\n  n, s, c0, c1 = like.n, like.s, like.c0, like.c1\n  log_0 = _binomial_loglikelihood(n=n, s=s, bias=c0) + jnp.log1p(-bias)\n  log_1 = _binomial_loglikelihood(n=n, s=s, bias=c1) + jnp.log(bias)\n\n  return jax.nn.softmax(jnp.array([log_0, log_1]))\n\nfig, axs = plt.subplots(\n  1, 3, dpi=120, figsize=(8, 2.4), sharex=True, sharey=True,\n)\n\nfor ax in axs:\n  ax.spines[['top', 'right']].set_visible(False)\n  ax.set_xticks([0, 1])\n  ax.set_xlim([-0.5, 1.5])\n  ax.set_ylim([-0.1, 1.1])\n  ax.set_yticks([0, 0.25, 0.5, 0.75, 1])\n\n\nfor ax, n in zip(axs, [1, 3, 5]):\n  ax.set_title(f\"$N = {n}$, $S = {n-1}$\")\n  for i, noise in enumerate([0.01, 0.1, 0.25, 0.45]):\n    like = LikelihoodArgs(\n      n=n,\n      s=n - 1,\n      c0=noise,\n      c1=1-noise,\n    )\n    posterior = posterior_t(bias=0.5, like=like)\n\n    ax.plot([0, 1], posterior, label=f\"{noise:.2f}\", c=f\"C{i}\")\n    ax.scatter([0, 1], posterior, c=f\"C{i}\", s=4)\n  \nax = axs[-1]\nax.legend(frameon=False, bbox_to_anchor=(1.05, 1))\nfig.tight_layout()\n\n\n/home/pawel/micromamba/envs/data-science/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\nAn NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n\n\n\n\n\nThis formula is elegant: we can use it to solve the denoising problem, i.e., to infer the value of the \\(T\\) variable. Why did we introduce the \\(\\theta\\) variable with its own prior? We may be interested not only in inferring a missing coin toss, but also in learning about the unknown bias \\(\\theta\\) of the coin from this experiment.\nFor example, if \\(T\\) were directly observed, we would have \\(P(\\theta \\mid T) = \\mathrm{Beta}(\\theta \\mid \\alpha + T, \\beta + (1-T) )\\):\n\n\nCode\nfig, axs = plt.subplots(1, 3, figsize=(5, 2.3), dpi=200, sharex=True, sharey=True)\n\nfor ax in axs:\n  ax.spines[['top', 'right']].set_visible(False)\n  ax.set_xlim([-0.02, 1.02])\n\nbins = jnp.linspace(0, 1, 31)\n\nkey = jax.random.PRNGKey(1)\nkey, *subkeys = jax.random.split(key, 4)\n\nax = axs[0]\nax.set_title(\"Prior\\nBeta(1, 1)\")\nsamples = dist.Beta(1, 1).sample(subkeys[0], sample_shape=(10_000,))\nax.hist(samples, bins=bins, density=True)\n\nax = axs[1]\nax.set_title(\"Posterior for $T=0$\\nBeta(1, 2)\")\nsamples = dist.Beta(1, 2).sample(subkeys[0], sample_shape=(10_000,))\nax.hist(samples, bins=bins, density=True)\n\nax = axs[2]\nax.set_title(\"Posterior for $T=1$\\nBeta(2, 1)\")\nsamples = dist.Beta(2, 1).sample(subkeys[0], sample_shape=(10_000,))\nax.hist(samples, bins=bins, density=True)\n\nfig.tight_layout()\n\n\n\n\n\nAs \\(T\\) is not directly observed, we have to look at \\(P(\\theta \\mid \\mathbf{X})\\): \\[\\begin{align*}\n  P(\\theta \\mid \\mathbf{X}) &= \\sum_{t} P(\\theta, T=t \\mid \\mathbf{X}) \\\\\n  &= \\sum_{t} P(\\theta \\mid T=t,  \\mathbf{X})P(T=t\\mid \\mathbf{X})\\\\\n  &= \\sum_{t} P(\\theta \\mid T=t) P(T=t\\mid \\mathbf{X}).\n\\end{align*}\n\\]\nThis is therefore a mixture of \\(\\mathrm{Beta}(\\alpha+1, \\beta)\\) and \\(\\mathrm{Beta}(\\alpha, \\beta + 1)\\) distributions:\n\n\nCode\n_noise = 0.2\n\ndef create_posterior_theta(\n  prior: BetaPriorArgs,\n  like: LikelihoodArgs,\n):\n  mixing = dist.Categorical(\n    probs=posterior_t(\n      bias=prior.mean, like=like\n  ))\n  return dist.MixtureSameFamily(\n    mixing_distribution=mixing,\n    component_distribution=dist.Beta(\n      # alpha\n      concentration1=jnp.array([prior.alpha, prior.alpha + 1]),\n      # beta\n      concentration0=jnp.array([prior.beta + 1, prior.beta]),\n    )\n  )\n\nfig, axs = plt.subplots(1, 3, figsize=(5, 2.3), dpi=200, sharex=True, sharey=True)\n\nfor ax in axs:\n  ax.spines[['top', 'right']].set_visible(False)\n  ax.set_xlim([-0.02, 1.02])\n\nkey, *subkeys = jax.random.split(key, 4)\n\nax = axs[0]\nax.set_title(\"$N=S=1$\")\nd = create_posterior_theta(\n  prior=BetaPriorArgs(alpha=1.0, beta=1.0),\n  like=LikelihoodArgs(\n    n=1, s=1,\n    c0=_noise, c1=1-_noise,\n  )\n)\nsamples = d.sample(subkeys[0], sample_shape=(10_000,))\nax.hist(samples, bins=bins, density=True)\n\nax = axs[1]\nax.set_title(\"$N=S=2$\")\nd = create_posterior_theta(\n  prior=BetaPriorArgs(alpha=1.0, beta=1.0),\n  like=LikelihoodArgs(\n    n=2, s=2,\n    c0=_noise, c1=1-_noise,\n  )\n)\nsamples = d.sample(subkeys[0], sample_shape=(10_000,))\nax.hist(samples, bins=bins, density=True)\n\nax = axs[2]\nax.set_title(\"$N=S=100$\")\nd = create_posterior_theta(\n  prior=BetaPriorArgs(alpha=1.0, beta=1.0),\n  like=LikelihoodArgs(\n    n=100, s=100,\n    c0=_noise, c1=1-_noise,\n  )\n)\nsamples = d.sample(subkeys[0], sample_shape=(10_000,))\nax.hist(samples, bins=bins, density=True)\n\nfig.tight_layout()\n\n\n\n\n\nImportantly, we see that the posterior on \\(\\theta\\) will not shrink even for large \\(N\\): increasing \\(N\\) affects how well we can determine the outcome of the toss \\(T\\). But even if we observe \\(T\\) perfectly, it’s only one toss, so it does not give too much information on the bias \\(\\theta\\).\nLet’s now look at this problem also from a bit different perspective. If we want to update \\(\\theta\\) directly from the observed data \\(\\mathbf{X}\\), we can marginalise \\(T\\) out in the likelihood: \\[\\begin{align*}\n  P(\\mathbf{X} \\mid \\theta ) &= \\sum_{t} P(\\mathbf{X} \\mid T=t) P(T=t\\mid \\theta) \\\\\n  &= \\theta P(\\mathbf{X} \\mid T=1) + (1-\\theta) P( \\mathbf{X}\\mid T=0) \\\\\n  &= \\theta c_1^{S}(1-c_1)^{N-S} + (1-\\theta) c_0^S(1-c_0)^{N-S}.\n\\end{align*}\n\\]\nWe have now only one continuous variable and we could use Hamiltonian Monte Carlo to sample from the distribution \\(P(\\theta \\mid \\mathbf{X})\\). We have already determined it analytically, as a mixture of beta distributions, but let’s quickly compare the results:\n\n\nCode\nfrom numpyro.infer import MCMC, NUTS\n\n\ndef create_model(like: LikelihoodArgs, prior: BetaPriorArgs):\n  theta = numpyro.sample(\"theta\", \n    dist.Beta(concentration1=prior.alpha, concentration0=prior.beta)\n  )\n  log1 = _binomial_loglikelihood(n=like.n, s=like.s, bias=like.c1) + jnp.log(theta)\n  log0 = _binomial_loglikelihood(n=like.n, s=like.s, bias=like.c0) + jnp.log1p(-theta)\n  numpyro.factor(\"loglikelihood\", jnp.logaddexp(log1, log0))\n\n\nprior_args = BetaPriorArgs(\n  alpha=1.0,\n  beta=1.0,\n)\n\nlike_args = LikelihoodArgs(\n  n=3,\n  s=2,\n  c0=0.1,\n  c1=0.9,\n)\n\nfig, axs = plt.subplots(1, 3, figsize=(5, 2.3), dpi=200, sharex=True, sharey=True)\nfor ax in axs:\n  ax.spines[['top', 'right']].set_visible(False)\n  ax.set_xlim([-0.02, 1.02])\n\nkey, *subkeys = jax.random.split(key, 4)\n\n\nax = axs[0]\nax.set_title(\"$P(\\\\theta \\\\mid T=1)$\")\nsamples = dist.Beta(2, 1).sample(subkeys[0], sample_shape=(100_000,))\nax.hist(samples, bins=bins, density=True, color=\"C2\")\n\nax = axs[1]\nax.set_title(\"$P(\\\\theta\\\\mid \\\\mathbf{X})$\\n(explicit)\")\nd = create_posterior_theta(\n  like=like_args,\n  prior=prior_args,\n)\nsamples = d.sample(subkeys[1], sample_shape=(100_000,))\nax.hist(samples, bins=bins, density=True)\n\nax = axs[2]\nax.set_title(\"$P(\\\\theta\\\\mid \\\\mathbf{X})$\\n(HMC)\")\n\nnuts_kernel = NUTS(create_model)\nmcmc = MCMC(nuts_kernel, num_warmup=1000, num_samples=10_000, num_chains=2)\nmcmc.run(subkeys[2], like=like_args, prior=prior_args)\nsamples = mcmc.get_samples()[\"theta\"]\nax.hist(samples, bins=bins, density=True)\n\n\n/tmp/ipykernel_11590/1934137946.py:51: UserWarning: There are not enough devices to run parallel chains: expected 2 but got 1. Chains will be drawn sequentially. If you are running MCMC in CPU, consider using `numpyro.set_host_device_count(2)` at the beginning of your program. You can double-check how many devices are available in your system using `jax.local_device_count()`.\n  mcmc = MCMC(nuts_kernel, num_warmup=1000, num_samples=10_000, num_chains=2)\n  0%|          | 0/11000 [00:00&lt;?, ?it/s]warmup:   0%|          | 1/11000 [00:00&lt;2:48:23,  1.09it/s, 3 steps of size 1.40e+01. acc. prob=0.98]warmup:   6%|▌         | 642/11000 [00:01&lt;00:11, 863.61it/s, 1 steps of size 1.58e+00. acc. prob=0.79]sample:  12%|█▏        | 1288/11000 [00:01&lt;00:05, 1765.94it/s, 1 steps of size 9.13e-01. acc. prob=0.91]sample:  18%|█▊        | 1948/11000 [00:01&lt;00:03, 2670.27it/s, 1 steps of size 9.13e-01. acc. prob=0.91]sample:  24%|██▎       | 2601/11000 [00:01&lt;00:02, 3484.36it/s, 3 steps of size 9.13e-01. acc. prob=0.91]sample:  30%|██▉       | 3261/11000 [00:01&lt;00:01, 4205.38it/s, 1 steps of size 9.13e-01. acc. prob=0.92]sample:  36%|███▌      | 3911/11000 [00:01&lt;00:01, 4774.65it/s, 3 steps of size 9.13e-01. acc. prob=0.91]sample:  42%|████▏     | 4572/11000 [00:01&lt;00:01, 5254.93it/s, 3 steps of size 9.13e-01. acc. prob=0.91]sample:  47%|████▋     | 5217/11000 [00:01&lt;00:01, 5580.35it/s, 1 steps of size 9.13e-01. acc. prob=0.91]sample:  53%|█████▎    | 5858/11000 [00:01&lt;00:00, 5771.69it/s, 1 steps of size 9.13e-01. acc. prob=0.91]sample:  59%|█████▉    | 6499/11000 [00:01&lt;00:00, 5951.85it/s, 1 steps of size 9.13e-01. acc. prob=0.91]sample:  65%|██████▌   | 7157/11000 [00:02&lt;00:00, 6131.07it/s, 3 steps of size 9.13e-01. acc. prob=0.91]sample:  71%|███████   | 7801/11000 [00:02&lt;00:00, 6191.86it/s, 3 steps of size 9.13e-01. acc. prob=0.91]sample:  77%|███████▋  | 8442/11000 [00:02&lt;00:00, 6142.81it/s, 3 steps of size 9.13e-01. acc. prob=0.91]sample:  82%|████████▏ | 9072/11000 [00:02&lt;00:00, 6188.51it/s, 3 steps of size 9.13e-01. acc. prob=0.91]sample:  88%|████████▊ | 9711/11000 [00:02&lt;00:00, 6247.36it/s, 3 steps of size 9.13e-01. acc. prob=0.91]sample:  94%|█████████▍| 10366/11000 [00:02&lt;00:00, 6336.80it/s, 3 steps of size 9.13e-01. acc. prob=0.91]sample: 100%|██████████| 11000/11000 [00:02&lt;00:00, 4188.79it/s, 1 steps of size 9.13e-01. acc. prob=0.91]\n  0%|          | 0/11000 [00:00&lt;?, ?it/s]warmup:   6%|▌         | 665/11000 [00:00&lt;00:01, 6642.86it/s, 3 steps of size 1.31e+00. acc. prob=0.79]sample:  12%|█▏        | 1330/11000 [00:00&lt;00:01, 6536.01it/s, 3 steps of size 1.01e+00. acc. prob=0.89]sample:  18%|█▊        | 1984/11000 [00:00&lt;00:01, 6444.17it/s, 1 steps of size 1.01e+00. acc. prob=0.89]sample:  24%|██▍       | 2629/11000 [00:00&lt;00:01, 6401.43it/s, 1 steps of size 1.01e+00. acc. prob=0.89]sample:  30%|██▉       | 3270/11000 [00:00&lt;00:01, 6373.51it/s, 3 steps of size 1.01e+00. acc. prob=0.89]sample:  36%|███▌      | 3917/11000 [00:00&lt;00:01, 6403.57it/s, 3 steps of size 1.01e+00. acc. prob=0.89]sample:  42%|████▏     | 4568/11000 [00:00&lt;00:00, 6437.18it/s, 1 steps of size 1.01e+00. acc. prob=0.89]sample:  47%|████▋     | 5219/11000 [00:00&lt;00:00, 6457.23it/s, 3 steps of size 1.01e+00. acc. prob=0.89]sample:  53%|█████▎    | 5869/11000 [00:00&lt;00:00, 6468.53it/s, 1 steps of size 1.01e+00. acc. prob=0.89]sample:  59%|█████▉    | 6516/11000 [00:01&lt;00:00, 6441.67it/s, 3 steps of size 1.01e+00. acc. prob=0.89]sample:  65%|██████▌   | 7189/11000 [00:01&lt;00:00, 6529.38it/s, 3 steps of size 1.01e+00. acc. prob=0.89]sample:  71%|███████▏  | 7858/11000 [00:01&lt;00:00, 6576.41it/s, 3 steps of size 1.01e+00. acc. prob=0.89]sample:  77%|███████▋  | 8522/11000 [00:01&lt;00:00, 6593.16it/s, 3 steps of size 1.01e+00. acc. prob=0.89]sample:  84%|████████▎ | 9197/11000 [00:01&lt;00:00, 6638.41it/s, 3 steps of size 1.01e+00. acc. prob=0.89]sample:  90%|████████▉ | 9868/11000 [00:01&lt;00:00, 6658.19it/s, 1 steps of size 1.01e+00. acc. prob=0.89]sample:  96%|█████████▌| 10540/11000 [00:01&lt;00:00, 6674.45it/s, 3 steps of size 1.01e+00. acc. prob=0.89]sample: 100%|██████████| 11000/11000 [00:01&lt;00:00, 6546.98it/s, 3 steps of size 1.01e+00. acc. prob=0.89]\n\n\n(array([0.24749999, 0.28499999, 0.31799995, 0.39600002, 0.43200003,\n        0.5084998 , 0.55500003, 0.64650004, 0.63000004, 0.77400005,\n        0.81750005, 0.79799933, 0.86250005, 0.86850005, 1.00800096,\n        1.08899909, 1.08749909, 1.2570012 , 1.22699898, 1.25250119,\n        1.26899894, 1.34400128, 1.36049886, 1.4324988 , 1.53300146,\n        1.48199876, 1.48800142, 1.60799866, 1.74450166, 1.6785016 ]),\n array([0.        , 0.03333334, 0.06666667, 0.10000001, 0.13333334,\n        0.16666667, 0.20000002, 0.23333335, 0.26666668, 0.30000001,\n        0.33333334, 0.36666667, 0.40000004, 0.43333337, 0.4666667 ,\n        0.5       , 0.53333336, 0.56666672, 0.60000002, 0.63333338,\n        0.66666669, 0.70000005, 0.73333335, 0.76666671, 0.80000007,\n        0.83333337, 0.86666673, 0.90000004, 0.9333334 , 0.9666667 ,\n        1.        ]),\n &lt;BarContainer object of 30 artists&gt;)\n\n\n\n\n\nHMC agrees well with the mixture of beta distributions. We see that \\(P(\\theta \\mid \\mathbf{X})\\) is slightly more diffuse compared with the case when \\(T\\) is directly observed.\n\n\nA bit different model\nIn the model above, for any single observation \\(X_n\\) we had \\[\nP(X_n\\mid \\theta) = \\theta c_1^{X_n}(1-c_1)^{1-X_n} + (1-\\theta) c_0^{X_n}(1-c_0)^{1-X_n}.\n\\]\nHowever, the joint likelihood was given by\n\\[\\begin{align*}\nP(\\mathbf{X} \\mid \\theta) &= \\theta \\prod_{n} c_1^{X_n}(1-c_1)^{1-X_n} + (1-\\theta) \\prod_{n} c_0^{X_n}(1-c_0)^{1-X_n} \\\\\n&= \\theta c_1^S (1-c_1)^{N-S} + (1-\\theta)c_0^S(1-c_0)^S,\n\\end{align*}\n\\]\nwhich is not the product of \\(\\prod_n P(X_n\\mid \\theta)\\), because all the variables \\(X_n\\) were noisy observations of a single coin toss outcome \\(T\\).\nLet’s now consider a deceptively similar model: \\[\\begin{align*}\n  \\phi &\\sim \\mathrm{Beta}(\\alpha, \\beta)\\\\\n  U_n \\mid \\phi &\\sim \\mathrm{Bernoulli}(\\phi) \\\\\n  Y_n \\mid U_n &\\sim \\mathrm{Bernoulli}(c_0(1-U_n) + c_1U_n)\n\\end{align*}\n\\]\nIn this case, for each observation \\(Y_n\\), we have a new coin toss \\(U_n\\).\nWe have \\(U_n\\sim \\mathrm{Bernoulli}(\\alpha/(\\alpha + \\beta))\\), so that \\(\\mathrm{law}\\, U_n = \\mathrm{law}\\, T\\). Similarly, \\(P(Y_n \\mid \\phi)\\) and \\(P(X_n \\mid \\theta)\\) will be very similar: \\[\\begin{align*}\nP(Y_n \\mid \\phi) &= \\sum_{u} P( Y_n \\mid U_n=u ) P(U_n=u \\mid \\phi) \\\\\n&= \\phi c_1^{Y_n}(1-c_1)^{1-Y_n} + (1-\\phi) c_0^{Y_n} (1-c_1)^{1-Y_n}.\n\\end{align*}\n\\]\nWe see that for \\(X_n = Y_n\\) and \\(\\theta = \\phi\\) the expressions are exactly the same.\nFor \\(N=1\\) there is no real difference between these two models. However, for \\(N\\ge 2\\) a difference appears, because throws \\(U_n\\) are independent and we have \\[\nP(\\mathbf{Y} \\mid \\phi) = \\prod_{n} \\left( \\phi c_1^{Y_n}(1-c_1)^{1-Y_n} + (1-\\phi) c_0^{Y_n} (1-c_1)^{1-Y_n} \\right).\n\\]\nThis is substantially different from \\(P(\\mathbf{X}\\mid \\theta)\\).\nPerhaps the following perspective is useful: the new model, with variables \\(U_n\\) marginalised out, corresponds to the following:\n\\[\\begin{align*}\n  \\phi &\\sim \\mathrm{Beta}(\\alpha, \\beta)\\\\\n  Y_n \\mid \\phi &\\sim \\mathrm{Bernoulli}(c_0(1-\\phi) + c_1\\phi)\n\\end{align*}\n\\]\nwhich is different from the model with a shared latent variable \\(T\\):\n\\[\\begin{align*}\n  \\theta &\\sim \\mathrm{Beta}(\\alpha, \\beta)\\\\\n  T\\mid \\theta &\\sim \\mathrm{Bernoulli}(\\theta)\\\\\n  X_n \\mid T &\\sim \\mathrm{Bernoulli}(c_0(1-T) + c_1T)\n\\end{align*}\n\\]\nHence, although the likelihood functions agree for any single observation, i.e., for every \\(x\\in \\{0, 1\\}\\), the likelihood functions \\(P(X_n=x\\mid \\theta)\\) and \\(P(Y_n=x\\mid \\phi)\\) are the same, the likelihood functions constructed using all observed variables, \\(P(\\mathbf{X}\\mid \\theta)\\) and \\(P(\\mathbf{Y}\\mid \\phi)\\), are usually different.\nAlso, the posteriors \\(P(\\theta \\mid \\mathbf{X})\\) and \\(P(\\phi \\mid \\mathbf{Y})\\) do differ: \\(\\phi\\) treats each outcome \\(Y_n\\) independently, so that the posterior can shrink quickly if \\(N\\) is large.\nCompare this with the posterior on \\(\\theta\\), which assumes that all \\(X_n\\) are noisy versions of a single throw \\(T\\), so it knows that there is little information about \\(\\theta\\) even if \\(N\\) is large: the posterior will always be a mixture of \\(\\mathrm{Beta}(\\alpha+1, \\beta)\\) and \\(\\mathrm{Beta}(\\alpha, \\beta+1)\\). In particular, if \\(\\alpha = \\beta = 1\\), the posterior on \\(\\theta\\) will still be very diffuse.\nFor example, consider \\(T = 1\\). We toss the coin \\(N = 10\\) times, but due to the noise \\(c_0 = 1 - c_1 = 0.1\\) we observed \\(S=8\\). We have the following \\(P(T=1\\mid \\mathbf{X})\\):\n\n\nCode\nprior = BetaPriorArgs(1.0, 1.0)\nlike = LikelihoodArgs(\n  n=10,\n  s=8,\n  c0=0.1,\n  c1=0.9,\n)\n\nposterior_t_value = posterior_t(\n  bias=prior.mean,\n  like=like,\n)\n\nprint(f\"P(T=1 | X) = {posterior_t_value[1]:.7f}\")\n\n\nP(T=1 | X) = 0.9999981\n\n\nHow would the posteriors on \\(\\theta\\) and \\(\\phi\\) look like? We will plot the above value as a dashed line:\n\n\nCode\ndef new_model(prior: BetaPriorArgs, like: LikelihoodArgs):\n  phi = numpyro.sample(\"phi\", dist.Beta(prior.alpha, prior.beta))\n\n  bias = like.c0 * (1 - phi) + like.c1 * phi\n  numpyro.factor(\"loglikelihood\",\n    _binomial_loglikelihood(n=like.n, s=like.s, bias=bias)\n  )\n\n\nfig, axs = plt.subplots(1, 3, figsize=(5, 2.3), dpi=200, sharex=True, sharey=True)\nfor ax in axs:\n  ax.spines[['top', 'right']].set_visible(False)\n  ax.set_xlim([-0.02, 1.02])\n  ax.axvline(posterior_t_value[1], c=\"white\", linestyle=\"--\", linewidth=2)\n\nkey, *subkeys = jax.random.split(key, 4)\n\nax = axs[0]\nax.set_title(\"$P(\\\\theta \\\\mid T=1)$\")\nsamples = dist.Beta(2, 1).sample(subkeys[0], sample_shape=(100_000,))\nax.hist(samples, bins=bins, density=True, color=\"C2\")\n\nax = axs[1]\nax.set_title(\"$P(\\\\theta\\\\mid \\\\mathbf{X}=\\\\mathbf{x})$\")\nd = create_posterior_theta(\n  like=like_args,\n  prior=prior_args,\n)\nsamples = d.sample(subkeys[1], sample_shape=(100_000,))\nax.hist(samples, bins=bins, density=True)\n\nax = axs[2]\nax.set_title(\"$P(\\\\phi\\\\mid \\\\mathbf{Y}=\\\\mathbf{x})$\")\n\nnuts_kernel = NUTS(new_model)\nmcmc = MCMC(nuts_kernel, num_warmup=1000, num_samples=5_000, num_chains=2)\nmcmc.run(subkeys[2], like=like, prior=prior)\nsamples = mcmc.get_samples()[\"phi\"]\nax.hist(samples, bins=bins, density=True, color=\"C4\")\n\nfig.tight_layout()\n\n\n/tmp/ipykernel_11590/3633385242.py:36: UserWarning: There are not enough devices to run parallel chains: expected 2 but got 1. Chains will be drawn sequentially. If you are running MCMC in CPU, consider using `numpyro.set_host_device_count(2)` at the beginning of your program. You can double-check how many devices are available in your system using `jax.local_device_count()`.\n  mcmc = MCMC(nuts_kernel, num_warmup=1000, num_samples=5_000, num_chains=2)\n  0%|          | 0/6000 [00:00&lt;?, ?it/s]warmup:   0%|          | 1/6000 [00:00&lt;1:28:40,  1.13it/s, 3 steps of size 8.10e+00. acc. prob=0.68]warmup:  11%|█         | 655/6000 [00:00&lt;00:05, 908.21it/s, 3 steps of size 1.76e+00. acc. prob=0.79]sample:  22%|██▏       | 1312/6000 [00:01&lt;00:02, 1844.48it/s, 7 steps of size 7.26e-01. acc. prob=0.93]sample:  33%|███▎      | 1972/6000 [00:01&lt;00:01, 2753.18it/s, 3 steps of size 7.26e-01. acc. prob=0.92]sample:  44%|████▍     | 2632/6000 [00:01&lt;00:00, 3577.19it/s, 3 steps of size 7.26e-01. acc. prob=0.92]sample:  55%|█████▍    | 3290/6000 [00:01&lt;00:00, 4280.01it/s, 3 steps of size 7.26e-01. acc. prob=0.92]sample:  66%|██████▌   | 3951/6000 [00:01&lt;00:00, 4863.02it/s, 3 steps of size 7.26e-01. acc. prob=0.92]sample:  77%|███████▋  | 4603/6000 [00:01&lt;00:00, 5298.99it/s, 3 steps of size 7.26e-01. acc. prob=0.92]sample:  88%|████████▊ | 5264/6000 [00:01&lt;00:00, 5655.87it/s, 3 steps of size 7.26e-01. acc. prob=0.92]sample:  99%|█████████▉| 5930/6000 [00:01&lt;00:00, 5936.81it/s, 3 steps of size 7.26e-01. acc. prob=0.92]sample: 100%|██████████| 6000/6000 [00:01&lt;00:00, 3335.62it/s, 3 steps of size 7.26e-01. acc. prob=0.92]\n  0%|          | 0/6000 [00:00&lt;?, ?it/s]warmup:  11%|█▏        | 677/6000 [00:00&lt;00:00, 6768.72it/s, 1 steps of size 5.60e-01. acc. prob=0.79]sample:  23%|██▎       | 1354/6000 [00:00&lt;00:00, 6768.16it/s, 3 steps of size 7.83e-01. acc. prob=0.92]sample:  34%|███▍      | 2034/6000 [00:00&lt;00:00, 6781.42it/s, 1 steps of size 7.83e-01. acc. prob=0.92]sample:  45%|████▌     | 2714/6000 [00:00&lt;00:00, 6787.05it/s, 7 steps of size 7.83e-01. acc. prob=0.92]sample:  57%|█████▋    | 3393/6000 [00:00&lt;00:00, 6778.94it/s, 3 steps of size 7.83e-01. acc. prob=0.92]sample:  68%|██████▊   | 4072/6000 [00:00&lt;00:00, 6782.54it/s, 3 steps of size 7.83e-01. acc. prob=0.92]sample:  79%|███████▉  | 4752/6000 [00:00&lt;00:00, 6786.99it/s, 7 steps of size 7.83e-01. acc. prob=0.92]sample:  91%|█████████ | 5431/6000 [00:00&lt;00:00, 6787.16it/s, 1 steps of size 7.83e-01. acc. prob=0.92]sample: 100%|██████████| 6000/6000 [00:00&lt;00:00, 6781.78it/s, 1 steps of size 7.83e-01. acc. prob=0.92]\n\n\n\n\n\nAs we expected, the posterior on \\(\\phi\\) is much more precise than posterior on \\(\\theta\\). It also is shifted towards the value \\(T=1\\), so (in this case) it behaves as sort of an approximation to \\(T\\)."
  },
  {
    "objectID": "posts/beta-bernoulli.html#which-model-is-better-for-denoising",
    "href": "posts/beta-bernoulli.html#which-model-is-better-for-denoising",
    "title": "Beta-Bernoulli distribution",
    "section": "Which model is better for denoising?",
    "text": "Which model is better for denoising?\nBoth models actually answer different questions: the first model tries to estimate \\(T\\), a single coin toss, and slightly updates the information about this coin bias, \\(\\theta\\).\nThe second model assumes independent coin tosses, where the bias is controlled by \\(\\phi\\). As such, it can quickly shrink the posterior on \\(\\phi\\). Moreover, it can be used to answer the question to impute individual coin tosses, \\(P(\\mathbf{U} \\mid \\mathbf{Y})\\).\nLet’s think what could happen if we fitted each model to the data generated from the other one: this is working with misspecified models (in the \\(\\mathcal M\\)-complete setting).\nConsider a setting where we have a lot of data, \\(N\\gg 1\\) and the false positive and false negative rates are small, with \\(c_0 \\ll c_1\\). If the data come from the second model, with individual variables \\(U_n\\sim \\mathrm{Bernoulli}(\\phi)\\), and we have \\(Y_n\\approx U_n\\), then the posterior on \\(T\\) will have most mass on the maximum likelihood solution: either \\(0\\) (which should happen for \\(\\phi \\ll 0.5\\)) or \\(1\\) (for \\(\\phi \\gg 0.5\\)). This model will be underfitting and the predictive distribution from this model will be quite bad: new \\(Y_{N+1}\\) would again be an approximation to \\(U_{N+1}\\), which is sampled from \\(\\mathrm{Bernoulli}(\\phi)\\), but the model would just return a noisy version of the inferred \\(T\\).\nOn the other hand, if we have a lot of data from the first model (with a single \\(T\\) variable) and we fit the second model, the posterior on \\(\\phi\\) may have most of the mass near \\(0\\) or \\(1\\), depending on the true value of \\(T\\). Hence, although \\(U_n\\sim \\mathrm{Bernoulli}(\\phi)\\) are sampled independently, once \\(\\phi\\) is near the true value of \\(T\\) (\\(0\\) or \\(1\\)), they can all be approximately equal to \\(T\\).\nSo, when there is a lot of data, noting where most of the mass of \\(\\phi\\) lies can be a good approximation to the maximum likelihood of \\(T\\).\nOf course, these \\(N\\gg 1\\) settings only tell what happens when we have a lot of data and we didn’t discuss the uncertainty: can we use \\(\\phi\\) to get well-calibrated uncertainty on \\(T\\)?\nI generally expect that the \\(\\phi\\) model can be a bit better in terms of handling slight misspecification, but doing inference directly on \\(T\\) will provide better results in terms of uncertainty quantification for small \\(N\\). But this is just a hypothesis: extensive simulations are not for today."
  },
  {
    "objectID": "posts/beta-bernoulli.html#appendix",
    "href": "posts/beta-bernoulli.html#appendix",
    "title": "Beta-Bernoulli distribution",
    "section": "Appendix",
    "text": "Appendix\n\nList of distributions\n\nBinomial distribution\nThe simplest choice: we have a coin with bias \\(b\\) and we toss it \\(N\\) times: \\[\\begin{align*}\n  X_n &\\sim \\mathrm{Bernoulli}(b) \\text{ for } n = 1, \\dotsc, N\\\\\n  S &= X_1 + \\cdots + X_N\n\\end{align*}\n\\]\nThen, we have \\(S \\sim \\mathrm{Binomial}(N, b)\\). As the individual throws are independent, it’s easy to prove that \\[\n\\mathbb E[S] = Nb, \\quad \\mathbb V[S] = Nb(1-b).\n\\]\n\n\nFinite mixture of binomial distributions\nAs above, consider \\(K\\) coins with biases \\(b_1, \\dotsc, b_K\\) and a dice used to choose the coin which will be tossed. This is a finite mixture of binomial distributions: \\[\\begin{align*}\n  D &\\sim \\mathrm{Categorical}(d_1, \\dotsc, d_K)\\\\\n  S \\mid D &\\sim \\mathrm{Binomial}(N, b_D)\n\\end{align*}\n\\]\nIn this case the expectation is exactly what one can expect: \\[\n\\mathbb E[S] = \\sum_{k=1}^K d_k \\cdot Nb_k = N\\bar b,\n\\] where \\(\\bar b = d_1b_1 + \\cdots + d_Kb_K\\).\nHowever, the formula for variance is more complex: from the law of total variance:\n\\[\\begin{align*}\n\\mathbb V[S] &= \\mathbb E[\\mathbb V[S\\mid B]] + \\mathbb V[ \\mathbb E[S\\mid B] ] \\\\\n&= \\sum_{k=1}^K d_k N b_k(1-b_k) + \\mathbb V[NB(1-B)]\n\\end{align*}\n\\]\n\n\nBeta-binomial distribution\nSimilarly as here, we can consider an infinite collection of coins, chosen from the beta distribution. Once we pick a coin, we toss it \\(N\\) times:\n\\[\\begin{align*}\n  B &\\sim \\mathrm{Beta}(\\alpha, \\beta)\\\\\n  S \\mid B &\\sim \\mathrm{Binomial}(N, B)\n\\end{align*}\n\\]\nThe marginal distribution is called the beta-binomial distribution: \\[\nS \\sim \\mathrm{BetaBinomial}(N, \\alpha, \\beta).\n\\]\nIt’s easy to prove that \\[\n\\mathbb E[S] = N \\frac{\\alpha}{\\alpha + \\beta},\n\\] but I don’t know an easy derivation of the formula for the variance: \\[\n\\mathbb V[S] = Nb(1-b)\\cdot (1 + (N-1)\\rho),\n\\] where \\(b=\\alpha/(\\alpha + \\beta)\\) and \\(\\rho=1/(1 + \\alpha + \\beta)\\).\nHence, choosing the coin first incurs additional variance (compared to the binomial distribution).\n\n\nPoisson binomial distribution\nThat was quite a few examples. Let’s do one more: the Poisson binomial distribution, because it is fun.\nIn this case one has \\(N\\) coins with biases \\(b_1, \\dotsc, b_N\\) and tosses each of them exactly once: \\[\\begin{align*}\n  X_n &\\sim \\mathrm{Bernoulli}(b_n) \\text{ for } n=1, \\dotsc, N\\\\\n  S &= X_1 + \\cdots + X_N.\n\\end{align*}\n\\]\nWe see that if all biases are equal, this reduces to the binomial distribution. However, this one is more flexible, as the expectation and variance are given now by \\[\n  \\mathbb E[S] = \\sum_{n=1}^N b_n, \\quad \\mathbb  V[S] = \\sum_{n=1}^N b_n(1-b_n).\n\\]\n\n\n\nBeta-Bernoulli sparsity magic\nConsider the following prior on coefficients in a linear model: \\[\\begin{align*}\n  \\gamma &\\sim \\mathrm{Beta}(\\alpha, \\beta)\\\\\n  \\theta_k \\mid \\gamma &\\sim \\gamma\\, Q_0 + (1-\\gamma)\\, Q_1 \\text{ for } k = 1, \\dotsc, K\n\\end{align*}\n\\]\nwhere \\(Q_1\\) is e.g., a \\(\\mathrm{Normal}(0, 10^2)\\) distribution corresponding to “slab” component and \\(Q_0\\), e.g., \\(\\mathrm{Normal}\\left(0, 0.01^2\\right)\\) is the “spike” component.\nIntuitively, we expect that fraction \\(\\gamma\\) of the parameters will be shrunk to \\(0\\) by the spike component \\(Q_0\\) and the rest (the \\(1-\\gamma\\) fraction) of the parameters will actually be used to predict values.\nMichael Betancourt wrote an amazing tutorial in which he introduces local latent variables, \\(\\lambda_k\\), that control whether \\(\\theta_k\\) should be shrunk or not:\n\\[\\begin{align*}\n  \\lambda_k &\\sim \\mathrm{Beta}(\\alpha, \\beta) \\text{ for } k = 1, \\dotsc, K\\\\\n  \\theta_k \\mid \\lambda_k &\\sim \\lambda_k \\, Q_0 + (1-\\lambda_k)\\, Q_1 \\text{ for } k = 1, \\dotsc, K.\n\\end{align*}\n\\]\nUsing small letters for PDFs, we can marginalize variables \\(\\lambda_k\\) as follows: \\[\n  p(\\mathbf{\\theta}) = \\prod_{k=1}^K p(\\theta_k) = \\prod_{k=1}^K \\left( \\int p(\\theta_k \\mid \\lambda_k) \\, \\mathrm{d}P(\\lambda_k) \\right)\n\\] and \\[\\begin{align*}\n  p(\\theta_k) &= \\int p(\\theta_k \\mid \\lambda_k) \\, \\mathrm{d}P(\\lambda_k) \\\\\n  &= q_0(\\theta_k) \\int \\lambda_k\\, \\mathrm{Beta}(\\lambda_k \\mid \\alpha, \\beta) \\, \\mathrm{d}\\lambda_k + q_1(\\theta_k) \\int (1-\\lambda_k)\\, \\mathrm{Beta}(\\lambda_k \\mid \\alpha, \\beta)\\, \\mathrm{d} \\lambda_k \\\\\n  &= q_0(\\theta_k) \\frac{\\alpha}{\\alpha + \\beta} + q_1(\\theta_k) \\left( 1 - \\frac{\\alpha}{\\alpha + \\beta} \\right),\n\\end{align*}\n\\] so that \\[\n  \\theta_k \\sim \\gamma\\, Q_0 + (1-\\gamma)\\, Q_1,\n\\]\nwhere \\(\\gamma = \\alpha / (\\alpha + \\beta)\\).\nBeta-Bernoulli distribution offers the following perspective: draw latent indicator variables \\(T_k \\mid \\lambda_k \\sim \\mathrm{Bernoulli}(\\lambda_k)\\), so that \\(\\theta_k \\mid T_k \\sim T_k\\, Q_0 + (1-T_k) \\, Q_1\\).\nWe recognize that \\(T_k \\sim \\mathrm{BetaBernoulli}(\\alpha, \\beta)\\) which is just \\(\\mathrm{Bernoulli}(\\gamma)\\) for \\(\\gamma =\\alpha/(\\alpha+\\beta)\\). By integrating out \\(T_k\\) variables (which is just trivial summation!), we have \\[\n  \\theta_k \\sim \\gamma\\, Q_0 + (1-\\gamma)\\, Q_1.\n\\]"
  },
  {
    "objectID": "posts/bernoulli-inequality.html",
    "href": "posts/bernoulli-inequality.html",
    "title": "Bernoulli’s inequality",
    "section": "",
    "text": "Bernoulli trials appeared on the blog several times, for example here or there. Today we are going to do something slightly different.\nBernoulli’s inequality is one of the classic inequalities encountered during maths classes to teach mathematical induction. Namely, if \\(n\\ge 1\\) is an integer and \\(x\\ge -1\\), then \\[\n(1+x)^n \\ge 1 + nx.\n\\]\nThe inequality is strict for \\(n\\ge 2\\) and \\(x\\neq 0\\). The proof by induction is simple, where the inductive step is based on \\[\\begin{align*}\n  (1 + x)^{n+1}\n  &= (1+x) \\cdot (1+x)^n \\\\\n  &\\ge (1+x)(1+nx) \\\\\n  &= 1 + (n+1)x + nx^2.\n\\end{align*}\n\\]\nIn fact, this is not the strongest form of the inequality, with the Wikipedia article listing many variants of it.\nToday I wanted to discuss a few applications of this inequality."
  },
  {
    "objectID": "posts/bernoulli-inequality.html#probability-of-no-event-occurring",
    "href": "posts/bernoulli-inequality.html#probability-of-no-event-occurring",
    "title": "Bernoulli’s inequality",
    "section": "Probability of (no) event occurring",
    "text": "Probability of (no) event occurring\nImagine that we observe a process which fails with probability \\(p\\) many times, say \\(n\\). If the failures occur independently, what is the probability of having only successes? The probability of having only successes is \\((1 - p)^n \\ge 1 - pn\\). The probability of observing at least one failure is then \\(1 - (1-p)^n \\le np\\). We can therefore bound the probability of failures by controlling \\(p\\).\nIn fact, we can say more, as explained here. Because of the Bernoulli’s inequality, we have \\[\n  (1 + x/n)^n \\ge 1 + x,\n\\] which gives (by passing to the limit \\(n\\to \\infty\\)) the inequality \\(e^x\\ge 1 + x\\) (note that for \\(x &lt; -1\\) it, in fact, also holds, as the right-hand-side is negative. To prove that the left-hand-side is positive, one can use Archimedean property of real numbers to show that for all large enough \\(n\\) we have \\(x / n &gt; -1\\)).\nThen, \\[\n  e^{-pn} = ( e^{-p} )^n \\ge (1 - p)^n \\ge 1 - pn,\n\\]\nwhich bounds \\((1-p)^n\\) from both sides.\n\nMonte Carlo estimators\nThis shows why Monte Carlo methods used to calculate volumes can fail in high dimensions. For example, the volume of a unit \\(m\\)-ball, \\(D_m = \\{ ||x|| &lt; 1 \\mid x \\in [-1, 1]^m \\}\\) is given by \\[\nV_m = \\begin{cases}\n  2 & \\text{ if } m = 1\\\\\n  \\pi &\\text { if } m = 2\\\\\n  \\frac{2\\pi}{m} V_{m-2} &\\text{ otherwise}\n\\end{cases}\n\\]\nIf we sample from the uniform distribution over the cube \\([-1, 1]^m\\), the probability of a sample landing inside the ball is \\(p = V_m / 2^m\\), which is tiny for large \\(m\\).\nIf we collect \\(n\\) Monte Carlo samples, the probability that we see any sample inside the ball (and our estimate is not \\(0\\)) is then smaller than \\(pm\\). In other words, in \\(m=100\\) dimensions even if we collect million of samples, there is a very high probability that we return \\(0\\) (so that the relative error of the Monte Carlo estimate is then 100%).\n\n\nPopoviciu’s inequality\nWe see that with large probability our estimate for \\(p\\) (and, in turn \\(V_m\\)) is \\(0\\). The relative error is hence 100% with large probability. However, what can we say about the absolute error? In this case \\(p\\) is very tiny, so that returning \\(0\\) perhaps is not bad. The variance of the Monte Carlo estimator for \\(n\\) samples is \\(p(1-p) / n\\). Note that \\[\n  \\sqrt{p(1-p)} \\le \\frac{p + (1-p)}{2} = 1/2,\n\\]\nso that the variance of the Monte Carlo estimator is bounded from above by \\(1/4n\\). Of course, it can still be unacceptably large to obtain small relative errors, but this gives us some assurance about the absolute error.\nMore generally, Popoviciu’s inequality can be used here. It says that for a real-valued random variable bounded between \\([L, U]\\), its variance cannot exceed \\((U-L)^2 / 4\\).\nAs the variance does not change when the variable is shifted, we can move the variable to \\([0, U-L]\\), and then rescale it by \\(U - L\\) to the interval \\([0, 1]\\) (which scales the variance quadratically). Hence, it suffices to prove that for a random variable defined on the interval \\([0, 1]\\), its variance cannot exceed \\(1/4\\) (which is the value attained for the Bernoulli random variable, assigning all the probability mass equally to the boundary of the interval).\nUsing the proof of the Bhatia-Davis inequality, we get\n\\[\n  0\\le \\mathbb E[ X (1-X) ] = \\mathbb E[X] - \\mathbb E[X^2].\n\\]\nWe see that \\(\\mathbb E[X^2] \\le \\mathbb E[X]\\). Let \\(p = \\mathbb E[X] \\in [0, 1]\\). Consequently, we have \\[\n  \\mathbb E[X^2] - \\mathbb E[X]^2 \\le p - p^2 = p(1-p) \\le 1/4  \n\\]\nas above."
  },
  {
    "objectID": "posts/bernoulli-inequality.html#generalized-inequality",
    "href": "posts/bernoulli-inequality.html#generalized-inequality",
    "title": "Bernoulli’s inequality",
    "section": "Generalized inequality",
    "text": "Generalized inequality\nSimilarly by induction, it is possible to prove that for \\(n\\) numbers \\(x_1, \\dotsc, x_n\\) such that:\n\nall \\(x_i &gt; -1\\),\nall \\(x_i\\) are of the same sign,\n\nwe have\n\\[\n(1+x_1)(1+x_2) \\cdots (1+x_n) \\ge 1 + x_1 + x_2 +\\cdots + x_n.\n\\]\nOf course, the original inequality is obtained from the case \\(x_1 = x_2 = \\cdots = x_n\\).\nI think this is an important inequality.\n\nThe variance of the pointwise mutual information profile\nIn our pointwise mutual information paper, we wanted to prove the bounds on the variance of the pointwise mutual information profile for the multivariate normal distribution, under the constraint that the mutual information is fixed at some pre-specified value.\nWe had to prove an inequality: \\[\na_1 a_2\\cdots a_n + (n-1) \\ge a_1 + a_2 + \\cdots + a_n\n\\]\nfor any numbers \\(a_i \\in (0, 1]\\).\nAs I did not know the generalized form of Bernoulli’s inequality back then, the proof in the appendix was done by induction. However, this could have been done in (essentially) one line!\nWrite \\(x_i = a_i - 1\\), so that we have \\(x_i \\in (-1, 0]\\). All the numbers are larger than \\(-1\\) and of the same sign. The Bernoulli’s inequality gives then:\n\\[\\begin{align*}\na_1 a_2\\cdots a_n\n&= (1 + x_1)(1+x_2)\\cdots (1+x_n) \\\\\n&\\ge 1 + x_1 + x_2 + \\cdots + x_n \\\\\n&=  1 + (a_1 - 1) + (a_2 - 1) + \\cdots + (a_n - 1) \\\\\n&= a_1 + a_2 + \\cdots + a_n - (n - 1).\n\\end{align*}\n\\]\n\n\nEvents (not) occurring (again)\nLet \\(y_i = \\mathbb P(Y_i)\\) for some events \\(Y_1, \\dotsc, Y_n\\). All the numbers are in the set \\([0, 1]\\), so that \\(x_i := -y_i \\in [-1, 0]\\). The Bernoulli’s inequality says then that\n\\[\n  \\prod_{i=1}^n (1 - y_i) \\ge 1 - \\sum_{i} y_i,\n\\]\nwhich is \\[\n  \\prod_{i=1}^n \\mathbb P( Y_i' ) \\ge 1 - \\sum_{i=1}^n \\mathbb P(Y_i).\n\\]\nwhere \\(Y'\\) is the complement of the event \\(Y\\). Swapping \\(Y_i\\) with \\(Y_i'\\) gives also \\[\n  \\prod_{i=1}^n \\mathbb P( Y_i ) \\ge 1 - \\sum_{i=1}^n \\mathbb P(Y_i').\n\\]\nThis inequality has an easy interpretation when all \\(Y_i\\) are independent. But to see it, we have to discuss Boole’s inequality, which says that for all events \\(Y_1, \\dots, Y_n\\) we have\n\\[\n\\mathbb P\\!\\left(\\bigcup_{i=1}^n Y_i\\right) \\le \\sum_{i=1}^n \\mathbb P(Y_i).\n\\]\n(In fact, here we can event take countably many events, as measures are \\(\\sigma\\)-sub-additive. But let’s talk about the finite case here.)\nIf \\(Y_i\\) are all independent, the inequality we have seen above reduces to \\[\n  \\prod_{i=1}^n \\mathbb P(Y_i) = \\mathbb P\\!\\left(\\bigcap_{i=1}^n Y_i\\right) = 1 - \\mathbb P\\!\\left( \\bigcup_{i=1}^n Y_i' \\right) \\ge 1 - \\sum_{i=1}^n \\mathbb P(Y_i').\n\\]"
  },
  {
    "objectID": "posts/dirichlet-process.html",
    "href": "posts/dirichlet-process.html",
    "title": "The Dirichlet process",
    "section": "",
    "text": "In this post we will quickly review different constructions of the Dirichlet process, following Teh et al. (2006) and Gelman et al. (2013, chap. 23)."
  },
  {
    "objectID": "posts/dirichlet-process.html#finite-dimensional-dirichlet-prior",
    "href": "posts/dirichlet-process.html#finite-dimensional-dirichlet-prior",
    "title": "The Dirichlet process",
    "section": "Finite-dimensional Dirichlet prior",
    "text": "Finite-dimensional Dirichlet prior\nConsider the simplest Gaussian mixture model: there are several normal distributions with unit variance \\(\\mathcal N(\\mu_k, 1)\\) for \\(k\\in \\{1, \\dotsc, K\\}\\) and mixture proportions vector \\(\\pi = (\\pi_1, \\dotsc, \\pi_K)\\) with \\(\\pi_k\\ge 0\\) and \\(\\sum_k \\pi_k=1\\).\nA convenient prior for \\(\\pi\\) is the Dirichlet distribution. We put some \\(F_0\\) prior on the parameters \\(\\{\\mu_k\\}\\) of the model, so the generative process looks like: \\[\\begin{align*}\n  \\pi \\mid \\alpha &\\sim \\mathrm{Dirichlet}(\\alpha_1, \\dotsc, \\alpha_K)\\\\\n  \\mu_k \\mid F_0 &\\sim F_0, & k=1, \\dotsc, K\\\\\n  Z_n \\mid \\pi &\\sim \\mathrm{Categorical}(\\pi_1, \\dotsc, \\pi_K), & n=1, \\dotsc, N\\\\\n  X_n\\mid Z_n=z_n, \\{\\mu_k\\} &\\sim \\mathcal N(\\mu_{z_n}, 1),\\quad & n=1, \\dotsc, N.\n\\end{align*}\\]\n\nAnother point of view\nRather than using individual random variables \\(Z_n\\) and a shared set of parameters \\(\\{\\mu_k\\}\\) we could reparametrize the model to use individual means \\(\\tilde \\mu_n = \\mu_{Z_n}\\). In other words, we could consider a probability measure with atoms ${_k}$ given by \\[F = \\sum_{k=1}^K \\pi_k \\delta_{\\mu_k}.\\]\nIf we only know the Dirichlet weights vector \\((\\alpha_1, \\dotsc, \\alpha_K)\\) and the base measure \\(F_0\\) we can think of \\(F\\) as of a random probability measure generated according to \\[\\begin{align*}\n  \\pi \\mid \\alpha &\\sim \\mathrm{Dirichlet}(\\alpha_1, \\dotsc, \\alpha_K)\\\\\n  \\mu_k &\\sim F_0, \\quad k = 1, \\dotsc, K\\\\\n  F &:= \\sum_{k=1}^K \\pi_k \\delta_{\\mu_k}.\n\\end{align*}\\]\nThen sampling individual data points amounts to the following model with \\(n=1, \\dotsc, N\\): \\[\\begin{align*}\n  F\\mid \\alpha, F_0 &\\sim \\text{the procedure above}\\\\\n  \\theta_n \\mid F &\\sim F, \\\\\n  X_n\\mid \\theta_n &\\sim \\mathcal N(\\theta_n, 1).\n\\end{align*}\\]\nNote that the values of \\(\\theta_n\\) come from the set \\(\\{\\mu_1, \\dotsc, \\mu_K\\}\\) as \\(F\\) is atomic."
  },
  {
    "objectID": "posts/dirichlet-process.html#dirichlet-process-prior",
    "href": "posts/dirichlet-process.html#dirichlet-process-prior",
    "title": "The Dirichlet process",
    "section": "Dirichlet process prior",
    "text": "Dirichlet process prior\n\nStick-breaking construction\nWith the following example in mind we will pass now to a general distribution \\(F_0\\) defined over some infinite space \\(\\mathcal M\\) (which can be \\(\\mathbb R\\) as above) and a single positive parameter \\(\\alpha &gt; 0\\).\nWe will generate a random measure \\(F\\) from \\(F_0\\) using the construction known as the Dirichlet process.\nSample for \\(k=1, 2, \\dotsc\\) \\[\\begin{align*}\nv_k \\mid \\alpha &\\sim \\mathrm{Beta}(1, \\alpha)\\\\\n\\mu_k \\mid F_0 &\\sim F_0\n\\end{align*}\\] and define \\[\\begin{align*}\n  p_1 &= v_1\\\\\n  p_k &= v_k \\prod_{i=1}^{k-1} (1-v_k) \\quad \\text{for } k\\ge 2,\\\\\n  F &= \\sum_{k=1}^\\infty p_k \\delta_{\\mu_k}\n\\end{align*}\\]\nWith probability 1 it holds that \\[\\sum_{k=1}^\\infty  p_k = 1,\\] i.e., \\((p_k)\\) is a valid proportions vector.\nWe say that the distribution \\(F\\) was drawn from the Dirichlet process: \\[F \\sim \\mathrm{DP}(\\alpha, F_0).\\]\n\n\nInfinite limit\nThe atomic distributions generated with finite-dimensional proportions \\((\\pi_k)_{k=1, 2, \\dotsc, K}\\) and infinite sequence of weights \\((p_k)_{k=1, 2, \\dotsc, \\infty}\\) look optically similar. There is a close relation between these two generative processes.\nConsider a random measure \\(F^K\\) defined using a symmetric Dirichlet distribution: \\[\\begin{align*}\n  \\pi^K \\mid \\alpha &\\sim \\mathrm{Dirichlet}(\\alpha/K, \\cdots, \\alpha/K)\\\\\n  \\mu^K_k \\mid F_0 &\\sim F_0\\\\\n  F^K &= \\sum_{k=1}^K \\pi^K_k\\delta_{\\mu^K_k}\n\\end{align*}\\]\nNow if \\(F^{\\infty} \\sim \\mathrm{DP}(\\alpha, F_0)\\) and \\(u\\) is any measurable function integrable with respect to \\(F_0\\), then the sequence of random variables \\[ \\int_{\\mathcal M} u\\, \\mathrm{d} F^{K} \\] converges in distribution (that is, weakly) to \\[ \\int_{\\mathcal M} u\\, \\mathrm{d} F^{\\infty}.\\]\n\nWhere the difference really is\nWe see that \\((p_k)\\) looks deceptively similar as \\((\\pi_k^K)\\) for large \\(K\\). There are some differences, though. First of all, \\((p_k)\\) is infinite and the number of atoms appearing in the analysis of a particular data set is implicitly controlled by the number of data points. If \\(F_0\\) is non-atomic,one can expect \\(O(\\alpha\\log N)\\) atoms in a data set with \\(N\\) points. In the finite-dimensional case more than \\(K\\) clusters are impossible.\nHowever, for \\(K\\gg N\\) it’s natural to expect that several entries from \\((\\pi^K_k)\\) should be matching several entries of \\((p_k)\\). However, the intuition that \\(p_1 = \\pi_1^K\\), \\(p_2 = \\pi_2^K\\), … is wrong. In the stick-breaking construction of the Dirichlet process we expect the first few entries to have the most of the mass, while in the finite-dimensional case the Dirichlet prior is symmetric — we don’t know which weights \\(\\pi_k^K\\) will have vanishing mass.\nAlthough it seems obvious I spent quite some time trying to understand why the stick-breaking sampling procedure from the Dirichlet distribution gives different results!\nThe stick-breaking sampling procedure for the \\(\\mathrm{Dirichlet}(\\alpha/K, \\dotsc, \\alpha/K)\\) distribution works as follows: \\[\\begin{align*}\n  u_k &\\sim \\mathrm{Beta}( \\alpha/K, \\alpha\\cdot (1-k/K) )\\\\\n  \\pi_1 &= u_1\\\\\n  \\pi_k &= u_k \\prod_{j &lt; k} (1-u_k), \\quad k = 2, \\dotsc, K-1\\\\\n  \\pi_K &= 1 - (\\pi_1 + \\dotsc + \\pi_{K-1})\n\\end{align*}\\]\nwhich for \\(k \\ll K\\) corresponds to sampling from (approximately) \\(\\mathrm{Beta}(\\alpha/K, \\alpha)\\), rather than \\(\\mathrm{Beta}(1, \\alpha)\\).\nPitman (1996) describes size-biased permutations, which perhaps can be used to establish link between \\((\\pi_k)\\) for large \\(K\\) and \\((p_k)\\), but I haven’t understood it yet.\n\n\n\nDefining property\nWe have seen in what sense the Dirichlet process prior can be thought as of an infinite-dimensional generalization of the Dirichlet prior. However, there is another link.\nRecall that the defining property of a Gaussian process is that it is a continuous-time stochastic process \\(\\{X_t\\}_{t\\in [0, 1]}\\) such that for every finite set of indices \\(t_1, t_2, \\dotsc, t_m\\) random vector \\((X_{t_1}, \\dotsc, X_{t_m})\\) is distributed according to multivariate normal distribution. (In particular every \\(X_t\\) is a normal random variable). While this defining property is not sufficient without a proof of existence (e.g., an explicit construction), it is useful in many calculations involving them.\nWe will now give the defining property of the Dirichlet process. Take a probability measure \\(F_0\\) over \\(\\mathcal M\\) and the concentration parameter \\(\\alpha &gt; 0\\). We say that \\(\\mathrm{DP}(\\alpha, F_0)\\) is a Dirichlet process if every sample \\(F\\sim \\mathrm{DP}(\\alpha, F_0)\\) is a probability measure over \\(\\mathcal M\\) such that for every partition \\(A_1, \\cdots, A_m\\) of \\(\\mathcal M\\) the following holds: \\[ \\left( F(A_1), \\dotsc, F(A_K) \\right) \\sim \\mathrm{Dirichlet}\\big(\\alpha F_0(A_1), \\dotsc, \\alpha F_0(A_K) \\big) \\]\nIn particular if \\(A\\subseteq \\mathcal X\\) is any measurable subset, then we can use the partition \\(\\{A, \\mathcal M\\setminus A\\}\\) to get \\[ F(A) \\sim \\mathrm{Beta}\\big( \\alpha F_0(A), \\alpha(1-F_0(A)) \\big),\\] so that \\[\\mathbb E[ F(A) ] = F_0(A)\\] and \\[\\mathrm{Var}[F(A)] = \\frac{ F_0(A)\\cdot (1-F_0(A)) }{1+\\alpha}\\]\nHence, each draw \\(F\\) is centered around \\(F_0\\) and the variance is small for large parameter values \\(\\alpha\\).\n\n\nPólya urn scheme\nFinally, we give an interpretation in terms of Pólya urn scheme.\nAbove we considered the sampling process from the finite-dimensional Dirichlet distribution: \\[\\begin{align*}\n  F\\mid \\alpha, F_0 &\\sim \\text{construct atomic measure},\\\\\n  \\theta_n \\mid F &\\sim F,\n\\end{align*}\\] where each of the \\(\\theta_n\\) was actually some atom of the distribution \\(\\mu_k\\).\nThis interpretation is also easy to understand when the atomic measure \\(F\\) is drawn from the Dirichlet process using the stick-breaking construction.\nConsider now a sampling procedure of \\(\\theta_n\\) where we do not have direct access to \\(F\\), but only to the distribution \\(F_0\\), concentration parameter \\(\\alpha &gt; 0\\) and previous draws \\(\\theta_1, \\dotsc, \\theta_{n-1}\\). It holds that \\[\\theta_n \\mid \\alpha, F_0, \\theta_1, \\dotsc, \\theta_{n-1} \\sim \\frac{\\alpha}{ (n-1) + \\alpha }F_0 + \\sum_{u=1}^{n-1} \\frac{1}{(n-1)+\\alpha}\\delta_{ \\theta_n }.\\]\nIf \\(\\alpha\\) is a positive integer we can interpret this sampling procedure as follows: we want to draw the \\(n\\)th ball and we have an urn with \\(\\alpha\\) transparent balls and \\(n-1\\) balls of different colors. We draw a random ball. If it is transparent, we use \\(G_0\\) to sample a colored ball from \\(F_0\\), note it down, and put it to the urn.\nThis also suggests a clustering property: if there is a color \\(\\mu_k\\) such that there are already \\(m_k\\) balls inside the urn (i.e., \\(m_k\\) is the number of indices \\(1\\le i\\le n-1\\) such that \\(\\theta_i = \\mu_k\\)), then we have a larger chance to draw a ball of this color: \\[\\theta_n \\mid \\alpha, F_0, \\theta_1, \\dotsc, \\theta_{n-1} \\sim \\frac{\\alpha}{(n-1) + \\alpha}F_0 + \\sum_{k} \\frac{m_k}{ (n-1) + \\alpha } \\delta_{ \\mu_k }.\\]\nWe also see that for the concentration parameter \\(\\alpha \\gg n\\) this sampling procedure approximates independent sampling from \\(F_0\\).\n\nAsymptotic number of clusters\nThe above formulation can be used to argue why the number of clusters grows as \\(O(\\alpha\\log n)\\) if \\(F_0\\) is non-atomic. Define \\(D_1 = 1\\) and for \\(n\\ge 1\\) \\[D_n = \\begin{cases} 1 &\\text{ if } \\theta_n \\notin \\{\\theta_1, \\dotsc, \\theta_{n-1}\\}\\\\\n0 &\\text{otherwise}\\end{cases}\\] From the above construction we know the probability of drawing a new atom, so \\[\\mathbb E[D_n] = \\alpha / (\\alpha + n-1)\\] The number of distinct atoms in \\(F\\) is then \\[\\mathbb E[C_n] = \\mathbb E[D_1 + \\dotsc + D_n] = \\alpha \\sum_{i=1}^n \\frac{1}{\\alpha + n - 1}.\\] We recognise that this sum is similar to the harmonic series and (this can be proven formally) also grows as \\(O(\\log n)\\), so that \\(\\mathbb E[C_n] = O(\\alpha\\log n)\\). To provide a more precise result: \\[\\lim_{n\\to\\infty}\\frac{ \\mathbb E[C_n] }{\\alpha \\log n} = 1.\\] For this and related results consult these notes.\n\n\n\nChinese restaurant process\nThe procedure above is also closely related to the Chinese restaurant process, where the metaphor is that there are \\(K\\) occupied tables (where a dish \\(\\mu_k\\) is served) and there are \\(m_k\\) people sitting around the \\(k\\)th table. When a new customer enters the restaurant, they can either join an existing table (with probability proportional to \\(m_k\\)) or start a new (\\((K+1)\\)th) table with probability proportional to \\(\\alpha\\), where a new dish \\(\\mu_{K+1}\\sim F_0\\) will be served."
  },
  {
    "objectID": "posts/dirichlet-process.html#afterword",
    "href": "posts/dirichlet-process.html#afterword",
    "title": "The Dirichlet process",
    "section": "Afterword",
    "text": "Afterword\nThe Dirichlet process is a useful construction, which can be used as a nonparametric prior in clustering problems — instead of specifying a fixed number of clusters one can specify the growth rate via \\(\\alpha\\).\nIn practice the results (including the number of inferred clusters in a particular data set) need to be treated with caution: the clusters found in the data set do not need to have the “real world” meaning (or perhaps “clusters” are a wrong abstration at all, with heterogeneity attributable e.g., to some continuous covariates which could be measured). Careful validation is often needed, epsecially that these models may be non-robust to misspecification (see this paper on coarsening) or the inferences may be hard to do (see this overview of their intrinsic non-identifiability).\nAnyway, although difficult, clustering can provide useful information about a given problem, so we often need do it. For example, take a look at this application of Chinese restaurant process to the clustering of single-cell DNA profiles."
  },
  {
    "objectID": "posts/non-reversible-parallel-tempering.html",
    "href": "posts/non-reversible-parallel-tempering.html",
    "title": "Non-reversible parallel tempering",
    "section": "",
    "text": "I have revived my interest in Ising models, which can be now trained using discrete Fisher divergence. However, once the model is trained, I would like to generate synthetic samples and evaluate the quality of the fit.\nThe standard solution for sampling from such distributions is our usual suspect, Markov chain Monte Carlo (MCMC). However, MCMC can get “trapped” in a single mode of the distribution and never escape it within the (finite) simulation time. Ising models are somewhat hard to visualise, so let’s focus on some one-dimensional problem and the simple Metropolis algorithm with Gaussian random walk proposals, namely \\(q(x'\\mid x) = \\mathcal N\\!\\left(x' \\mid x, \\sigma^2\\right)\\).\nWe can use MCMC to obtain samples for the following problems:\nCode\nimport numpy as np\n\nimport jax\nimport jax.random as jrandom\nimport jax.numpy as jnp\nfrom jaxtyping import Float, Int, Array\n\nimport numpyro\nimport numpyro.distributions as dist\n\nimport matplotlib.pyplot as plt\nplt.style.use(\"dark_background\")\n\nfrom typing import Any\nfrom collections import OrderedDict\n\nRandomKey = jax.Array\nKernel = callable  # kernel(key, x) -&gt; new_x\nKernelParam = Any\n\nclass JAXRNG:\n    \"\"\"JAX stateful random number generator.\n\n    Example:\n      key = jax.random.PRNGKey(5)\n      rng = JAXRNG(key)\n      a = jax.random.bernoulli(rng.key, shape=(10,))\n      b = jax.random.bernoulli(rng.key, shape=(10,))\n    \"\"\"\n\n    def __init__(self, key: RandomKey) -&gt; None:\n        \"\"\"\n        Args:\n            key: initialization key\n        \"\"\"\n        self._key = key\n\n    @property\n    def key(self) -&gt; RandomKey:\n        \"\"\"Generates a new key.\"\"\"\n        key, subkey = jax.random.split(self._key)\n        self._key = key\n        return subkey\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Used by the repr() method.\"\"\"\n        return f\"{type(self).__name__}(key={self._key})\"\n\n    def __str__(self) -&gt; str:\n        \"\"\"Used by the str() method.\"\"\"\n        return repr(self)\n\n\ndef mcmc_sampling_loop(\n  key: RandomKey,\n  x0: Float[Array, \" *dim\"],\n  kernel: Kernel,\n  n_samples: int,\n  warmup: int = 2_000,\n) -&gt; Float[Array, \"n_samples *dim\"]:\n  \"\"\"Markov chain Monte Carlo sampling loop.\"\"\"\n  def f(x, subkey):\n    x_new = kernel(subkey, x)\n    return x_new, x_new\n  \n  key_warmup, key_sampling = jrandom.split(key)\n  # Run warm-up: update the starting point, but do not collect samples:\n  x0, _ = jax.lax.scan(f, x0, jrandom.split(key_warmup, warmup))\n  # Collect the samples:\n  _, samples = jax.lax.scan(f, x0, jrandom.split(key_sampling, n_samples))\n  \n  return samples\n\n\ndef sample_exact(\n  key: RandomKey,\n  distributions: OrderedDict,\n  n_samples: int,\n) -&gt; Float[Array, \"n_distributions n_samples\"]:\n  \"\"\"Samples from the ground-truth distributions using (exact) ancestral sampling in NumPyro.\n  \n  Args:\n    key: JAX random key\n    distributions: an ordered dictionary mapping names to distribution factories. For example, `OrderedDict({\"name\": factory})`, where `factory()` returns a NumPyro distribution.\n    n_samples: number of samples to collect  \n  \"\"\"\n  xs_all = np.empty((len(distributions), n_samples))\n\n  for idx, (_, dist_factory) in enumerate(distributions.items()):\n    key, subkey = jrandom.split(key)\n    distrib = dist_factory()\n    xs = distrib.sample(subkey, sample_shape=(n_samples,))\n    \n    xs_all[idx, :] = np.array(xs)\n\n  return xs_all\n\n\ndef make_multirun(\n  key: RandomKey,\n  sampling_fn,\n  distributions: OrderedDict,\n  params: list,\n) -&gt; Float[Array, \"params distributions *samples\"]:\n  \"\"\"This function applies a sampling function over all distributions and parameters.\n\n  Args:\n    sampling_fn: function used to provide samples. It has the signature\n        sampling_fn(key, log_prob, param) -&gt; Float[Array, \" *samples\"]\n      where \"*samples\" encodes all the dimension of the sample\n  \"\"\"\n  all_samples = []\n  for param in params:\n    samples_param = []\n    for _, dist_factory in distributions.items():\n    # Define log-PDF\n      def log_p(x):\n        distribution = dist_factory()\n        return distribution.log_prob(x)\n      \n      key, subkey = jrandom.split(key)\n\n      samples = np.array(sampling_fn(key, log_p, param))\n      # Append the samples\n      samples_param.append(samples)\n    # Now add the row\n    all_samples.append(np.array(samples_param))\n\n  return np.array(all_samples)\n\n\ndef mcmc_multirun(\n  key: RandomKey,\n  kernel_generator,\n  distributions: OrderedDict,\n  params: list[KernelParam],\n  n_samples: int,\n  warmup: int,\n  x0: float = 0.5\n) -&gt; Float[Array, \"params distributions n_samples\"]:\n  \"\"\"A high-level function running an array of MCMC samplers\n  over different distributions and parameter settings.\n  \"\"\"\n  def sampling_fn(key, log_prob, param):\n    kernel = kernel_generator(log_prob, param)\n    return mcmc_sampling_loop(\n      key=key, x0=jnp.asarray(x0), kernel=kernel, n_samples=n_samples, warmup=warmup\n    )\n\n  return make_multirun(\n    key=key, sampling_fn=sampling_fn, distributions=distributions, params=params\n  )\n\n\n/home/pawel/micromamba/envs/data-science/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\nCode\ndef make_multipanel_figure(\n  plotting_fn,\n  distributions: OrderedDict,\n  params: list[KernelParam],\n  samples_exact: Float[Array, \"distributions n_samples_exact\"] | None,\n  samples_mcmc: Float[Array, \"params distributions n_samples_mcmc\"],\n  n_spines: int = 1,\n  sharex=\"col\",\n  dpi=350,\n  param_name = \"\",\n):\n  def _param_to_str(p):\n    if isinstance(p, float):\n      return f\"{p:.1f}\"\n    else:\n      return str(p)\n\n  fig, axs = plt.subplots(\n    len(params),\n    len(distributions),\n    sharex=sharex,\n    dpi=dpi,\n  )\n\n  for param_idx, param_value in enumerate(params):\n    if param_name is None:\n      axs[param_idx, 0].set_ylabel(_param_to_str(param_value))\n    else:\n      axs[param_idx, 0].set_ylabel(f\"{param_name}={_param_to_str(param_value)}\")\n    \n    for dist_idx, (dist_name, _) in enumerate(DISTRIBUTIONS.items()):\n      ax = axs[param_idx, dist_idx]\n\n      if param_idx == 0:\n        ax.set_title(dist_name)\n\n      if samples_exact is None:\n        se = None\n      else:\n        se = samples_exact[dist_idx]\n\n      plotting_fn(\n        ax,\n        se,\n        samples_mcmc[param_idx, dist_idx]\n      )\n\n  for ax in axs.ravel():\n    if n_spines == 1:\n      ax.spines[[\"top\", \"left\", \"right\"]].set_visible(False)\n      ax.set_yticks([])\n    elif n_spines == 2:\n      ax.spines[[\"top\", \"right\"]].set_visible(False)\n\n  fig.tight_layout()\n\n\ndef plot_hist_panel_function(\n  ax: plt.Axes,\n  exact_samples: Float[Array, \" n_samples_exact\"],\n  mcmc_samples: Float[Array, \" n_samples_mcmc\"],\n  bins: int = 30\n):\n  ax.hist(exact_samples, density=True, bins=bins, histtype=\"step\", color=\"white\")\n  ax.hist(mcmc_samples, density=True, bins=bins, histtype=\"stepfilled\", color=\"C3\", alpha=0.4)\n\n\ndef plot_trace_panel_function(\n  ax: plt.Axes,\n  exact_samples: Float[Array, \" n_samples_exact\"],\n  mcmc_samples: Float[Array, \" n_samples_mcmc\"],\n):\n  ax.plot(mcmc_samples, color=\"C3\")\nCode\ndef mixture2_dist() -&gt; dist.Distribution:\n  p = 0.3\n  probs = jnp.asarray([p, 1.0 - p])\n  sep = 25\n  mus = jnp.asarray([-sep, sep], dtype=float)\n  mixing = dist.Categorical(probs=probs)\n  return dist.MixtureSameFamily(\n    mixing_distribution=mixing,\n    component_distribution=dist.Normal(mus, 1.0),\n  )\n\n\ndef mixture3_dist() -&gt; dist.Distribution:\n  probs = jnp.asarray([0.15, 0.3, 0.55])\n  sep = 35\n  mus = jnp.asarray([-sep, 0., sep])\n  mixing = dist.Categorical(probs=probs)\n  return dist.MixtureSameFamily(\n    mixing_distribution=mixing,\n    component_distribution=dist.Normal(mus, 1.0),\n  )\n\nRNG_JAX = JAXRNG(jrandom.PRNGKey(42))\nRNG_NPY = np.random.default_rng(101)\n\nDISTRIBUTIONS = {\n  \"Normal\": lambda: dist.Normal(0, 1),\n  \"Mixture (2 comp.)\": mixture2_dist,\n  \"Mixture (3 comp.)\": mixture3_dist,\n}\nSAMPLES_EXACT = sample_exact(\n  key=RNG_JAX.key,\n  distributions=DISTRIBUTIONS,\n  n_samples=3_000,\n)\n\ndef generate_kernel(\n  logp_fn,\n  sigma: float,\n) -&gt; Kernel:\n  \"\"\"Generates a random-walk kernel.\n\n  Args:\n    logp_fn: function mapping a point to log-density\n    sigma: scale of the random walk\n  \"\"\"\n  def kernel(\n    key: RandomKey,\n    x: Float[Array, \" dim\"],\n  ) -&gt; Float[Array, \" dim\"]:\n    key1, key2 = jrandom.split(key)\n    # Generate a proposal\n    x_ = x + sigma * jrandom.normal(key1, shape=x.shape)\n    \n    # Evaluate the acceptance ratio\n    log_p1 = logp_fn(x)\n    log_p2 = logp_fn(x_)\n    alpha = jnp.exp(log_p2 - log_p1)\n    \n    # Decide whether to accept the proposal\n    u = jrandom.uniform(key2)\n    return jax.lax.select(u &lt;= alpha, x_, x)\n  \n  return kernel\n\nSIGMAS = [0.1, 1, 10, 100]\n\ndef plot_histograms(\n  samples_exact,\n  samples_mcmc,\n  params,\n  param_name,\n):\n  return make_multipanel_figure(\n    plotting_fn=plot_hist_panel_function,\n    distributions=DISTRIBUTIONS,\n    params=params,\n    samples_exact=samples_exact,\n    samples_mcmc=samples_mcmc,\n    n_spines=1,\n    param_name=param_name,\n  )\n\ndef plot_traces(\n  samples_mcmc,\n  params,\n  param_name,\n):\n  return make_multipanel_figure(\n    plotting_fn=plot_trace_panel_function,\n    distributions=DISTRIBUTIONS,\n    params=params,\n    samples_exact=None,\n    samples_mcmc=samples_mcmc,\n    n_spines=2,\n    param_name=param_name,\n  )\n\n_samples_mcmc = mcmc_multirun(\n  key=RNG_JAX.key,\n  kernel_generator=generate_kernel,\n  distributions=DISTRIBUTIONS,\n  params=SIGMAS,\n  n_samples=8_000,\n  warmup=5_000,\n  x0=0.5,\n)\n\nplot_histograms(SAMPLES_EXACT, _samples_mcmc, params=SIGMAS, param_name=\"$\\\\sigma$\")\n\n\nAn NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\nSome problems are already visible, but let’s take a look at the trace plots, tracking how the parameter has changed over time:\nCode\nplot_traces(_samples_mcmc, params=SIGMAS, param_name=\"$\\\\sigma$\")\nIt turns out that:\nPerfectly, we would use a sampling scheme allowing for both efficient local exploration and frequent jumps between the modes. There are different reasonable strategies:\nToday we focus on the last method and look at a new variant of it."
  },
  {
    "objectID": "posts/non-reversible-parallel-tempering.html#parallel-tempering-as-originally-designed",
    "href": "posts/non-reversible-parallel-tempering.html#parallel-tempering-as-originally-designed",
    "title": "Non-reversible parallel tempering",
    "section": "Parallel tempering as originally designed",
    "text": "Parallel tempering as originally designed\nConsider a space \\(\\mathcal X\\) with a probability distribution of interest \\(p\\) (by abuse of notation we write \\(p\\) both for the probability distribution and for its density with respect to some convenient measure on \\(\\mathcal X\\)). We have a Markov kernel \\(K\\) allowing us to explore \\(\\mathcal X\\) locally, but which has a trouble to pass through low-density regions separating distinct modes.\nThis issue can be addressed by extending the original space \\(\\mathcal X\\) to a larger space \\(\\mathcal X^{N+1} = \\mathcal X \\times \\cdots \\times \\mathcal X\\) and targeting a product distribution \\(\\mathbf{p}(\\mathbf x) = p_0(x_0)\\cdots p_{N-1}(x_{N-1}) p_{N}(x_N)\\), where \\(p_N = p\\) is the original distribution of interest and \\(p_0, \\dotsc, p_{N-1}\\) are auxiliary distributions, designed to be easier to sample from. The main idea is that \\(p_0\\) should be chosen so that it is known to be easy to sample (e.g., i.i.d. samples are easy to generate) and the consecutive distributions, \\(p_{i}\\) and \\(p_{i+1}\\), should be closely related: the separate modes of \\(p = p_N\\) can be then “connected” by going through \\(p_{N-1}, p_{N-2}, \\dotsc\\) to \\(p_0\\), which is then to sample from, and back.\nFor example, a typical choice for a sequence \\(p_0, \\dotsc, p_N=p\\) is to use an annealing schedule \\(0 = \\beta_0 &lt; \\beta_1 &lt; \\dotsc &lt; \\beta_N = 1\\) and employ the following distribution: \\[\n  p_n(x) = \\frac{1}{\\mathcal Z(\\beta_n)} \\left(\\frac{p(x)}{p_0(x)}\\right)^{\\beta_n} p_0(x) = \\frac{1}{\\mathcal Z(\\beta_n)} p(x)^{\\beta_n} p_0(x)^{1-\\beta_n}\n\\]\nSimilarly as in SMC samplers, the schedule does matter a lot, controlling how much consecutive distributions are related. However, there is an important difference between SMC samplers and parallel tempering: they are orthogonal to each other, in the sense that parallel tempering at any single time keeps the states at across all the temperatures, while SMC samplers always have all the particles at the same temperature, which then rises over time. (I think this observation was made by Nicholas Chopin in one of his lecture, although I can’t find the exact reference. Many apologies for misquoting or misattributing this statement)\nA Markov chain is now defined on \\(\\mathcal X^{N+1}\\), with a state \\(\\mathbf{x} = (x_0, \\dotsc, x_N)\\). We consider two transitions:\n\nApplying Markov kernels \\(K_n\\) to entries \\(x_i\\), targeting distributions \\(p_i\\), so that we do local exploration.\nSwapping entries \\(x_{i}\\) with \\(x_{i+1}\\), so that we can eventually pass from \\(x_N\\) (which is targeting \\(\\pi\\)) to \\(x_0\\) (which is easy to explore) and back.\n\nNote that if the second kind of moves were not allowed, we would have just \\(N+1\\) independent Markov chains (each defined on the space \\(\\mathcal X\\)) and targeting the \\(\\mathbf{p}\\) distribution “individually”: the first chain would be efficient (exploring \\(p_0\\)) and the last one would mix very, very slow. As the chains are coupled, they are not individually Markov anymore and they travel between different tempered distributions (hopefully eventually reaching the same distribution). We can use them then to extract samples just from the \\(p=p_N\\) distribution.\nTo ensure that the Markov chain on \\(\\mathcal X^{N+1}\\) explores \\(\\mathbf{p}\\) properly, Charles Geyer proposed to swap components \\(i\\) and \\(j\\) according to a Metropolis update. If \\(\\mathbf{x}\\) is the current state and \\(\\mathbf{x}'\\) is the state with entries \\(i\\) and \\(j\\) swapped, the Metropolis ratio is given by \\[\n  r(i, j) = \\frac{ \\mathbf{p}(\\mathbf x')}{ \\mathbf{p}(\\mathbf x)} = \\frac{ p_i(x_j) p_j(x_i)}{ p_i(x_i) p_j(x_j)}.\n\\]\nTypically, only adjacent indices are swapped, as for \\(i\\) very distant from \\(j\\) we expect that \\(r\\) would close to zero. It is also informative to write this ratio in terms of the target and the reference distributions. As \\(\\log p_n(x) = \\beta_n \\log p(x) + (1-\\beta_n) \\log p_0(x) - \\log \\mathcal Z(\\beta_n)\\), we have \\[\\begin{align*}\n\\log r &= \\beta_i (\\log p(x_j) - \\log p(x_i)) + (1-\\beta_i) (\\log p_0(x_j) - \\log p_0(x_i)) \\\\\n&+ \\beta_j( \\log p(x_i) - \\log p(x_j) ) + (1-\\beta_j)( \\log p_0(x_i) - \\log p_0(x_j) ) \\\\\n&= -(\\beta_i - \\beta_j) \\left( \\log \\frac{p(x_i)}{p_0(x_i)} - \\log \\frac{p(x_j)}{p_0(x_j)} \\right)  \n\\end{align*}\n\\]\nThis is a very convenient formula if \\(p_0\\) corresponds to the prior distribution and \\(p\\) is the posterior distribution, as their ratio is then simply the likelihood1.\nThis is enough theory for now – let’s implement parallel tempering in JAX.\n\nJAX implementation\nWe need to make some design choices. We keep the state \\(\\mathbf{x} = (x_0, \\dotsc, x_N)\\) as a matrix \\((N+1)\\times (\\mathrm{dim}\\, \\mathcal X)\\). As we have access to two log-probability functions (\\(p = p_N\\) and the reference distribution \\(p_0\\)), we need to construct intermediate log-probability functions given an annealing schedule \\(0 = \\beta_0 &lt; \\beta_1 &lt; \\cdots &lt; \\beta_N\\). To make everything vectorisable, let’s construct the individual kernels \\(K_n\\) as \\(K_n = \\mathcal{K}(\\phi_n)\\) using a factory function \\(\\mathcal{K}\\) and kernel-specific parameters \\(\\phi_n\\).\nHence, this implementation is not as general as possible, but it should be compatible with JAX vectorised operations.\nTo swap the chains, we do a “full sweep”, attemping to swap \\(x_0 \\leftrightarrow x_1\\), then \\(x_1\\leftrightarrow x_2\\), and up to \\(x_{N-1}\\leftrightarrow x_N\\). This choice is actually important, as we will later see.\n\n\nCode\ndef generate_independent_annealed_kernel(\n  log_prob,\n  log_ref,\n  annealing_schedule,\n  kernel_generator,\n  params,\n) -&gt; tuple:\n  \"\"\"Generates the kernels via the kernel generator given appropriate parameters.\n\n  Args:\n    log_prob: log_prob of the target distribution\n    log_ref: log_prob of the easy-to-sample reference distribution\n    annealing_schedule: annealing schedule such that `annealing_schedule[0] = 0.0` and `annealing_schedule[-1] = 1`\n    kernel_generator: `kernel_generator(log_p, param)` returns a transition kernel of signature `kernel(key, state) -&gt; new_state`\n    params: parameters for the transition kernels. Note that `len(annealing_schedule) = len(params)`\n  \"\"\"\n  if len(annealing_schedule) != len(params):\n    raise ValueError(\"Parameters have to be of the same length as the annealing schedule\")\n  n_chains = len(annealing_schedule)\n\n  def transition_kernel(key, state, beta, param):\n    def log_p(y):\n      return beta * log_prob(y) + (1.0 - beta) * log_ref(y)\n    return kernel_generator(log_p, param)(key, state)\n\n  def kernel(key, state_joint):\n    key_vec = jrandom.split(key, n_chains)\n    return jax.vmap(transition_kernel, in_axes=(0, 0, 0, 0))(key_vec, state_joint, annealing_schedule, params)\n\n  return kernel\n\ndef generate_swap_chains_decision_kernel(\n  log_prob,\n  log_ref,\n  annealing_schedule,\n):\n  def log_p(y, beta):\n    return beta * log_prob(y) + (1.0 - beta) * log_ref(y)\n\n  def swap_decision(key, state, i: int, j: int) -&gt; bool:\n    beta1, beta2 = annealing_schedule[i], annealing_schedule[j]\n    x1, x2 = state[i], state[j]\n    log_numerator = log_p(x1, beta2) + log_p(x2, beta1) \n    log_denominator = log_p(x1, beta1) + log_p(x2, beta2)\n    log_r = log_numerator - log_denominator\n\n    r = jnp.exp(log_r)\n    return jrandom.uniform(key) &lt; r\n\n  return swap_decision\n\n\ndef generate_full_sweep_swap_kernel(\n  log_prob,\n  log_ref,\n  annealing_schedule,\n):\n  \"\"\"Applies a full sweep, attempting to swap chains 0 &lt;-&gt; 1, then 1 &lt;-&gt; 2 etc. one-after-another.\n  \"\"\"\n  n_chains = len(annealing_schedule)\n\n  if n_chains &lt; 2:\n    raise ValueError(\"At least two chains are needed.\")\n\n  swap_decision_fn = generate_swap_chains_decision_kernel(\n    log_prob=log_prob,\n    log_ref=log_ref,\n    annealing_schedule=annealing_schedule,\n  )\n\n  def kernel(key, state):\n    def f(state, i: int):\n      subkey = jrandom.fold_in(key, i)\n      decision = swap_decision_fn(subkey, state=state, i=i, j=i+1)\n      \n      # Candidate state: we swap values at i and i+1 positions\n      swapped_state = state.at[i].set(state[i+1])\n      swapped_state = swapped_state.at[i+1].set(state[i])\n\n      new_state = jax.lax.select(decision, swapped_state, state)\n      return new_state, None\n    \n    final_state, _ = jax.lax.scan(f, state, jnp.arange(n_chains - 1))\n    return final_state\n  \n  return kernel\n\n\ndef compose_kernels(kernels: list):\n  \"\"\"Composes kernels, applying them in order.\"\"\"\n  def kernel(key, state):\n    for ker in kernels:\n      key, subkey = jrandom.split(key)\n      state = ker(subkey, state)\n\n    return state\n\n  return kernel\n\n\nWe need also some annealing schedules:\n\n\nCode\ndef annealing_constant(n_chains: int, base: float = 1.0):\n  \"\"\"Constant annealing schedule, should be avoided.\"\"\"\n  return base * jnp.ones(n_chains)\n\ndef annealing_linear(n_chains: int):\n  \"\"\"Linear annealing schedule, should be avoided.\"\"\"\n  return jnp.linspace(0.0, 1.0, n_chains)\n\ndef annealing_exponential(n_chains: int, base: float = 2.0**0.5):\n  \"\"\"Annealing parameters form a geometric series (apart from beta[0] = 0).\n\n  Args:\n    n_chains: number of chains in the schedule\n    base: geometric progression base, float larger than 1\n  \"\"\"\n  if base &lt;= 1:\n    raise ValueError(\"Base should be larger than 1.\")\n\n  if n_chains &lt; 2:\n    raise ValueError(\"At least two chains are required.\")\n  elif n_chains == 2:\n    return jnp.array([0.0, 1.0])\n  else:\n    x = jnp.append(jnp.power(base, -jnp.arange(n_chains - 1)), 0.0)\n    return x[::-1]\n\n\nNow we can re-run the previous experiment, but with the usual random-walk MCMC replaced with parallel tempering:\n\n\nCode\ndef pt_multirun_sigmas(\n  key: RandomKey,\n  sigmas: list[float],\n  distributions: OrderedDict,\n  n_samples: int,\n  warmup: int,\n  reference_scale: float = 20.0,\n  n_chains: int = 10,\n  schedule_const: float = 1.1,\n  x0: float = 0.5,\n):\n  def log_ref(x):\n    return dist.Normal(0, reference_scale).log_prob(x)\n  \n  def sampling_fn(key, log_prob, sigma):\n    betas = annealing_exponential(n_chains, schedule_const)\n\n    # We know how to sample from the reference distribution \n    sigmas = sigma * jnp.ones_like(betas)\n    sigmas = sigmas.at[0].set(reference_scale)\n\n    K_ind = generate_independent_annealed_kernel(\n      log_prob=log_prob,\n      log_ref=log_ref,\n      annealing_schedule=betas,\n      kernel_generator=generate_kernel,\n      params=sigmas,\n    )\n    K_swap = generate_full_sweep_swap_kernel(\n      log_prob=log_prob,\n      log_ref=log_ref,\n      annealing_schedule=betas,\n    )\n    K_combined = compose_kernels([K_ind, K_swap])\n\n    return mcmc_sampling_loop(\n      key=key,\n      x0=x0 * jnp.ones(n_chains),\n      kernel=K_combined,\n      n_samples=n_samples,\n      warmup=warmup)\n\n  return make_multirun(\n    key,\n    sampling_fn=sampling_fn,\n    distributions=distributions,\n    params=sigmas,\n  )\n\n_samples_pt = pt_multirun_sigmas(\n  key=RNG_JAX.key,\n  sigmas=SIGMAS,\n  distributions=DISTRIBUTIONS,\n  n_samples=4_000,\n  warmup=2000,\n)\n\n\nplot_histograms(SAMPLES_EXACT, _samples_pt[..., -1], params=SIGMAS, param_name=\"$\\\\sigma$\")\n\n\n\n\n\nMuch better! Let’s take a look at the traces:\n\n\nCode\nplot_traces(_samples_pt[..., -1], params=SIGMAS, param_name=\"$\\\\sigma$\")\n\n\n\n\n\nIn fact, we can plot the traces corresponding to the auxiliary distributions. This distribution is closer to the reference, about in the middle:\n\n\nCode\n_middle_chain = _samples_pt.shape[-1] // 2\nplot_traces(_samples_pt[..., _middle_chain], params=SIGMAS, param_name=\"$\\\\sigma$\")\n\n\n\n\n\nAnd this one is just the reference distribution, from which we know how to sample well:\n\n\nCode\nplot_traces(_samples_pt[..., 0], params=SIGMAS, param_name=\"$\\\\sigma$\")\n\n\n\n\n\n\n\nWhat if?\nLet’s understand parallel tempering better. The method has quite a few hyperparameters and we can understand what happens if we change them.\n\nReference distribution\nConsider a “too narrow” reference distribution, which puts low mass in the region where the modes of the target distribution arise (in the multimodal case).\n\n\nCode\n_samples_pt = pt_multirun_sigmas(\n  key=RNG_JAX.key,\n  sigmas=SIGMAS,\n  distributions=DISTRIBUTIONS,\n  n_samples=4_000,\n  warmup=2000,\n  reference_scale=1.0,\n)\n\nplot_histograms(SAMPLES_EXACT, _samples_pt[..., -1], params=SIGMAS, param_name=\"$\\\\sigma$\")\n\n\n\n\n\n\n\nCode\nplot_traces(_samples_pt[..., -1], params=SIGMAS, param_name=\"$\\\\sigma$\")\n\n\n\n\n\nWe see some problems arising here. On the other hand, if choose a very wide distribution…\n\n\nCode\n_samples_pt = pt_multirun_sigmas(\n  key=RNG_JAX.key,\n  sigmas=SIGMAS,\n  distributions=DISTRIBUTIONS,\n  n_samples=4_000,\n  warmup=2000,\n  reference_scale=300.0,\n)\n\nplot_histograms(SAMPLES_EXACT, _samples_pt[..., -1], params=SIGMAS, param_name=\"$\\\\sigma$\")\n\n\n\n\n\n\n\nCode\nplot_traces(_samples_pt[..., -1], params=SIGMAS, param_name=\"$\\\\sigma$\")\n\n\n\n\n\n… the behaviour of parallel tempering is somewhat better. Of course, not as good as when the reference \\(p_0\\) was closer to the target \\(p=p_N\\), but my current intuition is that it is better to use a “too wide” reference, rather than “too narrow”.\n\n\nNumber of chains\nLet’s try a smaller number of chains:\n\n\nCode\n_samples_pt = pt_multirun_sigmas(\n  key=RNG_JAX.key,\n  sigmas=SIGMAS,\n  distributions=DISTRIBUTIONS,\n  n_samples=4_000,\n  warmup=2000,\n  n_chains=4,\n)\n\nplot_histograms(SAMPLES_EXACT, _samples_pt[..., -1], params=SIGMAS, param_name=\"$\\\\sigma$\")\n\n\n\n\n\n\n\nCode\nplot_traces(_samples_pt[..., -1], params=SIGMAS, param_name=\"$\\\\sigma$\")\n\n\n\n\n\nThis seems to be not bad, although the autocorrelation between samples is larger.\nOn the other hand, if we use a larger number of chains…\n\n\nCode\n_samples_pt = pt_multirun_sigmas(\n  key=RNG_JAX.key,\n  sigmas=SIGMAS,\n  distributions=DISTRIBUTIONS,\n  n_samples=4_000,\n  warmup=2000,\n  n_chains=50,\n)\n\nplot_histograms(SAMPLES_EXACT, _samples_pt[..., -1], params=SIGMAS, param_name=\"$\\\\sigma$\")\n\n\n\n\n\n\n\nCode\nplot_traces(_samples_pt[..., -1], params=SIGMAS, param_name=\"$\\\\sigma$\")\n\n\n\n\n\n… we see excellent results. As we will see below, there exist parallel tempering schemes in which a large number of chains results in a bad performance. However, the “full sweep” variant does not seem to suffer from this issue. (I may need to revise this intuition one day, when precise theory is available, but right now I am happy with it).\n\n\nAnnealing schedule\nFinally, let’s take a look at the annealing schedule:\n\n\nCode\nfor base in [1.01, 1.1, 1.2, 1.4]:\n  schedule = np.asarray(annealing_exponential(n_chains=10, base=base)).tolist()\n  schedule_str = \", \".join(map(lambda x: f\"{x:.1f}\", schedule))\n  print(f\"{base}:\\t{schedule_str}\")\n\n\n1.01:   0.0, 0.9, 0.9, 0.9, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0\n1.1:    0.0, 0.5, 0.5, 0.6, 0.6, 0.7, 0.8, 0.8, 0.9, 1.0\n1.2:    0.0, 0.2, 0.3, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 1.0\n1.4:    0.0, 0.1, 0.1, 0.1, 0.2, 0.3, 0.4, 0.5, 0.7, 1.0\n\n\nLet’s use the constant 1.2:\n\n\nCode\n_samples_pt = pt_multirun_sigmas(\n  key=RNG_JAX.key,\n  sigmas=SIGMAS,\n  distributions=DISTRIBUTIONS,\n  n_samples=4_000,\n  warmup=2000,\n  schedule_const=1.2,\n)\n\nplot_histograms(SAMPLES_EXACT, _samples_pt[..., -1], params=SIGMAS, param_name=\"$\\\\sigma$\")\n\n\n\n\n\n\n\nCode\nplot_traces(_samples_pt[..., -1], params=SIGMAS, param_name=\"$\\\\sigma$\")\n\n\n\n\n\nPerformance does not seem to differ by much.\nEven if we use constant 2…\n\n\nCode\n_samples_pt = pt_multirun_sigmas(\n  key=RNG_JAX.key,\n  sigmas=SIGMAS,\n  distributions=DISTRIBUTIONS,\n  n_samples=4_000,\n  warmup=2000,\n  schedule_const=2.0,\n)\n\nplot_histograms(SAMPLES_EXACT, _samples_pt[..., -1], params=SIGMAS, param_name=\"$\\\\sigma$\")\n\n\n\n\n\n\n\nCode\nplot_traces(_samples_pt[..., -1], params=SIGMAS, param_name=\"$\\\\sigma$\")\n\n\n\n\n\n… the performance is reasonable. This is interesting.\n\n\n\nSummary\nMy (current and subjective) intuition is that: - The most important thing is to ensure efficient sampling from the reference, \\(p_0\\), which should be somewhat close to \\(p\\). - It is better to use a bit “too wide” \\(p_0\\) rather than a “too narrow” one. - We need efficient local exploration. Parallel tempering can however improve on these aspects. - The annealing schedule and the number of chains also do matter. I feel that the points considered above may be a bit more important for the final performance than tuning the optimisation schedule (e.g., 10-20 chains and around 1.1-1.2 annealing constant seem to be quite reasonable defaults), but choosing the number of chains and annealing schedule properly does matter. Also, we can often easily change the number of chains and the annealing schedule, while changing \\(p_0\\) can be much more tricky (especially that in Bayesian inference problems, it often is taken to be the prior distribution, which cannot be changed arbitrarily).\nThese intuitions are based on “easy” low-dimensional problems considered above. However, for modern high-dimensional problems with many modes these intuitions may not generalise well. It is also much harder to ensure then that \\(p_0\\) is reasonably wide and that the kernels are efficient enough.\nIt would be nice to have a good default for the annealing schedule. Another aspect, which we do not consider above, but is important for very high-dimensional problems, is to use distributed computing on multiple machines or parallel computing on multiple cores. While it is easy to parallelise application of the kernels \\(K_n\\) to distinct components \\(x_n\\), our current “full sweep” swapping strategy has to be executed iteratively."
  },
  {
    "objectID": "posts/non-reversible-parallel-tempering.html#non-reversible-parallel-tempering",
    "href": "posts/non-reversible-parallel-tempering.html#non-reversible-parallel-tempering",
    "title": "Non-reversible parallel tempering",
    "section": "Non-reversible parallel tempering",
    "text": "Non-reversible parallel tempering\nIn Non-reversible parallel tempering: a scalable highly parallel MCMC scheme , Saifuddin Syed, Alexandre Bouchard-Côté, George Deligiannidis and Arnaud Doucet propose an interesting alternative to the sampling scheme described above, suitable for distributed computing. I enjoyed reading the paper very much, but there is also an excellent lecture available.\nConsider a distributed swapping scheme in which we do not want to sequentially attempt swapping chains \\(i \\leftrightarrow i+1\\) sequentially for all \\(i=0, \\dotsc, N-1\\), but rather employ either an:\n\nEven move: attempt swapping the states \\(2k \\leftrightarrow 2k+1\\) (which can be done simultaneously for all \\(k\\) on different machines).\nOdd move: attempt swapping the states \\(2k-1 \\leftrightarrow 2k\\) (which also can be done simultaneously for all \\(k\\) on different machines).\n\nNote that both moves are different from what we did above: each full sweep swapped the states consequtively. In particular, there was a chance (very small, though) to travel from \\(p_0\\) to \\(p_N\\) in one full sweep (incurring \\(N-1\\) swaps). Currently \\(x_0\\) can either be swapped with \\(x_1\\) (even move is accepted) or be left in place in one step (the odd move is executed or the even move is rejected).\nThe authors consider alternating between these moves basing on the following:\n\nStochastic even-odd swap (SEO): an unbiased coin is tossed to decide whether to execute the even or the odd move.\nDeterministic even-odd swap (DEO): even time steps result in even moves and odd time steps result in odd moves.\n\nIt turns out that SEO is a very inefficient choice when a large number of chains is used and DEO is much more preferred (see also this section).\n\nJAX implementation of DEO\nLet’s implement DEO in JAX.\nOur first task is to execute even and odd moves, which deserves a subsection on its own.\n\nControlled swapping problem\nWe have a state \\(\\mathbf{x} = (x_0, \\dotsc, x_N)\\) and we want to execute some moves. Let’s keep the information about the swaps in a binary mask matrix \\(\\mathbf{m} = (m_0, \\dotsc, m_{N-1})\\) such that \\(m_i = 1\\) if and only if we want to swap \\(x_{i} \\leftrightarrow x_{i+1}\\) (and \\(m_i = 0\\) otherwise).\nAn even move in which (somewhat unlikely) all proposals are accepted has then a mask \\(\\mathbf{m}_\\text{even} = (1, 0, 1, 0, \\cdots)\\) and an odd move has a mask \\(\\mathbf{m}_\\text{odd} = (0, 1, 0, 1, \\cdots)\\). However, as only some moves have been accepted, some ones can be replaced by zeros. Namely, if we have a binary matrix \\(\\mathbf{m}_\\text{accept}\\), we have to take the entry-wise AND operation. For example, \\(\\mathbf{m} := \\mathbf{m}_\\text{accept}\\, \\&\\, \\mathbf{m}_\\text{even}\\) for accepted even moves.\nNote that it is not possible to have two consecutive ones.\nI find applying the swaps according to \\(\\mathbf{m}\\) rather tricky. Consider the following algorithm:\nfn swapping_naive(x[], m[]) -&gt; y[]:\n  for i = 0, ..., N-1:\n    if m[i] = 0:\n      y[i]   := x[i]\n      y[i+1] := x[i+1]\n    else:                 // m[i] = 1\n      y[i]   := x[i+1]\n      y[i+1] := x[i]\nDoes it work?\nNo. For the input data\nm := [1, 0]\nx := [a, b, c]\nwe want y = [b, a, c]. However, we have:\ny := [b, a]       // i = 0, m[0] = 1\ny := [b, b, c]    // i = 1, m[1] = 0\nThe issue was that y[1] was updated both at i=0 and i=1 stages. Let’s think how we can improve this:\nfn swapping_good(x[], m[]) -&gt; y[]:\n  y := copy(x)\n\n  for i = 0, ..., N-1:\n    if m[i] = 1:\n      y[i]   := x[i+1]\n      y[i+1] := x[i]\n    else:                // m[i] = 0\n      y[i]   := y[i]     // Note that we do not change the values\n      y[i+1] := y[i+1]   // simply copying them over\nIn this case, the algorithm works as following:\ny = [a, b, c]   // Before the loop\ny = [b, a, c]   // i = 0, m[0] = 1\ny = [b, a, c]   // i = 1, m[1] = 0\nLet’s prove that this algorithm is indeed correct by showing that each y[i] attains the correct value. We consider three cases:\nCase y[0]: we have y[0] := x[0] at the beginning. The only moment when it can be modified is at step i = 0. We have y[0] := y[0] = x[0] if m[0] = 0 and y[0] := x[0+1] = x[1] if m[0] = 1.\nCase y[N]: similarly as above, y[N] := x[N] at the beginning and the only moment when it can be modified is at i = N-1 step. If m[N-1] = 0, then y[N] = x[N] and if m[N-1] = 1, then we overwrite the value to y[N] :=  x[N-1].\nCase y[j] for 0 &lt; j &lt; N: at the beginning y[j] := x[j] and can be modified only at steps i=j-1 and i=j. We have three cases:\n\nm[j-1, j] = [0, 0]. Then, y[j] = x[j] as it has not been modified at either step.\nm[j-1, j] = [1, 0]. Then y[j] := x[j-1] at i=j-1 and stays unchanged at i=j, so in the end sequence we have y[j] = x[j-1].\nm[j-1, j] = [0, 1]. Then, y[j] = x[j] at step i=j-1. Then, at step i=j we have y[j] := x[j+1].\n\nNote that it is important that consecutive ones are not allowed.\n\n\nCode\ndef _test_controlled_swapping(func):\n  test_cases = [\n    # Triples (x, m, y)\n    ([1, 2], [0,], [1, 2]),\n    ([1, 2], [1,], [2, 1]),\n    ([1, 2, 3], [0, 0], [1, 2, 3]),\n    ([1, 2, 3], [1, 0], [2, 1, 3]),\n    ([1, 2, 3], [0, 1], [1, 3, 2]),\n    ([1, 2, 3, 4], [1, 0, 1], [2, 1, 4, 3]),\n    ([1, 2, 3, 4], [0, 1, 0], [1, 3, 2, 4]),\n  ]\n  for x, m, y in test_cases:\n    y_ = func(jnp.asarray(x), jnp.asarray(m))\n    y_ = np.asarray(y_).tolist()\n    if tuple(y_) != tuple(y):\n      raise ValueError(f\"f(x={x}, m={m}) = {y_}. Expected {y}.\")\n\ndef controlled_swapping_scan(\n  x: Float[Array, \"n_chains *dims\"],\n  m: Int[Array, \" n_chains-1\"],\n) -&gt; Float[Array, \"n_chains *dims\"]:\n  \"\"\"Swaps the entries of `x`, as described by binary mask `m`.\n\n  Args:\n    x: array of shape (n_chains, dim)\n    m: binary mask of shape (n_chains - 1,) controlling which chains should be swapped. We have `m[i] = 1` if `x[i]` and `x[i+1]` should be swapped.\n    \n  Note:\n    Consecutive values 1 in `m` are not allowed.\n    Namely, it cannot hold that `m[i] = m[i+1] = 1`.\n  \"\"\"\n  def f(y, i):\n    value = jax.lax.select(m[i], x[i + 1], y[i])       # y[i]\n    value_next = jax.lax.select(m[i], x[i], y[i + 1])  # y[i + 1]\n        \n    y = y.at[i].set(value)\n    y = y.at[i + 1].set(value_next)\n        \n    return y, None\n\n  # Run the scan over the range of M\n  y, _ = jax.lax.scan(f, x, jnp.arange(m.shape[0]))\n  \n  return y\n\n_test_controlled_swapping(controlled_swapping_scan)\n\n\nThis implementation seems to work and shows how powerful [jax.lax.scan](https://jax.readthedocs.io/en/latest/_autosummary/jax.lax.scan.html) can be. However, JAX is built to accelerate linear algebra and perhaps we can come up with an appropriately vectorised operation. Let’s go through the three cases once again.\nCase y[0]: we want x[0] if m[0] = 0 and x[1] if m[1] = 1. In other words, we have y[0] = x[m[0]].\nCase y[N]: we want x[N-1] if m[N-1] = 1 and x[N] if m[N-1] = 0. Hence, y[N] = x[N - m[N-1]].\nCase y[j] for 0 &lt; j &lt; N: as before, we have three cases, controlled by m[j-1] and m[j]. I claim that y[j] = x[j + m[j] - m[j-1]]. For both zeros, we do not swap anything and have y[j] = x[j]. For m[j-1, j] = [0, 1] we want to have y[j] = x[j+1] and for m[j-1, j] = [1, 0] we have y[j] = x[j-1].\nThe implementation is now trivial:\n\n\nCode\ndef _create_indices(m):\n  N = m.shape[0] + 1\n  base_indices = jnp.arange(1, N-1)  # Length N-2\n  ind_middle = base_indices + m[1:] - m[:-1]\n\n  ind = jnp.concatenate((\n        jnp.array([m[0]]),\n        ind_middle,\n        jnp.array([N - 1 - m[-1]])  \n  ))\n\n  return ind\n\ndef controlled_swapping(\n  x: Float[Array, \"n_chains *dims\"],\n  m: Int[Array, \" n_chains-1\"],\n) -&gt; Float[Array, \"n_chains *dims\"]:\n  \"\"\"Swaps the entries of `x`, as described by binary mask `m`.\n\n  Args:\n    x: array of shape (n_chains, dim)\n    m: binary mask of shape (n_chains - 1,) controlling which chains should be swapped. We have `m[i] = 1` if `x[i]` and `x[i+1]` should be swapped.\n    \n  Note:\n    Consecutive values 1 in `m` are not allowed.\n    Namely, it cannot hold that `m[i] = m[i+1] = 1`.\n  \"\"\"\n  indices = _create_indices(m)\n  return x[indices, ...]\n\n_test_controlled_swapping(controlled_swapping)\n\n\n\n\nDEO swapping kernel and the sampling loop\nAt this stage, we can implement the swapping kernel. Note that this kernel has a different signature than usual, additionally taking the timestep as input and calculating the rejection rates, which will turn out to be useful when we optimise the annealing schedule. Due to the fact that the swaps are not “interfering”, we can use vectorised operations.\n\n\nCode\ndef generate_deo_extended_kernel(\n  log_prob,\n  log_ref,\n  annealing_schedule,\n):\n  def log_p(y, beta):\n    return beta * log_prob(y) + (1.0 - beta) * log_ref(y)\n\n  log_p_vmap = jax.vmap(log_p, in_axes=(0, 0))\n\n  def extended_kernel(\n    key,\n    state,\n    timestep: int,\n  ) -&gt; tuple:\n    \"\"\"Extended deterministic even-odd swap kernel, which\n    for even timesteps makes even swaps (2i &lt;-&gt; 2i+1)\n    and for odd timesteps makes odd swaps (2i-1 &lt;-&gt; 2i)\n\n    Args:\n      key: random key\n      state: state\n      timestep: timestep number, used to decide whether to make even or odd move\n\n    Returns:\n      new_state, the same shape as `state`\n      rejection_rates, shape (n_chains-1,)\n    \"\"\"\n    n_chains = state.shape[0]\n\n    idx1 = jnp.arange(n_chains - 1)\n    idx2 = idx1 + 1\n\n    xs1 = state[idx1]\n    xs2 = state[idx2]\n\n    betas1 = annealing_schedule[idx1]\n    betas2 = annealing_schedule[idx2]\n\n    log_numerator = log_p_vmap(xs1, betas2) + log_p_vmap(xs2, betas1) \n    log_denominator = log_p_vmap(xs1, betas1) + log_p_vmap(xs2, betas2)\n    log_accept = log_numerator - log_denominator\n    accept_prob = jnp.minimum(jnp.exp(log_accept), 1.0)\n    rejection_rates = 1.0 - accept_prob\n\n    # Where the swaps would be accepted through M-H\n    accept_mask = jrandom.bernoulli(key, p=accept_prob)\n    # Where the swaps can be accepted due to even-odd moves\n    even_odd_mask = jnp.mod(idx1, 2) == jnp.mod(timestep, 2)\n    total_mask = accept_mask & even_odd_mask\n\n    # Now the tricky part: we need to execute the swaps\n    new_state = controlled_swapping(state, total_mask)\n    return new_state, rejection_rates\n\n  return extended_kernel\n\n\ndef deo_sampling_loop(\n  key: RandomKey,\n  x0,\n  kernel_local,\n  kernel_deo,\n  n_samples: int,\n  warmup: int,\n) -&gt; tuple:\n  \"\"\"The sampling loop for DEO parallel tempering.\n\n  Returns:\n    samples\n    rejection_rates\n  \"\"\"\n  def f(x, timestep: int):\n    subkey = jrandom.fold_in(key, timestep)\n\n    key_local, key_deo = jrandom.split(subkey)\n\n    # Apply local exploration kernel\n    x = kernel_local(key_local, x)\n\n    # Apply the DEO swap\n    x, rejection_rates = kernel_deo(\n      key_deo,\n      x,\n      timestep,\n    )\n\n    return x, (x, rejection_rates)\n  \n  # Run warmup\n  x0, _ = jax.lax.scan(f, x0, jnp.arange(warmup))\n\n  # Collect samples\n  _, (samples, rejection_rates) = jax.lax.scan(f, x0, jnp.arange(n_samples))\n  \n  return samples, rejection_rates\n\n\ndef nonreversible_pt_multirun_sigmas(\n  key: RandomKey,\n  sigmas: list[float],\n  distributions: OrderedDict,\n  n_samples: int,\n  warmup: int,\n  reference_scale: float = 20.0,\n  n_chains: int = 10,\n  schedule_const: float = 1.1,\n  x0: float = 0.5,\n  annealing_schedule: Float[Array, \" n_chains\"] = None,\n):\n  if annealing_schedule is None:\n    betas = annealing_exponential(n_chains=n_chains, base=schedule_const)\n  else:\n    betas = jnp.asarray(annealing_schedule)\n\n  def log_ref(x):\n    return dist.Normal(0, reference_scale).log_prob(x)\n  \n  def sampling_fn(key, log_prob, sigma):\n    # We know how to sample from the reference distribution \n    sigmas = sigma * jnp.ones_like(betas)\n    sigmas = sigmas.at[0].set(reference_scale)\n\n    K_ind = generate_independent_annealed_kernel(\n      log_prob=log_prob,\n      log_ref=log_ref,\n      annealing_schedule=betas,\n      kernel_generator=generate_kernel,\n      params=sigmas,\n    )\n    K_deo = generate_deo_extended_kernel(\n      log_prob=log_prob,\n      log_ref=log_ref,\n      annealing_schedule=betas,\n    )\n\n    key, subkey = jrandom.split(key)\n    samples, rejections = deo_sampling_loop(\n      key=subkey,\n      x0=x0 * jnp.ones(n_chains, dtype=float),\n      kernel_local=K_ind,\n      kernel_deo=K_deo,\n      n_samples=n_samples,\n      warmup=warmup,\n    )\n    return samples\n\n  return make_multirun(\n    key,\n    sampling_fn=sampling_fn,\n    distributions=distributions,\n    params=sigmas,\n  )\n\n\n\n\nCode\n_samples_npt = nonreversible_pt_multirun_sigmas(\n  key=RNG_JAX.key,\n  sigmas=SIGMAS,\n  distributions=DISTRIBUTIONS,\n  n_samples=4000,\n  warmup=2000,\n)\n\nplot_histograms(SAMPLES_EXACT, _samples_npt[..., -1], params=SIGMAS, param_name=\"$\\\\sigma$\")\n\n\n\n\n\n\n\nCode\nplot_traces(_samples_npt[..., -1], params=SIGMAS, param_name=\"$\\\\sigma$\")\n\n\n\n\n\nThis seems pretty good to me!\n\n\n\nWhy non-reversible parallel tempering?\nInterestingly, DEO has better performance than SEO, which are termed in the paper, respectively, non-reversible and reversible parallel tempering schemes. The discussion whether to use non-reversible or reversible kernels has a long history and I still find the topic mysterious. Probably it is worth to write a separate blog post on the topic, but:\n\nThis wonderful 2000 paper from Persi Diaconis, Susan Holmes and Radford Neal shows examples where non-reversible methods are more efficient than reversible ones.\nIn a great 2016 paper from Gareth Roberts and Jeffrey Rosenthal there are examples where “systematic scan” Gibbs samplers (which often are non-reversible, although not always: recall palindromic kernels of the form \\(K_1 K_2 K_1\\)) can outperform “random scan” (always reversible) Gibbs samplers. Examples with the opposite behaviour are also provided.\nIn a 2016 C. Andrieu’s paper there is a theorem showing that for two kernels fulfilling some technical assumptions, systematic scans are more efficient than random ones. This could perhaps offer an orthogonal perspective on why DEO is more efficient than SEO, but I am not sure.\n\nThis discussion whether reversible or non-reversible scheme could be used is one way of looking at the problem. Another is through the perspective of reducing the diffusive random walk behaviour by introducing a momentum variable. Momentum is a common theme in computational statistics and machine learning, with examples such as MALA and Hamiltonian Monte Carlo in Markov chain Monte Carlo world or stochastic gradient descent with momentum in optimisation.\nDEO reduces random walk in swapping the chains (which is studied through the perspective of an index process) and can be thought as of introducing a (discrete) momentum variable. I will skip the precise description of the index process, replacing it with a picture I based on the figures from the paper. We will simulate the path of the chain under the sequential scheme we studied before, SEO and DEO:\n\n\nCode\ndef _find_trajectory(states, tracked: int = None):\n  n_chains = states.shape[1]\n\n  if tracked is None:\n    tracked = n_chains // 2\n\n  return jnp.einsum(\"ng,g-&gt;n\", states == tracked, jnp.arange(n_chains))\n\ndef generate_figure_momentum(p: float = 0.85, n_chains: int = 5, n_timesteps: int = 30):\n  rng = np.random.default_rng(8)\n\n  fig, axs = plt.subplots(3, 1, sharex=True, sharey=True)\n\n  x_axis = np.arange(n_timesteps)\n\n  for ax in axs:\n    for chain in range(n_chains):\n      ax.scatter(x_axis, chain * np.ones_like(x_axis), c=\"w\", s=3)\n    ax.spines[[\"top\", \"right\"]].set_visible(False)\n\n  # Sample full sweep\n  state = jnp.arange(n_chains)\n  states = [state]\n  for timestep in range(1, n_timesteps):\n    for i in range(n_chains - 1):\n      if rng.binomial(1, p):\n        new_state = state.at[i].set(state[i+1])\n        new_state = new_state.at[i+1].set(state[i])\n        state = new_state\n    states.append(state)\n\n  states = jnp.stack(states)\n  trajectory = _find_trajectory(states)\n  \n  ax = axs[0]\n  ax.plot(x_axis, trajectory)\n  ax.set_title(\"Full sweep\")\n\n  # Sample SEO\n  state = jnp.arange(n_chains)\n  states = [state]\n  for timestep in range(1, n_timesteps):\n    mode = rng.binomial(1, 0.5)\n    for i in range(n_chains - 1):\n      if (i % 2 == mode) and rng.binomial(1, p):\n        new_state = state.at[i].set(state[i+1])\n        new_state = new_state.at[i+1].set(state[i])\n        state = new_state\n    states.append(state)\n\n  states = jnp.stack(states)\n  trajectory = _find_trajectory(states)\n  \n  ax = axs[1]\n  ax.plot(x_axis, trajectory)\n  ax.set_title(\"Reversible stochastic even-odd swaps (SEO)\")\n\n  # Sample DEO\n  state = jnp.arange(n_chains)\n  states = [state]\n  for timestep in range(1, n_timesteps):\n    for i in range(n_chains - 1):\n      if (i % 2 == timestep % 2) and rng.binomial(1, p):\n        new_state = state.at[i].set(state[i+1])\n        new_state = new_state.at[i+1].set(state[i])\n        state = new_state\n    states.append(state)\n\n  states = jnp.stack(states)\n  trajectory = _find_trajectory(states)\n  \n  ax = axs[2]\n  ax.plot(x_axis, trajectory)\n  ax.set_title(\"Non-reversible deterministic even-odd swaps (DEO)\")\n\n  fig.tight_layout()\n  return fig\n\nfig = generate_figure_momentum(p=0.8, n_chains=4, n_timesteps=20)\n\n\n\n\n\n\n\nCode\nfig = generate_figure_momentum(p=0.9, n_chains=10, n_timesteps=50)\n\n\n\n\n\nDefinitely, SEO has a trouble going between the reference and the target distribution. On the other hand, in these simulations (which are very simplistic, though!), DEO does not show a clear advantage over the full sweep (other than being more parallelisable). Note that both SEO and DEO can move an index only by \\(\\pm 1\\), while the full sweep can increase the index arbitrarily. However, it also decreases the indices at most by \\(1\\). It may perhaps be interesting to consider a deterministic forward-backward full sweep scheme, in which even timesteps make a full forward sweep (as we did in the first prototype) and odd timesteps make a backward sweep.\nI think studying the behaviour of the DEO scheme is an important contribution of this paper, but there several more:\n\nBy introducing and studying the index process, the authors devise the DEO sampling scheme together with a method for choosing the annealing schedule basing on preliminary runs.\nThe proposed sampling scheme is highly parallelisable and can be used in distributed computing environments. The experiments in the paper cover many complex high-dimensional distributions. Moreover, a new Julia package, Pigeons.jl makes application of non-reversible parallel tempering practical in the distributed setting.\n\nLet’s see, however, how to tune the annealing schedule, which has a wonderful theory outlined in Section 4 of the paper.\n\n\nAnnealing schedule optimisation\nAssuming efficient local exploration of individual components, the authors build a theory how quickly the chain can cycle between \\(p_0\\) and \\(p=p_N\\). The key quantity is the instateneous rejection rate function \\[\n  \\lambda(\\beta) = \\frac{1}{2} \\mathbb E_{X, Y \\sim_\\mathrm{i.i.d.} p_\\beta } \\left[\\left| \\log \\frac{ p(X) p_0(Y)}{ p(Y) p_0(X) } \\right|\\right],\n\\]\nwhich depends on the annealing parameter \\(\\beta\\) (which controls the measure over which we integrate), but also on how different \\(p\\) and \\(p_0\\) are. Define \\[\n  \\Lambda(\\beta) = \\int_{0}^{\\beta} \\lambda(\\beta') \\,\\mathrm{d}\\beta'.\n\\]\nIf \\(\\tilde \\Lambda = \\Lambda(1)\\), then for small \\(\\max_i |\\beta_i - \\beta_{i+1}|\\) it holds that the “round-trip rate”, describing how often going from \\(p_0\\) to \\(p\\) and back, for SEO is about \\[\n  f_\\mathrm{SEO} \\approx \\frac{1}{2N + 2\\tilde \\Lambda}.\n\\]\nUsing a large \\(N\\) for SEO leads to diffusive behaviour with close-to-zero round-trip rate! I find this result amazing. What is even more interesting, for DEO: \\[\n  f_\\mathrm{DEO} \\approx \\frac{1}{2 \\cdot \\left(1+\\tilde \\Lambda\\right)}.\n\\]\nDEO with large \\(N\\) does not have the diffusive behaviour, with the round-trip rate being controlled by the communication barrier \\(\\tilde \\Lambda\\), which depends on \\(p_0\\) and \\(p\\). Hence, for large \\(\\tilde\\Lambda\\) many, many iterations may be necessary to obtain enough round trips and good mixing.\nInterestingly, the \\(\\Lambda\\) function can be estimated from a run using a fine-grained annealing schedule. It turns out that if \\(\\rho(\\beta, \\beta')\\) is the expected rejection rate of swapping the chains between \\(\\beta\\) and \\(\\beta'\\) (so that, of course, \\(\\rho(\\beta, \\beta') = \\rho(\\beta', \\beta)\\)), then \\[\n  \\rho(\\beta, \\beta') = | \\Lambda(\\beta) - \\Lambda(\\beta') | + O(|\\beta - \\beta'|^3).\n\\]\nIn particular, \\[\n  \\tilde \\Lambda \\approx \\sum_{i=0}^{N-1} \\rho(\\beta_{i}, \\beta_{i+1}),\n\\]\nwhere the error is of order \\(O\\!\\left(N \\cdot \\left(\\max_i |\\beta_i - \\beta_{i+1}|\\right)^3 \\right)\\).\nIn other words, \\(\\Lambda(\\beta)\\) can be estimated from the rejection probabilities.\nTo optimise the schedule, the authors note that the round-trip rate under DEO is given by \\[\n  f = \\frac{1}{2\\left(1 + \\sum_{i=0}^{N-1} \\frac{\\rho(\\beta_{i}, \\beta_{i+1})}{ 1-\\rho(\\beta_i, \\beta_{i+1}) } \\right)}\n\\]\nfor any schedule. (Note that the approximation of \\(f_\\mathrm{DEO}\\) when differences \\(|\\beta_i - \\beta_{i+1}|\\) are small, can be read from this formula: all \\(\\rho\\) are small, so we can ignore terms \\(1-\\rho\\) and then we obtain \\(\\tilde \\Lambda\\)).\nAs we want to maximise \\(f\\), we need to find a schedule minimising the denominator. On the other hand, there is a constraint that for any fine-grained schedule it holds that\n\\[\n  \\tilde \\Lambda \\approx \\sum_{i=0}^{N-1} \\rho(\\beta_{i}, \\beta_{i+1}),\n\\]\nso that this is a constrained optimisation problem. It turns out that the optimum is attained when \\(\\rho(\\beta_i, \\beta_{i+1})\\) are all equal. Using the relationship between \\(\\rho\\) and differences in \\(\\Lambda\\), we see that we should aim at \\[\n  \\Lambda(\\beta_i) \\approx \\frac{i}{N} \\tilde \\Lambda.\n\\]\n\nJAX implementation\n\n\nCode\nfrom scipy.interpolate import PchipInterpolator\nfrom scipy.optimize import bisect\n\ndef estimate_lambda_values(rejection_rates, offset: float = 1e-3):\n  # Make sure that the estimated rejection rates are non-zero\n  rejection_rates = jnp.maximum(rejection_rates, offset)\n  # We have Lambda(0) = 0 and then estimate the rest by cumulative sums\n  extended = jnp.concatenate((jnp.zeros(1), rejection_rates))\n  return jnp.cumsum(extended)\n\n\ndef get_lambda_function(\n  annealing_schedule,\n  lambda_values,\n):\n  \"\"\"Approximates the Lambda function from several estimates at the schedule by interpolating the values with a monotonic cubic spline (as advised in the paper).\"\"\"\n  return PchipInterpolator(annealing_schedule, lambda_values)\n\n\ndef annealing_optimal(\n  n_chains: int,\n  previous_schedule,\n  rejection_rates,\n  _offset: float = 1e-3,\n):\n  \"\"\"Finds the optimal annealing schedule basing on the approximation of the Lambda function.\"\"\"\n  lambda_values = estimate_lambda_values(rejection_rates, offset=_offset)\n  lambda_fn = get_lambda_function(\n    previous_schedule,\n    lambda_values,\n  )\n\n  lambda1 = lambda_values[-1]\n\n  new_schedule = [0.0]\n\n  for k in range(1, n_chains - 1):\n    def fn(x):\n      desired_value = k * lambda1 / (n_chains - 1)\n      return lambda_fn(x) - desired_value\n    \n    new_point = bisect(\n      fn,\n      new_schedule[-1],\n      1.0,\n    )\n\n    if new_point &gt;= 1.0:\n      raise ValueError(\"Encountered value 1.0.\")\n\n    new_schedule.append(new_point)\n\n  new_schedule.append(1.0)\n\n  if len(new_schedule) != n_chains:\n    raise Exception(\"This should not happen.\")\n\n  return jnp.asarray(new_schedule, dtype=float)\n\n\nLet’s apply these utilities to suggest better annealing schedules for one of the problems above.\n\n\nCode\ndef run_nonversible_pt(\n  key: RandomKey,\n  sigma: float,\n  distribution_factory,\n  n_samples: int,\n  warmup: int,\n  annealing_schedule: Float[Array, \" n_chains\"],\n  reference_scale: float = 20.0,\n  x0: float = 0.5,\n):\n  betas = jnp.asarray(annealing_schedule)\n  n_chains = betas.shape[0]\n\n  def log_ref(x):\n    return dist.Normal(0, reference_scale).log_prob(x)\n  \n  def sampling_fn(key, log_prob, sigma):\n    # We know how to sample from the reference distribution \n    sigmas = sigma * jnp.ones_like(betas)\n    sigmas = sigmas.at[0].set(reference_scale)\n\n    K_ind = generate_independent_annealed_kernel(\n      log_prob=log_prob,\n      log_ref=log_ref,\n      annealing_schedule=betas,\n      kernel_generator=generate_kernel,\n      params=sigmas,\n    )\n    K_deo = generate_deo_extended_kernel(\n      log_prob=log_prob,\n      log_ref=log_ref,\n      annealing_schedule=betas,\n    )\n\n    key, subkey = jrandom.split(key)\n    samples, rejections = deo_sampling_loop(\n      key=subkey,\n      x0=x0 * jnp.ones(n_chains, dtype=float),\n      kernel_local=K_ind,\n      kernel_deo=K_deo,\n      n_samples=n_samples,\n      warmup=warmup,\n    )\n    return samples, rejections\n\n  def log_prob(x):\n    return distribution_factory().log_prob(x)\n\n  return sampling_fn(key, log_prob, sigma)\n\n\nWe use 20 chains and an exponential schedule, supposed to decay too quickly:\n\n\nCode\n_N_CHAINS = 20\n_CURRENT_DIST = mixture3_dist\n\ninitial_schedule = annealing_exponential(n_chains=_N_CHAINS, base=3.0)\nsamples, rejections = run_nonversible_pt(\n  key=RNG_JAX.key,\n  sigma=1.0,\n  distribution_factory=_CURRENT_DIST,\n  n_samples=5_000,\n  warmup=2_000,\n  annealing_schedule=initial_schedule,\n)\n\n\nLet’s calculate a new schedule and collect a new sample:\n\n\nCode\n_mean_rejections = rejections.mean(axis=0)\n_lambda_vals = estimate_lambda_values(_mean_rejections)\n_lambda_fn = get_lambda_function(initial_schedule, _lambda_vals)\n\nnew_schedule = annealing_optimal(\n  initial_schedule.shape[0],\n  initial_schedule,\n  _mean_rejections,\n)\n\nnew_samples, new_rejections = run_nonversible_pt(\n  key=RNG_JAX.key,\n  sigma=1.0,\n  distribution_factory=_CURRENT_DIST,\n  n_samples=5_000,\n  warmup=2_000,\n  annealing_schedule=new_schedule,\n)\n\n_new_mean_rejections = new_rejections.mean(axis=0)\n_new_lambda_vals = estimate_lambda_values(_new_mean_rejections)\n_new_lambda_fn = get_lambda_function(new_schedule, _new_lambda_vals)\n\nvery_new_schedule = annealing_optimal(\n  new_schedule.shape[0],\n  new_schedule,\n  _new_mean_rejections,\n)\n\n\nWe have now:\n\ntwo samples: using initial (very quickly decaying schedule) and an optimised one,\ntwo estimates of the \\(\\Lambda(\\beta)\\) function (each estimate depends on the schedule and the rejection rates collected during sampling),\nthree schedules: the initial one (decaying one), the optimised schedule (for which we also collected a sample), and a third, “very optimised”, schedule (which we estimated using the second one).\n\nWe hope that:\n\nThe second sample will be better than the first one (as the schedule now should be better).\nThe \\(\\Lambda(\\beta)\\) estimates will somewhat agree.\nThe second schedule will be much different than the second one. On the other hand, we can hope that the third schedule will be close to the second one (as it depends on the \\(\\Lambda\\) function and we hope that the estimates are reasonable).\n\nLet’s see how this works:\n\n\nCode\nfig, axs = plt.subplots(4, 2, figsize=(2 * 3, 4 * 2), dpi=350, sharex=\"row\", sharey=\"row\")\n\n_sample_true = _CURRENT_DIST().sample(RNG_JAX.key, (5_000,)) \n_BINS = np.concatenate([\n  np.linspace(-40, -20, 30),\n  np.linspace(-10, 10, 30),\n  np.linspace(20, 40, 30),\n])\n\naxs[0, 0].set_ylabel(\"Samples\")\naxs[1, 0].set_ylabel(\"Trace\")\naxs[2, 0].set_ylabel(\"$\\\\Lambda(\\\\beta)$ function\")\naxs[3, 0].set_ylabel(\"After opt.\")\n\nfor ax, smp, title in zip(axs[0, :], [samples, new_samples], [\"Initial schedule\", \"New schedule\"]):\n  ax.hist(_sample_true, histtype=\"step\", color=\"w\", bins=_BINS, density=True)\n  ax.hist(smp[..., -1], bins=_BINS, density=True)\n  ax.spines[[\"top\", \"left\", \"right\"]].set_visible(False)\n  ax.set_yticks([])\n  ax.set_title(title)\n\nfor ax, smp in zip(axs[1, :], [samples, new_samples]):\n  ax.plot(smp[..., -1], color=\"C3\")\n  ax.spines[[\"top\", \"right\"]].set_visible(False)\n  ax.set_xlabel(\"Time\")\n\nfor ax, fn in zip(axs[2, :], [_lambda_fn, _new_lambda_fn]):\n  x_ax = np.linspace(0, 1, 30)\n  ax.plot(x_ax, fn(x_ax), c=\"w\")\n  ax.spines[[\"top\", \"right\"]].set_visible(False)\n  ax.set_xlabel(\"$\\\\beta$\")\n\nfor ax, [x_sch, y_sch] in zip(\n  axs[3, :],\n  [\n    [initial_schedule, new_schedule],\n    [new_schedule, very_new_schedule],\n  ]\n):\n  ax.set_xlabel(\"Schedule before opt.\")\n  ax.spines[[\"top\", \"right\"]].set_visible(False)\n  ax.scatter(\n    x_sch,\n    y_sch,\n    c=\"C1\",\n    s=3**2,\n  )\n  ax.plot(x_sch, x_sch, linestyle=\":\", c=\"w\", linewidth=0.8)\n\nfig.tight_layout()\n\n\n\n\n\nThis looks pretty good to me! We see that the mixing has improved and the sample is of better quality. The \\(\\Lambda(\\beta)\\) differs a bit (resulting in a refined optimised schedule), but I would argue that this disagreement is reasonable. It also nice to see that the third schedule is close to the second one. In the paper, it is suggested to run the optimisation multiple times and the authors propose solutions how to allocate the computational budget within the preliminary runs and the last sampling phase.\n\n\n\nSummary\nI am very excited about parallel tempering! Some related thoughts:\n\nCan we design other update schemes than DEO, which round-trip rate could be improved \\(\\Lambda\\)? How to calculate the round-trip rate for a “full sweep” update scheme? Could alternating “full sweep forward” and “full sweep backward” improve the performance of the algorithm, or somewhat degenerate to the diffuse behaviour?\nDEO removing dependency on \\(N\\) from SEO reminds me of preconditioned Crank-Nicolson algorithm, which works better than the random-walk Metropolis in high dimensions. Is it possible to somewhat formalise a possible connection between these ideas?\nHow to use parallel tempering when working with Gibbs samplers? Building a bridge through tempering between prior and posterior can break the conjugacy employed at some steps. Also, Gibbs samplers do not need to explore the prior (acting as the reference distribution) effectively enough.\nHow well can the data point tempering work, where the data set is artificially shrunk? This could be a potential bridge for Gibbs samplers (as we can simply consider different Gibbs samplers conditioned on subsets of the data sets as the local exploration kernels), but in the context of SMC samplers, there is a great 2007 paper from Ajay Jasra, David Stephens and Chris Holmes, showing that (at least for SMC samplers) the data point tempering seems to work worse than the likelihood tempering.\nRegarding the choice of the tempering scheme, there is a more general likelihood tempering scheme to bridge the prior and the posterior. It looks very interesting, but I have not read this paper yet."
  },
  {
    "objectID": "posts/non-reversible-parallel-tempering.html#footnotes",
    "href": "posts/non-reversible-parallel-tempering.html#footnotes",
    "title": "Non-reversible parallel tempering",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIt is tempting to use priors \\(p_0\\) which are easy to sample from, but for which \\(\\log p_0(x)\\) can be intractable. However, that one has to evaluate \\(\\log p_0(x)\\) to construct the Markov kernels targeting intermediate distributions \\(p_n\\) for \\(0 &lt; n &lt; N\\). Only for swapping the chains we can rely just on the likelihood.↩︎"
  },
  {
    "objectID": "posts/getting-started-with-Bayesian-statistics.html",
    "href": "posts/getting-started-with-Bayesian-statistics.html",
    "title": "Getting started with Bayesian statistics",
    "section": "",
    "text": "Getting started\n\nTake any two examples from PyMC Gallery. For example, GLM and hierarchical partial pooling. Implement them in a Jupyter notebook. Rather than copying and running the code, type it on your own and think what it is supposed to do.\nRead Michael Betancourt’s Inferring gravity from data. Reproduce it in PyMC — the data are available on GitHub.\nIf you would like a lecture series to watch, there’s Richard McElreath’s Statistical rethinking.\n\n\n\nConsult regularly\n\nLearning Bayesian Statistics is a truly excellent podcast hosted by Alexandre Andorra. Many leading statisticians, including Frank Harrell, Jessica Hullman, Kevin Murphy and Aki Vehtari. I find it the best place to learn about various perspectives on modelling techniques and important problems people are working on. I also enjoy listening to Data and Science with Glen Wright Colopy, which covers a wide range of topics and featured many prominent statisticians, such as Deborah Mayo, Chris Holmes, Andrew Gelman and David Dunson.\nAndrew Gelman’s blog and Frank Harrell’s blog.\nAnd, of course, whenever I open Bayesian data analysis, Probabilistic machine learning and Michael Betancourt’s notes, I learn something new.\n\n\n\nHandbooks\n\nGeneral references, covering many topics\n\nBayesian Data Analysis from Andrew Gelman et al.\nBiostatistics for Biomedical Research written by Frank Harrell.\nMichael Betancourt’s notes.\nProbabilistic Machine Learning written by Kevin Murphy.\n\n\n\nPrincipled modelling workflow\n\nThe Bayesian workflow manuscript.\nMichael Betancourt’s workflow description.\nKris Sankaran and Susan Holmes wrote a great paper Generative models: an interdisciplinary perspective.\nThere’s David Blei’s Build, compute, critique, repeat paper.\n\n\n\nInference methods\n\nVariational inference: a review for statisticians.\nHandbook of Markov chain Monte Carlo (unfortunately, this book is not freely available).\nAn introduction to sequential Monte Carlo (unfortunately, this book is not freely available).\n\n\n\n\nGreat to watch\nIf you are in mood for something as engaging as a TV series episode, but more informative, take a look at these talks:\n\nDavid Dunson’s Debunking the hype.\nKristin Lennox’s Everything wrong with statistics.\nAndrew Gelman’s Solve all your statistics problems using p-values."
  },
  {
    "objectID": "posts/setting-up-repository.html",
    "href": "posts/setting-up-repository.html",
    "title": "Setting up a new project repository",
    "section": "",
    "text": "First, a disclaimer: I am not an expert and what I write below is likely to be suboptimal in many ways. On the other hand, in academia we rarely have access to an experienced team of software engineers, DevOps, testers and documentation writers, so perhaps some of the tricks below will be useful.\nSecondly, setting up a new scientific project is generally much more complex than setting up a repository: discussing the project idea with the academic adviser, writing a good research plan, checking with an ethics committee, applying for a grant…\nWe will not cover any of these here, assuming that you have completed the necessary steps and you and your collaborators just want to start coding."
  },
  {
    "objectID": "posts/setting-up-repository.html#on-collaboration-and-mentorship",
    "href": "posts/setting-up-repository.html#on-collaboration-and-mentorship",
    "title": "Setting up a new project repository",
    "section": "On collaboration and mentorship",
    "text": "On collaboration and mentorship\nSometimes you will be the sole developer of the project. More frequently, you will be a member of a team. This includes mentoring students, which includes taking more responsibility, but we will discuss some of these aspects below.\nI think it is important to set the expectations on all sides involved. In the meeting it’s worth to discuss the following:\n\nFinding the time for a regular meeting. (Or, if a regular time slot is impossible, scheduling the next meeting.)\nDoes the whole team adhere to some best coding practices?\nWhat are the views on authorship of the subsequent publication(s)?\n\nHow is it going to be reflected in the expected contributions and time committed to the project?\nI don’t like politics, but this topic will eventually appear, so it’s good to discuss it early.\n\n\nRegarding deciding on coding practices, we often use the following:\n\nWe work within Git feature branch workflow.\nWe try to submit small pull requests and have pull request reviewed happening as soon as possible.\nWe put ideas for next changes as GitHub issues.\nWhen we want to discuss large code changes, sometimes we submit a PR just with function stubs and docstrings. This PR is not really going to be merged, but we can discuss proposed changes using reviewing capabilities, before the time is spent on implementing them.\nWe often do pair programming, which is great.\nWe have continuous integration set using GitHub Actions, which forces us to use consistent style (sometimes to write enough tests and documentation).\n\n\nStudent mentorship\nSpeaking of mentoring students, I have a separate document on this topic. I think it is good to discuss expectations as early as possible and remember that part of our job is mentoring. Of course, it’s great that the students can contribute to the project, but in the end, they should learn useful skills via mentorship.\nSpeaking of coding aspects of mentorship:\n\nSchedule enough time in the calendar to the student at least once a week.\nDiscuss not only the science, but also other issues they may be facing. Isn’t the workload too high?\nDo detailed code reviews (and pair programming, if possible), so they can learn software development from you.\nFor complicated changes, sometimes it may help to write code stubs. I.e., design interfaces, write most important function names (together with type annotations and docstrings) and ask the student to study them and fill in the missing bits.\n\nDesigning right abstractions is hard and it may take a lot of yor time.\nBut remember that this skill is hard even for experienced software developers, and many students haven’t had yet enough experience to handle these problems.\n\n\nTry to find balance with being hand-on and hands-off.\n\nI find it quite difficult and this balance is going to depend on the project difficulty and the student’s needs.\nGenerally, being hands-on is important, so that they feel supported. Mentorship is a key part of our job.\nOn the other hand, micromanagement is bad.\nEven if somebody is not not micromanaging, sometimes being too available can harm the student.\n\nThey need to know how to use available resources and work independently. (Whether they stay in academia or move to industry.)\nDiscuss with the students that they are expected to check obvious resources (like Wikipedia, Google, ChatGPT, project documentation…) before reaching out.\nIn some teams there is something like a formal “15-minute rule” and “1-hour rule”, but I think it’s better to rely on discussing the expectations early.\nIn some cases, answering the questions with a delay of a few hours seemed to reinforce the students to search for the answers on their own. I’m not convinced about this method, though, and would prefer to rely on clear communication of expectations."
  },
  {
    "objectID": "posts/setting-up-repository.html#setting-up-a-repository",
    "href": "posts/setting-up-repository.html#setting-up-a-repository",
    "title": "Setting up a new project repository",
    "section": "Setting up a repository",
    "text": "Setting up a repository\nGreat: the research plan tells the team what they need to do, expectations are clear, but we still haven’t set up the repository.\nI recommend taking studying how Equinox is set up, which is the model solution.\n\nSee .github/workflows to see how GitHub Actions are set.\nNote that pre-commit is used and configured.\nProject manager is used. Equinox uses Hatch, I usually go for Poetry.\n\nMy personal steps to creating a new repository are the following:\n\nCreate a new Micromamba environment. (There are many other alternatives to virtual environments, though.)\nInitialise the project. I use Poetry and the src layout, i.e., poetry new --src project_name.\nCreate a GitHub repository and commit the changes.\nSet up tests using PyTest.\n\nYou can create a simple function and one unit test.\n\nSet up pre-commit checks.\n\nFor linting and formatting, I use Ruff. (Alternatives include Black, flake8 and isort.)\nFor type checking I usually go for Pyright. (Alternatives include MyPy and PyType.)\n\nSet up documentation using Material for Mkdocs.\nMake sure that GitHub Actions are configured and properly run. (As a starting point, consult the configurations of Equinox or pMHN.)\nSet up pull request merging criteria on GitHub (depending on what has been agreed with the team):\n\nNobody can commit to main branch without a pull request.\nPre-commit checks and unit tests run properly.\nThe code is reviewed and conversations are resolved.\nThe branch has all changes from main (disputable).\nThe pull request is squashed into one commit on main, rather than merged (disputable).\n\n\nRegarding the data science workflow (which often includes developing some Python package, but also running experiments and generating the figures):\n\nWe usually develop Python code in src/ and use test/ and docs/ for unit tests for its documentation.\nTo run experiments, we use Snakemake workflows in the workflows/ directory.\n\nEnsuring reproducibility is important.\nMicromamba works very nicely with Snakemake.\nAlternatives include NextFlow (I personally liked Snakemake more, though).\n\nKedro sounds interesting. At the moment of writing this post I haven’t had much experience with it, but I’m planning to try it."
  },
  {
    "objectID": "posts/adjoint-group-homomorphisms.html",
    "href": "posts/adjoint-group-homomorphisms.html",
    "title": "Adjoint group homomorphisms",
    "section": "",
    "text": "Consider a group \\(G\\). Its delooping is a one-object category, \\(\\mathbf{B}G\\), such that it has a single object \\(\\bullet_G\\) and each \\(g\\in G\\) is treated as a morphism \\(\\bullet_G \\to \\bullet_G\\). The morphism composition is then the usual group composition. A group homomorphism \\(f\\colon G\\to H\\) is then essentially the same as a functor \\(\\mathbf{B}f\\colon \\mathbf{B}G\\to \\mathbf{B}G\\) (with the difference that the functor \\(\\mathbf{B}f\\) additionally maps the object \\(\\bullet_G\\) to the object \\(\\bullet_H\\)).\nConsider now two group homomorphisms, \\(f\\colon G\\to H\\) and \\(u\\colon H\\to G\\). If we treat them as functors, \\(\\mathbf{B}f\\) and \\(\\mathbf{B}u\\), under which circumstances are they adjoint? In other words, what are the properties of homomorphisms of \\(f\\) and \\(u\\), such that \\(\\mathbf{B}f \\dashv \\mathbf{B}u\\)?\nFor an adjunction we need a natural isomorphism \\(\\psi_{\\bullet_G\\bullet_H}\\colon \\mathrm{Hom}_{\\mathbf{B}H}( \\mathbf{B}f(\\bullet_G), \\bullet_H) \\to \\mathrm{Hom}_{\\mathbf{B}G}(\\bullet_G, \\mathbf{B}g(\\bullet_H) )\\).This is just a convoluted way of writing that we need a bijection \\(\\psi\\colon H\\to G\\) with additional properties, making it the natural isomorphism.\nWhat properties? Let’s introduce some notation. For an arrow \\(h\\in H\\) we write \\(h^\\flat = \\psi(h)\\). If \\(\\phi\\colon G\\to H\\) be the inverse, \\(\\phi = \\psi^{-1}\\), and \\(g\\in G\\), we write \\(g^\\sharp = \\phi(g)\\). Of course, we have \\((h^\\flat)^\\sharp = h\\) and \\((g^\\sharp)^\\flat = g\\) for all \\(h\\) and \\(g\\).\nThe first property (naturality in \\(\\mathbf{B}H\\)) reads that for every \\(h_1\\) and \\(h_2\\) in \\(H\\), we have \\[\n    u(h_1) h_2^\\flat = (h_1h_2)^\\flat,\n\\] or equivalently, \\[\n  u(h_1) \\psi(h_2) = \\psi(h_1h_2).  \n\\]\nThe second property (naturality in \\(\\mathbf{B}G\\)) reads that for every \\(g\\in G\\) and \\(h \\in H\\) it holds that \\[\n    (h \\circ f(g))^\\flat = h^\\flat g.\n\\]\nThis looks asymmetric, but we can fix it. As \\(G\\) and \\(H\\) have to be bijections, we can write \\(h = g_1^\\sharp\\). Rename \\(g\\) to \\(g_2\\). Then, \\[\n    (g_1^\\sharp f(g_2))^\\flat = g_1 g_2,\n\\]\nor equivalently \\[\n    g_1^\\sharp f(g_2) = (g_1 g_2)^\\sharp,\n\\]\nor equivalently \\[\n    \\phi(g_1) f(g_2) = \\phi(g_1g_2).\n\\]\nTo summarise, we require \\[\n    \\phi(g_1)f(g_2) = \\phi(g_1 g_2), \\quad u(h_1) \\psi(h_2) = \\psi(h_1 h_2).\n\\]\nfor all \\(g_1, g_2\\in G\\) and \\(h_1, h_2 \\in H\\). Note that these conditions differ from the definition of a homomorphism, having \\(f(g_2)\\) instead of \\(\\phi(g_2)\\) and \\(u(h_1)\\) instead of \\(\\psi(h_1)\\). They are also slightly asymmetric, which is related to the fact that adjoint functors also have this asymmetry: we want \\(f\\) to be the left adjoint and \\(g\\) to be the right adjoint.\nThese conditions are, actually, very restrictive: if we take $g_1 to be the identity element, then we have \\[\n    \\phi(1_G) f(g) = \\phi(g)\n\\]\nfor all \\(g\\in G\\). We can therefore write \\(f\\) as a composition of bijective functions, i.e., \\(f\\) is also bijective. And as \\(f\\) is a bijective group homomorphism, it has to be a group isomorphism.\nAnalogous argument proves that any right adjoint \\(u\\) has to be a group isomorphism as well. Let’s now think how \\(f\\) and \\(u\\) can be related. A natural candidate for a right adjoint \\(u\\) for \\(f\\) would be its inverse.\nProposition: If \\(f\\colon G\\to H\\) is a group isomorphism, then we have an adjunction \\(f \\dashv f^{-1}\\).\nProof: Define \\(\\phi(g) = f(g)\\), so that \\(\\psi(h) = f^{-1}(h)\\). The naturality equations are then obvious.\nIf \\(u\\) is a group isomorphism, then also \\(u^{-1}\\) is a group isomorphism, and we obtain \\(u^{-1} \\dashv u\\) adjunction. The above adjunction (i.e., use the inverse) is almost the only one existing: adjoint functors are unique up to a natural isomorphism.\nLet’s therefore think what it means for two group homomorphisms \\(f_1\\colon G\\to H\\) and \\(f_2\\colon G\\to H\\) to be naturally isomorphic. As usual, we have only a single object \\(\\bullet_G\\) in the category \\(\\mathbf{B}G\\), indexing a single arrow \\(a\\colon \\bullet_G \\to \\bullet_G\\), i.e., \\(a\\in G\\). As it is a natural transformation, we have to have \\[\n    f_1(g) a = a f_2(g)\n\\]\nfor every \\(g\\in G\\) and \\(f_2(g) = a^{-1} f_1(g) a\\). By writing \\(\\sigma_a\\colon h\\to h\\) to be the inner automorphism \\(h\\mapsto a^{-1} h a\\), we see that \\(f_2 = \\sigma_a f_1\\) for some \\(a\\).\nThis allows us to formulate the theorem characterising adjunctions:\nTheorem: Every group isomorphism \\(f\\colon G\\to H\\) has both left and right adjoints, given by \\(\\sigma_{\\hat g} f^{-1}\\) for arbitrary elements \\({\\hat g}\\in G\\). Conversely, if \\(f\\dashv u\\) is any adjunction, then \\(f\\) and \\(u\\) are group isomorphisms and \\(u = \\sigma_{\\hat g} f^{-1}\\) for some \\(\\hat g \\in G\\).\nAt this point we see that the only examples of adjoint group homomorphisms arise from group isomorphisms and (appropriately conjugated) inverses."
  },
  {
    "objectID": "posts/adjoint-group-homomorphisms.html#where-are-the-monads",
    "href": "posts/adjoint-group-homomorphisms.html#where-are-the-monads",
    "title": "Adjoint group homomorphisms",
    "section": "Where are the monads?",
    "text": "Where are the monads?\nFinally, let’s take a look at a monad arising from an adjunction. Before we start, let’s fix some notation. Take \\(f \\dashv u\\) for \\(u = \\sigma_{\\hat g} f^{-1}\\) and \\(\\hat{g} \\in G\\). Hence, we have \\(uf = \\sigma_{\\hat g}\\colon G\\to G\\). The other composition can be obtained as \\[\n    fu(h) = f\\left( \\hat g^{-1} f^{-1}(h) \\hat g \\right) = f(\\hat g)^{-1} h f(\\hat g) = \\sigma_{\\hat h} h\n\\]\nfor \\(\\hat h = f(\\hat g)\\).\nWe need one more thing: the \\(\\phi = \\psi^{-1}\\) bijection. Let’s take \\(\\phi(g) = \\hat h f(g)\\) and \\(\\psi(h) = \\hat g^{-1} f^{-1}(h)\\). We have \\[\n    \\psi( \\phi(g) ) = \\hat g^{-1} f^{-1} ( \\hat h f(g) ) = \\hat g^{-1} f^{-1}(\\hat h) g = \\hat g^{-1} \\hat g g = g\n\\]\nand\n\\[\n    \\phi( \\psi(h) ) = \\hat h f( \\hat g^{-1} f^{-1}(h) ) = \\hat h f(\\hat g)^{-1} h = h,\n\\]\nso that these are indeed inverse to each other. We can also verify that\n\\[\n    \\phi(g_1)f(g_2) = \\hat h f(g_1)f(g_2) = \\hat h f(g_1 g_2) = \\phi(g_1 g_2)\n\\] and \\[\n    u(h_1) \\psi(h_2) = \\left(\\hat g^{-1} f^{-1}(h_1) \\hat g\\right) \\left( \\hat g^{-1} f^{-1}(h_2) \\right) = \\hat g^{-1} f^{-1}(h_1h_2) = \\psi(h_1 h_2).\n\\]\nThe unit of the adjunction is given by \\[\n    \\eta = (1_H)^{\\flat} = \\psi(1_H) = \\hat g^{-1}\n\\]\nand is a natural transformation from the identity functor \\(1_{\\mathbf{B}G}\\) to the functor given by \\(uf = \\sigma_{\\hat g}\\), which can be verified as \\[\n    \\hat g^{-1} (-) =  \\left( \\hat g^{-1} ( - ) \\hat g\\right) \\hat g^{-1}.\n\\]\nWhat is the counit? Analogously, it is \\(\\epsilon = (1_G)^\\sharp = \\hat h\\). Let’s quickly verify the triangle identities:\n\\(\\epsilon f(\\eta) = \\hat h f( \\hat g^{-1} ) = 1_H\\) and \\(u(\\epsilon) \\eta = (\\hat g^{-1} f^{-1}(\\epsilon) \\hat g) \\hat g^{-1} = \\hat g^{-1} f^{-1}(\\hat h) = 1_G\\).\nThe monad now is given by a functor \\(t = uf = \\sigma_{\\hat g}\\colon G\\to G\\), the unit \\(\\eta = \\hat g^{-1}\\), and the multiplication \\(\\mu = u\\epsilon f\\), which is a natural transformation from \\(t^2\\) to \\(t\\). More explicitly, multiplication is given by \\(\\mu = u(\\epsilon) = \\sigma_{\\hat g}f^{-1}(\\epsilon) = \\sigma_{\\hat g}( \\hat g ) = \\hat g\\). It is a natural transformation as \\[\n    \\mu \\, t^2(g) = t(g)\\, \\mu  \n\\]\nfor every \\(g\\in G\\): both sides evaluate to \\(\\hat g^{-1}\\, g\\, \\hat g^2\\).\nIf \\(\\hat g\\) is taken to be the identity element, then this monad is just the identity monad. What is the meaning of this one? I am not sure…"
  },
  {
    "objectID": "posts/small-power.html",
    "href": "posts/small-power.html",
    "title": "From \\(t\\)-test to “This is what ‘power=0.06’ looks like”",
    "section": "",
    "text": "Andrew Gelman wrote an amazing blogpost, where he argues that with large noise-to-signal ratio (low power studies) statistically significant results:\nThis is especially troublesome because as there is a tradition of publishing only statistically significant results1, many of the reported effects will have the wrong sign or be seriously exaggerated.\nBut we’ll take a look at these phenomena later. First, let’s review what the \\(t\\)-test, confidence intervals, and statistical power are."
  },
  {
    "objectID": "posts/small-power.html#a-quick-overview-of-t-test",
    "href": "posts/small-power.html#a-quick-overview-of-t-test",
    "title": "From \\(t\\)-test to “This is what ‘power=0.06’ looks like”",
    "section": "A quick overview of \\(t\\)-test",
    "text": "A quick overview of \\(t\\)-test\nRecall that if we have access to i.i.d. random variables\n\\[\nX_1, \\dotsc, X_n \\sim \\mathcal N(\\mu, \\sigma^2),\n\\] we introduce sample mean \\[\nM = \\frac{X_1 + \\cdots + X_n}{n}\n\\] and sample standard deviation: \\[\nS = \\sqrt{ \\frac{1}{n-1} \\sum_{i=1}^n \\left(X_i - M\\right)^2}.\n\\]\nIt follows that \\[\nM\\sim \\mathcal N(\\mu, \\sigma^2/n)\n\\] and \\[\nS^2\\cdot (n-1)/\\sigma^2 \\sim \\chi^2_{n-1}\n\\] are independent. We can construct a pivot \\[\nT_\\mu = \\frac{M-\\mu}{S / \\sqrt{n}},\n\\] which is distributed according to Student’s \\(t\\) distribution2 with \\(n-1\\) degrees of freedom, \\(t_{n-1}\\).\nChoose a number \\(0 &lt; \\alpha &lt; 1\\) and write \\(u\\) for the solution to the equation \\[\nP(T_\\mu \\in (-u, u)) = 1-\\alpha.\n\\]\nThe \\(t\\) distribution is continuous and symmetric around \\(0\\), so that \\[\n\\mathrm{CDF}(-u) = 1 - \\mathrm{CDF}(u)\n\\] and \\[\nP(T_\\mu \\in (-u, u)) = \\mathrm{CDF}(u) - \\mathrm{CDF}(-u) = 2\\cdot \\mathrm{CDF}(u) - 1.\n\\]\nFrom this we have \\[\n\\mathrm{CDF}(u) = 1 - \\alpha / 2.\n\\]\nHence, we have \\[\nP(\\mu \\in ( M - \\delta, M + \\delta )) = 1 - \\alpha,\n\\] where \\[\n\\delta = u \\cdot\\frac{S}{\\sqrt{n}}\n\\] is the half-width of the confidence interval.\nIn this way we have constructed a confidence interval with coverage \\(1-\\alpha\\). Note that \\(u\\) coresponds to the \\((1-\\alpha/2)\\)th quantile. For example, if we want coverage of \\(95\\%\\), we take \\(\\alpha=5\\%\\) and \\(u\\) will be the 97.5th percentile.\n\nHypothesis testing\nConsider a hypothesis \\(H_0\\colon \\mu = 0\\).\nWe will reject \\(H_0\\) if \\(0\\) is outside of the confidence interval defined above. Note that \\[\nP( 0 \\in (M-\\delta, M + \\delta)  \\mid H_0 ) = 1-\\alpha\n\\] and \\[\nP(0 \\not\\in (M-\\delta, M + \\delta) \\mid H_0 ) = 1 - (1-\\alpha) = \\alpha,\n\\] meaning that such a test will have false discovery rate \\(\\alpha\\).\n\n\nInterlude: \\(p\\)-values\nThe above test is often presented in terms of \\(p\\)-values. Define the \\(t\\)-statistic \\[\nT_0 = \\frac{M}{S/\\sqrt{n}},\n\\] which does not require the knowledge of a true \\(\\mu\\). Note that if \\(H_0\\) is true, then \\(T_0\\) will be distributed according to \\(t_{n-1}\\) distribution. (If \\(H_0\\) is false, then it won’t be. We will take a closer look at this case when we discuss power).\nWe have \\(T_0 \\in (-u, u)\\) if and only if \\(0\\) is inside the confidence interval defined above: we can reject \\(H_0\\) whenever \\(|T_0| \\ge u\\). As the procedure hasn’t really changed, we also have \\[\nP(T_0 \\in (-u, u) \\mid H_0 ) = P( |T_0| &lt; u \\mid H_0 ) = 1-\\alpha\n\\] and this test again has false discovery rate \\(\\alpha\\).\nNow define \\[\n\\Pi = 2\\cdot (1 - \\mathrm{CDF}( |T_0| ) ) = 2\\cdot \\mathrm{CDF}(-|T_0|).\n\\] Note that we have\n\\[\\begin{align*}\n  \\Pi &lt; \\alpha &\\Leftrightarrow 2\\cdot (1-\\mathrm{CDF}(|T_0|)) &lt; 2\\cdot (1-\\mathrm{CDF}(u)) \\\\\n  &\\Leftrightarrow 1- \\mathrm{CDF}(|T_0|) &lt; 1 - \\mathrm{CDF}(u) \\\\\n  &\\Leftrightarrow \\mathrm{CDF}(-|T_0|) &lt; \\mathrm{CDF}(-u)\\\\\n  &\\Leftrightarrow -|T_0| &lt; -u \\\\\n  &\\Leftrightarrow |T_0| &gt; u,\n\\end{align*}\n\\]\nso that we can compare the \\(p\\)-value \\(\\Pi\\) against false discovery rate \\(\\alpha\\).\nBoth comparisons are equivalent and equally hard to compute: the hard part of comparing \\(|T_0|\\) against \\(u\\) is calculation of \\(u\\) from \\(\\alpha\\) (which requires the access to the CDF). The hard part of comparing the \\(p\\)-value \\(\\Pi\\) against \\(\\alpha\\) is calculation of \\(\\Pi\\) from \\(|T_0|\\) (which requires the access to the CDF). I should also add that, as we will see below, the CDF is actually easy to access in Python.\nLet’s also observe that we have \\[\nP( \\Pi \\le \\alpha \\mid H_0 ) = P( |T_0| \\ge u \\mid H_0 ) = \\alpha,\n\\] meaning that if the null hypothesis is true, then \\(\\Pi\\) has the uniform distribution over the interval \\((0, 1)\\).\n\n\nSummary\n\nChoose a coverage level \\(1-\\alpha\\).\nCollect a sample of size \\(n\\).\nCalculate \\(u\\) as using \\(\\mathrm{CDF}(u)=1-\\alpha/2\\), or equivalently, \\(\\alpha = 2\\cdot ( 1 - \\mathrm{CDF}(u) )\\), where we use the CDF of the Student distribution with \\(n-1\\) degrees of freedom.\nCalculate sample mean and sample standard deviation, \\(M\\) and \\(S\\).\nConstruct the confidence interval as \\(M\\pm uS/\\sqrt{n}\\).\nTo test for \\(H_0\\colon \\mu = 0\\) check if \\(\\mu_0\\) is outside of the confidence interval (to reject \\(H_0\\)).\nAlternatively, construct \\(T_0 = M\\sqrt{n}/S\\) and compare \\(|T_0| &gt; u\\) (to reject \\(H_0\\)).\nAlternatively, construct \\(\\Pi = 2\\cdot (1-\\mathrm{CDF}(|T_0|))\\) and check whether \\(\\Pi &lt; \\alpha\\) (to reject \\(H_0\\)).\n\n\n\nA bit of code\nLet’s implement the above formulae using SciPy’s \\(t\\) distribution to keep things as related to the formulae above as possible. Any discrepancies will suggest an error in the derivations or a bug in the code.\n\n\nCode\nimport numpy as np\n\nfrom scipy import stats\n\ndef alpha_to_u(alpha: float, nobs: int) -&gt; float:\n  if np.min(alpha) &lt;= 0 or np.max(alpha) &gt;= 1:\n    raise ValueError(\"Alpha has to be inside (0, 1).\")\n  return stats.t.ppf(1 - alpha / 2, df=nobs - 1)\n\n\ndef u_to_alpha(u: float, nobs: int) -&gt; float:\n  if np.min(u) &lt;= 0:\n    raise ValueError(\"u has to be positive\")\n  return 2 * (1 - stats.t.cdf(u, df=nobs - 1))\n\n# Let's test whether the functions seem to be compatible\nfor alpha in [0.01, 0.05, 0.1, 0.99]:\n  for nobs in [5, 10]:\n    u = alpha_to_u(alpha, nobs=nobs)\n    a = u_to_alpha(u, nobs=nobs)\n\n    if abs(a - alpha) &gt; 1e-4:\n      raise ValueError(f\"Discrepancy for {nobs=} {alpha=}\")\n\n\ndef calculate_t(xs: np.ndarray):\n  # Sample mean and sample standard deviation\n  n = len(xs)\n  m = np.mean(xs)\n  s = np.std(xs, ddof=1)\n\n  # Calculate the t value assuming the null hypothesis mu = 0\n  t = m / (s / np.sqrt(n))\n  return t\n\ndef calculate_p_value_from_t(t: float, nobs: int) -&gt; float:\n  return 2 * (1 - stats.t.cdf(np.abs(t), df=nobs-1))\n\ndef calculate_p_value_from_data(xs: np.ndarray) -&gt; float:\n  n = len(xs)\n  t = calculate_t(xs)\n  return calculate_p_value_from_t(t=t, nobs=n)\n\ndef calculate_ci_delta_from_params(\n  s: float,\n  nobs: int,\n  alpha: float,\n) -&gt; tuple[float, float]:\n  u = alpha_to_u(alpha, nobs=nobs)\n  delta = u * s / np.sqrt(nobs)\n  return delta\n\ndef calculate_ci_delta_from_data(xs: np.ndarray, alpha: float) -&gt; float:\n  m = np.mean(xs)\n  s = np.std(xs, ddof=1)\n  delta = calculate_ci_delta_from_params(s=s, nobs=len(xs), alpha=alpha)\n  return delta\n\ndef calculate_confidence_interval_from_data(\n  xs: np.ndarray,\n  alpha: float,\n) -&gt; tuple[float, float]:\n  delta = calculate_ci_delta_from_data(xs, alpha=alpha)\n  return (m-delta, m+delta)\n\n\nWe have three equivalent tests for rejecting \\(H_0\\). Let’s see how they perform on several samples. We’ll simulate \\(N\\) times a fresh data set \\(X_1, \\dotsc, X_n\\) from \\(\\mathcal N\\left(0, \\sigma^2\\right)\\) and calculate the confidence interval, the \\(T_0\\) statistic and the \\(p\\)-value for each of these. We will order the samples by increasing \\(p\\)-value (equivalently, with decreasing \\(|T_0|\\) statistic), to make dependencies in the plot easier to see.\nAdditionally, we will choose relatively large \\(\\alpha\\) and mark the regions such that the test does not reject \\(H_0\\). In terms of confidence intervals there is no such region, as each sample which has its own confidence interval, which can contain \\(0\\) or not.\n\n\nCode\nimport matplotlib.pyplot as plt\nplt.style.use(\"dark_background\")\n\nrng = np.random.default_rng(42)\n\nnsimul = 100\nnobs = 5\n\nsamples = rng.normal(loc=0, scale=1.0, size=(nsimul, nobs))\np_values = np.asarray([calculate_p_value_from_data(x) for x in samples])\nindex = np.argsort(p_values)\n\n\nalpha: float = 0.1\nu_thresh = alpha_to_u(alpha, nobs=nobs)\n\nsamples = samples[index, :]\np_values = p_values[index]\nt_stats = np.asarray([calculate_t(x) for x in samples])\nsample_means = np.mean(samples, axis=1)\nci_deltas = np.asarray([calculate_ci_delta_from_data(x, alpha=alpha) for x in samples])\n\nx_axis = np.arange(1, nsimul + 1)\nfig, axs = plt.subplots(3, 1, figsize=(3, 4), dpi=150, sharex=True)\n\n# P-values plot\nax = axs[0]\nax.fill_between(x_axis, alpha, 0, color=\"lime\", alpha=0.2)\nax.plot(x_axis, np.linspace(0, 1, num=len(x_axis)), linestyle=\"--\", linewidth=0.5, color=\"white\")\nax.scatter(x_axis, p_values, c=\"yellow\", s=1)\nax.set_xlim(-0.5, nsimul + 0.5)\nax.set_ylabel(\"$p$-value\")\n\nax.axvline(alpha * nsimul + 0.5, color=\"maroon\", linestyle=\"--\", linewidth=0.5)\n\n# T statistics\nax = axs[1]\nax.fill_between(x_axis, -u_thresh, u_thresh, color=\"lime\", alpha=0.2)\nax.scatter(x_axis, t_stats, c=\"yellow\", s=1)\nax.plot(x_axis, np.zeros_like(x_axis), c=\"white\", linestyle=\"--\", linewidth=0.5)\nax.set_ylabel(\"$t$ statistic\")\n\nax.axvline(alpha * nsimul + 0.5, color=\"maroon\", linestyle=\"--\", linewidth=0.5)\n\n# Confidence intervals\nax = axs[2]\nax.set_xlabel(\"Sample index\")\nax.plot(x_axis, np.zeros_like(x_axis), c=\"white\", linestyle=\"--\", linewidth=0.5)\nax.errorbar(x_axis, sample_means, yerr=ci_deltas, fmt=\"o\", markersize=1, linewidth=0.5, c=\"yellow\")\nax.set_ylabel(\"Conf. int.\")\n\nax.axvline(alpha * nsimul + 0.5, color=\"maroon\", linestyle=\"--\", linewidth=0.5)\n\nfor ax in axs:\n  ax.spines[['top', 'right']].set_visible(False)\n\nfig.tight_layout()\n\n\n\n\n\nLet’s also quickly verify that, indeed, the distribution of \\(p\\)-values is uniform over \\((0, 1)\\) (which, in a way, can be already deduced from the plot with ordered \\(p\\) values) and that the distribution of the \\(t\\) statistic is indeed \\(t_{n-1}\\).\n\n\nCode\nfig, axs = plt.subplots(1, 3, figsize=(5, 2), dpi=150)\n\nax = axs[0]\nax.set_title(\"CDF ($p$-value)\")\nx_ax = np.linspace(0, 1, 5)\nax.plot(x_ax, x_ax, color=\"white\", linestyle=\"--\", linewidth=1)\nax.ecdf(p_values, c=\"maroon\")\n\nax = axs[1]\nax.set_title(\"CDF ($t$-stat.)\")\nx_ax = np.linspace(-3.5, 3.5, 101)\nax.plot(x_ax, stats.t.cdf(x_ax, df=nobs-1), color=\"white\", linestyle=\"--\", linewidth=1)\nax.ecdf(t_stats, c=\"maroon\")\n\nax = axs[2]\nax.set_title(\"CDF (mean)\")\nx_ax = np.linspace(-1.2, 1.2, 101)\nax.plot(x_ax, stats.norm.cdf(x_ax, scale=1/np.sqrt(nobs)), color=\"white\", linestyle=\"--\", linewidth=1)\nax.ecdf(sample_means, c=\"maroon\")\n\nfor ax in axs:\n  ax.spines[['top', 'right']].set_visible(False)\n\nfig.tight_layout()"
  },
  {
    "objectID": "posts/small-power.html#statistical-power",
    "href": "posts/small-power.html#statistical-power",
    "title": "From \\(t\\)-test to “This is what ‘power=0.06’ looks like”",
    "section": "Statistical power",
    "text": "Statistical power\nAbove we have seen a procedure used to reject the null hypothesis \\(H_0\\colon \\mu = 0\\) either by constructing the confidence interval or defining the variable \\(T_0\\) with the property that \\(P( |T_0| &gt; u \\mid H_0) = \\alpha.\\)\nConsider now the data coming from a distribution with any other \\(\\mu\\), i.e., we will not condition on \\(H_0\\) anymore. To make this explicit in the notation, we will condition on \\(H_\\mu\\), rather than \\(H_0\\).\nHow does the distribution of \\(T_0\\) look like now? Recall that have independent variables \\[\n\\frac{M-\\mu}{\\sigma/\\sqrt{n}} \\sim \\mathcal N(0, 1)\n\\] and \\[\nS^2\\cdot (n-1)/\\sigma^2 \\sim \\chi^2_{n-1}\n\\] so that we have, of course, \\[\nT_\\mu = \\frac{M-\\mu}{S/\\sqrt{n}} =  \n\\frac{ \\frac{M-\\mu}{\\sigma/\\sqrt{n}}}{\\sqrt{ \\frac{ S^2\\cdot (n-1)/\\sigma^2 }{n-1} }}\n\\sim t_{n-1}.\n\\] For \\(T_0\\) we have \\[\nT_0 = \\frac{ \\frac{M-\\mu}{\\sigma/\\sqrt{n}} + \\frac{\\mu}{\\sigma/\\sqrt{n}}}{\\sqrt{ \\frac{ S^2\\cdot (n-1)/\\sigma^2 }{n-1} }}\n\\] which is distributed according to the noncentral \\(t\\) distribution with noncentrality parameter \\(\\theta = \\mu\\sqrt{n} / \\sigma\\). Note that this distribution is generally asymmetric and different from the location-scale generalisation of the (central) \\(t\\) distribution. Let’s write \\(F_{n-1, \\theta}\\) for the CDF of this function. We will reject the null hypothesis \\(H_0\\) with probability \\[\\begin{align*}\nP(|T_0| &gt; u \\mid H_\\mu) &= P(T_0 &gt; u \\mid H_\\mu ) + P(T_0 &lt; -u \\mid H_\\mu) \\\\\n&= 1 - P(T_0 &lt; u \\mid H_\\mu) + P(T_0 &lt; -u \\mid H_\\mu) \\\\\n&= 1 - F_{n-1,\\theta}(u) + F_{n-1,\\theta}(-u),\n\\end{align*}\n\\]\nwhich gives us statistical power of the test.\nWe see that power depends on chosen \\(\\alpha\\) (as it controls \\(u\\), the value we compare against the \\(|T_0|\\) statistic), on \\(n\\) (as it controls both the number of degrees of freedom in the CDF \\(F_{n-1,\\theta}\\) and the noncentrality parameter \\(\\theta\\)) and on the effect size, by which we understand the standardized mean difference, i.e., \\(\\mu/\\sigma\\).\n\nAnother bit of code\nLet’s implement power calculation. In fact, statsmodels has very convenient utilities for power calculation, so in practice implementing it is never needed:\n\n\nCode\nfrom statsmodels.stats.power import TTestPower\n\ndef calculate_power(\n  effect_size: float,\n  nobs: int,\n  alpha: float,\n) -&gt; float:\n  theta = np.sqrt(nobs) * effect_size\n  u = alpha_to_u(alpha, nobs=nobs)\n\n  def cdf(x: float) -&gt; float:\n    return stats.nct.cdf(x, df=nobs-1, nc=theta)\n  \n  return 1 - cdf(u) + cdf(-u)\n\nfor nobs in [5, 10]:\n  for alpha in [0.01, 0.05, 0.1]:\n    for effect_size in [0, 0.1, 1.0, 3.0]:\n      power = calculate_power(\n        effect_size=effect_size, nobs=nobs, alpha=alpha,\n      )\n      power_ = TTestPower().power(\n        effect_size=effect_size, nobs=nobs, alpha=alpha, alternative=\"two-sided\")\n      \n      if abs(power - power_) &gt; 0.001:\n        raise ValueError(f\"For {nobs=} {alpha=} {effect_size=} we noted discrepancy {power} != {power_}\")\n\n\nLet’s quickly check how power depends on the effect size in the following setting. We collect \\(X_1, \\dotsc, X_n\\sim \\mathcal N(\\mu, 1^2)\\), so that standardized mean difference is \\(\\mu = \\mu/1\\) and we will use standard \\(\\alpha = 5\\%\\).\n\n\nCode\nfig, axs = plt.subplots(1, 3, figsize=(7, 3), dpi=150, sharey=True)\n\nax = axs[0]\nmus = np.linspace(0, 1.5)\nax.plot(\n  mus, calculate_power(effect_size=mus, nobs=10, alpha=0.05)\n)\nax.set_xlabel(r\"$\\mu/\\sigma$\")\nax.set_ylabel(\"Power\")\n\nax = axs[1]\nns = np.arange(5, 505)\nfor eff in [0.1, 0.5, 1.0]:\n  ax.plot(\n    ns, calculate_power(effect_size=eff, nobs=ns, alpha=0.05),\n    label=f\"$\\mu/\\sigma={eff:.1f}$\"\n  )\nax.set_xscale(\"log\")\nn_labels = np.asarray([5, 10, 20, 100, 500])\nax.set_xticks(n_labels, n_labels)\nax.set_xlabel(\"$n$\")\n\nax = axs[2]\nalphas = np.linspace(0.01, 0.99, 99)\nfor eff in [0.1, 0.5, 1.0]:\n  ax.plot(\n    alphas, calculate_power(effect_size=eff, nobs=5, alpha=alphas),\n    label=f\"$\\mu/\\sigma={eff:.1f}$\"\n  )\nax.legend(frameon=False)\nax.set_xlabel(r\"$\\alpha$\")\n\nfor ax in axs:\n  ax.spines[['top', 'right']].set_visible(False)\n\nfig.tight_layout()\n\n\n/home/pawel/micromamba/envs/data-science/lib/python3.10/site-packages/scipy/stats/_continuous_distns.py:7313: RuntimeWarning: invalid value encountered in _nct_cdf\n  return np.clip(_boost._nct_cdf(x, df, nc), 0, 1)"
  },
  {
    "objectID": "posts/small-power.html#consequences-of-low-power",
    "href": "posts/small-power.html#consequences-of-low-power",
    "title": "From \\(t\\)-test to “This is what ‘power=0.06’ looks like”",
    "section": "Consequences of low power",
    "text": "Consequences of low power\nConsider a study with \\(\\sigma=1\\), \\(\\mu=0.1\\) and \\(n=10\\). At \\(\\alpha = 5\\%\\) we have power of:\n\n\nCode\npower = calculate_power(effect_size=0.1, nobs=10, alpha=0.05)\nif abs(power - 0.06) &gt; 0.005:\n  raise ValueError(f\"We want power to be around 6%\")\nprint(f\"Power: {100 * power:.2f}%\")\n\n\nPower: 5.93%\n\n\nwhich is similar to the value used in Andrew Gelman’s post. Let’s simulate a lot of data sets and confirm that the power of the test is indeed around 6%:\n\n\nCode\nnsimul = 200_000\nnobs = 10\nalpha = 0.05\n\ntrue_mean = 0.1\n\nsamples = rng.normal(loc=true_mean, scale=1.0, size=(nsimul, nobs))\np_values = np.asarray([calculate_p_value_from_data(x) for x in samples])\n\nprint(f\"Power from simulation: {100 * np.mean(p_values &lt; alpha):.2f}%\")\n\n\nPower from simulation: 5.97%\n\n\n(As a side note, I already regret not implementing \\(p\\)-value calculation in JAX – I can’t use vmap!)\n\n\nCode\nfig, axs = plt.subplots(1, 2, figsize=(5, 2), dpi=150, sharex=True)\n\nbins = np.linspace(-1.5, 1.5, 31)\n\nax = axs[0]\nsample_means = np.mean(samples, axis=1)\nax.hist(\n  sample_means,\n  bins=bins,\n  histtype=\"step\",\n)\nax.axvline(true_mean)\nax.set_title(\"Histogram of sample means\")\nax.set_xlabel(\"Sample mean\")\nax.set_ylabel(\"PDF\")\n\nax = axs[1]\nx_ax = np.linspace(-1.5, 1.5, 51)\nax.plot(\n  x_ax, stats.norm.cdf(x_ax, loc=0.1, scale=1/np.sqrt(nobs))\n)\nax.ecdf(sample_means)\nax.set_xlabel(\"Sample mean\")\nax.set_ylabel(\"CDF\")\nax.set_title(\"Empirical CDF\")\n\nfor ax in axs:\n  ax.spines[['top', 'right']].set_visible(False)\n\nfig.tight_layout()\n\n\n\n\n\nWe see that sample mean, represented by the random variable \\(M\\), is indeed distributed according to \\(\\mathcal N\\left(0.1, \\left(1/\\sqrt{10}\\right)^2\\right)\\). The standard error of the mean, \\(1/\\sqrt{10}\\approx 0.31\\) is three times larger than the population mean \\(\\mu=0.1\\), so we have a lot of noise here.\nLet’s see what happens if a “statistically significant” result is somehow obtained.\n\n\nCode\nstat_signif_samples = samples[p_values &lt; alpha, :]\n\nfig, ax = plt.subplots(figsize=(2, 2), dpi=150)\n\nsample_means = np.mean(stat_signif_samples, axis=1)\nax.hist(\n  sample_means,\n  bins=bins,\n  histtype=\"step\",\n)\nax.axvline(true_mean)\nax.set_title(\"Stat. signif. results\")\nax.set_xlabel(\"Sample mean\")\nax.set_ylabel(\"PDF\")\n\n\nax.spines[['top', 'right']].set_visible(False)\nfig.tight_layout()\n\n\n\n\n\nConditioning only on statistically significant results, let’s take a look at:\n\nhow many of them have sample mean of wrong sign,\nhow many of them have sample mean seriously exaggerated (e.g., at least 5 times),\n\nsimilarly as Andrew Gelman did in his blog post:\n\n\nCode\nfrac_wrong_sign = np.mean(sample_means &lt; 0)\nprint(f\"Wrong sign: {100*frac_wrong_sign:.1f}%\")\n\nfrac_exaggerated = np.mean(sample_means &gt;= 5 * true_mean)\nprint(f\"Exaggerated (5x): {100 * frac_exaggerated:.1f}%\")\n\n\nWrong sign: 20.8%\nExaggerated (5x): 69.5%\n\n\nOh, that’s not good! We see that a statistically significant result (which itself has only 6% occurrence probability, if the study is executed properly), will have about 20% chance of being of a wrong sign and around 2/3 chance of being quite exaggerated.\nActually, Andrew Gelman looked at results exaggerated 9 times. Let’s make a plot summarizing these probabilities:\n\n\nCode\nfig, ax = plt.subplots(figsize=(2, 2), dpi=150)\n\nratio_exag = np.linspace(1, 11, 31)\nfrac_exag = [np.mean(sample_means &gt;= r * true_mean) for r in ratio_exag]\n\nax.plot(\n  ratio_exag,\n  frac_exag,\n)\nax.set_ylim(0, 1)\nax.set_xlabel(\"Exag. factor\")\nax.set_ylabel(\"Probability\")\nxticks = [1, 3, 5, 7, 9]\nax.set_xticks(xticks, xticks)\n\nax.spines[['top', 'right']].set_visible(False)\nfig.tight_layout()\n\n\n\n\n\nFinally, there is a great plot by Art Owen showing how confidence intervals of statistically significant results look like when power is low. Let’s quickly reproduce it:\n\n\nCode\nci_deltas = np.asarray([calculate_ci_delta_from_data(x, alpha=alpha) for x in stat_signif_samples])\n\nn_to_plot = min(50, len(stat_signif_samples))\n\nx_axis = np.arange(1, n_to_plot + 1)\nfig, ax = plt.subplots(1, figsize=(3, 2), dpi=150)\n\nax.set_xlabel(\"Sample index\")\n\nax.plot(x_axis, np.zeros_like(x_axis), c=\"white\", linestyle=\"--\", linewidth=0.5)\nax.plot(x_axis, np.zeros_like(x_axis) + true_mean, c=\"maroon\", linestyle=\"-\", linewidth=0.5)\n\nax.errorbar(x_axis, sample_means[:n_to_plot], yerr=ci_deltas[:n_to_plot], fmt=\"o\", markersize=1, linewidth=0.5, c=\"yellow\")\nax.set_ylabel(\"Conf. int.\")\n\nax.spines[['top', 'right']].set_visible(False)\nfig.tight_layout()\n\n\n\n\n\nOf course, the confidence intervals cannot contain \\(0\\) (otherwise the results wouldn’t have been statistically significant). How many of them contain \\(\\mu=0.1\\)? In case we look at all the results (including the majority of nonsignificant results), \\(1-\\alpha=95\\%\\) of confidence intervals contains the true value. However, once we restrict our attention to only significant results, this coverage drops to\n\n\nCode\nfrac_contain = np.mean(\n  (sample_means - ci_deltas &lt; true_mean) \n  & (true_mean &lt; sample_means + ci_deltas)\n)\nprint(f\"Coverage: {100 * frac_contain:.1f}%\")\n\n\nCoverage: 37.2%"
  },
  {
    "objectID": "posts/small-power.html#summary-1",
    "href": "posts/small-power.html#summary-1",
    "title": "From \\(t\\)-test to “This is what ‘power=0.06’ looks like”",
    "section": "Summary",
    "text": "Summary\nLow-powered studies have, of course, high probability of not rejecting \\(H_0\\) when the alternative is true. But even when \\(H_0\\) is rejected, estimated mean will be often of wring sign or exaggerated. This indeed shows that principled experimental design and power analysis are crucial to execute before any data collection!"
  },
  {
    "objectID": "posts/small-power.html#footnotes",
    "href": "posts/small-power.html#footnotes",
    "title": "From \\(t\\)-test to “This is what ‘power=0.06’ looks like”",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nA practice, which can result in p-hacking and HARKing. On a related manner see this post on negative results and the paper on “the garden of forking paths”.↩︎\nStudent’s \\(t\\) distribution with \\(k\\) degrees of freedom arises as the distribution of the variable \\(T = A/\\sqrt{B / k}\\), where \\(A\\sim \\mathcal N(0, 1)\\) and \\(B\\sim \\chi^2_k\\) are independent. In this case, \\(\\frac{M-\\mu}{\\sigma/\\sqrt{n}}\\sim N(0, 1)\\) and \\(S^2\\cdot (n-1)/ \\sigma^2 \\sim \\chi^2_{n-1}\\) are independent. Parameter \\(\\sigma\\) cancels out.↩︎"
  },
  {
    "objectID": "posts/conditioning-multivariate-normal.html",
    "href": "posts/conditioning-multivariate-normal.html",
    "title": "Starting (on finite domains) with Gaussian processes",
    "section": "",
    "text": "Let \\(Y = (Y_1, \\dotsc, Y_n)^T\\) be a random variable distributed according to the multivariate normal distribution \\(\\mathcal N(\\mu, \\Sigma)\\), where \\(\\mu\\in \\mathbb R^n\\) and \\(\\Sigma\\) is a real symmetric positive-definite1 \\(n\\times n\\) matrix.\nWe will think of this distribution in the following manner: we have a domain \\(\\mathcal X = \\{1, 2, \\dotsc, n\\}\\) and for each \\(x\\in \\mathcal X\\) we have a random variable \\(Y_i\\) and the joint distribution \\(P(Y_1, \\dotsc, Y_n)\\) is multivariate normal.\nAssume that we have measured \\(Y_x\\) variables for indices \\(x_1, \\dotsc, x_k\\), with corresponding values \\(Y_{x_1}=y_1, \\dotsc, Y_{x_k}=y_k\\), and we are interested in predicting the values at locations \\(x'_1, \\dotsc, x'_q\\), i.e., modelling the conditional probability distribution \\[\nP(Y_{x'_1}, \\dotsc, Y_{x'_q} \\mid Y_{x_1}=y_1, \\dotsc, Y_{x_k}=y_k).\n\\]\nWe will also allow \\(k=0\\), i.e., we would like to access marginal distributions. This can be treated as an extension of the problems answered by bend and mix models we studied here."
  },
  {
    "objectID": "posts/conditioning-multivariate-normal.html#formalising-the-problem",
    "href": "posts/conditioning-multivariate-normal.html#formalising-the-problem",
    "title": "Starting (on finite domains) with Gaussian processes",
    "section": "Formalising the problem",
    "text": "Formalising the problem\nTo formalise the problem a bit:\n\n\n\n\n\n\nConditional calculation\n\n\n\nConsider a set of measured values \\(M = \\{(x_1, y_1), \\dotsc, (x_k, y_k)\\}\\) and a non-empty query set \\(Q = \\{x'_1, \\dotsc, x'_q\\} \\subseteq \\mathcal X\\).\nWe assume that \\(Q\\cap M_x = \\varnothing\\) and \\(|M_x| = |M|\\), where \\(M_x = \\{x \\in \\mathcal X \\mid (x, y)\\in M \\text{ for some } y \\}\\).\nWe would like to be able to sample from the conditional probability distribution \\[\nP(Y_{x_1'}, \\dotsc, Y_{x_q'} \\mid Y_{x_1}=y_1, \\dotsc, Y_{x_k}=y_k)\n\\] as well as to evaluate the (log-)density at any point.\nWe allow \\(M=\\varnothing\\), which corresponds then to marginal distributions.\n\n\nThis problem can be solved for multivariate normal distributions by noticing that all conditional (and marginal) distributions will also be multivariate normal. Let’s introduce some notation.\nFor a tuple \\(\\pi = (\\pi_1, \\dotsc, \\pi_m) \\in \\mathcal X^m\\) such that \\(\\pi_i\\neq \\pi_j\\) for \\(i\\neq j\\), we will write \\(Y_\\pi\\) for a random vector \\((Y_{\\pi_1}, \\dotsc, Y_{\\pi_m})\\). Note that this operation can be implemented using a linear mapping \\(A_\\pi \\colon \\mathbb R^n\\to \\mathbb R^m\\) with \\[\nA_\\pi \\begin{pmatrix} Y_1 \\\\ \\vdots \\\\ Y_n \\end{pmatrix} = \\begin{pmatrix}\n  Y_{\\pi_1} \\\\ \\vdots \\\\ Y_{\\pi_m}\n\\end{pmatrix}\n\\] and \\((A_\\pi)_{oi} = \\mathbf 1[ i = \\pi_o]\\). Hence, \\(Y_\\pi\\) vector is distributed according to \\(\\mathcal N(A_\\pi\\mu, A_\\pi\\Sigma A_\\pi^T)\\).\nThe above operation suffices for calculating arbitrary marginal distributions and distributions corresponding to permuting the components.\nConsider now the case where we want to calculate a “true” conditional distribution (i.e., with \\(M\\neq \\varnothing\\)), so the marginalisation does no suffice.\nWe can use the tuple \\(\\pi = (x_1', \\dotsc, x_q', x_1, \\dotsc, x_k)\\) to select the right variables and reorder them into a \\(q\\)-dimensional block of unobserved (“query”) variables and a \\(k\\)-dimensional block of observed (“key”) variables2.\n\n\n\n\n\n\nConditioning multivariate normal\n\n\n\nLet \\(Y=(Y_1, Y_2) \\in \\mathbb R^{k}\\times \\mathbb R^{n-k}\\) be a random vector split into blocks of dimensions \\(k\\) and \\(n-k\\). If \\(Y\\sim \\mathcal N(\\mu, \\Sigma)\\), where \\[\n\\mu = (\\mu_1, \\mu_2)\n\\] and \\[\n\\Sigma = \\begin{pmatrix}\n  \\Sigma_{11} & \\Sigma_{12} \\\\\n  \\Sigma_{21} & \\Sigma_{22}\n\\end{pmatrix},\n\\]\nthen for every \\(y \\in \\mathbb R^{n-k}\\) it holds that\n\\[\nY_1 \\mid Y_2=y \\sim \\mathcal N(\\mu', \\Sigma'),\n\\] where \\[\n  \\mu' = \\mu_1 + {\\color{Apricot}\\Sigma_{12}\\Sigma_{22}^{-1}}(y-\\mu_2)\n\\] and \\[\n\\Sigma' = \\Sigma_{11} - {\\color{Apricot}\\Sigma_{12} \\Sigma_{22}^{-1}} \\Sigma_{21}.\n\\]\n\n\nWe see that in both formulae the matrix of regression coefficients3 \\(\\color{Apricot}\\Sigma_{12}\\Sigma_{22}^{-1}\\) appears. We will discuss calculation of this term below."
  },
  {
    "objectID": "posts/conditioning-multivariate-normal.html#lets-write-some-code",
    "href": "posts/conditioning-multivariate-normal.html#lets-write-some-code",
    "title": "Starting (on finite domains) with Gaussian processes",
    "section": "Let’s write some code",
    "text": "Let’s write some code\nNow let’s implement a prototype:\n\n\nCode\nimport numpy as np\nimport numpy.linalg as npla\nfrom jaxtyping import Float, Int, Array\n\nfrom scipy import stats\nimport scipy.linalg as spla\n\n\nclass MultivariateNormal:\n  def __init__(\n    self,\n    mu: Float[Array, \" dim\"],\n    cov: Float[Array, \"dim dim\"],\n  ) -&gt; None:\n    eigvals, _ = npla.eig(cov)\n    if np.min(eigvals) &lt;= 0:\n      raise ValueError(f\"Covariance should be positive-definite.\")\n    \n    self.mu = np.asarray(mu)\n    self.cov = np.asarray(cov)\n    dim = self.mu.shape[0]\n\n    assert self.mu.shape == (dim,)\n    assert self.cov.shape == (dim, dim)\n\n  @property\n  def dim(self) -&gt; int:\n    return self.mu.shape[0]\n\n  def sample(\n    self,\n    rng: np.random.Generator,\n    size: int = 1,\n  ) -&gt; np.ndarray:\n    return rng.multivariate_normal(\n      mean=self.mu, cov=self.cov, size=size,\n    )\n\n  def logpdf(self, y: Float[Array, \" dim\"]) -&gt; float:\n    return stats.multivariate_normal.logpdf(\n      y,\n      mean=self.mu,\n      cov=self.cov,\n      allow_singular=False,\n    )\n\n\ndef _contruct_projection_matrix(\n  n: int,\n  indices: Int[Array, \" k\"],\n) -&gt; Int[Array, \"k n\"]:\n  indices = np.asarray(indices, dtype=int)  \n\n  # Output dimension\n  k = indices.shape[0]\n  if np.unique(indices).shape[0] != k:\n    raise ValueError(\"Indices should be unique.\")\n\n  inp = np.arange(n, dtype=int)\n  \n  ret = np.asarray(inp[None, :] == indices[:, None], dtype=int)\n  assert ret.shape == (k, n)\n  return ret\n\n\ndef select(\n  dist: MultivariateNormal,\n  indices: Int[Array, \" k\"],\n) -&gt; MultivariateNormal:\n  proj = np.asarray(\n    _contruct_projection_matrix(\n      n=dist.dim,\n      indices=indices,\n    ),\n    dtype=float,\n  )\n  \n  new_mu = np.einsum(\"oi,i -&gt; o\", proj, dist.mu)\n  new_cov = np.einsum(\"oi,iI,OI -&gt; oO\", proj, dist.cov, proj)\n  \n  return MultivariateNormal(\n    mu=new_mu,\n    cov=new_cov,\n  )\n\ndef _regression_coefs(\n  sigma12: Float[Array, \"Q K\"],\n  sigma22: Float[Array, \"K K\"],\n) -&gt; Float[Array, \"Q K\"]:\n  return spla.solve(sigma22, sigma12.T).T\n\ndef _condition_gaussian(\n  dist: MultivariateNormal,\n  m: int,\n  y: Float[Array, \" vals\"]\n) -&gt; MultivariateNormal:\n  assert y.shape[0] == dist.dim - m\n\n  mu1 = dist.mu[:m]\n  mu2 = dist.mu[m:]\n  sigma11 = dist.cov[:m, :m]\n  sigma12 = dist.cov[:m, m:]\n  sigma22 = dist.cov[m:, m:]\n\n  reg = _regression_coefs(\n    sigma12=sigma12, sigma22=sigma22,\n  )\n\n  mu_ = mu1 + reg @ (y - mu2)\n  sigma_ = sigma11 - reg @ sigma12.T\n\n  return MultivariateNormal(\n    mu=mu_, cov=sigma_,\n  )\n\ndef condition(\n  dist: MultivariateNormal,\n  query: Int[Array, \" Q\"],\n  key: Int[Array, \" K\"],\n  values: Float[Array, \" K\"],\n) -&gt; MultivariateNormal:\n  q, k = query.shape[0], key.shape[0]\n  assert values.shape == (k,), \"Values have wrong shape\"\n\n  total_index = np.concatenate((query, key))\n\n  if np.unique(total_index).shape[0] != k + q:\n    raise ValueError(\"Indices must be unique.\")\n  if np.min(total_index) &lt; 0 or np.max(total_index) &gt;= dist.dim:\n    raise ValueError(\"Indices must be from the set 0, 1, ..., dim-1.\")\n\n  ordered_dist = select(dist, indices=total_index)\n\n  if k == 0:\n    return ordered_dist\n  else:\n    return _condition_gaussian(\n      dist=ordered_dist,\n      m=q,\n      y=values,\n    )\n\n\nNow we can do conditioning. For example, imagine that we have \\(\\mu = 0\\) and we know \\(\\Sigma\\): \\(Y_1\\) correlates with \\(Y_3\\), and \\(Y_2\\) anticorrelates with \\(Y_4\\) and \\(Y_5\\) doesn’t correlate with anything else. We measure \\(Y_1\\) and \\(Y_2\\), so we can use this correlation structure to impute \\(Y_3\\), \\(Y_4\\) and \\(Y_5\\).\nFirst, let’s plot the covariance matrix and the samples:\n\n\nCode\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nplt.style.use(\"dark_background\")\n\nmixing = np.asarray([\n  [1.5, -0.7, 1.5, 0, 0],\n  [0, 1, 0, -1, 0],\n  [0, 0, 0, 0, 1],\n], dtype=float)\n\ncov = np.einsum(\"io,iO-&gt;oO\", mixing, mixing) + 0.1 * np.eye(5)\n\nfig, axs = plt.subplots(1, 2, figsize=(4.5, 2), dpi=200)\nticklabels = [f\"$Y_{i}$\" for i in range(1, 5+1)]\n\nax = axs[0]\nsns.heatmap(cov, ax=ax, xticklabels=ticklabels, yticklabels=ticklabels, vmin=-1.5, vmax=1.5, center=0, square=True, annot=True, fmt=\".1f\")\n\ndist = MultivariateNormal(mu=np.zeros(5), cov=cov)\nrng = np.random.default_rng(42)\nsamples = dist.sample(rng, size=10)\n\nax = axs[1]\nfor sample in samples:\n  x_ax = np.arange(1, 6)\n  ax.plot(x_ax, sample, alpha=0.5, c=\"C1\")\n  ax.scatter(x_ax, sample, alpha=0.5, c=\"C1\", s=4)\n\nax.spines[['top', 'right']].set_visible(False)\nax.set_xlim(0.9, 5.1)\nax.set_xticks(x_ax, ticklabels)\n\nfig.tight_layout()\n\n\n\n\n\nImagine now that we observed \\(Y_1=1.5\\) and \\(Y_2=1\\). We expect that \\(Y_3\\) should move upwards (the posterior should be shifted so that most of the mass is above \\(0\\)), \\(Y_4\\) to go downwards and \\(Y_5\\) to stay as it was. Let’s plot covariance matrix and draws from the conditional posterior \\(P(Y_3, Y_4, Y_5\\mid Y_1=1.5, Y_2=1)\\):\n\n\nCode\ny_obs = np.asarray([1.5, 1])\ncond = condition(\n  dist=dist,\n  query=np.asarray([2, 3, 4]),\n  key=np.asarray([0, 1]),\n  values=y_obs,\n)\n\nfig, axs = plt.subplots(1, 2, figsize=(4.5, 2), dpi=200)\n\nax = axs[0]\nsns.heatmap(\n  cond.cov,\n  ax=ax,\n  xticklabels=ticklabels[2:],\n  yticklabels=ticklabels[2:],\n  vmin=-1.5,\n  vmax=1.5,\n  center=0,\n  annot=True,\n  square=True,\n)\n\nax = axs[1]\nax.spines[['top', 'right']].set_visible(False)\nax.set_xlim(0.9, 5.1)\nax.set_xticks(x_ax)\n\nax.scatter([1, 2], y_obs, c=\"maroon\", s=4)\n\nsamples = cond.sample(rng, size=10)\nfor sample in samples:\n  ax.plot([3, 4, 5], sample, alpha=0.5, c=\"C1\")\n  ax.scatter([3, 4, 5], sample, alpha=0.5, c=\"C1\", s=4)\n\nax.set_xticks(x_ax, ticklabels)\n\nfig.tight_layout()\n\n\n\n\n\nWe see that there is slight anticorrelation between \\(Y_3\\) and \\(Y_4\\): by sampling from the conditional distribution we obtain a coherent sample. This is different than drawing independent samples from \\(P(Y_3 \\mid Y_1=y_1, Y_2=y_2)\\) and \\(P(Y_4\\mid Y_1=y_1, Y_2=y_2)\\). Perhaps it’ll be easier to visualise it on a scatter plot:\n\n\nCode\nfig, axs = plt.subplots(1, 2, figsize=(4.5, 3), dpi=200, sharex=True, sharey=True)\nsamples = cond.sample(rng, size=15_000)\n\nax = axs[0]\nax.scatter(samples[:, 0], samples[:, 1], s=2, alpha=0.01)\nax.set_title(\"Joint sample\")\nax.set_xlabel(r\"$Y_3$\")\nax.set_ylabel(r\"$Y_4$\")\n\nax = axs[1]\nsamples2 = cond.sample(rng, size=samples.shape[0])\nax.scatter(samples[:, 0], samples2[:, 1], s=2, alpha=0.01)\nax.set_title(\"Independent\")\nax.set_xlabel(r\"$Y_3$\")\nax.set_ylabel(r\"$Y_4$\")\n\ncorr = cond.cov[0, 1] / np.sqrt(cond.cov[0, 0] * cond.cov[1, 1])\nfig.suptitle(r\"$\\text{Corr}(Y_3, Y_4) = $\" + f\"{corr:.2f}\")\nfig.tight_layout()\n\nfor ax in axs:\n  ax.spines[['top', 'right']].set_visible(False)\n\n\n\n\n\nOk, it’s hard to see, but visible – the (negative) correlation is just quite weak in this case.\nLet’s do the last visualisation before we move to Gaussian processes. As mentioned, the magical thing is the access to the whole posterior distribution \\(P(Y_3, Y_4, Y_5 \\mid Y_1=y_1, Y_2=y_2)\\): we can evaluate arbitrary probabilities and sample consistent vectors from this distribution. We can visualise samples, but sometimes a simpler summary statistic would be useful. Each of the distributions \\(P(Y_i \\mid Y_1=y_1, Y_2=y_2)\\) is one-dimensional Gaussian, so we can plot its mean and standard deviation. Or, even better, let’s plot \\(\\mu_i\\pm 2\\sigma_i\\) to see where approximately 95% of probability lies.\nWe’ll plot these regions both before and after conditioning:\n\n\nCode\nfig, axs = plt.subplots(1, 2, figsize=(4, 1.5), dpi=170, sharex=True, sharey=True)\n\n# Before conditioning\nax = axs[0]\n\nax.plot(np.arange(1, 1+5), np.zeros(5), linestyle=\"--\", c=\"gray\", alpha=0.5, linewidth=0.8)\n\nax.errorbar(\n  np.arange(1, 1+5),\n  dist.mu,\n  yerr=2 * np.sqrt(np.diagonal(dist.cov)),\n  fmt=\"o\",\n)\n\n# After conditioning\nax = axs[1]\nax.plot(np.arange(1, 1+5), np.zeros(5), linestyle=\"--\", c=\"gray\", alpha=0.5, linewidth=0.8)\nax.scatter([1, 2], y_obs, c=\"maroon\", s=4)\nax.errorbar(\n  [3, 4, 5],\n  cond.mu,\n  yerr=2 * np.sqrt(np.diagonal(cond.cov)),\n  fmt=\"o\",\n)\n\nfor ax in axs:\n  ax.spines[['top', 'right']].set_visible(False)\n  ax.set_xlim(0.8, 5.3)\n  ax.set_xticks(x_ax, ticklabels)\n\nfig.tight_layout()\n\n\n\n\n\nAs mentioned, these plots don’t really allow us to look at correlations between different variables, but they are still useful: we can easily see that the posterior of \\(Y_3\\) moved upwards and \\(Y_4\\) moved downwards! Variable \\(Y_5\\), which is independent of \\((Y_1, Y_2, Y_3, Y_4)\\), doesn’t change: if we want to know it, we just have to measure it."
  },
  {
    "objectID": "posts/conditioning-multivariate-normal.html#gaussian-processes",
    "href": "posts/conditioning-multivariate-normal.html#gaussian-processes",
    "title": "Starting (on finite domains) with Gaussian processes",
    "section": "Gaussian processes",
    "text": "Gaussian processes\nFor \\(\\mathcal X = \\{1, \\dotsc, n\\}\\) we considered an indexed collection of random variables \\(\\{Y_x\\}_{x\\in \\mathcal X}\\). Let’s call it a stochastic process.\nThis stochastic process has the property that the joint distribution over all variables was multivariate normal. From that we could deduce that distributions \\(P(Y_{x_1}, \\dotsc, Y_{x_m})\\) were again multivariate normal, what in turn allowed us to do prediction via conditioning (which resulted, again, in multivariate normal distributions).\nLet’s move beyond a finite dimension: take \\(\\mathcal X=\\mathbb R\\) and consider a stochastic process \\(\\{Y_x\\}_{x\\in \\mathcal X}\\). We will say that it’s a Gaussian process if for every finite set \\(\\{x_1, \\dotsc, x_m\\}\\subseteq \\mathcal X\\) the joint distribution \\(P(Y_{x_1}, \\dotsc, Y_{x_m})\\) is multivariate normal. More generally, we can take other domains \\(\\mathcal X\\) (e.g., \\(\\mathbb R^n\\)) and speak of Gaussian random fields.\nIn either case, the trick is that we never work with infinitely many random variables at once: for example, if we observe values \\(y_1, \\dotsc, y_k\\) at locations \\(x_1, \\dotsc, x_k\\) and we want to predict the values at points \\(x'_1, \\dotsc, x'_q\\), we will construct the joint multivariate normal distribution \\(P(Y_{x_1}, \\dotsc, Y_{x_m}, Y_{x'_1}, \\dotsc, Y_{x'_q})\\) and condition on observed values to get the conditional distribution \\(P(Y_{x'_1}, \\dotsc, Y_{x'_q} \\mid Y_{x_1} = y_1, \\dotsc, Y_{x_k}=y_k)\\).\nNow the questions is: how can we define a consistent stochastic process with these great properties? When \\(\\mathcal X\\) was finite, we could just define the joint probability distribution over all variables via mean and covariance. But now \\(\\mathcal X\\) is not finite!\nConsider therefore two functions, giving the mean and covariance: \\(m \\colon \\mathcal X\\to \\mathbb R\\) and \\(k\\colon \\mathcal X\\to \\mathcal X\\to \\mathbb R^+\\). The premise is to build multivariate normal distributions \\(P(Y_{x_1}, \\dotsc, Y_{x_m})\\) by using the mean vector \\(\\mu_i = m(x_i)\\) and covariance matrix \\(\\Sigma_{ij} = k(x_i, x_j)\\).\nFirst of all, we see that not all covariance functions are suitable: we want covariance matrices to be symmetric and positive-definite, so we should use positive-definite kernels.\nSecondly, we don’t know if these probability distributions can be coherently glued to a stochastic process. The answer to this problem is provided by Daniell-Kolmogorov extension theorem, which says when a family of probability distributions can be coherently glued yielding a stochastic process. In this case parameterising covariances via \\(\\Sigma_{ij}=k(x_i, x_j)\\) has the properties mentioned in the theorem. On the other hand, parameterising precision matrices via \\(k(x_i, x_j)\\) doesn’t generally yield a coherent stochastic process.\nThere are many important technical details, which I should mention here. Instead, I’ll refer to a great introduction to Gaussian processes at Dan Simpson’s blog, and implement something."
  },
  {
    "objectID": "posts/conditioning-multivariate-normal.html#modelling-functions",
    "href": "posts/conditioning-multivariate-normal.html#modelling-functions",
    "title": "Starting (on finite domains) with Gaussian processes",
    "section": "Modelling functions",
    "text": "Modelling functions\nThere are many libraries for working with Gaussian processes, including GPJax, GPyTorch and GPy. We will however just use the code developed above, plus some simple covariance functions.\nOur task will be the following: we are given some function on the interval \\((0, 1)\\). We observe some values \\(M=\\{(x_1, y_1), \\dotsc, (x_k, y_k)\\}\\) inside the intervals \\((0, u)\\) and \\((1-u, 1)\\) and we want to predict the function behaviour in the interval \\((u, 1-u)\\), from which we do not have any data.\n\n\nCode\nimport matplotlib.pyplot as plt\nimport dataclasses\n\n@dataclasses.dataclass\nclass Task:\n  xs_all: Float[Array, \" points\"]\n  ys_all: Float[Array, \" points\"]\n  xs_obs: Float[Array, \" key\"]\n  ys_obs: Float[Array, \" key\"]\n  xs_query: Float[Array, \" query\"]\n\ndef create_task(\n  f,\n  thresh: float = 0.25,\n  k_2: int = 5,\n  n_query: int = 101,\n) -&gt; Task:\n  assert 0.02 &lt; thresh &lt; 0.98, \"Threshold should be in (0.02, 0.98)\"\n\n  xs_all = np.linspace(0.01, 1)\n  \n  xs_obs = np.concatenate((np.linspace(0.01, thresh, k_2), np.linspace(1 - thresh, 0.99, k_2)))\n  xs_query = np.linspace(thresh + 0.01, 0.99 - thresh, n_query)\n\n  return Task(\n    xs_all=xs_all,\n    ys_all=f(xs_all),\n    xs_obs=xs_obs,\n    ys_obs=f(xs_obs),\n    xs_query=xs_query,\n  )\n\ndef plot_task(ax: plt.Axes, task: Task):\n  ax.plot(\n    task.xs_all, task.ys_all, linestyle=\"--\", c=\"maroon\", linewidth=1.0, alpha=0.8\n  )\n  ax.scatter(task.xs_obs, task.ys_obs, s=8, c=\"maroon\")\n  ax.spines[['top', 'right']].set_visible(False)\n  ax.set_xlim(-0.02, 1.02)\n\n\nfig, ax = plt.subplots(figsize=(2.2, 1.4), dpi=200)\n\ntask1 = create_task(lambda x: np.cos(2*np.pi * x))\nplot_task(ax, task1)\nfig.tight_layout()\n\n\n\n\n\nWe can approach this problem in two ways: first, we can impute missing values evaluated at some points.\nFor example, we can define a grid over \\((u, 1-u)\\) with \\(q\\) query points \\(x'_1, \\dotsc, x'_q\\) and sample from the conditional distribution \\(P(Y_{x'_1}, \\dotsc, Y_{x'_q} \\mid Y_{x_1}=y_1, \\dotsc, Y_{x_k}=y_k)\\) several times. This is one good way of plotting, showing us the behaviour of the whole sample at once.\nAnother way of plotting, which we also have already seen, is to take a single point \\(x'\\) and look at the normal distribution \\(P(Y_{x'} \\mid Y_{x_1}=y_1, \\dotsc, Y_{x_k}=y_k)\\), summarized by the mean and standard deviation: we can plot \\(\\mu(x') \\pm 2\\sigma(x')\\) as a function of \\(x'\\) (similarly to the finite-dimensional case). This approach doesn’t allow us to look at joint behaviour at different locations, but is quite convenient to summarise uncertainty at a single specific point. For example, this may be informative enough to determine a good location of the next sample to collect in Bayesian optimisation framework (unless one wants to consider multiple points).\nLet’s implement an example kernel and plot predictions in both ways:\n\n\nCode\ndef kernel(x, x_):\n  return np.exp(-20 * np.square(x-x_))\n\nfig, axs = plt.subplots(1, 2, figsize=(2*3, 2), dpi=120, sharex=True, sharey=True)\n\nfor ax in axs:\n  plot_task(ax, task1)\n\nxs_eval = np.concatenate((task1.xs_query, task1.xs_obs))\n\ncov = kernel(xs_eval[:, None], xs_eval[None, :]) + 1e-6 * np.eye(len(xs_eval))\ndist = MultivariateNormal(np.zeros_like(xs_eval), cov)\n\ncond = condition(\n  dist=dist,\n  query=np.arange(len(task1.xs_query)),\n  key=np.arange(len(task1.xs_query), len(xs_eval)),\n  values=task1.ys_obs\n)\n\nrng = np.random.default_rng(1)\nsamples = cond.sample(rng, size=30)\n\nax = axs[0]\nfor sample in samples:\n  ax.plot(task1.xs_query, sample, alpha=0.8, color=\"C1\", linewidth=0.1)\n\n\nax = axs[1]\nax.plot(task1.xs_query, cond.mu, c=\"C1\", linestyle=\":\")\nuncert = 2 * np.sqrt(np.diagonal(cond.cov))\nax.fill_between(task1.xs_query, cond.mu - uncert, cond.mu + uncert, color=\"C1\", alpha=0.2)\n\nfig.tight_layout()\n\n\n\n\n\nNice! I’ve been thinking about showing how different kernels result in differing predictions. But this post is already a bit too long, so I may write another one on this topic. In any case, there’s a Kernel Cookbook created by David Duvenaud."
  },
  {
    "objectID": "posts/conditioning-multivariate-normal.html#appendix",
    "href": "posts/conditioning-multivariate-normal.html#appendix",
    "title": "Starting (on finite domains) with Gaussian processes",
    "section": "Appendix",
    "text": "Appendix\n\nMatrix of regression coefficients\nLet’s take a quick look at the matrix of regression coefficients, \\(\\Sigma_{12}\\Sigma_{22}^{-1}\\).\nWe could implement it via inversion, but there is a better solution.\nNamely, note that if \\(X=\\Sigma_{12} \\Sigma_{22}^{-1}\\), then\n\\[\n  \\Sigma_{22} X^T = \\Sigma_{22} \\Sigma_{22}^{-1} \\Sigma_{12}^T = \\Sigma_{12}^T\n\\]\nHence, \\(X^T\\) is a solution to a matrix equation, which we can implement using scipy.linalg.solve. This is considered a better practice as it increases the numerical precision and can be faster (which is visible for large matrices; for small matrices the solution using matrix inversion was often faster).\n\n\nCode\nimport time\nimport numpy as np\nimport numpy.linalg as npla\nimport scipy.linalg as spla\n\n\ndef calc_inv(sigma_12, sigma_22):\n  return sigma_12 @ npla.inv(sigma_22)\n\n\ndef calc_solve(sigma_12, sigma_22):\n  return spla.solve(sigma_22, sigma_12.T).T\n\nrng = np.random.default_rng(42)\n\nn_examples = 3\nsize = 20\nm = 10\n\n_coefs = rng.normal(size=(n_examples, size, size))\nsigmas = np.einsum(\"kab,kac-&gt;kbc\", _coefs, _coefs)\nfor sigma in sigmas:\n  assert np.min(npla.eigvals(sigma)) &gt; 0\n\n  sigma_12 = sigma[:m, m:]\n  sigma_22 = sigma[m:, m:]\n\n  sol1 = calc_inv(sigma_12, sigma_22)\n  sol2 = calc_solve(sigma_12, sigma_22)\n\n  assert np.allclose(sol1, sol2), \"Solutions differ.\""
  },
  {
    "objectID": "posts/conditioning-multivariate-normal.html#footnotes",
    "href": "posts/conditioning-multivariate-normal.html#footnotes",
    "title": "Starting (on finite domains) with Gaussian processes",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nFor every non-zero \\(x\\in \\mathbb R^n\\) we have \\(x^T\\Sigma x &gt; 0\\), where the inequality is strict. As \\(\\Sigma\\) is a real symmetric matrix, one of the versions of the spectral theorem yields a decomposition \\(\\Sigma = R^TDR\\) for a diagonal matrix \\(D\\) and orthogonal \\(R\\). Hence, equivalently, we all eigenvalues have to be positive. See this link for more discussion.↩︎\nI like to call them “query”, “keys” and “values” vectors, which makes the language a bit more similar to transformers. From that we just need one conditioning operation:↩︎\nWhy is it called in this manner? What are the slopes of \\(\\mu'\\) change, when we vary observed values \\(y\\)?↩︎"
  },
  {
    "objectID": "posts/strict-linear-independence-measures.html",
    "href": "posts/strict-linear-independence-measures.html",
    "title": "Strict linear independence of measures",
    "section": "",
    "text": "Here we discussed some possible approaches to tackle the quantification problem. Today let’s take a more theoretical look on it, as proposed in A Unified View of Label Shift Estimation."
  },
  {
    "objectID": "posts/strict-linear-independence-measures.html#quantification",
    "href": "posts/strict-linear-independence-measures.html#quantification",
    "title": "Strict linear independence of measures",
    "section": "Quantification",
    "text": "Quantification\nWe have many objects of distinct types \\(y\\in \\mathcal Y = \\{1, \\dotsc, K\\}\\), for example chairs, tables, cups, plates… However, in our setting the label is not available: we only have a list of features. For example, the features may include weight (which helps to distinguish chairs from beds), number of legs (which helps to distinguish cups from chairs), the main construction material.\nWe will assume that each object is given a point in the feature space \\(\\mathcal X\\). Of course, each type of object \\(y\\) may result in a lot of different features observed: there are heavier and lighter tables. Some chairs have three legs. Cups can be made out of wood, glass or metal. In other words, for each category \\(y\\) we have a probability measure \\(Q_y\\) on \\(\\mathcal X\\), representing the conditional probability distribution \\(P(X\\mid Y=y)\\).\nLet’s assume that the probability distributions \\(Q_y\\) are known, but we are trying to find the proportions of different object, \\(P(Y=y)\\), in the data set.\nIn other words, we have access to the finite sample from the mixture distribution\n\\[\nP(X) = \\sum_{y\\in \\mathcal Y} P(X\\mid Y=y) P(Y=y) = \\sum_{y\\in \\mathcal Y} \\pi_y Q_y,\n\\]\nwhere \\(\\pi = (\\pi_1, \\dotsc, \\pi_K)\\) is the list of proportions we are trying to find. Note that all entries have to be non-negative and that \\(\\pi_1 + \\dotsc + \\pi_K=1\\), what results in \\(K-1\\) degrees of freedom.\nHaving access to the finite samples is one of the actual difficulties of solving quantification problems. Another one is working with misspecified models, i.e., actually there may be more than \\(K\\) classes (but some of them we are not aware of), or out distributions \\(Q_y\\) may take a different form than assumed.\nHowever, let’s forget about these difficulties for now, and see if we can solve the quantification problem under the ideal circumstances."
  },
  {
    "objectID": "posts/strict-linear-independence-measures.html#identifiability",
    "href": "posts/strict-linear-independence-measures.html#identifiability",
    "title": "Strict linear independence of measures",
    "section": "Identifiability",
    "text": "Identifiability\n\n\n\n\n\n\nIdeal quantification problem: Let \\(P\\) and \\(Q_1, \\dotsc, Q_K\\) be probability measures on \\(\\mathcal X\\), such that there exist a decomposition into a mixture \\[\nP = \\sum_{y\\in \\mathcal Y} \\pi_y Q_y.\n\\]\nWhat are the necessary conditions on \\(Q_y\\) to ensure that the mixture components vector \\(\\pi\\) can be uniquely recovered given \\(Q_y\\) and \\(P\\)?\n\n\n\nFirst, let’s consider the case in which \\(\\mathcal X\\) is finite, with \\(|\\mathcal X| = D\\). If we order the elements of \\(\\mathcal X\\), we can represent each distribution \\(Q_y\\) by a vector of probabilities \\(Q_y(\\{x\\})\\) for \\(x\\in \\mathcal X\\). Equivalently, we are constructing conditional probability vectors \\(P(X=x\\mid Y=y)\\) and have to solve a set of equations\n\\[\nP(X=x) = \\sum_{y\\in \\mathcal Y} P(X=x\\mid Y=y) \\pi_y.\n\\]\nClearly, if the probability vectors representing \\(Q_y\\) are linearly independent, the solution for \\(\\pi\\) has to be unique (assuming that it exists, which is related to the assumption that we have no misspecification). The technique based on solving such a solution of linear equations has been proposed in various forms over the years."
  },
  {
    "objectID": "posts/strict-linear-independence-measures.html#strict-linear-independence-of-measures",
    "href": "posts/strict-linear-independence-measures.html#strict-linear-independence-of-measures",
    "title": "Strict linear independence of measures",
    "section": "Strict linear independence of measures",
    "text": "Strict linear independence of measures\nWhat if \\(\\mathcal X\\) is not finite? In particular, what if \\(\\mathcal X\\) is a continuous space in which singletons \\(\\{x\\}\\) have probability zero? In this case, the measures \\(Q_y\\) do not have such a convenient finite-dimensional vector representation and the concept of linear independence seems to be less useful.\nThe authors of A Unified View of Label Shift Estimation propose the following notion of strict linear independence of probability measures: for every vector \\(\\lambda \\in \\mathbb R^K\\) such that \\(\\lambda\\neq 0\\) it holds that \\[\n\\int_{\\mathcal X} \\left| \\sum_{y\\in \\mathcal Y} p(z\\mid y) \\right|  \\, \\mathrm{d}x \\neq 0.\n\\]\nI personally prefer a bit different formulation (although perhaps a bit more complicated). Assume that we have a \\(\\sigma\\)-finite measure \\(\\mu\\) on \\(\\mathcal X\\), such that all \\(Q_k \\ll \\mu\\). Often there is a natural reference measure in many problems (e.g., the Lebesgue measure on \\(\\mathbb R^n\\), with the assumption that all \\(Q_k\\) have PDFs), but generally at least one exists, for example \\(\\mu = Q_1 + \\dotsc + Q_K\\) (or it can be normalised by \\(K\\) to yield a probability measure!)\nThe equation above is a requirement that \\[\n\\int_{\\mathcal X} \\left| \\sum_{y\\in \\mathcal Y} \\lambda_y \\frac{\\mathrm d Q_y}{\\mathrm d \\mu} \\right|  \\, \\mathrm{d}\\mu \\neq 0\n\\] which in turn can be written as \\[\n\\left| \\sum_{y\\in \\mathcal Y} \\lambda_y Q_y \\right|(X) \\neq 0.\n\\]\nIt’s not hard to prove that the above condition is equivalent to an existence of a measurable set \\(A_\\lambda\\) such that \\[\n\\lambda_1 Q_1(A_\\lambda) + \\cdots + \\lambda_K Q_K(A_\\lambda) \\neq 0.\n\\]\nHence, we will prefer to use the equivalent definition:\n\n\n\n\n\n\nDefinition: We say that probability measures \\(Q_1, \\dotsc, Q_K\\) are strictly linearly independent if for every vector \\(\\lambda \\neq 0\\) there exists a measurable subset \\(A_\\lambda\\subseteq \\mathcal X\\) such that \\[\n\\lambda_1 Q_1(A_\\lambda) + \\cdots + \\lambda_K Q_K(A_\\lambda) \\neq 0.\n\\]\n\n\n\nLet’s think why this is a sufficient condition for the uniqueness of \\(\\pi\\). Assume that the true composition vector is \\(\\pi\\) and suppose that we have a candidate composition vector \\(\\gamma\\) such that \\(\\gamma\\neq \\pi\\). Take now \\(\\lambda = \\pi - \\gamma \\in \\mathbb R^K\\). From strict linear independence, we know that there exists \\(A_\\lambda\\) such that \\[\nP(A_\\lambda) = \\sum_{y} \\pi_y Q_y(A_\\lambda) \\neq \\sum_{y} \\gamma_y Q_y(A_\\lambda).\n\\]\nHence, the observed measure \\(P\\) is different from the mixture parameterised by \\(\\gamma\\)."
  },
  {
    "objectID": "posts/strict-linear-independence-measures.html#examples",
    "href": "posts/strict-linear-independence-measures.html#examples",
    "title": "Strict linear independence of measures",
    "section": "Examples",
    "text": "Examples\nFinally, let’s think about examples of strictly linearly independent measures.\n\nDiscrete spaces\nProbably the simplest example is for discrete measures on finite spaces: if \\(\\mathcal X\\) is finite, strict linear independence and linear independence are equivalent.\nThe proof is easy: consider the probability vectors \\(q^y_x = P(X=x\\mid Y=y) = Q_y(\\{x\\})\\). If the vectors are linearly independent, for every \\(\\lambda \\neq 0\\) we have \\(\\lambda_1 q^1 + \\cdots + \\lambda_K q^K\\neq 0\\), meaning that there exists a component \\(x\\in \\mathcal X\\) such that \\(\\lambda_1 q^1_x + \\cdots + \\lambda_K q^K_x \\neq 0\\). So, we define \\(A_\\lambda = \\{x\\}\\).\nConversely, if we have \\(\\lambda\\neq 0\\) and we use strict linear independence to ensure the existence of a set \\(A_\\lambda\\) such that \\[\n    0 \\neq \\lambda_1 Q_1(A_\\lambda) + \\cdots + \\lambda_K Q_K(A_\\lambda) = \\sum_{x\\in A_\\lambda} (\\lambda_1 q^1_x + \\cdots + \\lambda_K q^K_x),\n\\] then we see that for at least one component \\(x\\) we have \\(\\lambda_1 q^1_x + \\cdots + \\lambda_K q^K_x\\neq 0\\), which suffices for linear independence.\n\n\nA lemma\nFor continuous spaces the situation is a bit more complex. However, let’s prove a useful lemma, which is in fact a generalisation of the previous result.\n\n\n\n\n\n\nLemma: Assume that \\(\\mathcal X\\) is a standard Borel space and \\(Q_1, \\dotsc, Q_K\\) have continuous PDFs \\(q_1, \\dotsc, q_K\\), with respect to a \\(\\sigma\\)-finite and strictly positive measure \\(\\mu\\). Then, if \\(q_1, \\dotsc, q_K\\) are linearly independent as vectors in the space of continuous real-valued functions \\(C(\\mathcal X, \\mathbb R)\\), then the measures \\(Q_1, \\dotsc, Q_K\\) are strictly linearly independent.\n\n\n\nProof: Take any \\(\\lambda\\neq 0\\) and write \\(u = |\\lambda_1 q_1 + \\cdots + \\lambda_K q_K|\\). From the linear independence it follows that there exists \\(x_0\\in \\mathcal X\\) such that \\(u(x_0) &gt; 0\\). Now use continuity of \\(u\\) to find an open neighborhood \\(A\\) of \\(x_0\\) such that for all \\(x\\in A\\) we have \\(u(x) &gt; u(x_0) / 2\\). As \\(u\\) is non-negative and \\(\\mu\\) is strictly positive, we have \\(\\mu(A) &gt; 0\\), so that \\[\n\\int_X u\\, \\mathrm{d}\\mu \\ge \\int_{A} u\\, \\mathrm{d}\\mu \\ge \\frac{u(x_0)}{2} \\cdot \\mu(A) &gt; 0.\n\\]\nI personally find this lemma useful: verifying linear independence of functions is a well-studied problem in mathematics. For example, if \\(\\mathcal X\\subseteq \\mathbb R\\) is an interval and the densities \\(q_y\\) are sufficiently smooth, one can use Wronskian (introduced by Józef Wroński in 1812, so it’s a classic tool) to study linear independence.\n\n\nExponential variables\nConsider a space \\(\\mathcal X = \\mathbb R^+\\) and the family of exponential random variables, which have densities \\(q_y(x)=\\mu_k\\exp(-\\mu_y x)\\). Now assume that all the parameters are different. We will prove that these densities are linearly independent functions.\n\nWronskian approach\nNote that the \\(m\\)-th derivative is \\(q^{(m)}(x) = (-\\mu_y)^{m} q_y(x)\\). The Wronskian is in this case given by \\[\\begin{align*}\nW(x) &= \\det \\begin{pmatrix}\n    q_1(x) & \\cdots & q_K(x)\\\\\n    -\\mu_1 q_1(x) & \\cdots & -\\mu_K q_K(x) \\\\\n    \\vdots & \\ddots & \\vdots \\\\\n    (-\\mu_1)^{K-1} q_1(x) & \\cdots & (-\\mu_K)^{K-1} q_K(x)\n\\end{pmatrix} \\\\\n  &= \\left(\\prod_{y} q_y(x)\\right) \\cdot \\det \\begin{pmatrix}\n    1 & \\cdots & 1\\\\\n    -\\mu_1 & \\cdots & -\\mu_K \\\\\n    \\vdots & \\ddots & \\vdots \\\\\n    (-\\mu_1)^{K-1}  & \\cdots & (-\\mu_K)^{K-1}\n    \\end{pmatrix}\n\\end{align*}\n\\]\nNote that all \\(q_y(x) &gt; 0\\) and that the determinant of the last matrix has to be positive, as it’s a Vandermonde polynomial: \\[\n\\prod -(\\mu_i - \\mu_j) \\neq 0,\n\\] from the assumption that the means are different.\n\n\nAsymptotic behaviour\nLet’s do another proof, this time going to the limit, similarly to this solution.\nWithout loss of generality, assume \\(0 &lt; \\mu_1 &lt; \\mu_2 &lt; \\dotsc &lt; \\mu_K\\).\nIf there’s \\(\\lambda \\in \\mathbb R^K\\) such that \\[\n\\sum_k \\lambda_k \\mu_k \\exp(-\\mu_k x) = 0,\n\\] identically, then we can multiply both sides by \\(\\exp(\\mu_1 x)\\) to obtain \\[\n\\sum_k \\lambda_k \\mu_k \\exp\\big(-(\\mu_k-\\mu_1) x\\big) = 0.\n\\] For \\(x\\to \\infty\\) the first term becomes \\(\\lambda_1\\mu_1\\) and the rest of the terms goes to \\(0\\). Hence, we have \\(\\lambda_1 = 0\\). Repeating this procedure for \\(\\lambda_2\\), \\(\\lambda_3\\) and other coefficients, we end up with \\(\\lambda = 0\\), proving linear independence.\n\n\nEigenvectors and eigenvalues\nSheldon Axler provides a wonderful proof: each \\(q_y\\) is an eigenvector of the differentiation operator: \\[\n\\frac{\\mathrm{d}}{\\mathrm{d}x} q_y(x) = -\\mu_y q_y(x).\n\\]\nAs all these eigenvalues are distinct, the eigenvectors have to be independent (a useful lemma, one proof follows via induction on \\(K\\)).\n\n\n\nHow about the normal distributions?\nHere is a proof strategy for the normal distributions on \\(\\mathbb R\\), which employs asymptotics. However, I expect the result should generally hold for multivariate normal distributions, provided that the mean vectors are different. But how to prove that? Possibly the strategy employing asymptotics would work, but I am not sure about the details. Similarly, I expect that multivariate Student distributions with different location vectors should be strictly linearly independent.\nI somewhat feel that topic should have been already studied in measure theory and, perhaps, information geometry, although probably under a different name than strict linear independence. It would be interesting to see a reference on this topic!"
  },
  {
    "objectID": "posts/how-expressive-are-bernoulli-mixtures.html",
    "href": "posts/how-expressive-are-bernoulli-mixtures.html",
    "title": "How expressive are Bernoulli mixture models?",
    "section": "",
    "text": "Let \\(p\\in (0, 1)\\) be a parameter and \\(\\mathrm{Ber}(p)\\) be the Bernoulli distribution, with \\(\\mathrm{Ber}(y\\mid p) = p^y(1-p)^{1-y}\\) for \\(y\\in \\{0, 1\\}\\). Let’s define a slightly more general family of distributions, \\(\\overline{\\mathrm{Ber}}(p)\\), for \\(p\\in [0, 1]\\), as follows: \\[\n  \\overline{\\mathrm{Ber}}(p) = p\\delta_1 + (1-p)\\delta_0 = \\begin{cases}\n    \\delta_0 &\\text{ if } p=0\\\\\n    \\delta_1 &\\text{ if } p=1\\\\\n    \\mathrm{Ber}(p) &\\text{otherwise}\n  \\end{cases}\n\\]\nNamely, the \\(\\mathrm{Ber}\\) distributions model a procedure where a biased (e.g., by bending) coin with two sides is tossed, but \\(\\overline{\\mathrm{Ber}}\\) distributions allow tossing one-sided coins. As we have already discussed, \\(\\overline{\\mathrm{Ber}}\\) distributions are the most general probability distributions on the space \\(\\{0, 1\\}\\). Similarly, \\(\\mathrm{Ber}\\) distributions are the most general probability distributions on \\(\\{0, 1\\}\\) with full support, allowing for both outcomes to happen.\nThis leads to the question: is it very problematic if we use \\(\\mathrm{Ber}\\) distributions to model a data generating process which, in fact, belongs to the \\(\\overline{\\mathrm{Ber}}\\) family? We can expect that \\(\\mathrm{Ber}(u)\\) for \\(u \\ll 1\\) is rather a good model for \\(\\overline{\\mathrm{Ber}}(0)\\) (and, similarly, \\(\\mathrm{Ber}(1-u)\\) should be a good model for \\(\\overline{\\mathrm{Ber}}(1)\\)).\nSometimes, when we program simulators, we may want to distinguish the atomic distribution \\(\\delta_0\\) from \\(\\mathrm{Ber}(u)\\) with small \\(u\\) due to explicitly enforcing some constrains: not enforcing them can break the simulator in a specific place.\nHowever, if we think about a modelling problem, where we observe some data points \\(y_1, \\dotsc, y_N\\) and want to learn the underlying distribution, we may prefer to leave some probability that the future outcome may be non-zero, even if all the data points seen so far are zeros.\nToday we will focus on the second kind of problems, where we use \\(\\mathrm{Ber}\\) distributions, rather than \\(\\overline{\\mathrm{Ber}}\\), either for the reason outlined above or for convenience: \\(\\mathrm{Ber}\\) distributions never assign zero probability to any data point."
  },
  {
    "objectID": "posts/how-expressive-are-bernoulli-mixtures.html#bernoulli-distributions",
    "href": "posts/how-expressive-are-bernoulli-mixtures.html#bernoulli-distributions",
    "title": "How expressive are Bernoulli mixture models?",
    "section": "",
    "text": "Let \\(p\\in (0, 1)\\) be a parameter and \\(\\mathrm{Ber}(p)\\) be the Bernoulli distribution, with \\(\\mathrm{Ber}(y\\mid p) = p^y(1-p)^{1-y}\\) for \\(y\\in \\{0, 1\\}\\). Let’s define a slightly more general family of distributions, \\(\\overline{\\mathrm{Ber}}(p)\\), for \\(p\\in [0, 1]\\), as follows: \\[\n  \\overline{\\mathrm{Ber}}(p) = p\\delta_1 + (1-p)\\delta_0 = \\begin{cases}\n    \\delta_0 &\\text{ if } p=0\\\\\n    \\delta_1 &\\text{ if } p=1\\\\\n    \\mathrm{Ber}(p) &\\text{otherwise}\n  \\end{cases}\n\\]\nNamely, the \\(\\mathrm{Ber}\\) distributions model a procedure where a biased (e.g., by bending) coin with two sides is tossed, but \\(\\overline{\\mathrm{Ber}}\\) distributions allow tossing one-sided coins. As we have already discussed, \\(\\overline{\\mathrm{Ber}}\\) distributions are the most general probability distributions on the space \\(\\{0, 1\\}\\). Similarly, \\(\\mathrm{Ber}\\) distributions are the most general probability distributions on \\(\\{0, 1\\}\\) with full support, allowing for both outcomes to happen.\nThis leads to the question: is it very problematic if we use \\(\\mathrm{Ber}\\) distributions to model a data generating process which, in fact, belongs to the \\(\\overline{\\mathrm{Ber}}\\) family? We can expect that \\(\\mathrm{Ber}(u)\\) for \\(u \\ll 1\\) is rather a good model for \\(\\overline{\\mathrm{Ber}}(0)\\) (and, similarly, \\(\\mathrm{Ber}(1-u)\\) should be a good model for \\(\\overline{\\mathrm{Ber}}(1)\\)).\nSometimes, when we program simulators, we may want to distinguish the atomic distribution \\(\\delta_0\\) from \\(\\mathrm{Ber}(u)\\) with small \\(u\\) due to explicitly enforcing some constrains: not enforcing them can break the simulator in a specific place.\nHowever, if we think about a modelling problem, where we observe some data points \\(y_1, \\dotsc, y_N\\) and want to learn the underlying distribution, we may prefer to leave some probability that the future outcome may be non-zero, even if all the data points seen so far are zeros.\nToday we will focus on the second kind of problems, where we use \\(\\mathrm{Ber}\\) distributions, rather than \\(\\overline{\\mathrm{Ber}}\\), either for the reason outlined above or for convenience: \\(\\mathrm{Ber}\\) distributions never assign zero probability to any data point."
  },
  {
    "objectID": "posts/how-expressive-are-bernoulli-mixtures.html#approximation-bounds",
    "href": "posts/how-expressive-are-bernoulli-mixtures.html#approximation-bounds",
    "title": "How expressive are Bernoulli mixture models?",
    "section": "Approximation bounds",
    "text": "Approximation bounds\nFrom now on we assume that \\(\\mathcal Y\\) is a finite space and \\(P\\) and \\(Q\\) be two probability distributions on it.\n\nTotal variation distance\nThe total variation distance is defined as \\[\n  \\mathrm{TV}(P, Q) = \\max_{A \\subseteq \\mathcal Y} |P(A) - Q(A)|.\n\\]\nIt’s quite interesting that \\(\\mathrm{TV}(P, Q)\\) is essentially a scaled version of the \\(L_1\\) norm: \\[\n  \\mathrm{TV}(P, Q) = \\frac{1}{2} | P-Q |_{1} = \\frac 12 \\sum_{y\\in \\mathcal Y} | P(y) - Q(y)|.\n\\]\nThe proof essentially follows by defining a signed measure \\(\\mu = P - Q\\) and building the Hahn decomposition: let \\(A^+ = \\{y\\in \\mathcal Y\\mid \\mu \\ge 0\\}\\) and \\(A^- = \\mathcal Y- A^+ = \\{y\\in \\mathcal Y\\mid \\mu &lt; 0\\}\\). We have \\[\n  0 = P(\\mathcal Y) - Q(\\mathcal Y) = \\mu(\\mathcal Y) = \\mu(A^+) + \\mu(A^-),\n\\]\nso that \\(\\mu(A^+) = -\\mu(A^-)\\).\nNow for every \\(A\\subseteq \\mathcal Y\\) we have \\[\n  \\mu(A) = \\mu(A\\cap A^+) + \\mu(A\\cap A^-) \\le \\mu(A^+).\n\\]\nSimilarly, \\(-\\mu(A) \\le -\\mu(A^-) \\le \\mu(A^+)\\). Hence, we see that \\(A^+\\) can be taken as an optimal \\(A\\) when maximising the total variation distance and \\[\n  \\mathrm{TV}(P, Q) = \\sum_{y : P(y) \\ge Q(y) } \\left(P(y) - Q(y)\\right).\n\\]\nWe now have \\[\\begin{align*}\n  \\mathrm{TV}(P, Q) &= \\mu(A^+) \\\\\n  &= \\frac{1}{2}( \\mu(A^+) - \\mu(A^-) ) \\\\\n  &= \\frac{1}{2} \\left( \\sum_{y \\in A^+ } \\mu(y) + \\sum_{y\\in A^-} (-\\mu(y))  \\right) \\\\\n  &= \\frac 12 \\sum_{y\\in \\mathcal Y} |\\mu(y)| = \\frac 12 |\\mu|_1.\n\\end{align*}\n\\]\n\n\nKullback-Leibler divergence\nIf \\(P\\ll Q\\) (i.e., whenever \\(Q(y) = 0\\) we have \\(P(y) = 0\\). It’s a very convenient condition when \\(Q(y) &gt; 0\\) for all \\(y\\in \\mathcal Y\\)), we can define also the Kullback-Leibler divergence: \\[\n  \\mathrm{KL}(P\\parallel Q) = \\sum_{y\\in \\mathcal Y} P(y) \\frac{P(y)}{Q(y)},\n\\]\nunder the convention that \\(0\\log 0 = 0 \\log \\frac{0}{0} = 0\\). If \\(P \\not\\ll Q\\), we define \\(\\mathrm{KL}(P\\parallel Q) = +\\infty\\).\n\n\nPinsker’s inequalities\nPinsker’s inequality says that for arbitrary \\(P\\) and \\(Q\\) \\[\n  \\mathrm{TV}(P, Q) \\le \\sqrt{\\frac 12 \\mathrm{KL}(P\\parallel Q)}\n\\]\nor, equivalently, \\[\n  \\mathrm{TV}(P, Q)^2 \\le \\frac{1}{2} \\mathrm{KL}(P\\parallel Q).\n\\]\nThis inequality also generalises to infinite and not necessarily discrete spaces and has a beautiful proof due to David Pollard.\nThis inequality also generalises to infinite and not necessarily discrete spaces. However, as we work with finite spaces, we have also the inverse Pinsker’s inequality (see Lemma 4.1 here or Lemma 2 there) at our disposal: let \\(P\\ll Q\\) and \\(\\alpha_Q = \\min_{y\\colon Q(y) &gt; 0} Q(y)\\). Then, we have \\[\n  \\mathrm{KL}(P \\parallel Q) \\le \\frac{4}{\\alpha_Q} \\mathrm{TV}(P, Q)^2.\n\\]\nWhat is important here is that \\(\\alpha_Q\\) depends on \\(Q\\).\nIt is perhaps instructive to recall an elementary proof of this result (see Lemma 2): \\[\n  \\begin{align*}\n  \\mathrm{KL}(P\\parallel Q)&\\le \\chi^2(P\\parallel Q) \\\\\n  & := \\sum_{x} \\frac{ (P(x) - Q(x))^2 }{Q(x)} \\\\\n  &\\le \\frac{1}{\\alpha_Q} \\left(\\sum_x | P(x) - Q(x) |\\right)^2 \\\\\n  &= \\frac{1}{\\alpha_Q} |P-Q|_1^2,\n  \\end{align*}\n\\]\nwhere the inequality between KL and \\(\\chi^2\\) divergences is well-known (see e.g., here): for \\(x\\in (0, 1]\\) we have \\(\\log x \\le x-1\\) (which follows by evaluation at \\(x=1\\) and comparison of the derivatives on the interval \\((0, 1]\\)), so that \\[\n\\begin{align*}\n\\mathrm{KL}(P\\parallel Q) &= \\sum_x P(x)\\log \\frac{P(x)}{Q(x)} \\\\&\\le \\sum_{x} P(x) \\left( \\frac{P(x)}{Q(x)}-1 \\right) \\\\\n&= \\sum_x \\left(\\frac{ P^2(x)}{Q(x)} + Q(x) - 2P(x)\\right) \\\\\n&= \\sum_x \\frac{ (P(x) - Q(x))^2 }{Q(x)} \\\\\n&= \\chi^2(P\\parallel Q).\n\\end{align*}\n\\]\n\n\nApproximating the point distributions with Bernoulli distribution\nLet’s see how well we can compare the point distribution \\(\\delta_0\\) with \\(\\mathrm{Ber}(\\epsilon)\\) for \\(\\epsilon &gt; 0\\). We have \\[\n  \\mathrm{TV}( \\mathrm{Ber}(\\epsilon), \\delta_0 ) = \\frac{1}{2}\\left( |\\epsilon| + |1 - (1-\\epsilon)| \\right) = \\epsilon,\n\\]\nmeaning that this discrepancy measure can attain arbitrarily close value.\nWhen we look for maximum likelihood solution (or employ Bayesian inference), we are interested in \\(\\mathrm{KL}(\\delta_0 \\parallel \\mathrm{Ber}(\\epsilon)) &lt; \\infty\\), as \\(\\mathrm{Ber}(\\epsilon)\\) has full support. We can calculate this quantity exactly: \\[\n  \\mathrm{KL}(\\delta_0 \\parallel \\mathrm{Ber}(\\epsilon)) = \\log \\frac{1}{1-\\epsilon} = -\\log(1-\\epsilon) =  \\epsilon + \\frac{\\epsilon^2}2 + \\frac{\\epsilon^3}{3} + \\cdots,\n\\]\nwhich also can be made arbitrarily small. Namely, for every desired \\(\\ell &gt; 0\\) we can find an \\(\\tilde \\epsilon &gt; 0\\) such that for all \\(\\epsilon &lt; \\tilde \\epsilon\\) we have \\(\\mathrm{KL}(\\delta_0\\parallel \\mathrm{Ber}(\\epsilon)) &lt; \\ell\\). This is useful for establishing the KL support of the prior condition, appearing in Schwartz’s theorem.\nOn the other hand, we see that because \\(\\mathrm{Ber}(\\epsilon)\\not\\ll \\delta_0\\), we have \\[\n  \\mathrm{KL}(\\mathrm{Ber}(\\epsilon) \\parallel \\delta_0 ) = +\\infty,\n\\]\nmeaning that variational inference with arbitrary \\(\\epsilon\\) is always an “equally bad” approximation. Intuitively and very informally speaking, variational inference encourages approximations with lighter tails than the ground-truth distribution and it’s not possible to have lighter “tails” than a point mass!"
  },
  {
    "objectID": "posts/how-expressive-are-bernoulli-mixtures.html#moving-to-the-higher-dimensions",
    "href": "posts/how-expressive-are-bernoulli-mixtures.html#moving-to-the-higher-dimensions",
    "title": "How expressive are Bernoulli mixture models?",
    "section": "Moving to the higher dimensions",
    "text": "Moving to the higher dimensions\nLet’s now think about the probability distributions on the space of binary vectors, \\(\\mathcal Y= \\{0, 1\\}^G\\). Similarly as above, all the distributions with full support on a finite set have to be the categorical distributions (in this case the probability vector has \\(2^G\\) components, with \\(2^G-1\\) free parameters due to the usual constraint of summing up to one). Removing the full support requirement we obtain a discrete distributions with at most \\(2^G\\) atoms.\nLet’s consider perhaps the simplest distribution we could use to model the data points in the \\(\\mathcal Y\\) space: \\[\n\\mathrm{Ber}^{G}(y \\mid p) = \\prod_{g=1}^{G} \\mathrm{Ber}(y_g \\mid p_g).\n\\]\nIn this case, we assume that each entry is the outcome of a coin toss and the coin tosses are independent (even though coins do not have to be identical. For example, we allow \\(p_1 \\neq p_2\\)).\nAs such, this distribution is not particularly expressive and will not be suitable to model dependencies between different entries, which are very important when modelling spin lattices or genomic data. We generally may prefer to use a more expressive distribution, such energy-based models (including the Ising model) or mixture models.\nLet’s focus on mixture models. In a model with \\(K\\) components, we have parameters \\(p\\in (0, 1)^{K\\times G}\\) and \\(u \\in \\Delta^{K-1}\\) resulting in the distribution:\n\\[\n  \\mathrm{BerMix}(p, u) = \\sum_{k=1}^K u_k\\mathrm{Ber}^{G}(p_k),\n\\] i.e., \\[\n  \\mathrm{BerMix}(y\\mid p, u) = \\sum_{k=1}^K u_k\\mathrm{Ber}^{G}(y\\mid p_k),\n\\]\nOne usually hopes to find a relatively small number of components \\(K\\). However, in this blog post we allow arbitrarily large \\(K\\) and focus on the following questions: how expressive is this family of mixture models? Can it approximate arbitrary distributions well?\nLet’s consider the ground-truth data distribution \\[\n  D = \\sum_{k=1}^{2^G} \\pi_k \\delta_{y_k},\n\\]\nwhere \\(y_k \\in \\mathcal Y\\) are all the \\(2^G\\) possible atoms and \\(\\pi \\in \\bar \\Delta^{2^G - 1}\\) are probabilities of observing different atoms. We allow \\(\\pi_k = 0\\) for some \\(k\\) (i.e., we work with the closed probability simplex, rather than the open one), which result in a smaller number of observed atoms.\nWe want to find a Bernoulli mixture which will approximate this distribution well. Of course, using models with a restriction on \\(K\\ll 2^G\\) is more interesting. However, these models also seem to be much harder to study. Let’s instead use a model with all possible \\(K=2^G\\) components of the following form: \\[\n  P_\\epsilon = \\sum_{k=1}^{K} u(\\pi_k, \\epsilon) \\mathrm{Ber}^{G}( s_\\epsilon(y_k) ),\n\\]\nwhere \\(s_\\epsilon(y) = (1-\\epsilon) y + \\epsilon(1-y)\\) are noisy versions of the atoms (and can be understood as actually tossing coins with biases \\(\\epsilon\\) and \\(1-\\epsilon\\)) and \\(u(\\pi_k, \\epsilon) = \\frac{ \\pi_k + \\epsilon }{ 1 + \\epsilon K}\\) ensures that the mixing weights belong to the open simplex \\(\\Delta^{K-1}\\).\nInformally, we expect that for \\(\\epsilon \\approx 0\\) we would have \\(P_\\epsilon \\approx D\\), but let’s try to make it precise in terms of the discrepancy measures used earlier.\n\nBounding the total variation distance\nRecall that \\(\\mathrm{TV}( D, P_\\epsilon )\\) is half of the \\(L_1\\) norm. Let’s focus on \\(y_k \\in \\mathcal Y\\). We have \\[\n|P_\\epsilon(y_k) - D(y_k)| \\le | u(\\pi_k, \\epsilon) \\mathrm{Ber}^{G}(y_k \\mid s_\\epsilon(y_k) ) - \\pi_k | + \\sum_{a\\neq k} u(\\pi_a, \\epsilon) \\mathrm{Ber}^{G}(y_k \\mid s_\\epsilon(y_a))\n\\]\nWe have \\[\n  | u(\\pi_k, \\epsilon) \\mathrm{Ber}^{G}(y_k \\mid s_\\epsilon(y_k) ) - \\pi_k | = \\left| \\frac{\\pi_k + \\epsilon}{1+\\epsilon K} (1-\\epsilon)^G - \\pi_k \\right|,\n\\]\nwhich intuitively can be made arbitrarily small by appropriately \\(\\epsilon\\). More precisely, we have a Taylor approximation (employing the infinitesimal notation for big \\(O\\)): \\[\n  \\begin{align*}\n  | u(\\pi_k, \\epsilon) \\mathrm{Ber}^{G}(y_k \\mid s_\\epsilon(y_k) ) - \\pi_k | &= \\left| 1 - (2^G + G) \\pi_k \\right| \\epsilon  + O(\\epsilon^2) \\\\\n  &= (1 + G + 2^G)\\epsilon + O(\\epsilon^2)\n  \\end{align*}\n\\]\nNow for \\(a\\neq k\\) and \\(\\epsilon &lt; 1/2\\) we have \\[\n  u(\\pi_a, \\alpha) \\mathrm{Ber}^{G}(y_k \\mid s_\\epsilon(y_a) ) \\le \\mathrm{Ber}^{G}(y_k \\mid s_\\epsilon(y_a)) \\le \\epsilon (1-\\epsilon)^{G-1} \\le \\epsilon,\n\\]\nwhere the bound follows from the following reasoning: if \\(y_k = y_a\\) we have the probability \\((1-\\epsilon)^G\\) of obtaining the right outcome \\(y_k\\) by not encountering any bitflip due to the noise \\(\\epsilon\\). However, as \\(y_k\\neq y_a\\), there have to be \\(m\\ge 1\\) positions on which we have to use the \\(\\epsilon\\) noise to obtain the desired result \\(y_k\\). Hence, the probability in this case is \\(\\epsilon^m(1-\\epsilon)^{G-m} \\le \\epsilon (1-\\epsilon)^{G-1}\\) as we have \\(\\epsilon &lt; 1/2\\).\nTo sum up, we have \\[\n  | D(y_k) - P_\\epsilon(y_k) | \\le C \\epsilon + O(\\epsilon^2),\n\\]\nwhere \\(C\\) depends only on \\(G\\) (and it seems that \\(C=O(2^G)\\) is growing exponentially quickly with \\(G\\), so \\(\\epsilon\\) has to be very tiny). By summing up over all \\(k\\) we have \\[\n  \\mathrm{TV}(D, P_\\epsilon) \\le 2^G \\cdot C \\cdot \\epsilon + O(\\epsilon^2),\n\\]\nso that it can be made arbitrarily small.\n\n\nBounding the KL divergences\nLet’s think about the variational approximation with \\(P_\\epsilon\\) to \\(D\\). If \\(D\\) does not have full support, then we have \\(P_\\epsilon \\not\\ll D\\) and \\(\\mathrm{KL}( P_\\epsilon \\parallel D) = +\\infty\\).\nHowever, if \\(D\\) is supported on the whole \\(\\mathcal Y\\), we can use the inverse Pinsker’s inequality for some \\(\\alpha_D &gt; 0\\) and obtain \\[\n  \\mathrm{KL}( P_\\epsilon \\parallel D) \\le \\frac{1}{\\alpha_D} O(\\epsilon^2).\n\\]\nLet’s now think about \\(\\mathrm{KL}(D \\parallel P_\\epsilon)\\). In this case, we have \\(\\alpha_\\epsilon := \\alpha_{P_\\epsilon}\\) varying with \\(\\epsilon\\). Let’s try to bound it from below: \\[\n  \\alpha_\\epsilon \\ge P_\\epsilon(y_k) \\ge \\frac{ \\pi_k + \\epsilon}{1+\\epsilon K} (1-\\epsilon)^G,\n\\]\nso that\n\\[\n  \\frac{1}{\\alpha_\\epsilon} \\le \\frac{(1-\\epsilon)^{-G} (1+ K\\epsilon) }{\\pi_k + \\epsilon}.\n\\]\nIf \\(D\\) is fully supported, we have \\(\\pi_k &gt; 0\\) and \\(\\mathrm{KL}(P_\\epsilon \\parallel D) \\le O(\\epsilon^2)\\) should also decrease at the quadratic rate. However, if \\(\\pi_k = 0\\) for some \\(k\\), we still seem to have \\(\\mathrm{KL}(P_\\epsilon \\parallel D) \\le O(\\epsilon)\\).\nHence, it seems to me that Bernoulli mixtures (albeit with too many components to be practically useful) approximate well arbitrary distributions in terms of the total variation and forward KL divergence, while the backward KL divergence is well-approximated whenever the distribution has full support.\nHowever, I am not sure if I did not make a mistake somewhere in the calculations: if you something suspicious with this derivation, please let me know."
  },
  {
    "objectID": "publications/bayesian-quantification.html",
    "href": "publications/bayesian-quantification.html",
    "title": "Bayesian quantification with black-box estimators",
    "section": "",
    "text": "We have an unlabeled data set and an imperfect classifier. Can we provide Bayesian estimates of the class prevalence vector?"
  },
  {
    "objectID": "publications/bayesian-quantification.html#premise",
    "href": "publications/bayesian-quantification.html#premise",
    "title": "Bayesian quantification with black-box estimators",
    "section": "",
    "text": "We have an unlabeled data set and an imperfect classifier. Can we provide Bayesian estimates of the class prevalence vector?"
  },
  {
    "objectID": "publications/bayesian-quantification.html#abstract",
    "href": "publications/bayesian-quantification.html#abstract",
    "title": "Bayesian quantification with black-box estimators",
    "section": "Abstract",
    "text": "Abstract\nUnderstanding how different classes are distributed in an unlabeled data set is an important challenge for the calibration of probabilistic classifiers and uncertainty quantification. Approaches like adjusted classify and count, black-box shift estimators, and invariant ratio estimators use an auxiliary (and potentially biased) black-box classifier trained on a different (shifted) data set to estimate the class distribution and yield asymptotic guarantees under weak assumptions. We demonstrate that all these algorithms are closely related to the inference in a particular Bayesian model, approximating the assumed ground-truth generative process. Then, we discuss an efficient Markov Chain Monte Carlo sampling scheme for the introduced model and show an asymptotic consistency guarantee in the large-data limit. We compare the introduced model against the established point estimators in a variety of scenarios, and show it is competitive, and in some cases superior, with the state of the art."
  },
  {
    "objectID": "publications/bayesian-quantification.html#errata",
    "href": "publications/bayesian-quantification.html#errata",
    "title": "Bayesian quantification with black-box estimators",
    "section": "Errata",
    "text": "Errata\n\nLemma A.3 should read \\(\\int_{\\mathcal X} \\left| \\sum_{y\\in \\mathcal{Y} } \\lambda_y k_y(x) \\right|\\, \\mathrm{d}\\mu(x)\\), rather than \\(\\int_{\\mathcal X} \\left| \\sum_{y\\in \\mathcal{Y} } k_y(x) \\right| \\, \\mathrm{d}\\mu(x)\\).\nIn November 2024, I found in JASA an article by Fiksel et al. (2021), which in turns links to the Biostatistics paper of Datta et al. (2018). I think these articles are very related and important, but are missing from our literature review in Section 2."
  },
  {
    "objectID": "publications/bayesian-quantification.html#links",
    "href": "publications/bayesian-quantification.html#links",
    "title": "Bayesian quantification with black-box estimators",
    "section": "Links",
    "text": "Links\n\nWe have already discussed quantification in the post on Gibbs sampling and EM algorithm. It is also related to the strict linear independence of measures.\nBayesian quantification originated in the form of unsupervised recalibration. Albert described the background story (and related problems) at the Humans of AI podcast (18:15–25:45).\nBayesian quantification is available in the QuaPy package! Many thanks to Alejandro Moreo Fernández for his kindness and contributions in pull requests 28 and 29."
  },
  {
    "objectID": "publications/bayesian-quantification.html#citation",
    "href": "publications/bayesian-quantification.html#citation",
    "title": "Bayesian quantification with black-box estimators",
    "section": "Citation",
    "text": "Citation\n@article{bayesian-quantification,\n      title={Bayesian Quantification with Black-Box Estimators}, \n      author={Albert Ziegler and Pawe{\\l} Czy{\\.z}},\n      journal={Transactions on Machine Learning Research},\n      issn={2835-8856},\n      year={2024},\n      url={https://openreview.net/forum?id=Ft4kHrOawZ}\n}"
  },
  {
    "objectID": "publications/bayesian-quantification.html#acknowledgments",
    "href": "publications/bayesian-quantification.html#acknowledgments",
    "title": "Bayesian quantification with black-box estimators",
    "section": "Acknowledgments",
    "text": "Acknowledgments\nI had a lot of great time working on this! This is mostly due to Albert Ziegler and Ian Wright, who were wonderful mentors to work with. I am also grateful to Semmle/GitHub UK and the ETH AI Center for funding this work."
  },
  {
    "objectID": "publications/bayesian-quantification.html#behind-the-scenes",
    "href": "publications/bayesian-quantification.html#behind-the-scenes",
    "title": "Bayesian quantification with black-box estimators",
    "section": "Behind the scenes",
    "text": "Behind the scenes\n\nI’m grateful for this experience, especially as this was my entry point to probabilistic machine learning and statistical inference. There were quite a few versions of this paper and it may be tricky to see how the project has changed, but it started with asymptotic consistency guarantees in large-data limit, then we moved to a version of a maximum likelihood approach (and then to a penalised maximum likelihood, to regularise the predictions), and finally we formalised the problem in terms of probabilistic graphical models and employed Bayesian inference.\nThe project started in July 2018 and was accepted for publication in May 2024, what (as of June 2024) makes it my personal record for the time needed to publish a project. When I don’t want to feel to bad about this, I tell myself that (a) I needed to learn many new things to be able to reach the final version of the paper; (b) for most of this time it was a side project, and (c) hey, as the great Leslie Lamport explains on his website, he submitted the paper introducing Paxos algorithm for the first time in 1990 and finally published it in 1998. Of course, Paxos is in an entirely different league than our paper, but it’s good to know (especially for an early-career researcher) that even the greatest scientists with the best ideas also sometimes have to spend years trying to publish."
  },
  {
    "objectID": "publications/mutual-exclusivity.html",
    "href": "publications/mutual-exclusivity.html",
    "title": "Bayesian modeling of mutual exclusivity in cancer mutations",
    "section": "",
    "text": "When exploring the data to build scientific hypotheses about the mutual exclusivity patterns, can we go from hypothesis testing to iterative Bayesian modeling?"
  },
  {
    "objectID": "publications/mutual-exclusivity.html#premise",
    "href": "publications/mutual-exclusivity.html#premise",
    "title": "Bayesian modeling of mutual exclusivity in cancer mutations",
    "section": "",
    "text": "When exploring the data to build scientific hypotheses about the mutual exclusivity patterns, can we go from hypothesis testing to iterative Bayesian modeling?"
  },
  {
    "objectID": "publications/mutual-exclusivity.html#abstract",
    "href": "publications/mutual-exclusivity.html#abstract",
    "title": "Bayesian modeling of mutual exclusivity in cancer mutations",
    "section": "Abstract",
    "text": "Abstract\nWhen cancer develops, gene mutations do not occur independently, prompting researchers to pose scientific hypotheses about their interactions. Synthetic lethal interactions, which result in mutually exclusive mutations, have received considerable attention as they may inform about the structure of aberrant biological pathways in cancer cells and suggest therapeutic targets. However, finding patterns of mutually exclusive genes is a challenging task due to small available sample sizes, sequencing noise, and confounders present in observational studies. Here, we leverage recent advancements in probabilistic programming to propose a fully Bayesian framework for modeling mutual exclusivity based on a family of constrained Bernoulli mixture models. By forming continuous model expansion within the iterative Bayesian workflow, we quantify the uncertainty resulting from small sample sizes and perform careful model criticism. Our analysis indicates that alterations in the EGFR and IDH1 genes may exhibit mutual exclusivity in glioblastoma multiforme tumors. We argue that Bayesian analysis offers a conceptual, systematic, and computationally feasible approach to model building, complementing the findings obtained from classical hypothesis testing approaches."
  },
  {
    "objectID": "publications/mutual-exclusivity.html#links",
    "href": "publications/mutual-exclusivity.html#links",
    "title": "Bayesian modeling of mutual exclusivity in cancer mutations",
    "section": "Links",
    "text": "Links\n\nThis paper looks at the models introduced by E. Szczurek and N. Beerenwinkel in Modeling Mutual Exclusivity of Cancer Mutations (2014) through the lens of Bayesian workflow."
  },
  {
    "objectID": "publications/mutual-exclusivity.html#citation",
    "href": "publications/mutual-exclusivity.html#citation",
    "title": "Bayesian modeling of mutual exclusivity in cancer mutations",
    "section": "Citation",
    "text": "Citation\n@article {Bayesian-modeling-mutual-exclusivity,\n    author = {Czy{\\.z}, Pawe{\\l} and Beerenwinkel, Niko},\n    title = {Bayesian modeling of mutual exclusivity in cancer mutations},\n    elocation-id = {2024.10.29.620937},\n    year = {2024},\n    doi = {10.1101/2024.10.29.620937},\n    publisher = {Cold Spring Harbor Laboratory},\n    URL = {https://www.biorxiv.org/content/early/2024/11/02/2024.10.29.620937},\n    eprint = {https://www.biorxiv.org/content/early/2024/11/02/2024.10.29.620937.full.pdf},\n    journal = {bioRxiv}\n}"
  },
  {
    "objectID": "publications/beyond-normal.html",
    "href": "publications/beyond-normal.html",
    "title": "Beyond normal: on the evaluation of mutual information estimators",
    "section": "",
    "text": "How reliably can we estimate mutual information from non-Gaussian data?"
  },
  {
    "objectID": "publications/beyond-normal.html#premise",
    "href": "publications/beyond-normal.html#premise",
    "title": "Beyond normal: on the evaluation of mutual information estimators",
    "section": "",
    "text": "How reliably can we estimate mutual information from non-Gaussian data?"
  },
  {
    "objectID": "publications/beyond-normal.html#abstract",
    "href": "publications/beyond-normal.html#abstract",
    "title": "Beyond normal: on the evaluation of mutual information estimators",
    "section": "Abstract",
    "text": "Abstract\nMutual information is a general statistical dependency measure which has found applications in representation learning, causality, domain generalization and computational biology. However, mutual information estimators are typically evaluated on simple families of probability distributions, namely multivariate normal distribution and selected distributions with one-dimensional random variables. In this paper, we show how to construct a diverse family of distributions with known ground-truth mutual information and propose a language-independent benchmarking platform for mutual information estimators. We discuss the general applicability and limitations of classical and neural estimators in settings involving high dimensions, sparse interactions, long-tailed distributions, and high mutual information. Finally, we provide guidelines for practitioners on how to select appropriate estimator adapted to the difficulty of problem considered and issues one needs to consider when applying an estimator to a new data set."
  },
  {
    "objectID": "publications/beyond-normal.html#links",
    "href": "publications/beyond-normal.html#links",
    "title": "Beyond normal: on the evaluation of mutual information estimators",
    "section": "Links",
    "text": "Links\n\nThe story behind this project has appeared on the blog.\nWe have written a follow-up paper with new insights! It’s a bit more theoretical than this one, but I like its premise more."
  },
  {
    "objectID": "publications/beyond-normal.html#citation",
    "href": "publications/beyond-normal.html#citation",
    "title": "Beyond normal: on the evaluation of mutual information estimators",
    "section": "Citation",
    "text": "Citation\n@inproceedings{beyond-normal-2023,\n  title={Beyond Normal: On the Evaluation of Mutual Information Estimators},\n  author={Paweł Czyż and Frederic Grabowski and Julia E. Vogt and Niko Beerenwinkel and Alexander Marx},\n  booktitle={Thirty-seventh Conference on Neural Information Processing Systems},\n  year={2023}\n}"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "These posts have evolved from my digital garden1 into a bit longer form. Their main role is to educate myself and they may be incomplete, written from a very subjective point of view, or contain errors. I’d be grateful for feedback on how to improve them!"
  },
  {
    "objectID": "blog.html#footnotes",
    "href": "blog.html#footnotes",
    "title": "Blog",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSee this link for a difference. I maintain my digital garden in Obsidian. Please, let me know if you know a good way of integrating Obsidian with Quarto!↩︎"
  },
  {
    "objectID": "posts/markovs-inequality.html",
    "href": "posts/markovs-inequality.html",
    "title": "Markov’s inequality",
    "section": "",
    "text": "Let \\(\\mathbb R^+_a = [a, +\\infty) = \\{ x\\in \\mathbb R \\mid x\\ge a \\}\\) be the set of real numbers greater or equal to \\(a\\). In particular, \\(\\mathbb R^+_0\\) is the set of real non-negative numbers.\nConsider a situation with the following data:\n\n\\(\\mu\\) is a probability measure on \\(\\mathbb R\\).\n\\(a \\in \\mathbb R\\) is any real number.\n\\(f\\colon \\mathbb R\\to \\mathbb R_0^+\\) is a measurable function that is non-decreasing on \\(\\mathbb R^+_a\\).\n\nThen, we have \\[\n\\mathbb E_\\mu[f] \\ge f(a)\\cdot \\mu(\\mathbb R^+_a).\n\\]\nThe proof is quite simple. Define \\(g(x) = \\min(f(x), f(a))\\). This function is also non-negative and measurable.\nAs \\(f\\) is non-decreasing, we have \\(g(x) = f(a)\\) for all \\(x\\in \\mathbb R^+_a\\). Then, \\[\n\\mathbb E_\\mu [f] \\ge \\mathbb E_\\mu[g] \\ge \\int_{\\mathbb R^+_a} g \\,\\mathrm{d}\\mu = f(a) \\cdot \\mu(\\mathbb R^+_a).\n\\]\nLet \\(X\\colon \\Omega\\to \\mathbb R\\) be a real-valued random variable. If \\(\\mathbb P\\) is the probability measure on \\(\\Omega\\), then \\(\\mathbb P(X\\ge a) = (X_\\sharp\\mathbb P) (\\mathbb R^+_a)\\), where \\(X_\\sharp \\mathbb P(A) = \\mathbb P(X^{-1}(A))\\) defines the pushforward measure being just the law (probability distribution) of \\(X\\). We now have \\[\n    \\mathbb E_{\\mathbb P}[ f\\circ X] = \\mathbb E_{X_\\sharp \\mathbb P}[f],\n\\]\nwhich is nothing else than change of variables formula or the law of of the unconcious statistician.\nIn particular, the equality above takes the form \\[\n    \\mathbb E[f(X)] \\ge f(a) \\cdot \\mathbb P( X\\ge a)\n\\]\nand is a variant of the Markov’s inequality.\nThe classical version is obtained by applying the above inequality to the random variable \\(Y=|X|\\) and \\(f(y) = \\max(0, y)\\). Then \\[\n    \\mathbb E[|X|] \\ge a\\cdot \\mathbb P(|X|\\ge a).\n\\]\nTypically, this inequality is presented for \\(a &gt; 0\\) (note that otherwise it is trivial) and transformed into a bound on the tail probability: \\[\n    \\mathbb P(|X|\\ge a) \\le \\frac{\\mathbb E[|X|]}{a} .\n\\]\nSimilarly, for \\(a &gt; 0\\), \\(n\\ge 1\\) and \\(f(y) = \\max(0, y)^n\\), we obtain from the general inequality above \\[\n    \\mathbb P(|X|\\ge a) \\le \\frac{\\mathbb E[|X|^n]}{a^n} .\n\\]"
  },
  {
    "objectID": "posts/markovs-inequality.html#markovs-inequality",
    "href": "posts/markovs-inequality.html#markovs-inequality",
    "title": "Markov’s inequality",
    "section": "",
    "text": "Let \\(\\mathbb R^+_a = [a, +\\infty) = \\{ x\\in \\mathbb R \\mid x\\ge a \\}\\) be the set of real numbers greater or equal to \\(a\\). In particular, \\(\\mathbb R^+_0\\) is the set of real non-negative numbers.\nConsider a situation with the following data:\n\n\\(\\mu\\) is a probability measure on \\(\\mathbb R\\).\n\\(a \\in \\mathbb R\\) is any real number.\n\\(f\\colon \\mathbb R\\to \\mathbb R_0^+\\) is a measurable function that is non-decreasing on \\(\\mathbb R^+_a\\).\n\nThen, we have \\[\n\\mathbb E_\\mu[f] \\ge f(a)\\cdot \\mu(\\mathbb R^+_a).\n\\]\nThe proof is quite simple. Define \\(g(x) = \\min(f(x), f(a))\\). This function is also non-negative and measurable.\nAs \\(f\\) is non-decreasing, we have \\(g(x) = f(a)\\) for all \\(x\\in \\mathbb R^+_a\\). Then, \\[\n\\mathbb E_\\mu [f] \\ge \\mathbb E_\\mu[g] \\ge \\int_{\\mathbb R^+_a} g \\,\\mathrm{d}\\mu = f(a) \\cdot \\mu(\\mathbb R^+_a).\n\\]\nLet \\(X\\colon \\Omega\\to \\mathbb R\\) be a real-valued random variable. If \\(\\mathbb P\\) is the probability measure on \\(\\Omega\\), then \\(\\mathbb P(X\\ge a) = (X_\\sharp\\mathbb P) (\\mathbb R^+_a)\\), where \\(X_\\sharp \\mathbb P(A) = \\mathbb P(X^{-1}(A))\\) defines the pushforward measure being just the law (probability distribution) of \\(X\\). We now have \\[\n    \\mathbb E_{\\mathbb P}[ f\\circ X] = \\mathbb E_{X_\\sharp \\mathbb P}[f],\n\\]\nwhich is nothing else than change of variables formula or the law of of the unconcious statistician.\nIn particular, the equality above takes the form \\[\n    \\mathbb E[f(X)] \\ge f(a) \\cdot \\mathbb P( X\\ge a)\n\\]\nand is a variant of the Markov’s inequality.\nThe classical version is obtained by applying the above inequality to the random variable \\(Y=|X|\\) and \\(f(y) = \\max(0, y)\\). Then \\[\n    \\mathbb E[|X|] \\ge a\\cdot \\mathbb P(|X|\\ge a).\n\\]\nTypically, this inequality is presented for \\(a &gt; 0\\) (note that otherwise it is trivial) and transformed into a bound on the tail probability: \\[\n    \\mathbb P(|X|\\ge a) \\le \\frac{\\mathbb E[|X|]}{a} .\n\\]\nSimilarly, for \\(a &gt; 0\\), \\(n\\ge 1\\) and \\(f(y) = \\max(0, y)^n\\), we obtain from the general inequality above \\[\n    \\mathbb P(|X|\\ge a) \\le \\frac{\\mathbb E[|X|^n]}{a^n} .\n\\]"
  },
  {
    "objectID": "posts/markovs-inequality.html#section",
    "href": "posts/markovs-inequality.html#section",
    "title": "Markov’s inequality",
    "section": "",
    "text": "Is it possible that most people have grades higher than average? Yes: imagine a test given to a class of 20 students, where 19 students score 100% and the last one scores 80%.\nAssume that \\(X\\) is non-negative and \\(a &gt; 0\\). We have a bound \\[\n    \\mathbb P(X \\ge  a \\mathbb E[X] ) \\le \\frac{\\mathbb E[X]}{a\\mathbb E[X]} = \\frac{1}{a}.\n\\]\nFor example, the probability of getting \\(a = 2\\) times as many points as an average student is smaller than 50%."
  },
  {
    "objectID": "posts/markovs-inequality.html#chebyshevs-inequality",
    "href": "posts/markovs-inequality.html#chebyshevs-inequality",
    "title": "Markov’s inequality",
    "section": "Chebyshev’s inequality",
    "text": "Chebyshev’s inequality\nLet \\(Y = |X - \\mathbb E[X]|^2\\). Take any \\(a &gt; 0\\). Then, for every \\(b = a^2\\), we have \\[\n    \\mathbb P(Y \\ge b) \\le \\frac{\\mathbb E[Y]}{b},\n\\]\nwhich can be rewritten as \\[\n    \\mathbb P( |X - \\mathbb E[X]| \\ge a ) \\le \\frac{\\mathrm{Var}(X)}{a}.\n\\]"
  },
  {
    "objectID": "posts/markovs-inequality.html#relative-inequality",
    "href": "posts/markovs-inequality.html#relative-inequality",
    "title": "Markov’s inequality",
    "section": "Relative inequality",
    "text": "Relative inequality\nIs it possible that most university students have grades higher than average? Yes: imagine a test given to a class of 20 students, where 19 students score 100% and the last one scores 80%. However, is it possible that most students get five times as many points as the average?\nLet \\(X\\) be the non-negative number of points scored and \\(a &gt; 0\\). We have a bound \\[\n    \\mathbb P(X \\ge  a \\mathbb E[X] ) \\le \\frac{\\mathbb E[X]}{a\\mathbb E[X]} = \\frac{1}{a}.\n\\]\nHence, the probability of getting \\(a = 5\\) times as many points as an average student is at most 20%.\nSimilarly, if I were to think about a hypothetical investment (hypothetical – this post does not contain any investment or financial advice. It is just a simple story to illustrate a mathematical inequality, which is the topic of this post), where I would have to pay an upfront cost of \\(c\\) and obtain in return \\(X\\) (which can be also \\(0\\) or smaller than \\(c\\)), could I expect becoming very rich? Probably I couldn’t: the chance at least 100 times larger than the invested amount is \\[\n    \\mathbb P(X\\ge 100c) \\le \\frac{\\mathbb E[X]}{100c}.\n\\]\nEven if \\(\\mathbb E[X] = 2c\\), we see that the chance of becoming extremely wealthy by executing such a kind of investment once is rather small.\nFor more examples see this book."
  },
  {
    "objectID": "posts/markovs-inequality.html#randomized-inequality",
    "href": "posts/markovs-inequality.html#randomized-inequality",
    "title": "Markov’s inequality",
    "section": "Randomized inequality",
    "text": "Randomized inequality\nA. Ramdas and T. Manole wrote an amazing paper with stronger bounds. Assume that \\(X\\) non-negative and that \\(U \\sim \\mathrm{Uniform}([0, 1])\\) is independent of \\(X\\). Then, \\[\n    \\mathbb P(X\\ge aU) \\le \\frac{\\mathbb E[X]}{a}\n\\] for every \\(a &gt; 0\\).\nThe proof follows from the law of total expectation: \\[\\begin{align*}\n    \\mathbb P(X\\ge aU) &= \\mathbb E[\\mathbf 1[X\\ge aU]] \\\\\n    &= \\mathbb E[ \\mathbb E[\\mathbf 1[X\\ge aU]  \\mid X ]] \\\\\n    &= \\mathbb E[ \\mathbb P(X\\ge aU \\mid X ) ] \\\\\n    &= \\mathbb E[ \\mathbb P(U\\le X/a \\mid X)] \\\\\n    &= \\mathbb E[ \\min(X/a, 1)] \\\\\n    &\\le \\mathbb E[X/a] \\\\\n    &= \\mathbb E[X] / a.\n\\end{align*}\n\\]"
  },
  {
    "objectID": "posts/markovs-inequality.html#footnotes",
    "href": "posts/markovs-inequality.html#footnotes",
    "title": "Markov’s inequality",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis post does not contain any investment or financial advice. It is just a story to illustrate a mathematical fact.↩︎"
  },
  {
    "objectID": "posts/markovs-inequality.html#links",
    "href": "posts/markovs-inequality.html#links",
    "title": "Markov’s inequality",
    "section": "Links",
    "text": "Links\n\nWikipedia entry\nThe chapter with inequalities in the mentioned book.\nThe mentioned article.\nA book on inequalities."
  }
]