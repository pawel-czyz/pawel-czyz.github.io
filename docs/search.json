[
  {
    "objectID": "posts/histograms-vs-density-estimation.html",
    "href": "posts/histograms-vs-density-estimation.html",
    "title": "Histograms or kernel density estimators?",
    "section": "",
    "text": "I have recently seen Michael Betancourt’s talk in which he explains why kernel density estimators can be misleading when visualising samples and points to his wonderful case study which includes comparison between histograms and kernel density estimators, as well as many other things.\nI recommend reading this case study in depth; in this blog post we will only try to reproduce the example with kernel density estimators in Python."
  },
  {
    "objectID": "posts/histograms-vs-density-estimation.html#problem-setup",
    "href": "posts/histograms-vs-density-estimation.html#problem-setup",
    "title": "Histograms or kernel density estimators?",
    "section": "Problem setup",
    "text": "Problem setup\nWe will start with a Gaussian mixture with two components and draw the exact probability density function (PDF) as well as a histogram with a very large sample size.\n\n\nCode\nimport matplotlib.pyplot as plt\nimport numpy as np \nfrom scipy import stats\n\nplt.style.use(\"dark_background\")\n\nclass GaussianMixture:\n  def __init__(self, proportions, mus, sigmas) -&gt; None:\n    proportions = np.asarray(proportions)\n    self.proportions = proportions / proportions.sum()\n    assert np.min(self.proportions) &gt; 0\n\n    self.mus = np.asarray(mus)\n    self.sigmas = np.asarray(sigmas)\n\n    n = len(self.proportions)\n    self.n_classes = n\n    assert self.proportions.shape == (n,)\n    assert self.mus.shape == (n,)\n    assert self.sigmas.shape == (n,)\n\n  def sample(self, rng, n: int) -&gt; np.ndarray:\n    z = rng.choice(\n      self.n_classes,\n      p=self.proportions,\n      replace=True,\n      size=n,\n    )\n    return self.mus[z] + self.sigmas[z] * rng.normal(size=n)\n\n  def pdf(self, x):\n    ret = 0\n    for k in range(self.n_classes):\n      ret += self.proportions[k] * stats.norm.pdf(x, loc=self.mus[k], scale=self.sigmas[k])\n    return ret\n\nmixture = GaussianMixture(\n  proportions=[2, 1],\n  mus=[-2, 2],\n  sigmas=[1, 1],\n)\n\nrng = np.random.default_rng(32)\n\nlarge_data = mixture.sample(rng, 100_000)\n\nx_axis = np.linspace(np.min(large_data), np.max(large_data), 101)\npdf_values = mixture.pdf(x_axis)\n\nfig, ax = plt.subplots(figsize=(3, 2), dpi=100)\n\nax.hist(large_data, bins=150, density=True, histtype=\"stepfilled\", alpha=0.5, color=\"C0\")\nax.plot(x_axis, pdf_values, c=\"C2\", linestyle=\"--\")\n\nax.set_title(\"Probability density function\\nand histogram with large sample size\")\n\n\nText(0.5, 1.0, 'Probability density function\\nand histogram with large sample size')\n\n\n\n\n\nGreat, histogram with large sample size agreed well with the exact PDF!"
  },
  {
    "objectID": "posts/histograms-vs-density-estimation.html#plain-old-histograms",
    "href": "posts/histograms-vs-density-estimation.html#plain-old-histograms",
    "title": "Histograms or kernel density estimators?",
    "section": "Plain old histograms",
    "text": "Plain old histograms\nLet’s now move to a more challenging problem: we have only a moderate sample size available, say 100 points.\n\n\nCode\ndata = mixture.sample(rng, 100)\n\nfig, axs = plt.subplots(5, 1, figsize=(3.2, 3*5), dpi=100)\nbin_sizes = (3, 5, 10, 20, 50)\n\nfor bins, ax in zip(bin_sizes, axs):\n  ax.hist(data, bins=bins, density=True, histtype=\"stepfilled\", alpha=0.5, color=\"C0\")\n  ax.plot(x_axis, pdf_values, c=\"C2\", linestyle=\"--\")\n\n  ax.set_title(f\"{bins} bins\")\n\nfig.tight_layout()\n\n\n\n\n\nWe see that too few bins (three, but nobody will actually choose this number for 100 data points) we don’t see two modes and that for more than 20 and 50 bins the histogram looks quite noisy. Both 5 and 10 bins would make a sensible choice in this problem."
  },
  {
    "objectID": "posts/histograms-vs-density-estimation.html#kernel-density-estimators",
    "href": "posts/histograms-vs-density-estimation.html#kernel-density-estimators",
    "title": "Histograms or kernel density estimators?",
    "section": "Kernel density estimators",
    "text": "Kernel density estimators\nNow it’s the time for kernel density estimators. We will use several kernel families and several different bandwidths:\n\n\nCode\nfrom sklearn.neighbors import KernelDensity\n\n\nkernels = [\"gaussian\", \"tophat\", \"cosine\"]\nbandwidths = [0.1, 1.0, 3.0, \"scott\", \"silverman\"]\n\nfig, axs = plt.subplots(\n  len(kernels),\n  len(bandwidths),\n  figsize=(12, 8),\n  dpi=130,\n)\n\nfor i, kernel in enumerate(kernels):\n  axs[i, 0].set_ylabel(f\"Kernel: {kernel}\")\n  for j, bandwidth in enumerate(bandwidths):\n    ax = axs[i, j]\n\n    kde = KernelDensity(bandwidth=bandwidth, kernel=kernel)\n    kde.fit(data[:, None])\n\n    kde_pdf = np.exp(kde.score_samples(x_axis[:, None]))\n\n    ax.plot(x_axis, pdf_values, c=\"C2\", linestyle=\"--\")\n    ax.fill_between(x_axis, 0.0, kde_pdf, color=\"C0\", alpha=0.5)\n\n\nfor j, bandwidth in enumerate(bandwidths):\n  axs[0, j].set_title(f\"Bandwidth: {bandwidth}\")\n\nfig.tight_layout()\n\n\n\n\n\nI see the point now! Apart from the small bandwidth case (0.1 and sometimes Silverman) the issues with KDE plots are hard to diagnose. Moreover, conclusions from different plots are different: is the distribution multimodal? If so, how many modes are there? What are the “probability masses” of each modes? Observing only one of these plots can lead to wrong conclusions."
  },
  {
    "objectID": "posts/histograms-vs-density-estimation.html#links",
    "href": "posts/histograms-vs-density-estimation.html#links",
    "title": "Histograms or kernel density estimators?",
    "section": "Links",
    "text": "Links\n\nWhat’s wrong with a kernel density: a blog post by Andrew Gelman, explaining why he prefers histograms over kernel density plots.\nMichael Betancourt’s case study, which also discusses histograms with error bars."
  },
  {
    "objectID": "posts/board-games-monte-carlo.html",
    "href": "posts/board-games-monte-carlo.html",
    "title": "Understanding board games with Monte Carlo simulations",
    "section": "",
    "text": "I like playing board games, but I never remember the probabilities of different interesting events. Let’s code a very simple Monte Carlo simulation to evaluate probabilities used in them, so I can revisit to this website and use it to (maybe eventually) win."
  },
  {
    "objectID": "posts/board-games-monte-carlo.html#fight-or-flight",
    "href": "posts/board-games-monte-carlo.html#fight-or-flight",
    "title": "Understanding board games with Monte Carlo simulations",
    "section": "Fight or flight?",
    "text": "Fight or flight?\nIn the rare days when I find time to play Runebound, I find myself in situations fighting monsters and trying to decide whether I should try to fight them or escape. I know a monster’s strength (high), I know my strength (low), but I don’t know how likely it is that the difference can be compensated by throwing two ten-sided dice.\nLet’s estimate the chances of getting at least \\(X\\) points due to the dice throw.\n\n\nCode\nimport numpy as np\n\nn_simulations: int = 100_000\ndice: int = 10\n\nrng = np.random.default_rng(42)\noccurrences = np.zeros(2 * dice + 1, dtype=float)\n\nthrows = rng.integers(1, dice, endpoint=True, size=(n_simulations, 2))\ntotal = throws.sum(axis=1)\n\nfor t in total:\n    occurrences[:t+1] += 1\n\noccurrences /= n_simulations\n\nfor i, p in enumerate(occurrences):\n    if i &lt; 1:\n        continue\n    print(f\"{i}: {100*p:.1f}%\")\n\n\n1: 100.0%\n2: 100.0%\n3: 99.0%\n4: 97.0%\n5: 94.0%\n6: 90.1%\n7: 85.2%\n8: 79.2%\n9: 72.2%\n10: 64.1%\n11: 55.1%\n12: 45.2%\n13: 36.0%\n14: 28.0%\n15: 21.1%\n16: 15.1%\n17: 10.0%\n18: 6.0%\n19: 3.0%\n20: 1.0%\n\n\nIn this case it’s also very easy to actually calculate the probabilities without Monte Carlo simulation:\n\n\nCode\nprobabilities = np.zeros(2*dice + 1, dtype=float)\n\nfor result1 in range(1, dice + 1):\n    for result2 in range(1, dice + 1):\n        total = result1 + result2\n        probabilities[:total + 1] += 1/dice**2\n\nfor i, p in enumerate(occurrences):\n    if i &lt; 1:\n        continue\n    print(f\"{i}: {100*p:.1f}%\")\n\n\n1: 100.0%\n2: 100.0%\n3: 99.0%\n4: 97.0%\n5: 94.0%\n6: 90.1%\n7: 85.2%\n8: 79.2%\n9: 72.2%\n10: 64.1%\n11: 55.1%\n12: 45.2%\n13: 36.0%\n14: 28.0%\n15: 21.1%\n16: 15.1%\n17: 10.0%\n18: 6.0%\n19: 3.0%\n20: 1.0%\n\n\nThe exact solution requires \\(O(K^2)\\) operations, where one uses two dice with \\(K\\) sides1. For a larger number of dice this solution may not be as tractable, so Monte Carlo approximations may shine."
  },
  {
    "objectID": "posts/board-games-monte-carlo.html#where-should-my-cheese-be",
    "href": "posts/board-games-monte-carlo.html#where-should-my-cheese-be",
    "title": "Understanding board games with Monte Carlo simulations",
    "section": "Where should my cheese be?",
    "text": "Where should my cheese be?\nIn Cashflow one way to win the end-game is to quickly get to the tile with a cheese-shaped token. As this token can be placed in advance, I was wondering what the optimal location of it should be.\nIf I put the token on the first tile, I need to throw exactly one in my first throw or I will need to travel across the whole board to close the loop and have another chance (or try to win the game in another way).\nLet’s use Monte Carlo simulation to estimate where I should put the token so I can win in at most five moves:\n\n\nCode\nimport numpy as np \n\nN_SIMULATIONS: int = 100_000\nN_THROWS: int = 5\nDICE: int = 6  # Number of sides on the dice\nrng = np.random.default_rng(101)\n\nvisitations = np.zeros(N_THROWS * DICE + 1)\n\nfor simulation in range(N_SIMULATIONS):\n    position = 0\n    for throw_index in range(N_THROWS):\n        result = rng.integers(1, DICE, endpoint=True)\n        position += result\n        visitations[position] += 1\n\nfor i in range(N_THROWS * DICE + 1):\n    percentage = 100 * visitations[i] / N_SIMULATIONS\n    print(f\"{i}: {percentage:.1f}\")\n\n\n0: 0.0\n1: 16.5\n2: 19.3\n3: 22.8\n4: 26.4\n5: 30.8\n6: 36.2\n7: 25.2\n8: 26.8\n9: 28.1\n10: 28.6\n11: 28.4\n12: 27.9\n13: 25.8\n14: 25.1\n15: 24.0\n16: 21.8\n17: 19.6\n18: 16.5\n19: 13.9\n20: 11.2\n21: 8.5\n22: 6.2\n23: 4.3\n24: 2.7\n25: 1.6\n26: 0.9\n27: 0.5\n28: 0.2\n29: 0.1\n30: 0.0\n\n\nAgain, we could do this in the exact fashion — for example, for 30 we know that the probability is exactly \\(6^{-5}\\approx 0.013\\%\\), but it’s quite clear that the sixth tile gives decent chances of winning in the first few moves."
  },
  {
    "objectID": "posts/board-games-monte-carlo.html#footnotes",
    "href": "posts/board-games-monte-carlo.html#footnotes",
    "title": "Understanding board games with Monte Carlo simulations",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe implemented solution works in \\(O(K^3)\\) due to the probabilities[:total + 1] operation. If the performance did really matter here, we could store the occurrences and then calculate cumulative sums only once in the end.↩︎"
  },
  {
    "objectID": "posts/expectations-student-mentorship.html",
    "href": "posts/expectations-student-mentorship.html",
    "title": "Student mentorship: expectations document",
    "section": "",
    "text": "Welcome! This document is supposed to explain my general mentoring style and act as a skeleton around we can build the collaboration and mentorship rules.\nPlease, note:"
  },
  {
    "objectID": "posts/expectations-student-mentorship.html#mission-statement",
    "href": "posts/expectations-student-mentorship.html#mission-statement",
    "title": "Student mentorship: expectations document",
    "section": "Mission statement",
    "text": "Mission statement\nWhen I advise on a project I try to keep the following in mind:\n\nI want you to learn and become a better researcher and engineer at the end of the project.\nIt’s more important that we understand each other and are happy with the mentorship, rather than we get an additional feature."
  },
  {
    "objectID": "posts/expectations-student-mentorship.html#expectations",
    "href": "posts/expectations-student-mentorship.html#expectations",
    "title": "Student mentorship: expectations document",
    "section": "Expectations",
    "text": "Expectations\n\nWhat you can expect from me\n\nI’ll find regular time to meet with you and advise on the steps which may be worth taking. While I will be more “hands-on” and have more precise ideas when you start, I want you to become an independent thinker with a good knowledge on the topic – it’s also likely that you’ll know more about the topic than me by the end of your project!\nI’ll advise you on your code and writing, to make sure that your skills improve.\n\nI’ll keep an open mindset to your comments and suggestions. If you encounter any issues, let me know and we’ll work together on resolving them.\n\n\n\nWhat I’d like to expect from you\n\nHonesty. If something doesn’t work for you (e.g., the expectations and the workload are too high), I said something ridiculously wrong, or the experiments fail, let’s discuss. I’m still learning both how to be a good mentor and a good scientist.\nConforming to use good research and coding practices. We will work on open-source projects and I expect you to write good code (with documentation and tests) and run reproducible experiments. Developing these skills takes time and we will work together to make sure that your research and programming skills are improving.\nTaking the ownership of conforming to the university rules. You should remind me when your thesis is due three months before submitting it, so we can discuss the outline, and send me the first draft three weeks before the deadline, so I can review it.\n\n\n\nConflict resolution\nIn case of a conflict with an academic or a student, please contact me and we will work together to resolve the conflict. If you feel that you do not want me to be involved (e.g., the conflict is between you and me), I encourage you to contact my mentor, Professor Niko Beerenwinkel or ETH’s Ombudspersons.\n\n\nMisc\nI’m not an established researcher in the field (and I don’t have a PhD!). Apart from the fact that I may be wrong in different aspects (happy to learn!), the reference letters written by me are unlikely to be accepted e.g., if you apply for a PhD. If you need a reference letter at the end of the project, I’d suggest to ask Professor Niko Beerenwinkel (and CC me) whether he could provide one."
  },
  {
    "objectID": "posts/expectations-student-mentorship.html#general-research-advice",
    "href": "posts/expectations-student-mentorship.html#general-research-advice",
    "title": "Student mentorship: expectations document",
    "section": "General research advice",
    "text": "General research advice\nAlthough I will supply you with an initial reading list tailored to your project, I’d like to share below some general advice on research, knowledge work, and learning. (Remember – if you see that some of these do not work you, feel free to replace them with better practices. I’d also be grateful if you could share them with me, so I can update this document).\n\nResearch notes and journal\nI’d strongly encourage you to book some time at the start and the end of every working day to work on your research notes and write your observations in a journal.\nThis serves multple purposes: - I believe it will help you to improve your understanding of the domain. - At some point you will need to write your thesis. You will see that it is much easier to edit a series of connected research notes into a first draft, rather than starting with an empty page. - By practicing this over the duration of the project, you will end up with a skill which is useful regardless whether you decide to move into industry or stay in academia.\nTo start writing research notes, read an Andy Matuschak’s note or watch Martin Adams’ video. Popular software includes Obsidian and Zettlr (and you can use them for the journal as well).\nFor your research journal, you may find this blog post useful. Journal can also be helpful to track your feelings and attitude towards the project, so we can adjust the workload or troubleshoot the process – see this post.\n\n\nReading scientific literature\n\nI’d suggest to read this Andy Matuschak’s note and this short P.N. Edwards’ article.\nThis is also a skill which takes time to master, so I’d suggest to practice it regularly and go back and refresh the principles of effective reading.\nYou will see that there is always too much literature to read than the time permits, so it’s critical to think what you want to learn from a paper.\n\nAre there specific questions I want to have the answer to by reading this paper? (It’s always good to read papers with several questions in mind.)\nIs this some maths or statistics which is crucial to deeply understand for the project? If so, several hours may be required and there is nothing to be done.\nIs this a paper which main conclusion can be quickly understood just by looking at one figure and the abstract or conclusions?\nIs this a paper which is potentially useful if problem X arises? If so, it’s probably good to say in a research problem on topic X that this paper may be useful to deeply understand it then.\n\nI like to use Connected Papers to find papers related to the paper of interest. Another strategy is to use Google Scholar to find papers which cited the paper of interest or see what the superstars are doing.\nSpeaking of superstars, Twitter has become a place where new research results are often announced and have short “tl;dr” threads. I would suggest to create a recurrent task (e.g., half an hour every two weeks) and check what the superstars have been doing. (Note that the temptation to procrastinate can be huge. This is why I recommend to set only a specific time to check it.)\n\n\n\nFinding a sustainable working style\n\nAs Bastian Rieck advises, it’s crucial that you find a sustainable workflow. Working on a project over a few months is a long time and “it’s rather a marathon than a sprint”.\nI’m interested in seeing that your expertise grows and that work you produce is of good quality, rather than in counting the hours you put into the work:\n\nIf you think that my expectations are unrealistic, just talk to me – we don’t need to rush and the scope of the thesis can always be adjusted to be more realistic.\nMake sure that you prioritize your mental health and well-being.\nPlease, please, please, no work on weekends and holidays.\n\nYou will see that different people have different working styles. This is fine – they are also working on different projects, have different backgrounds, and have different goals. Don’t compare yourself with them and embrace your way of working as well as theirs.\n\n\n\nProgramming\n\nWe will use Git version control and GitHub. Please, make sure that you have an account and send me your username, so I can add you to the project.\nIf you had not worked with Git before, I recommend (a) Creating a “sandbox” repository and playing with different commands. R. Dudler’s “The simple guide” is a nice way to get started.\nLearning good software practices is like learning a new language – working with a dictionary won’t make one proficient in one day, but using it regularly can help to avoid common errors. I recommend Google Style Guide and (to know what should be avoided) Python anti-patterns.\n\n\n\nMisc\n\nIf you want to become a researcher, R. Hamming’s “You and your research” talk is a classic.\nPatrick Kidger and Andrej Karpathy also wrote on this topic."
  },
  {
    "objectID": "posts/expectations-student-mentorship.html#references",
    "href": "posts/expectations-student-mentorship.html#references",
    "title": "Student mentorship: expectations document",
    "section": "References",
    "text": "References\nI used the following resources to draft the document above. However, all the mistakes (scientific, mentoring, grammar) are to blame on myself.\n\nK.S. Masters and P.K. Kreeger’s “Ten Simple Rules” article\nYinghzhen Li’s blogpost\nThe document issued by Niko Beerenwinkel to his PhD students."
  },
  {
    "objectID": "posts/triangle-distributions.html",
    "href": "posts/triangle-distributions.html",
    "title": "Two distributions on a triangle",
    "section": "",
    "text": "Frederic, Alex and I have been discussing some experiments related to our work on mutual information estimators and Frederic suggested to look at one distribution. I misunderstood what he meant, but this mistake turned out to be quite an interesting object.\nSo let’s take a look at two distributions defined over a triangle \\[T = \\{ (x, y)\\in (0, 1)\\times (0, 1) \\mid y &lt; x \\}\\] and calculate their mutual information."
  },
  {
    "objectID": "posts/triangle-distributions.html#uniform-joint",
    "href": "posts/triangle-distributions.html#uniform-joint",
    "title": "Two distributions on a triangle",
    "section": "Uniform joint",
    "text": "Uniform joint\nConsider a probability distribution with constant probability density function (PDF) of the joint distribution: \\[p_{XY}(x, y) = 2 \\cdot \\mathbf{1}[y&lt;x].\\]\nWe have \\[p_X(x) = \\int\\limits_0^x p_{XY}(x, y)\\, \\mathrm{d}y = 2x\\] and \\[ p_Y(y) = \\int\\limits_0^1 p_{XY}(x, y) \\mathbf{1}[y &lt; x]  \\, \\mathrm{d}x = \\int\\limits_y^1 p_{XY}(x, y) \\, \\mathrm{d}x = 2(1-y).\\]\nHence, pointwise mutual information is given by \\[ i(x, y) = \\log \\frac{ p_{XY}(x, y) }{p_X(x) \\, p_Y(y) } = \\log \\frac{1}{2x(1-y)}\\] and mutual information is\n\\[I(X; Y) = \\int\\limits_0^1 \\mathrm{d}x \\int\\limits_x^1 i(x, y)\\, p_{XY}(x, y) \\mathrm{d}y = 1-\\log 2 \\approx 0.307.\\]\nFinally, let’s visualise this distribution to numerically validate the formulae above:\n\n\nCode\nfrom typing import Protocol\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nplt.style.use(\"dark_background\")\n\n\nclass Distribution(Protocol):\n  def sample(self, rng, n_samples: int) -&gt; np.ndarray:\n    pass\n\n  def p_xy(self, x: np.ndarray, y: np.ndarray) -&gt; np.ndarray:\n    pass\n\n  def p_x(self, x: np.ndarray) -&gt; np.ndarray:\n    pass\n\n  def p_y(self, y: np.ndarray) -&gt; np.ndarray:\n    pass\n\n  def pmi(self, x: np.ndarray, y: np.ndarray) -&gt; np.ndarray:\n    pass\n\n  @property\n  def mi(self) -&gt; float:\n    pass\n\n\nclass UniformJoint(Distribution):\n  def sample(self, rng, n_samples):\n    samples = rng.uniform(low=1e-9, size=(3 * n_samples, 2))\n    samples = np.asarray(list(filter(lambda point: point[1] &lt; point[0], samples)))\n    if len(samples) &lt; n_samples:\n      samples = self.sample(rng, n_samples)\n    \n    assert len(samples) &gt;= n_samples\n    return samples[:n_samples, ...]\n\n  def p_xy(self, x, y):\n    return np.where(y &lt; x, 2.0, 0.0)\n\n  def p_x(self, x):\n    return 2*x\n\n  def p_y(self, y):\n    return 2*(1-y)\n\n  def pmi(self, x, y):\n    return np.where(y &lt; x, -np.log(2*x*(1-y)), np.nan)\n\n  @property\n  def mi(self):\n    return 0.307\n\n\ndef visualise_dist(\n  rng,\n  dist: Distribution,\n  n_samples: int = 15_000,\n) -&gt; plt.Figure:\n  fig, axs = plt.subplots(2, 3, figsize=(3*2.2, 2*2.2))\n\n  samples = dist.sample(rng, n_samples=n_samples)\n\n  t_axis = np.linspace(1e-9, 1 - 1e-9, 51)\n\n  X, Y = np.meshgrid(t_axis, t_axis)\n\n  # Visualise joint probability\n  ax = axs[0, 0]\n  ax.scatter(samples[:, 0], samples[:, 1], rasterized=True, alpha=0.3, s=0.2, marker=\".\")\n  ax.set_xlim(0, 1)\n  ax.set_ylim(0, 1)\n  ax.set_title(\"Samples from $P_{XY}$\")\n  ax.set_xlabel(\"$x$\")\n  ax.set_ylabel(\"$y$\")\n\n  ax = axs[1, 0]\n  ax.imshow(dist.p_xy(X, Y), origin=\"lower\", extent=[0, 1, 0, 1], cmap=\"magma\")\n  ax.set_title(\"PDF $p_{XY}$\")\n  ax.set_xlabel(\"$x$\")\n  ax.set_ylabel(\"$y$\")\n\n  # Visualise marginal distributions\n  ax = axs[0, 1]\n  ax.set_xlim(0, 1)\n  ax.hist(samples[:, 0], bins=np.linspace(0, 1, 51), density=True, alpha=0.2, rasterized=True)\n  ax.plot(t_axis, dist.p_x(t_axis))\n  ax.set_xlabel(\"$x$\")\n  ax.set_title(\"PDF $p_X$\")\n\n  ax = axs[1, 1]\n  ax.set_xlim(0, 1)\n  ax.hist(samples[:, 1], bins=np.linspace(0, 1, 51), density=True, alpha=0.2, rasterized=True)\n  t_axis = np.linspace(0, 1, 51)\n  ax.plot(t_axis, dist.p_y(t_axis))\n  ax.set_xlabel(\"$y$\")\n  ax.set_title(\"PDF $p_Y$\")\n\n  # Visualise PMI\n  ax = axs[0, 2]\n  ax.set_xlim(0, 1)\n  ax.set_ylim(0, 1)\n  ax.imshow(dist.pmi(X, Y), origin=\"lower\", extent=[0, 1, 0, 1], cmap=\"magma\")\n  ax.set_title(\"PMI\")\n  ax.set_xlabel(\"$x$\")\n  ax.set_ylabel(\"$y$\")\n\n  ax = axs[1, 2]\n  pmi_profile = dist.pmi(samples[:, 0], samples[:, 1])\n  mi = np.mean(pmi_profile)\n  ax.set_title(f\"PMI histogram. MI={dist.mi:.2f}\")  \n  ax.axvline(mi, color=\"navy\", linewidth=1)\n  ax.axvline(dist.mi, color=\"salmon\", linewidth=1, linestyle=\"--\")\n  ax.hist(pmi_profile, bins=np.linspace(-2, 5, 21), density=True)\n  ax.set_xlabel(\"PMI value\")\n\n  return fig\n\nrng = np.random.default_rng(42)\ndist = UniformJoint()\n\nfig = visualise_dist(rng, dist)\nfig.tight_layout()"
  },
  {
    "objectID": "posts/triangle-distributions.html#uniform-margin",
    "href": "posts/triangle-distributions.html#uniform-margin",
    "title": "Two distributions on a triangle",
    "section": "Uniform margin",
    "text": "Uniform margin\nThe above distribution is interesting, but when I heard about the distribution over the triangle, I actually had the following generative model in mind: \\[\\begin{align*}\n  X &\\sim \\mathrm{Uniform}(0, 1),\\\\\n  Y \\mid X=x &\\sim \\mathrm{Uniform}(0, x).\n\\end{align*}\\]\nWe have \\(p_X(x) = 1\\) and therefore \\[p_{XY}(x, y) = p_{Y\\mid X}(y\\mid x) = \\frac{1}{x}\\,\\mathbf{1}[y &lt; x].\\]\nAgain, this distribution is defined on the triangle \\(T\\), although now the joint is not uniform.\nWe have \\[ p_Y(y) = \\int\\limits_y^1  \\frac{1}{x} \\, \\mathrm{d}x = -\\log y\\] and \\[i(x, y) = \\log \\frac{1}{-x \\log y} = -\\log \\big(x\\cdot (-\\log y)\\big )\n= - \\left(\\log(x) + \\log(-\\log y) \\right) = -\\log x - \\log(-\\log y).\\] This expression suggests that if \\(p_Y(y)\\) were uniform on \\((0, 1)\\) (but it is not), the pointwise mutual information \\(i(x, Y)\\) would be distributed according to Gumbel distribution.\nThe mutual information \\[\n  I(X; Y) = -\\int\\limits_0^1 \\mathrm{d}y \\int\\limits_y^1 \\frac{ \\log x + \\log(-\\log y)}{x} \\, \\mathrm{d}x = \\frac{1}{2} \\int\\limits_0^1 \\log y \\cdot \\log \\left(y \\log ^2 y\\right) \\, \\mathrm{d}y = \\gamma \\approx 0.577\n\\] is in this case the Euler–Mascheroni constant. I don’t know how to do this integral, but both Mathematica and Wolfram Alpha seem to be quite confident in it.\nPerhaps it shouldn’t be too surprising as \\(\\gamma\\) can appears in expressions involving mean of the Gumbel distribution. However, I’d like to understand this connection better.\nPerhaps another time; let’s finish this post with another visualisation:\n\n\nCode\nclass UniformMargin(Distribution):\n  def sample(self, rng, n_samples: int) -&gt; np.ndarray:\n    x = rng.uniform(size=(n_samples,))\n    y = rng.uniform(high=x)\n    return np.hstack([x.reshape((-1, 1)), y.reshape((-1, 1))])\n\n  def p_xy(self, x: np.ndarray, y: np.ndarray) -&gt; np.ndarray:\n    return np.where(y &lt; x, np.reciprocal(x), np.nan)\n\n  def p_x(self, x: np.ndarray) -&gt; np.ndarray:\n    return np.full_like(x, fill_value=1.0)\n\n  def p_y(self, y: np.ndarray) -&gt; np.ndarray:\n    return -np.log(y)\n\n  def pmi(self, x: np.ndarray, y: np.ndarray) -&gt; np.ndarray:\n    return np.where(y &lt; x, -np.log(-x * np.log(y)), np.nan)\n\n  @property\n  def mi(self):\n    return 0.577\n\n\nrng = np.random.default_rng(42)\ndist = UniformMargin()\n\nfig = visualise_dist(rng, dist)\nfig.tight_layout()\n\n\n/tmp/ipykernel_95941/2727834072.py:14: RuntimeWarning: divide by zero encountered in log\n  return -np.log(y)"
  },
  {
    "objectID": "posts/determinant-multilinear.html",
    "href": "posts/determinant-multilinear.html",
    "title": "Matrix determinant from the exterior algebra viewpoint",
    "section": "",
    "text": "In every linear algebra course matrix determinant is a must. Often it is introduced in the following form:\nThe definition above has a lot of advantages, but it also has an important drawback — the “why” of this construction is hidden and appears only later in a long list of its properties.\nWe’ll take an alternative viewpoint, which I have learned from Darling (1994, chap. 1), and is based around the exterior algebra."
  },
  {
    "objectID": "posts/determinant-multilinear.html#motivational-examples",
    "href": "posts/determinant-multilinear.html#motivational-examples",
    "title": "Matrix determinant from the exterior algebra viewpoint",
    "section": "Motivational examples",
    "text": "Motivational examples\nConsider \\(V=\\mathbb R^3\\). For vectors \\(v\\) and \\(w\\) we can define their vector product \\(v\\times w\\) with the following properties:\n\nBilinearity: \\((\\lambda v+v')\\times w = \\lambda (v\\times w) + v'\\times w\\) and \\(v\\times (\\lambda w+w') = \\lambda (v\\times w) + v\\times w'\\).\nAntisymmetry: \\(v\\times w = -w\\times v\\).\n\nGeometrically we can think of it as of a signed area of the parallelepiped spanned by \\(v\\) and \\(w\\).\nFor three vectors \\(v, w, u\\) we can form signed volume: \\[\\langle v, w, u\\rangle = v\\cdot (w\\times u),\\] which has similar properties:\n\nTrilinearity: \\(\\langle \\lambda v+v', w, u \\rangle = \\lambda \\langle v, w, u \\rangle + \\langle v', w, u\\rangle\\) (and similarly in \\(w\\) and \\(u\\) arguments).\nAntisymmetry: when we swap any two arguments the sign changes, e.g., \\(\\langle v, w, u\\rangle = -\\langle w, v, u\\rangle = \\langle w, u, v\\rangle = -\\langle u, w, v\\rangle\\).\n\nExterior algebra will be a generalisation of the above construction beyond the three-dimensional space \\(V=\\mathbb R^3\\)."
  },
  {
    "objectID": "posts/determinant-multilinear.html#exterior-algebra",
    "href": "posts/determinant-multilinear.html#exterior-algebra",
    "title": "Matrix determinant from the exterior algebra viewpoint",
    "section": "Exterior algebra",
    "text": "Exterior algebra\nLet’s start with the natural definition:\n\nDefinition 2 (Antisymmetric multilinear function) Let \\(V\\) and \\(U\\) be vector spaces and \\(f\\colon V\\times V \\times \\cdots \\times V \\to U\\) be a function. We will say that it is multilinear if for all \\(i = 1, 2, \\dotsc, n\\) it holds that \\[\nf(v_1, v_2, \\dotsc, \\lambda v_i + v_i', v_{i+1}, \\dotsc, v_n) = \\lambda f(v_1, \\dotsc, v_i, \\dotsc, v_n) + f(v_1, \\dotsc, v_i', \\dotsc, v_n).\n\\] We will say that it is antisymmetric if it changes the sign whenever we swap any two arguments: \\[\nf(v_1, \\dotsc, v_i, \\dotsc, v_j, \\dotsc, v_n) = -f(v_1, \\dotsc, v_j, \\dotsc, v_i, \\dotsc, v_n).\n\\]\n\nAs we have seen above both \\((v, w)\\mapsto v\\times w\\) and \\((v, w, u)\\mapsto v\\cdot (w\\times u)\\) are antisymmetric multilinear functions.\nNote that for every \\(\\sigma\\in S_n\\) it holds that \\[\nf(v_1, \\dotsc, v_n) = \\mathrm{sgn}\\,\\sigma \\, f(v_{\\sigma(1)}, \\dotsc, v_{\\sigma(n)})\n\\] as \\(\\mathrm{sgn}\\,\\sigma\\) counts transpositions modulo 2.\n\nExercise 1 Let \\(f\\colon V\\times V\\to U\\) be multilinear. Show that the following are equivalent:\n\n\\(f\\) is antisymmetric, i.e., \\(f(v, w) = -f(w, v)\\) for every \\(v, w \\in V\\).\n\\(f\\) is alternating, i.e., \\(f(v, v) = 0\\) for every \\(v\\in V\\).\n\nGeneralise to multilinear mappings \\(f\\colon V\\times V \\times \\cdots\\times V\\to U\\).\n\n\n\n\n\n\nHint\n\n\n\n\n\nExpand \\(f(v+w, v+w)\\) using multilinearity.\n\n\n\n\nNow we are ready to construct (a particular) exterior algebra.\n\nDefinition 3 (Second exterior power) Let \\(V\\) be a vector space. Its second exterior power \\(\\bigwedge^2 V\\) we be the vector space of expressions \\[\n\\lambda_1 v_1\\wedge w_1 + \\cdots + \\lambda_n v_n\\wedge w_n\n\\] with the following rules:\n\nThe wedge \\(\\wedge\\) operator is bilinear, i.e., \\((\\lambda v+v')\\wedge w = \\lambda v\\wedge w + v'\\wedge w\\) and \\(v\\wedge (\\lambda w+w') = \\lambda v\\wedge w + v\\wedge w'\\).\n\\(\\wedge\\) is antisymmetric, i.e., \\(v\\wedge w = -w\\wedge v\\) (or, equivalently, \\(v\\wedge v=0\\)).\nIf \\(e_1, \\dotsc, e_n\\) is a basis of \\(V\\), then \\[\\begin{align*}\n    &e_1\\wedge e_2, e_1\\wedge e_3, \\dotsc, e_1\\wedge e_n, \\\\\n    &e_2\\wedge e_3, \\dotsc, e_2\\wedge e_n\\\\\n    &\\qquad\\vdots\\\\\n    &e_{n-1}\\wedge e_n\n    \\end{align*}\n    \\] is a basis of \\(\\bigwedge^2 V\\).\n\n\nNote that \\(v\\wedge w\\) has the interpretation of a signed area of the parallelepiped spanned by \\(v\\) and \\(w\\). Such parallelepipeds can be formally added and there is a resemblance between the wedge product and the vector product in \\(\\mathbb R^3\\).\nWe just need to prove that such a space actually exists (this construction can be skipped at the first reading): similarly to the tensor space, build the free vector space on the set \\(V\\times V\\). Now quotient it by expressions like \\((v, v)\\), \\((\\lambda v, w) - (v, \\lambda w)\\), \\((v+v', w) - (v, w) - (v', w)\\) and \\((v, w+w') - (v, w) - (v, w')\\).\nThen define \\(v\\wedge w\\) to be the equivalence class \\([(v, w)]\\).\n\n\n\n\n\n\nNote\n\n\n\n\n\nIf we had introduced the determinant by other means, we could construct the exterior algebra \\(\\bigwedge^k V\\) also as the space of antisymmetric multilinear functions \\(V^*\\times V^*\\to \\mathbb R\\) (where \\(V^*\\) is the dual space) by\n\\[\n(v\\wedge w)(\\alpha, \\beta) := \\det \\begin{pmatrix}  \\alpha(v_1) & \\alpha(v_2) \\\\ \\beta(v_1) & \\beta(v_2) \\end{pmatrix}\n\\]\n\n\n\nAnalogously we can construct:\n\nDefinition 4 (Exterior power) Let \\(V\\) be a vector space. We define \\(\\bigwedge^0 V = \\mathbb R\\), \\(\\bigwedge^1 V = V\\) and for \\(k\\ge 2\\) its \\(k\\)th exterior power \\(\\bigwedge^k V\\) as the vector space of expressions \\[\n\\lambda_1 a_1\\wedge a_2\\wedge \\cdots\\wedge a_k + \\cdots + \\lambda_n v_1\\wedge v_2 \\wedge \\cdots\\wedge v_k\n\\] such that the wedge operator \\(\\wedge\\) is multilinear and antisymmetric (alternating) and that if \\(e_1, \\dotsc, e_n\\) is a basis of \\(V\\), then the set \\[\n\\{ e_{i_1}\\wedge e_{i_2}\\wedge \\cdots \\wedge e_{i_k}\\mid i_1 &lt; i_2 &lt; \\cdots &lt; i_k \\}\n\\]\nis a basis of \\(\\bigwedge^k V\\).\n\n\nExercise 2 Show that if \\(\\dim V = n\\), then \\(\\dim \\bigwedge^k V = \\binom{n}{k}\\). (And that in particular for \\(k &gt; n\\) we have \\(\\bigwedge^k V = 0\\), the trivial vector space).\n\nThe introduced space can be used to convert between antisymmetric multilinear and linear functions by the means of the universal property:\n\nTheorem 1 (Universal property) Let \\(f\\colon V\\times V \\cdots\\times V\\to U\\) be an antisymmetric multilinear function. Then, there exists a unique linear mapping \\(\\tilde f\\colon \\bigwedge^k V\\to U\\) such that for every set of vectors \\(v_1, \\dotsc, v_k\\) \\[\nf(v_1, \\dotsc, v_k) = \\tilde f(v_1\\wedge \\dotsc \\wedge v_k).\n\\]\n\n\nProof. (Can be skipped at the first reading.)\nAs \\(f\\) is multlilinear, its values are determined by the values on the tuples \\((e_{i_1}, \\dotsc, e_{i_k})\\), where \\(\\{e_1, \\dotsc, e_n\\}\\) is a basis of \\(V\\).\nWe can use antisymmetry to show that by “sorting out” the elements such that \\(i_1 \\le i_2\\cdots \\le i_k\\) and defining \\(\\tilde f(e_{i_1} \\wedge \\dotsc, \\wedge e_{i_k}) = f(e_{i_1}, \\dotsc, e_{i_k})\\) we obtain a well-defined mapping. Linearity is easy to proof.\nNow the uniqueness is proven by observing that antisymmetry and multilinearity uniquely prescribe the values at the basis elements of \\(\\bigwedge^k V\\).\n\nIts importance is the following: to show that a linear map \\(\\bigwedge^k V\\to U\\) is well-defined, one can construct a multilinear antisymmetric map \\(V\\times V\\times \\cdots \\times V\\to U\\)."
  },
  {
    "objectID": "posts/determinant-multilinear.html#determinants",
    "href": "posts/determinant-multilinear.html#determinants",
    "title": "Matrix determinant from the exterior algebra viewpoint",
    "section": "Determinants",
    "text": "Determinants\nFinally, we can define the determinant. Note that if \\(\\dim V = n\\), then \\(\\dim \\bigwedge^n V = 1\\).\n\nDefinition 5 (Determinant) Let \\(n=\\dim V\\) and \\(A\\colon V\\to V\\) be a linear mapping. We consider the mapping \\[\n(v_1, \\dotsc, v_n) \\mapsto (Av_1) \\wedge \\cdots \\wedge (Av_n).\n\\]\nAs it is antisymmetric and multilinear, we know that it induces a unique linear mapping \\(\\bigwedge^n V\\to \\bigwedge^n V\\).\nBecause \\(\\bigwedge^n V\\) is one-dimensional, this mapping must be multiplication by a number. Namely, we define the determinant \\(\\det A\\) to be the number such that for every set of vectors \\(v_1, \\dotsc, v_n\\) \\[\nAv_1 \\wedge \\cdots \\wedge Av_n = \\det A\\, (v_1\\wedge \\cdots \\wedge v_n).\n\\]\n\nIn other words, determinant measures the volume stretch of the parallelepiped spanned by the vectors after they are transformed by the mapping.\nI like this geometric intuition, especially that it is clear that determinant depends only on the linear map, rather than a particular matrix representation — it is independent on the chosen basis.\nWe can now show a number of lemmata.\n\nProposition 1 If \\(\\mathrm{id}_V\\colon V\\to V\\) is the identity mapping, then \\(\\det \\mathrm{id}_V = 1\\).\n\n\nProof. Obvious from the definition! Similarly, it’s clear that \\(\\det \\left(\\lambda\\cdot \\mathrm{id}_V\\right) = \\lambda^{\\dim V}\\).\n\n\nProposition 2 For every two mappings \\(A, B\\colon V\\to V\\) it holds that \\(\\det (B\\circ A) = \\det B\\cdot \\det A\\).\n\n\nProof. For every set of vectors we have \\[\n\\begin{align*}\n\\det (B\\circ A) \\, v_1\\wedge \\cdots \\wedge v_n &= (BAv_1) \\wedge \\cdots \\wedge (BAv_n) \\\\\n&= B(Av_1) \\wedge \\cdots \\wedge B(Av_n) \\\\\n&= \\det B \\, (Av_1) \\wedge \\cdots \\wedge (Av_n) \\\\\n&= \\det B\\cdot \\det A\\, v_1\\wedge \\cdots\\wedge v_n.\n\\end{align*}\n\\]\n\n\nProposition 3 (Only invertible matrices have non-zero determinants) A mapping is an isomorphism if and only if it has non-zero determinant.\n\n\nProof. If the mapping is invertible, then \\(A\\circ A^{-1} = \\mathrm{id}\\) and we have \\(\\det A \\cdot \\det A^{-1} = 1\\), so its determinant must be non-zero.\nNow assume that the mapping is non-invertible. This means that there exists a non-zero vector \\(k\\in \\ker A\\) such that \\(Ak=0\\). Let’s complete \\(k\\) to a basis \\(k, e_1, \\dotsc, e_{n-1}\\). Then \\[\n\\det A\\, k\\wedge e_1\\wedge \\cdots\\wedge e_{n-1} = (Ak) \\wedge \\cdots \\wedge (Ae_{n-1}) = 0,\n\\] which means that \\(\\det A=0\\) as \\(\\{k\\wedge e_1\\wedge \\dotsc \\wedge e_{n-1}\\}\\) is a basis of \\(\\bigwedge^n V\\).\n\nLet’s now connect the usual definition of the determinant to the one coming from exterior algebra:\n\nProposition 4 (Recovering the standard expression) Let \\(e_1, \\dotsc, e_n\\) be a basis of \\(V\\) and \\((A^{i}_j)\\) be the matrix of coordinates, i.e., \\[\nAe_k = \\sum_i A^{i}_k e_i.\n\\] Then the determinant \\(\\det A\\) can be calculated as \\[\n\\det A = \\sum_{\\sigma\\in S_n} \\mathrm{sgn}\\,\\sigma \\, A^{\\sigma(1)}_1 A^{\\sigma(2)}_2 \\dotsc A^{\\sigma(n)}_n.\n\\]\n\n\nProof. Observe that \\[\\begin{align*}\n\\det A e_1 \\wedge \\cdots \\wedge e_n &= Ae_1 \\wedge \\cdots \\wedge Ae_n\\\\\n&= \\left( \\sum_{i_1} A^{i_1}_1 e_{i_1} \\right) \\wedge \\cdots \\wedge \\left( \\sum_{i_n} A^{i_n}_n e_{i_n} \\right)\\\\\n&= \\sum_{i_1, \\dotsc, i_n} A^{i_1}A^{i_2}\\cdots A^{i_n} \\, e_{i_1} \\wedge \\cdots \\wedge e_{i_n}.\n\\end{align*}\\]\nNow we see that repeated indices give zero contribution to this sum, so we can only consider the indices which are permutations of \\(1, 2, \\dotsc, n\\). We also see that \\(e_{i_1} \\wedge \\cdots \\wedge e_{i_n}\\) can be then written as \\(\\pm 1\\, e_1\\wedge \\dotsc \\wedge e_n\\), where the sign is the number of required transpositions, that is the sign of the permutation. This ends the proof.\n\nGoing just a bit further into exterior algebra we can also show that matrix transposition does not change the determinant.\nTo represent matrix transposition, we will use the dual mapping: if \\(A\\colon V\\to V\\) there is the dual mapping \\(A^*\\colon V^*\\to V^*\\), given as \\[\n  (A^*\\omega)(v) := \\omega(Av).\n\\]\nWe can therefore build the \\(n\\)th exterior power of \\(V^*\\): \\(\\bigwedge^n (V^*)\\) and consider the determinant \\(\\det A^*\\).\nWe will formally show that\n\nProposition 5 (Determinant of the transpose) Let \\(A\\colon V\\to V\\) be a linear map and \\(A^*\\colon V^*\\to V^*\\) be its dual. Then \\[\n\\det A^* = \\det A.\n\\]\n\n\nProof. To do this we will need an isomorphism \\[\n\\iota \\colon {\\bigwedge}^n (V^*) \\to \\left({\\bigwedge}^n V\\right)^*\n\\] given on basis elements by \\[\n\\iota( \\omega^1 \\wedge \\cdots \\wedge \\omega^n ) (v_1\\wedge \\cdots \\wedge v_n) = \\det \\big(\\omega^i(v_j) \\big)_{i, j = 1, \\cdots, n},\n\\] where on the right side we use any already known formula for the determinant. It is easy to show that this mapping is well-defined and linear, as it descends from a multilinear alternating mapping.\nHaving this, the proof becomes straightforward calculation: \\[\n\\begin{align*}\n  \\det A^* \\iota\\left(  \\omega^1\\wedge \\cdots\\wedge \\omega^n  \\right)(v_1\\wedge \\cdots\\wedge v_n ) &=\n  \\iota\\bigg( \\det A^* \\, \\omega^1\\wedge \\cdots\\wedge \\omega^n  \\bigg)(v_1\\wedge \\cdots\\wedge v_n ) \\\\\n  &=\\iota\\bigg( A^*\\omega^1 \\wedge \\cdots\\wedge A^*\\omega^n  \\bigg )(v_1\\wedge \\cdots\\wedge v_n) \\\\\n  &= \\det \\bigg((A^*\\omega^i)(v_j)\\bigg) = \\det \\bigg( \\omega^i(Av_j ) \\bigg) \\\\\n  &= \\iota\\left(\\omega^1\\wedge \\cdots\\wedge \\omega^n \\right)(Av_1\\wedge\\cdots\\wedge Av_n) \\\\\n  &= \\iota\\left(\\omega^1\\wedge \\cdots\\wedge \\omega^n \\right)(\\det A\\, v_1\\wedge\\cdots\\wedge v_n) \\\\\n  &= \\det A~ \\iota\\left(\\omega^1\\wedge \\cdots\\wedge \\omega^n\\right)(v_1\\wedge\\cdots\\wedge v_n)\n\\end{align*}\n\\]\n\nEstablishing such isomorphisms is quite a nice technique, which also can be used to prove\n\nProposition 6 (Determinant of a block-diagonal matrix) Let \\(A\\colon V\\to V\\) and \\(B\\colon W\\to W\\) be two linear mappings and \\(A\\oplus B\\colon V\\oplus W\\to V\\oplus W\\) be the mapping given by \\[\n(A\\oplus B)(v, w) = (Av, Bw).\n\\]\nThen \\(\\det (A\\oplus B) = \\det A\\cdot \\det B\\).\n\n\nProof. We will use this approach: there exists an isomorphism \\[\n{\\bigwedge}^p (V\\oplus W) \\simeq \\bigoplus_k {\\bigwedge}^k V \\otimes {\\bigwedge}^{p-k} W,\n\\] so if we take \\(n=\\dim V\\) and \\(m=\\dim W\\) and note that \\(\\bigwedge^{p} V = 0\\) for \\(p &gt; n\\) (and similarly for \\(W\\)) we have \\[\n\\iota\\colon {\\bigwedge}^{n+m} (V\\oplus W) \\simeq {\\bigwedge}^n V\\otimes {\\bigwedge}^m W.\n\\] If \\(i\\colon V\\to V\\oplus W\\) and \\(j\\colon W\\to V\\oplus W\\) are the two “canonical” inclusions, this isomorphism is given as \\[\n\\iota\\big( iv_1 \\wedge \\cdots \\wedge iv_n\\wedge jw_1 \\wedge \\cdots \\wedge jw_m \\big) = (v_1\\wedge \\cdots\\wedge v_n) \\otimes (w_1\\wedge\\cdots\\wedge w_m).\n\\] Now we calculate: \\[\\begin{align*}\n(A\\oplus B)( iv_1 \\wedge \\cdots \\wedge iv_n\\wedge jw_1 \\wedge \\cdots \\wedge jw_m ) &=\niAv_1 \\wedge \\cdots \\wedge iAv_n \\wedge jBw_1\\wedge\\cdots\\wedge jBw_m \\\\\n&= \\iota^{-1}\\big( Av_1\\wedge \\cdots\\wedge Av_n \\otimes Bw_1\\wedge\\cdots\\wedge Bw_m   \\big) \\\\\n&= \\iota^{-1}\\big(\\det A\\cdot \\det B\\, v_1\\wedge \\cdots \\wedge v_n \\otimes w_1\\wedge\\cdots\\wedge w_m) \\\\\n&= \\det A\\cdot \\det B \\, \\iota^{-1}\\big( v_1\\wedge \\cdots \\wedge v_n \\otimes w_1\\wedge\\cdots\\wedge w_m \\big)\\\\\n&= \\det A\\cdot \\det B \\, iv_1\\wedge \\cdots \\wedge iv_n \\wedge jw_1\\wedge\\cdots\\wedge jw_m.\n\\end{align*}\n\\]\n\n\nProposition 7 (Determinant of an upper-triangular matrix) Let \\(A\\colon V\\to V\\) be a linear mapping and \\(e_1, \\dotsc, e_n\\) be a basis of \\(V\\) such that matrix \\((A^i_j)\\) is upper-triangular, that is \\[\n\\begin{align*}\n  Ae_1 &= A^1_1 e_1\\\\\n  Ae_2 &= A^1_2 e_1 + A^2_2 e_2\\\\\n  &\\vdots\\\\\n  Ae_n &= A^1_n e_1 + A^2_ne_2 + \\dotsc + A^n_n e_n\n\\end{align*}\n\\] Then \\[\n\\det A = \\prod_{i=1}^n A^i_i.\n\\]\n\nOnce proven, this result can also be used for lower-triangular matrices due to Proposition 5.\n\nProof. Recall that whenever there is \\(i_j=i_k\\), then \\(e_{i_1}\\wedge \\cdots\\wedge e_{i_n} = 0\\). Hence, there is only one term that may be non-zero: \\[\nAe_1\\wedge Ae_2 \\wedge \\cdots \\wedge Ae_n = A^1_1 e_1 \\wedge \\cdots \\wedge A^n_n e_n = \\prod_{i=1}^n A^i_i\\, e_1\\wedge \\cdots\\wedge e_n.\n\\]"
  },
  {
    "objectID": "posts/determinant-multilinear.html#acknowledgements",
    "href": "posts/determinant-multilinear.html#acknowledgements",
    "title": "Matrix determinant from the exterior algebra viewpoint",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nI would like to thank Adam Klukowski for helpful editing suggestions."
  },
  {
    "objectID": "posts/dirichlet-process.html",
    "href": "posts/dirichlet-process.html",
    "title": "The Dirichlet process",
    "section": "",
    "text": "In this post we will quickly review different constructions of the Dirichlet process, following Teh et al. (2006) and Gelman et al. (2013, chap. 23)."
  },
  {
    "objectID": "posts/dirichlet-process.html#finite-dimensional-dirichlet-prior",
    "href": "posts/dirichlet-process.html#finite-dimensional-dirichlet-prior",
    "title": "The Dirichlet process",
    "section": "Finite-dimensional Dirichlet prior",
    "text": "Finite-dimensional Dirichlet prior\nConsider the simplest Gaussian mixture model: there are several normal distributions with unit variance \\(\\mathcal N(\\mu_k, 1)\\) for \\(k\\in \\{1, \\dotsc, K\\}\\) and mixture proportions vector \\(\\pi = (\\pi_1, \\dotsc, \\pi_K)\\) with \\(\\pi_k\\ge 0\\) and \\(\\sum_k \\pi_k=1\\).\nA convenient prior for \\(\\pi\\) is the Dirichlet distribution. We put some \\(F_0\\) prior on the parameters \\(\\{\\mu_k\\}\\) of the model, so the generative process looks like: \\[\\begin{align*}\n  \\pi \\mid \\alpha &\\sim \\mathrm{Dirichlet}(\\alpha_1, \\dotsc, \\alpha_K)\\\\\n  \\mu_k \\mid F_0 &\\sim F_0, & k=1, \\dotsc, K\\\\\n  Z_n \\mid \\pi &\\sim \\mathrm{Categorical}(\\pi_1, \\dotsc, \\pi_K), & n=1, \\dotsc, N\\\\\n  X_n\\mid Z_n=z_n, \\{\\mu_k\\} &\\sim \\mathcal N(\\mu_{z_n}, 1),\\quad & n=1, \\dotsc, N.\n\\end{align*}\\]\n\nAnother point of view\nRather than using individual random variables \\(Z_n\\) and a shared set of parameters \\(\\{\\mu_k\\}\\) we could reparametrize the model to use individual means \\(\\tilde \\mu_n = \\mu_{Z_n}\\). In other words, we could consider a probability measure with atoms ${_k}$ given by \\[F = \\sum_{k=1}^K \\pi_k \\delta_{\\mu_k}.\\]\nIf we only know the Dirichlet weights vector \\((\\alpha_1, \\dotsc, \\alpha_K)\\) and the base measure \\(F_0\\) we can think of \\(F\\) as of a random probability measure generated according to \\[\\begin{align*}\n  \\pi \\mid \\alpha &\\sim \\mathrm{Dirichlet}(\\alpha_1, \\dotsc, \\alpha_K)\\\\\n  \\mu_k &\\sim F_0, \\quad k = 1, \\dotsc, K\\\\\n  F &:= \\sum_{k=1}^K \\pi_k \\delta_{\\mu_k}.\n\\end{align*}\\]\nThen sampling individual data points amounts to the following model with \\(n=1, \\dotsc, N\\): \\[\\begin{align*}\n  F\\mid \\alpha, F_0 &\\sim \\text{the procedure above}\\\\\n  \\theta_n \\mid F &\\sim F, \\\\\n  X_n\\mid \\theta_n &\\sim \\mathcal N(\\theta_n, 1).\n\\end{align*}\\]\nNote that the values of \\(\\theta_n\\) come from the set \\(\\{\\mu_1, \\dotsc, \\mu_K\\}\\) as \\(F\\) is atomic."
  },
  {
    "objectID": "posts/dirichlet-process.html#dirichlet-process-prior",
    "href": "posts/dirichlet-process.html#dirichlet-process-prior",
    "title": "The Dirichlet process",
    "section": "Dirichlet process prior",
    "text": "Dirichlet process prior\n\nStick-breaking construction\nWith the following example in mind we will pass now to a general distribution \\(F_0\\) defined over some infinite space \\(\\mathcal M\\) (which can be \\(\\mathbb R\\) as above) and a single positive parameter \\(\\alpha &gt; 0\\).\nWe will generate a random measure \\(F\\) from \\(F_0\\) using the construction known as the Dirichlet process.\nSample for \\(k=1, 2, \\dotsc\\) \\[\\begin{align*}\nv_k \\mid \\alpha &\\sim \\mathrm{Beta}(1, \\alpha)\\\\\n\\mu_k \\mid F_0 &\\sim F_0\n\\end{align*}\\] and define \\[\\begin{align*}\n  p_1 &= v_1\\\\\n  p_k &= v_k \\prod_{i=1}^{k-1} (1-v_k) \\quad \\text{for } k\\ge 2,\\\\\n  F &= \\sum_{k=1}^\\infty p_k \\delta_{\\mu_k}\n\\end{align*}\\]\nWith probability 1 it holds that \\[\\sum_{k=1}^\\infty  p_k = 1,\\] i.e., \\((p_k)\\) is a valid proportions vector.\nWe say that the distribution \\(F\\) was drawn from the Dirichlet process: \\[F \\sim \\mathrm{DP}(\\alpha, F_0).\\]\n\n\nInfinite limit\nThe atomic distributions generated with finite-dimensional proportions \\((\\pi_k)_{k=1, 2, \\dotsc, K}\\) and infinite sequence of weights \\((p_k)_{k=1, 2, \\dotsc, \\infty}\\) look optically similar. There is a close relation between these two generative processes.\nConsider a random measure \\(F^K\\) defined using a symmetric Dirichlet distribution: \\[\\begin{align*}\n  \\pi^K \\mid \\alpha &\\sim \\mathrm{Dirichlet}(\\alpha/K, \\cdots, \\alpha/K)\\\\\n  \\mu^K_k \\mid F_0 &\\sim F_0\\\\\n  F^K &= \\sum_{k=1}^K \\pi^K_k\\delta_{\\mu^K_k}\n\\end{align*}\\]\nNow if \\(F^{\\infty} \\sim \\mathrm{DP}(\\alpha, F_0)\\) and \\(u\\) is any measurable function integrable with respect to \\(F_0\\), then the sequence of random variables \\[ \\int_{\\mathcal M} u\\, \\mathrm{d} F^{K} \\] converges in distribution (that is, weakly) to \\[ \\int_{\\mathcal M} u\\, \\mathrm{d} F^{\\infty}.\\]\n\nWhere the difference really is\nWe see that \\((p_k)\\) looks deceptively similar as \\((\\pi_k^K)\\) for large \\(K\\). There are some differences, though. First of all, \\((p_k)\\) is infinite and the number of atoms appearing in the analysis of a particular data set is implicitly controlled by the number of data points. If \\(F_0\\) is non-atomic,one can expect \\(O(\\alpha\\log N)\\) atoms in a data set with \\(N\\) points. In the finite-dimensional case more than \\(K\\) clusters are impossible.\nHowever, for \\(K\\gg N\\) it’s natural to expect that several entries from \\((\\pi^K_k)\\) should be matching several entries of \\((p_k)\\). However, the intuition that \\(p_1 = \\pi_1^K\\), \\(p_2 = \\pi_2^K\\), … is wrong. In the stick-breaking construction of the Dirichlet process we expect the first few entries to have the most of the mass, while in the finite-dimensional case the Dirichlet prior is symmetric — we don’t know which weights \\(\\pi_k^K\\) will have vanishing mass.\nAlthough it seems obvious I spent quite some time trying to understand why the stick-breaking sampling procedure from the Dirichlet distribution gives different results!\nThe stick-breaking sampling procedure for the \\(\\mathrm{Dirichlet}(\\alpha/K, \\dotsc, \\alpha/K)\\) distribution works as follows: \\[\\begin{align*}\n  u_k &\\sim \\mathrm{Beta}( \\alpha/K, \\alpha\\cdot (1-k/K) )\\\\\n  \\pi_1 &= u_1\\\\\n  \\pi_k &= u_k \\prod_{j &lt; k} (1-u_k), \\quad k = 2, \\dotsc, K-1\\\\\n  \\pi_K &= 1 - (\\pi_1 + \\dotsc + \\pi_{K-1})\n\\end{align*}\\]\nwhich for \\(k \\ll K\\) corresponds to sampling from (approximately) \\(\\mathrm{Beta}(\\alpha/K, \\alpha)\\), rather than \\(\\mathrm{Beta}(1, \\alpha)\\).\nPitman (1996) describes size-biased permutations, which perhaps can be used to establish link between \\((\\pi_k)\\) for large \\(K\\) and \\((p_k)\\), but I haven’t understood it yet.\n\n\n\nDefining property\nWe have seen in what sense the Dirichlet process prior can be thought as of an infinite-dimensional generalization of the Dirichlet prior. However, there is another link.\nRecall that the defining property of a Gaussian process is that it is a continuous-time stochastic process \\(\\{X_t\\}_{t\\in [0, 1]}\\) such that for every finite set of indices \\(t_1, t_2, \\dotsc, t_m\\) random vector \\((X_{t_1}, \\dotsc, X_{t_m})\\) is distributed according to multivariate normal distribution. (In particular every \\(X_t\\) is a normal random variable). While this defining property is not sufficient without a proof of existence (e.g., an explicit construction), it is useful in many calculations involving them.\nWe will now give the defining property of the Dirichlet process. Take a probability measure \\(F_0\\) over \\(\\mathcal M\\) and the concentration parameter \\(\\alpha &gt; 0\\). We say that \\(\\mathrm{DP}(\\alpha, F_0)\\) is a Dirichlet process if every sample \\(F\\sim \\mathrm{DP}(\\alpha, F_0)\\) is a probability measure over \\(\\mathcal M\\) such that for every partition \\(A_1, \\cdots, A_m\\) of \\(\\mathcal M\\) the following holds: \\[ \\left( F(A_1), \\dotsc, F(A_K) \\right) \\sim \\mathrm{Dirichlet}\\big(\\alpha F_0(A_1), \\dotsc, \\alpha F_0(A_K) \\big) \\]\nIn particular if \\(A\\subseteq \\mathcal X\\) is any measurable subset, then we can use the partition \\(\\{A, \\mathcal M\\setminus A\\}\\) to get \\[ F(A) \\sim \\mathrm{Beta}\\big( \\alpha F_0(A), \\alpha(1-F_0(A)) \\big),\\] so that \\[\\mathbb E[ F(A) ] = F_0(A)\\] and \\[\\mathrm{Var}[F(A)] = \\frac{ F_0(A)\\cdot (1-F_0(A)) }{1+\\alpha}\\]\nHence, each draw \\(F\\) is centered around \\(F_0\\) and the variance is small for large parameter values \\(\\alpha\\).\n\n\nPólya urn scheme\nFinally, we give an interpretation in terms of Pólya urn scheme.\nAbove we considered the sampling process from the finite-dimensional Dirichlet distribution: \\[\\begin{align*}\n  F\\mid \\alpha, F_0 &\\sim \\text{construct atomic measure},\\\\\n  \\theta_n \\mid F &\\sim F,\n\\end{align*}\\] where each of the \\(\\theta_n\\) was actually some atom of the distribution \\(\\mu_k\\).\nThis interpretation is also easy to understand when the atomic measure \\(F\\) is drawn from the Dirichlet process using the stick-breaking construction.\nConsider now a sampling procedure of \\(\\theta_n\\) where we do not have direct access to \\(F\\), but only to the distribution \\(F_0\\), concentration parameter \\(\\alpha &gt; 0\\) and previous draws \\(\\theta_1, \\dotsc, \\theta_{n-1}\\). It holds that \\[\\theta_n \\mid \\alpha, F_0, \\theta_1, \\dotsc, \\theta_{n-1} \\sim \\frac{\\alpha}{ (n-1) + \\alpha }F_0 + \\sum_{u=1}^{n-1} \\frac{1}{(n-1)+\\alpha}\\delta_{ \\theta_n }.\\]\nIf \\(\\alpha\\) is a positive integer we can interpret this sampling procedure as follows: we want to draw the \\(n\\)th ball and we have an urn with \\(\\alpha\\) transparent balls and \\(n-1\\) balls of different colors. We draw a random ball. If it is transparent, we use \\(G_0\\) to sample a colored ball from \\(F_0\\), note it down, and put it to the urn.\nThis also suggests a clustering property: if there is a color \\(\\mu_k\\) such that there are already \\(m_k\\) balls inside the urn (i.e., \\(m_k\\) is the number of indices \\(1\\le i\\le n-1\\) such that \\(\\theta_i = \\mu_k\\)), then we have a larger chance to draw a ball of this color: \\[\\theta_n \\mid \\alpha, F_0, \\theta_1, \\dotsc, \\theta_{n-1} \\sim \\frac{\\alpha}{(n-1) + \\alpha}F_0 + \\sum_{k} \\frac{m_k}{ (n-1) + \\alpha } \\delta_{ \\mu_k }.\\]\nWe also see that for the concentration parameter \\(\\alpha \\gg n\\) this sampling procedure approximates independent sampling from \\(F_0\\).\n\nAsymptotic number of clusters\nThe above formulation can be used to argue why the number of clusters grows as \\(O(\\alpha\\log n)\\) if \\(F_0\\) is non-atomic. Define \\(D_1 = 1\\) and for \\(n\\ge 1\\) \\[D_n = \\begin{cases} 1 &\\text{ if } \\theta_n \\notin \\{\\theta_1, \\dotsc, \\theta_{n-1}\\}\\\\\n0 &\\text{otherwise}\\end{cases}\\] From the above construction we know the probability of drawing a new atom, so \\[\\mathbb E[D_n] = \\alpha / (\\alpha + n-1)\\] The number of distinct atoms in \\(F\\) is then \\[\\mathbb E[C_n] = \\mathbb E[D_1 + \\dotsc + D_n] = \\alpha \\sum_{i=1}^n \\frac{1}{\\alpha + n - 1}.\\] We recognise that this sum is similar to the harmonic series and (this can be proven formally) also grows as \\(O(\\log n)\\), so that \\(\\mathbb E[C_n] = O(\\alpha\\log n)\\). To provide a more precise result: \\[\\lim_{n\\to\\infty}\\frac{ \\mathbb E[C_n] }{\\alpha \\log n} = 1.\\] For this and related results consult these notes.\n\n\n\nChinese restaurant process\nThe procedure above is also closely related to the Chinese restaurant process, where the metaphor is that there are \\(K\\) occupied tables (where a dish \\(\\mu_k\\) is served) and there are \\(m_k\\) people sitting around the \\(k\\)th table. When a new customer enters the restaurant, they can either join an existing table (with probability proportional to \\(m_k\\)) or start a new (\\((K+1)\\)th) table with probability proportional to \\(\\alpha\\), where a new dish \\(\\mu_{K+1}\\sim F_0\\) will be served."
  },
  {
    "objectID": "posts/dirichlet-process.html#afterword",
    "href": "posts/dirichlet-process.html#afterword",
    "title": "The Dirichlet process",
    "section": "Afterword",
    "text": "Afterword\nThe Dirichlet process is a useful construction, which can be used as a nonparametric prior in clustering problems — instead of specifying a fixed number of clusters one can specify the growth rate via \\(\\alpha\\).\nIn practice the results (including the number of inferred clusters in a particular data set) need to be treated with caution: the clusters found in the data set do not need to have the “real world” meaning (or perhaps “clusters” are a wrong abstration at all, with heterogeneity attributable e.g., to some continuous covariates which could be measured). Careful validation is often needed, epsecially that these models may be non-robust to misspecification (see this paper on coarsening) or the inferences may be hard to do (see this overview of their intrinsic non-identifiability).\nAnyway, although difficult, clustering can provide useful information about a given problem, so we often need do it. For example, take a look at this application of Chinese restaurant process to the clustering of single-cell DNA profiles."
  },
  {
    "objectID": "posts/no-free-lunch-in-research.html",
    "href": "posts/no-free-lunch-in-research.html",
    "title": "The no free lunch theorem for scientific research",
    "section": "",
    "text": "Yesterday David and I went for pizza after work. As typical for our conversations, we spent quite some time discussing applied statistics and machine learning, and reached our usual conclusion that logistic regression is a wonderful model in so many problems.\nHowever, finding logistic regression or other “simple” methods in research papers can be quite hard, as we tend to look for methodological novelty. As Kristin Lennox nicely summarized, “you don’t get a lot of points for doing really good statistics on really important problems, if these statistics were invented in 1950s”. (In particular, Cynthia Rudin investigated how much applied research goes into the most presitigious machine learning conferences).\nThis is one of the reasons for the phenomenon which everybody in the field knows too well: from time to time you take a paper claiming state-of-the-art performance (“They are 0.02% better on CIFAR-10 than others! Let’s apply it to my problem”), and then finding out that the method requires heavy hyperparameter tuning and hours of playing with the brittleness that makes the method impossible to use in practice. And, what’s even worse, the performance isn’t that different from a simple baseline.\nSimilarly, there are voices from many statisticians raising the issue that several of the grandiose results, which often involve solving important problems with the state-of-the-art methodology, may be simply invalid.\nTo summarize, a perfect paper should use novel methodology, aim at solving an important problem, and be correct (which should go without saying). The no free lunch of scientific research says that you can pick two out of three, at most.\nThis is of course not universal – there exist great papers, rare gems, wonderful to read and to apply their methodology in practice. Plus, I don’t want to dichotomise here: methodological novelty, problem importance and correctness have many facets and subtleties, and may also be hard to assess upfront.\nSo when I’m planning my next research project, I’m going to be realistic: should I develop a novel cool method (illustrated on a toy problem), try to solve an important problem using well-known tools, or (perhap correctly) tackle a simple problem with a well-known tool?"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome!",
    "section": "",
    "text": "My name is Paweł Czyż and I am a data scientist interested in modelling complex biological data using techniques originating in probabilistic machine learning and applied Bayesian statistics.\nCurrently I am a Doctoral Fellow of the ETH AI Center working in Computational Biology Group and Computational Cancer Genomics Laboratory.\nPrior to starting my PhD I was an AI Resident at Microsoft Research Cambridge and studied mathematical physics at University of Oxford."
  },
  {
    "objectID": "Almost-Nonidentifiable.html",
    "href": "Almost-Nonidentifiable.html",
    "title": "Paweł Czyż",
    "section": "",
    "text": "import jax\nimport jax.numpy as jnp\nfrom jax import random\n\n\ndef sample_covariates(key, correlation: float, n_points: int) -&gt; jnp.ndarray:\n    cov = jnp.asarray([\n        [1.0, correlation],\n        [correlation, 1.0],\n    ])\n    return random.multivariate_normal(key, mean=jnp.zeros(2), cov=cov, shape=(n_points,))\n\n\ndef sample_response(\n    key, \n    covariates: jnp.ndarray,\n    coefs: jnp.ndarray,\n    intercept: float,\n    noise: float\n) -&gt; jnp.ndarray:\n    noise_var = noise * random.normal(key, shape=(covariates.shape[0],))\n    return intercept + jnp.einsum(\"j,nj -&gt; n\", coefs, covariates) + noise_var\n\n\ndef solve(\n    covariates,\n    response,\n) -&gt; tuple[float, jnp.ndarray]:\n    n = covariates.shape[0]\n    ones = jnp.ones((n, 1))\n    # Append 1 so that the matrix has columns:\n    # 1, cov1, cov2\n    xs = jnp.hstack((ones, covariates))\n\n    coefs = jnp.linalg.inv(xs.T @ xs) @ xs.T @ response\n    return coefs[0], coefs[1:]\n\n\ndef simulate_and_solve(\n    key,\n    correlation: float,\n    n_points: int,\n    intercept: float,\n    coefs: jnp.ndarray,\n    noise: float,\n) -&gt; tuple[float, jnp.ndarray]:\n    key1, key2 = random.split(key)\n    \n    X = sample_covariates(key1, correlation=correlation, n_points=n_points)\n    y = sample_response(key2, covariates=X, coefs=coefs, intercept=intercept, noise=noise)\n\n    return solve(X, y)\n\n\ncoefs = jnp.asarray([0.5, 0.8])\ncorrelation = 0.75\nn_points = 30\nintercept = 0.0\nnoise = 0.3\nn_simulations: int = 1000\n\nkey = random.PRNGKey(123)\nkey, *subkeys = random.split(key, n_simulations + 1)\n\nb_, coefs_ = jax.vmap(simulate_and_solve, in_axes=(0, None, None, None, None, None))(\n    jnp.asarray(subkeys), correlation, n_points, intercept, coefs, noise    \n)\n\nfig, axs = plt.subplots(2, 2, figsize=(6, 6))\n\nax = axs[0, 0]\nax.hist(b_, bins=20, alpha=0.8, color=\"navy\", density=True)\nax.axvline(intercept, c=\"crimson\", linestyle=\"--\")\nax.set_xlabel(\"Intercept\")\n\nax = axs[0, 1]\nax.hist(coefs_[:, 0], bins=20, alpha=0.8, color=\"navy\", density=True)\nax.axvline(coefs[0], c=\"crimson\", linestyle=\"--\")\nax.set_xlabel(\"Coefficient $\\\\alpha_1$\")\n\nax = axs[1, 0]\nax.hist(coefs_[:, 1], bins=20, alpha=0.8, color=\"navy\", density=True)\nax.axvline(coefs[1], c=\"crimson\", linestyle=\"--\")\nax.set_xlabel(\"Coefficient $\\\\alpha_2$\")\n\nax = axs[1, 1]\nax.scatter(coefs_[:, 0], coefs_[:, 1], c=\"navy\", alpha=0.3, s=3)\nax.axvline(coefs[0], c=\"crimson\", linestyle=\"--\")\nax.axhline(coefs[1], c=\"crimson\", linestyle=\"--\")\n\nax.set_xlabel(\"Coefficient $\\\\alpha_1$\")\nax.set_ylabel(\"Coefficient $\\\\alpha_2$\")\n\n\nfig.tight_layout()\n\n\n\n\n\ncoef_\n\nArray([1.266081 , 1.2590657], dtype=float32)\n\n\n\nimport matplotlib.pyplot as plt\n\n\nfig, axs = plt.subplots(1, 3, figsize=(6, 2), dpi=250)\n\nax = axs[0]\nax.scatter(X[:, 0], X[:, 1], s=0.1, c=\"navy\", alpha=0.8, marker=\".\")\nax.set_xlabel(\"$X_1$\")\nax.set_ylabel(\"$X_2$\")\nax.plot(X[:, 0], 0.5 * X[:, 0], linewidth=1, c=\"k\", linestyle=\"-\")\n\nax = axs[1]\nax.scatter(X[:, 0], y, s=0.1, c=X[:, 1], cmap=\"magma\")\nax.set_xlabel(\"$X_1$\")\nax.set_ylabel(\"$Y$\")\n\nax.plot(X[:, 0], coefs[0] * X[:, 0], c=\"k\", linestyle=\"-\", linewidth=1)\n\nax = axs[2]\nax.scatter(X[:, 1], y, s=0.1, c=X[:, 0], cmap=\"magma\")\nax.set_xlabel(\"$X_2$\")\nax.set_ylabel(\"$Y$\")\nax.plot(X[:, 1], coefs[1] * X[:, 1], c=\"k\", linestyle=\"-\", linewidth=1)\n\nfig.tight_layout()\n\n\n\n\n\ncoef_\n\nArray([1.518926  , 0.69501114], dtype=float32)"
  },
  {
    "objectID": "private/prezenty.html",
    "href": "private/prezenty.html",
    "title": "Lista prezentów",
    "section": "",
    "text": "Bardzo mi miło, że rozważasz podarowanie mi prezentu! Jestem jednak minimalistą (walczącym z książkoholizmem) i staram się nie posiadać zbyt wielu rzeczy (lub duplikatów książek). Postanowiłem przygotować listę rzeczy, z których jednak będę bardzo zadowolony."
  },
  {
    "objectID": "private/prezenty.html#zawsze-w-modzie",
    "href": "private/prezenty.html#zawsze-w-modzie",
    "title": "Lista prezentów",
    "section": "Zawsze w modzie",
    "text": "Zawsze w modzie\n\nKartka lub list.\nPotwierdzenie datku na wybraną fundację charytatywną. Na przykład UNICEF, Against Malaria czy Krajowy Fundusz na rzecz Dzieci."
  },
  {
    "objectID": "private/prezenty.html#artykuły-biurowe",
    "href": "private/prezenty.html#artykuły-biurowe",
    "title": "Lista prezentów",
    "section": "Artykuły biurowe",
    "text": "Artykuły biurowe\n\nCienkopisy.\nOłówki"
  },
  {
    "objectID": "private/prezenty.html#książki-i-komiksy",
    "href": "private/prezenty.html#książki-i-komiksy",
    "title": "Lista prezentów",
    "section": "Książki i komiksy",
    "text": "Książki i komiksy\n\nNeil Gaiman, Sandman, tom 1, 2, 3 lub 4"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "I’m still maintaining my digital garden in a non-public manner1, but from time to time I encounter an idea which I’d just love to share with others or I am asked frequently about to make it a topic of a separate post."
  },
  {
    "objectID": "blog.html#footnotes",
    "href": "blog.html#footnotes",
    "title": "Blog",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSometimes my ideas are just silly and need more time before I decide whether they are useful at all or they are just noise. Sometimes it’s joint work with other people and I can’t share their intellectual property without approval. And I still haven’t figured out how to integrate Obsidian with Quarto, which are wonderful tools.↩︎"
  },
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "Publications",
    "section": "",
    "text": "Beyond normal: on the evaluation of mutual information estimators   \n    P. Czyż, F. Grabowski, J.E. Vogt, N. Beerenwinkel and A. Marx \n    Code\n    Manuscript\n    Jun, 2023\n  \n  \n    Bayesian quantification with black-box estimators   \n    A. Ziegler and P. Czyż \n    Code\n    Manuscript\n    Feb, 2023\n  \n  \n    CanSig: discovery of shared transcriptional states across cancer patients from single-cell RNA sequencing data   \n    J. Yates, F. Barkmann, P. Czyż, ..., N. Beerenwinkel and V. Boeva \n    Code\n    Manuscript\n    Apr, 2022\n  \n  \n    Limits to the rate of information transmission through the MAPK pathway   \n    F. Grabowski, P. Czyż, M. Kochańczyk and T. Lipniacki \n    Code\n    Manuscript\n    Mar, 2019\n  \n\n\nNo matching items"
  }
]