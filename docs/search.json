[
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "Publications",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Author\n        \n         \n          Publication\n        \n         \n          Year\n        \n     \n  \n    \n      \n      \n    \n\n\n  \n    Bayesian quantification with black-box estimators\n    A. Ziegler, P. Czyż\n    Preprint, submitted\n    (2023)\n    \n      Details\n    \n    \n       Preprint\n    \n    \n       Materials\n    \n  \n  \n    Beyond normal: on the evaluation of mutual information estimators\n    P. Czyż, F. Grabowski, J.E. Vogt, N. Beerenwinkel, A. Marx\n    NeurIPS\n    (2023)\n    \n      Details\n    \n    \n      DOI\n    \n    \n       Preprint\n    \n    \n       Materials\n    \n  \n  \n    On the properties and estimation of pointwise mutual information profiles\n    P. Czyż, F. Grabowski, J.E. Vogt, N. Beerenwinkel, A. Marx\n    Preprint, submitted\n    (2023)\n    \n      Details\n    \n    \n       Preprint\n    \n    \n       Materials\n    \n  \n  \n    Limits to the rate of information transmission through the MAPK pathway\n    F. Grabowski, P. Czyż, M. Kochańczyk, T. Lipniacki\n    J. R. Soc. Interface\n    (2019)\n    \n      Details\n    \n    \n      DOI\n    \n    \n       Preprint\n    \n    \n       Materials\n    \n  \n\n\nNo matching items\n\n\n\nThis page was created using the following template."
  },
  {
    "objectID": "private/prezenty.html",
    "href": "private/prezenty.html",
    "title": "Lista prezentów",
    "section": "",
    "text": "Drogi Święty Mikołaju, bardzo mi miło, że rozważasz podarowanie mi prezentu! Staram się zostać minimalistą (walczącym z książkoholizmem), a to oznacza, że nie lubię posiadać zbyt wielu rzeczy. Jeśli więc nadal rozważasz podarowanie mi czegoś, przygotowałem listę rzeczy, z których jednak będę bardzo zadowolony.\n\nKartka lub list.\nInformacja o datku na dowolnie wybraną fundację charytatywną. Na przykład UNICEF, Krajowy Fundusz na rzecz Dzieci czy Against Malaria, ale istnieje znacznie więcej różnych metod pomagania.\nOłówek.\nCienkopis.\nNaboje atramentowe LAMY T10, najchętniej w kolorze granatowym, czarnym lub zielonym."
  },
  {
    "objectID": "publications/limits-mapk.html",
    "href": "publications/limits-mapk.html",
    "title": "Limits to the rate of information transmission through the MAPK pathway",
    "section": "",
    "text": "@article{Grabowski-2019-systems-biology,\nauthor = {Grabowski, Frederic  and Czyż, Paweł  and Kochańczyk, Marek  and Lipniacki, Tomasz },\ntitle = {Limits to the rate of information transmission through the {MAPK} pathway},\njournal = {Journal of The Royal Society Interface},\nvolume = {16},\nnumber = {152},\npages = {20180792},\nyear = {2019},\ndoi = {10.1098/rsif.2018.0792},\nURL = {https://royalsocietypublishing.org/doi/abs/10.1098/rsif.2018.0792},\neprint = {https://royalsocietypublishing.org/doi/pdf/10.1098/rsif.2018.0792}\n}"
  },
  {
    "objectID": "publications/limits-mapk.html#citation",
    "href": "publications/limits-mapk.html#citation",
    "title": "Limits to the rate of information transmission through the MAPK pathway",
    "section": "",
    "text": "@article{Grabowski-2019-systems-biology,\nauthor = {Grabowski, Frederic  and Czyż, Paweł  and Kochańczyk, Marek  and Lipniacki, Tomasz },\ntitle = {Limits to the rate of information transmission through the {MAPK} pathway},\njournal = {Journal of The Royal Society Interface},\nvolume = {16},\nnumber = {152},\npages = {20180792},\nyear = {2019},\ndoi = {10.1098/rsif.2018.0792},\nURL = {https://royalsocietypublishing.org/doi/abs/10.1098/rsif.2018.0792},\neprint = {https://royalsocietypublishing.org/doi/pdf/10.1098/rsif.2018.0792}\n}"
  },
  {
    "objectID": "publications/limits-mapk.html#abstract",
    "href": "publications/limits-mapk.html#abstract",
    "title": "Limits to the rate of information transmission through the MAPK pathway",
    "section": "Abstract",
    "text": "Abstract\nTwo important signalling pathways of NF-κB and ERK transmit merely 1 bit of information about the level of extracellular stimulation. It is thus unclear how such systems can coordinate complex cell responses to external cues. We analyse information transmission in the MAPK/ERK pathway that converts both constant and pulsatile EGF stimulation into pulses of ERK activity. Based on an experimentally verified computational model, we demonstrate that, when input consists of sequences of EGF pulses, transmitted information increases nearly linearly with time. Thus, pulse-interval transcoding allows more information to be relayed than the amplitude–amplitude transcoding considered previously for the ERK and NF-κB pathways. Moreover, the information channel capacity \\(C\\), or simply bitrate, is not limited by the bandwidth \\(B = 1/\\tau\\), where \\(\\tau\\approx 1\\, h\\) is the relaxation time. Specifically, when the input is provided in the form of sequences of short binary EGF pulses separated by intervals that are multiples of \\(\\tau/n\\) (but not shorter than \\(\\tau\\)), then for \\(n = 2\\), \\(C \\approx 1.39\\) bit/h; and for \\(n = 4\\), \\(C \\approx 1.86\\) bit/h. The capability to respond to random sequences of EGF pulses enables cells to propagate spontaneous ERK activity waves across tissue."
  },
  {
    "objectID": "publications/bayesian-quantification.html",
    "href": "publications/bayesian-quantification.html",
    "title": "Bayesian quantification with black-box estimators",
    "section": "",
    "text": "Understanding how different classes are distributed in an unlabeled data set is an important challenge for the calibration of probabilistic classifiers and uncertainty quantification. Approaches like adjusted classify and count, black-box shift estimators, and invariant ratio estimators use an auxiliary (and potentially biased) black-box classifier trained on a different (shifted) data set to estimate the class distribution and yield asymptotic guarantees under weak assumptions. We demonstrate that all these algorithms are closely related to the inference in a particular Bayesian model, approximating the assumed ground-truth generative process. Then, we discuss an efficient Markov Chain Monte Carlo sampling scheme for the introduced model and show an asymptotic consistency guarantee in the large-data limit. We compare the introduced model against the established point estimators in a variety of scenarios, and show it is competitive, and in some cases superior, with the state of the art."
  },
  {
    "objectID": "publications/bayesian-quantification.html#abstract",
    "href": "publications/bayesian-quantification.html#abstract",
    "title": "Bayesian quantification with black-box estimators",
    "section": "",
    "text": "Understanding how different classes are distributed in an unlabeled data set is an important challenge for the calibration of probabilistic classifiers and uncertainty quantification. Approaches like adjusted classify and count, black-box shift estimators, and invariant ratio estimators use an auxiliary (and potentially biased) black-box classifier trained on a different (shifted) data set to estimate the class distribution and yield asymptotic guarantees under weak assumptions. We demonstrate that all these algorithms are closely related to the inference in a particular Bayesian model, approximating the assumed ground-truth generative process. Then, we discuss an efficient Markov Chain Monte Carlo sampling scheme for the introduced model and show an asymptotic consistency guarantee in the large-data limit. We compare the introduced model against the established point estimators in a variety of scenarios, and show it is competitive, and in some cases superior, with the state of the art."
  },
  {
    "objectID": "publications/bayesian-quantification.html#acknowledgments",
    "href": "publications/bayesian-quantification.html#acknowledgments",
    "title": "Bayesian quantification with black-box estimators",
    "section": "Acknowledgments",
    "text": "Acknowledgments\nI had a lot of great time working on this! This is mostly due to Albert Ziegler and Ian Wright, who were wonderful mentors to work with. I am also grateful to Semmle/GitHub UK and the ETH AI Center for funding this work."
  },
  {
    "objectID": "publications/bayesian-quantification.html#links",
    "href": "publications/bayesian-quantification.html#links",
    "title": "Bayesian quantification with black-box estimators",
    "section": "Links",
    "text": "Links\n\nThe Bayesian quantification manuscript is available here. It evolved from unsupervised recalibration, as we arrived to the quantification coming from the related problem of classifier recalibration under prior probability shifts.\nThe experiments are fully reproducible thanks to Snakemake workflows! Check out the code.\nBayesian quantification is available in the QuaPy package! Many thanks to Alejandro Moreo Fernández for his kindness and contributions in pull requests 28 and 29.\nWe discussed quantification in the post on Gibbs sampling and EM algorithm."
  },
  {
    "objectID": "publications/bayesian-quantification.html#citation",
    "href": "publications/bayesian-quantification.html#citation",
    "title": "Bayesian quantification with black-box estimators",
    "section": "Citation",
    "text": "Citation\n@misc{bayesian-quantification,\n      title={Bayesian Quantification with Black-Box Estimators}, \n      author={Albert Ziegler and Paweł Czyż},\n      year={2023},\n      eprint={2302.09159},\n      archivePrefix={arXiv},\n      primaryClass={stat.ML}\n}"
  },
  {
    "objectID": "posts/strict-linear-independence-measures.html",
    "href": "posts/strict-linear-independence-measures.html",
    "title": "Strict linear independence of measures",
    "section": "",
    "text": "Here we discussed some possible approaches to tackle the quantification problem. Today let’s take a more theoretical look on it, as proposed in A Unified View of Label Shift Estimation."
  },
  {
    "objectID": "posts/strict-linear-independence-measures.html#quantification",
    "href": "posts/strict-linear-independence-measures.html#quantification",
    "title": "Strict linear independence of measures",
    "section": "Quantification",
    "text": "Quantification\nWe have many objects of distinct types \\(y\\in \\mathcal Y = \\{1, \\dotsc, K\\}\\), for example chairs, tables, cups, plates… However, in our setting the label is not available: we only have a list of features. For example, the features may include weight (which helps to distinguish chairs from beds), number of legs (which helps to distinguish cups from chairs), the main construction material.\nWe will assume that each object is given a point in the feature space \\(\\mathcal X\\). Of course, each type of object \\(y\\) may result in a lot of different features observed: there are heavier and lighter tables. Some chairs have three legs. Cups can be made out of wood, glass or metal. In other words, for each category \\(y\\) we have a probability measure \\(Q_y\\) on \\(\\mathcal X\\), representing the conditional probability distribution \\(P(X\\mid Y=y)\\).\nLet’s assume that the probability distributions \\(Q_y\\) are known, but we are trying to find the proportions of different object, \\(P(Y=y)\\), in the data set.\nIn other words, we have access to the finite sample from the mixture distribution\n\\[\nP(X) = \\sum_{y\\in \\mathcal Y} P(X\\mid Y=y) P(Y=y) = \\sum_{y\\in \\mathcal Y} \\pi_y Q_y,\n\\]\nwhere \\(\\pi = (\\pi_1, \\dotsc, \\pi_K)\\) is the list of proportions we are trying to find. Note that all entries have to be non-negative and that \\(\\pi_1 + \\dotsc + \\pi_K=1\\), what results in \\(K-1\\) degrees of freedom.\nHaving access to the finite samples is one of the actual difficulties of solving quantification problems. Another one is working with misspecified models, i.e., actually there may be more than \\(K\\) classes (but some of them we are not aware of), or out distributions \\(Q_y\\) may take a different form than assumed.\nHowever, let’s forget about these difficulties for now, and see if we can solve the quantification problem under the ideal circumstances."
  },
  {
    "objectID": "posts/strict-linear-independence-measures.html#identifiability",
    "href": "posts/strict-linear-independence-measures.html#identifiability",
    "title": "Strict linear independence of measures",
    "section": "Identifiability",
    "text": "Identifiability\n\n\n\n\n\n\nIdeal quantification problem: Let \\(P\\) and \\(Q_1, \\dotsc, Q_K\\) be probability measures on \\(\\mathcal X\\), such that there exist a decomposition into a mixture \\[\nP = \\sum_{y\\in \\mathcal Y} \\pi_y Q_y.\n\\]\nWhat are the necessary conditions on \\(Q_y\\) to ensure that the mixture components vector \\(\\pi\\) can be uniquely recovered given \\(Q_y\\) and \\(P\\)?\n\n\n\nFirst, let’s consider the case in which \\(\\mathcal X\\) is finite, with \\(|\\mathcal X| = D\\). If we order the elements of \\(\\mathcal X\\), we can represent each distribution \\(Q_y\\) by a vector of probabilities \\(Q_y(\\{x\\})\\) for \\(x\\in \\mathcal X\\). Equivalently, we are constructing conditional probability vectors \\(P(X=x\\mid Y=y)\\) and have to solve a set of equations\n\\[\nP(X=x) = \\sum_{y\\in \\mathcal Y} P(X=x\\mid Y=y) \\pi_y.\n\\]\nClearly, if the probability vectors representing \\(Q_y\\) are linearly independent, the solution for \\(\\pi\\) has to be unique (assuming that it exists, which is related to the assumption that we have no misspecification). The technique based on solving such a solution of linear equations has been proposed in various forms over the years."
  },
  {
    "objectID": "posts/strict-linear-independence-measures.html#strict-linear-independence-of-measures",
    "href": "posts/strict-linear-independence-measures.html#strict-linear-independence-of-measures",
    "title": "Strict linear independence of measures",
    "section": "Strict linear independence of measures",
    "text": "Strict linear independence of measures\nWhat if \\(\\mathcal X\\) is not finite? In particular, what if \\(\\mathcal X\\) is a continuous space in which singletons \\(\\{x\\}\\) have probability zero? In this case, the measures \\(Q_y\\) do not have such a convenient finite-dimensional vector representation and the concept of linear independence seems to be less useful.\nThe authors of A Unified View of Label Shift Estimation propose the following notion of strict linear independence of probability measures: for every vector \\(\\lambda \\in \\mathbb R^K\\) such that \\(\\lambda\\neq 0\\) it holds that \\[\n\\int_{\\mathcal X} \\left| \\sum_{y\\in \\mathcal Y} p(z\\mid y) \\right|  \\, \\mathrm{d}x \\neq 0.\n\\]\nI personally prefer a bit different formulation (although perhaps a bit more complicated). Assume that we have a \\(\\sigma\\)-finite measure \\(\\mu\\) on \\(\\mathcal X\\), such that all \\(Q_k \\ll \\mu\\). Often there is a natural reference measure in many problems (e.g., the Lebesgue measure on \\(\\mathbb R^n\\), with the assumption that all \\(Q_k\\) have PDFs), but generally at least one exists, for example \\(\\mu = Q_1 + \\dotsc + Q_K\\) (or it can be normalised by \\(K\\) to yield a probability measure!)\nThe equation above is a requirement that \\[\n\\int_{\\mathcal X} \\left| \\sum_{y\\in \\mathcal Y} \\lambda_y \\frac{\\mathrm d Q_y}{\\mathrm d \\mu} \\right|  \\, \\mathrm{d}\\mu \\neq 0\n\\] which in turn can be written as \\[\n\\left| \\sum_{y\\in \\mathcal Y} \\lambda_y Q_y \\right|(X) \\neq 0.\n\\]\nIt’s not hard to prove that the above condition is equivalent to an existence of a measurable set \\(A_\\lambda\\) such that \\[\n\\lambda_1 Q_1(A_\\lambda) + \\cdots + \\lambda_K Q_K(A_\\lambda) \\neq 0.\n\\]\nHence, we will prefer to use the equivalent definition:\n\n\n\n\n\n\nDefinition: We say that probability measures \\(Q_1, \\dotsc, Q_K\\) are strictly linearly independent if for every vector \\(\\lambda \\neq 0\\) there exists a measurable subset \\(A_\\lambda\\subseteq \\mathcal X\\) such that \\[\n\\lambda_1 Q_1(A_\\lambda) + \\cdots + \\lambda_K Q_K(A_\\lambda) \\neq 0.\n\\]\n\n\n\nLet’s think why this is a sufficient condition for the uniqueness of \\(\\pi\\). Assume that the true composition vector is \\(\\pi\\) and suppose that we have a candidate composition vector \\(\\gamma\\) such that \\(\\gamma\\neq \\pi\\). Take now \\(\\lambda = \\pi - \\gamma \\in \\mathbb R^K\\). From strict linear independence, we know that there exists \\(A_\\lambda\\) such that \\[\nP(A_\\lambda) = \\sum_{y} \\pi_y Q_y(A_\\lambda) \\neq \\sum_{y} \\gamma_y Q_y(A_\\lambda).\n\\]\nHence, the observed measure \\(P\\) is different from the mixture parameterised by \\(\\gamma\\)."
  },
  {
    "objectID": "posts/strict-linear-independence-measures.html#examples",
    "href": "posts/strict-linear-independence-measures.html#examples",
    "title": "Strict linear independence of measures",
    "section": "Examples",
    "text": "Examples\nFinally, let’s think about examples of strictly linearly independent measures.\n\nDiscrete spaces\nProbably the simplest example is for discrete measures on finite spaces: if \\(\\mathcal X\\) is finite, strict linear independence and linear independence are equivalent.\nThe proof is easy: consider the probability vectors \\(q^y_x = P(X=x\\mid Y=y) = Q_y(\\{x\\})\\). If the vectors are linearly independent, for every \\(\\lambda \\neq 0\\) we have \\(\\lambda_1 q^1 + \\cdots + \\lambda_K q^K\\neq 0\\), meaning that there exists a component \\(x\\in \\mathcal X\\) such that \\(\\lambda_1 q^1_x + \\cdots + \\lambda_K q^K_x \\neq 0\\). So, we define \\(A_\\lambda = \\{x\\}\\).\nConversely, if we have \\(\\lambda\\neq 0\\) and we use strict linear independence to ensure the existence of a set \\(A_\\lambda\\) such that \\[\n    0 \\neq \\lambda_1 Q_1(A_\\lambda) + \\cdots + \\lambda_K Q_K(A_\\lambda) = \\sum_{x\\in A_\\lambda} (\\lambda_1 q^1_x + \\cdots + \\lambda_K q^K_x),\n\\] then we see that for at least one component \\(x\\) we have \\(\\lambda_1 q^1_x + \\cdots + \\lambda_K q^K_x\\neq 0\\), which suffices for linear independence.\n\n\nA lemma\nFor continuous spaces the situation is a bit more complex. However, let’s prove a useful lemma, which is in fact a generalisation of the previous result.\n\n\n\n\n\n\nLemma: Assume that \\(\\mathcal X\\) is a standard Borel space and \\(Q_1, \\dotsc, Q_K\\) have continuous PDFs \\(q_1, \\dotsc, q_K\\), with respect to a \\(\\sigma\\)-finite and strictly positive measure \\(\\mu\\). Then, if \\(q_1, \\dotsc, q_K\\) are linearly independent as vectors in the space of continuous real-valued functions \\(C(\\mathcal X, \\mathbb R)\\), then the measures \\(Q_1, \\dotsc, Q_K\\) are strictly linearly independent.\n\n\n\nProof: Take any \\(\\lambda\\neq 0\\) and write \\(u = |\\lambda_1 q_1 + \\cdots + \\lambda_K q_K|\\). From the linear independence it follows that there exists \\(x_0\\in \\mathcal X\\) such that \\(u(x_0) &gt; 0\\). Now use continuity of \\(u\\) to find an open neighborhood \\(A\\) of \\(x_0\\) such that for all \\(x\\in A\\) we have \\(u(x) &gt; u(x_0) / 2\\). As \\(u\\) is non-negative and \\(\\mu\\) is strictly positive, we have \\(\\mu(A) &gt; 0\\), so that \\[\n\\int_X u\\, \\mathrm{d}\\mu \\ge \\int_{A} u\\, \\mathrm{d}\\mu \\ge \\frac{u(x_0)}{2} \\cdot \\mu(A) &gt; 0.\n\\]\nI personally find this lemma useful: verifying linear independence of functions is a well-studied problem in mathematics. For example, if \\(\\mathcal X\\subseteq \\mathbb R\\) is an interval and the densities \\(q_y\\) are sufficiently smooth, one can use Wronskian (introduced by Józef Wroński in 1812, so it’s a classic tool) to study linear independence.\n\n\nExponential variables\nConsider a space \\(\\mathcal X = \\mathbb R^+\\) and the family of exponential random variables, which have densities \\(q_y(x)=\\mu_k\\exp(-\\mu_y x)\\). Now assume that all the parameters are different. We will prove that these densities are linearly independent functions.\n\nWronskian approach\nNote that the \\(m\\)-th derivative is \\(q^{(m)}(x) = (-\\mu_y)^{m} q_y(x)\\). The Wronskian is in this case given by \\[\\begin{align*}\nW(x) &= \\det \\begin{pmatrix}\n    q_1(x) & \\cdots & q_K(x)\\\\\n    -\\mu_1 q_1(x) & \\cdots & -\\mu_K q_K(x) \\\\\n    \\vdots & \\ddots & \\vdots \\\\\n    (-\\mu_1)^{K-1} q_1(x) & \\cdots & (-\\mu_K)^{K-1} q_K(x)\n\\end{pmatrix} \\\\\n  &= \\left(\\prod_{y} q_y(x)\\right) \\cdot \\det \\begin{pmatrix}\n    1 & \\cdots & 1\\\\\n    -\\mu_1 & \\cdots & -\\mu_K \\\\\n    \\vdots & \\ddots & \\vdots \\\\\n    (-\\mu_1)^{K-1}  & \\cdots & (-\\mu_K)^{K-1}\n    \\end{pmatrix}\n\\end{align*}\n\\]\nNote that all \\(q_y(x) &gt; 0\\) and that the determinant of the last matrix has to be positive, as it’s a Vandermonde polynomial: \\[\n\\prod -(\\mu_i - \\mu_j) \\neq 0,\n\\] from the assumption that the means are different.\n\n\nAsymptotic behaviour\nLet’s do another proof, this time going to the limit, similarly to this solution.\nWithout loss of generality, assume \\(0 &lt; \\mu_1 &lt; \\mu_2 &lt; \\dotsc &lt; \\mu_K\\).\nIf there’s \\(\\lambda \\in \\mathbb R^K\\) such that \\[\n\\sum_k \\lambda_k \\mu_k \\exp(-\\mu_k x) = 0,\n\\] identically, then we can multiply both sides by \\(\\exp(\\mu_1 x)\\) to obtain \\[\n\\sum_k \\lambda_k \\mu_k \\exp\\big(-(\\mu_k-\\mu_1) x\\big) = 0.\n\\] For \\(x\\to \\infty\\) the first term becomes \\(\\lambda_1\\mu_1\\) and the rest of the terms goes to \\(0\\). Hence, we have \\(\\lambda_1 = 0\\). Repeating this procedure for \\(\\lambda_2\\), \\(\\lambda_3\\) and other coefficients, we end up with \\(\\lambda = 0\\), proving linear independence.\n\n\nEigenvectors and eigenvalues\nSheldon Axler provides a wonderful proof: each \\(q_y\\) is an eigenvector of the differentiation operator: \\[\n\\frac{\\mathrm{d}}{\\mathrm{d}x} q_y(x) = -\\mu_y q_y(x).\n\\]\nAs all these eigenvalues are distinct, the eigenvectors have to be independent (a useful lemma, one proof follows via induction on \\(K\\)).\n\n\n\nHow about the normal distributions?\nHere is a proof strategy for the normal distributions on \\(\\mathbb R\\), which employs asymptotics. However, I expect the result should generally hold for multivariate normal distributions, provided that the mean vectors are different. But how to prove that? Possibly the strategy employing asymptotics would work, but I am not sure about the details. Similarly, I expect that multivariate Student distributions with different location vectors should be strictly linearly independent.\nI somewhat feel that topic should have been already studied in measure theory and, perhaps, information geometry, although probably under a different name than strict linear independence. It would be interesting to see a reference on this topic!"
  },
  {
    "objectID": "posts/conditioning-multivariate-normal.html",
    "href": "posts/conditioning-multivariate-normal.html",
    "title": "Starting (on finite domains) with Gaussian processes",
    "section": "",
    "text": "Let \\(Y = (Y_1, \\dotsc, Y_n)^T\\) be a random variable distributed according to the multivariate normal distribution \\(\\mathcal N(\\mu, \\Sigma)\\), where \\(\\mu\\in \\mathbb R^n\\) and \\(\\Sigma\\) is a real symmetric positive-definite1 \\(n\\times n\\) matrix.\nWe will think of this distribution in the following manner: we have a domain \\(\\mathcal X = \\{1, 2, \\dotsc, n\\}\\) and for each \\(x\\in \\mathcal X\\) we have a random variable \\(Y_i\\) and the joint distribution \\(P(Y_1, \\dotsc, Y_n)\\) is multivariate normal.\nAssume that we have measured \\(Y_x\\) variables for indices \\(x_1, \\dotsc, x_k\\), with corresponding values \\(Y_{x_1}=y_1, \\dotsc, Y_{x_k}=y_k\\), and we are interested in predicting the values at locations \\(x'_1, \\dotsc, x'_q\\), i.e., modelling the conditional probability distribution \\[\nP(Y_{x'_1}, \\dotsc, Y_{x'_q} \\mid Y_{x_1}=y_1, \\dotsc, Y_{x_k}=y_k).\n\\]\nWe will also allow \\(k=0\\), i.e., we would like to access marginal distributions. This can be treated as an extension of the problems answered by bend and mix models we studied here."
  },
  {
    "objectID": "posts/conditioning-multivariate-normal.html#formalising-the-problem",
    "href": "posts/conditioning-multivariate-normal.html#formalising-the-problem",
    "title": "Starting (on finite domains) with Gaussian processes",
    "section": "Formalising the problem",
    "text": "Formalising the problem\nTo formalise the problem a bit:\n\n\n\n\n\n\nConditional calculation\n\n\n\nConsider a set of measured values \\(M = \\{(x_1, y_1), \\dotsc, (x_k, y_k)\\}\\) and a non-empty query set \\(Q = \\{x'_1, \\dotsc, x'_q\\} \\subseteq \\mathcal X\\).\nWe assume that \\(Q\\cap M_x = \\varnothing\\) and \\(|M_x| = |M|\\), where \\(M_x = \\{x \\in \\mathcal X \\mid (x, y)\\in M \\text{ for some } y \\}\\).\nWe would like to be able to sample from the conditional probability distribution \\[\nP(Y_{x_1'}, \\dotsc, Y_{x_q'} \\mid Y_{x_1}=y_1, \\dotsc, Y_{x_k}=y_k)\n\\] as well as to evaluate the (log-)density at any point.\nWe allow \\(M=\\varnothing\\), which corresponds then to marginal distributions.\n\n\nThis problem can be solved for multivariate normal distributions by noticing that all conditional (and marginal) distributions will also be multivariate normal. Let’s introduce some notation.\nFor a tuple \\(\\pi = (\\pi_1, \\dotsc, \\pi_m) \\in \\mathcal X^m\\) such that \\(\\pi_i\\neq \\pi_j\\) for \\(i\\neq j\\), we will write \\(Y_\\pi\\) for a random vector \\((Y_{\\pi_1}, \\dotsc, Y_{\\pi_m})\\). Note that this operation can be implemented using a linear mapping \\(A_\\pi \\colon \\mathbb R^n\\to \\mathbb R^m\\) with \\[\nA_\\pi \\begin{pmatrix} Y_1 \\\\ \\vdots \\\\ Y_n \\end{pmatrix} = \\begin{pmatrix}\n  Y_{\\pi_1} \\\\ \\vdots \\\\ Y_{\\pi_m}\n\\end{pmatrix}\n\\] and \\((A_\\pi)_{oi} = \\mathbf 1[ i = \\pi_o]\\). Hence, \\(Y_\\pi\\) vector is distributed according to \\(\\mathcal N(A_\\pi\\mu, A_\\pi\\Sigma A_\\pi^T)\\).\nThe above operation suffices for calculating arbitrary marginal distributions and distributions corresponding to permuting the components.\nConsider now the case where we want to calculate a “true” conditional distribution (i.e., with \\(M\\neq \\varnothing\\)), so the marginalisation does no suffice.\nWe can use the tuple \\(\\pi = (x_1', \\dotsc, x_q', x_1, \\dotsc, x_k)\\) to select the right variables and reorder them into a \\(q\\)-dimensional block of unobserved (“query”) variables and a \\(k\\)-dimensional block of observed (“key”) variables2.\n\n\n\n\n\n\nConditioning multivariate normal\n\n\n\nLet \\(Y=(Y_1, Y_2) \\in \\mathbb R^{k}\\times \\mathbb R^{n-k}\\) be a random vector split into blocks of dimensions \\(k\\) and \\(n-k\\). If \\(Y\\sim \\mathcal N(\\mu, \\Sigma)\\), where \\[\n\\mu = (\\mu_1, \\mu_2)\n\\] and \\[\n\\Sigma = \\begin{pmatrix}\n  \\Sigma_{11} & \\Sigma_{12} \\\\\n  \\Sigma_{21} & \\Sigma_{22}\n\\end{pmatrix},\n\\]\nthen for every \\(y \\in \\mathbb R^{n-k}\\) it holds that\n\\[\nY_1 \\mid Y_2=y \\sim \\mathcal N(\\mu', \\Sigma'),\n\\] where \\[\n  \\mu' = \\mu_1 + {\\color{Apricot}\\Sigma_{12}\\Sigma_{22}^{-1}}(y-\\mu_2)\n\\] and \\[\n\\Sigma' = \\Sigma_{11} - {\\color{Apricot}\\Sigma_{12} \\Sigma_{22}^{-1}} \\Sigma_{21}.\n\\]\n\n\nWe see that in both formulae the matrix of regression coefficients3 \\(\\color{Apricot}\\Sigma_{12}\\Sigma_{22}^{-1}\\) appears. We will discuss calculation of this term below."
  },
  {
    "objectID": "posts/conditioning-multivariate-normal.html#lets-write-some-code",
    "href": "posts/conditioning-multivariate-normal.html#lets-write-some-code",
    "title": "Starting (on finite domains) with Gaussian processes",
    "section": "Let’s write some code",
    "text": "Let’s write some code\nNow let’s implement a prototype:\n\n\nCode\nimport numpy as np\nimport numpy.linalg as npla\nfrom jaxtyping import Float, Int, Array\n\nfrom scipy import stats\nimport scipy.linalg as spla\n\n\nclass MultivariateNormal:\n  def __init__(\n    self,\n    mu: Float[Array, \" dim\"],\n    cov: Float[Array, \"dim dim\"],\n  ) -&gt; None:\n    eigvals, _ = npla.eig(cov)\n    if np.min(eigvals) &lt;= 0:\n      raise ValueError(f\"Covariance should be positive-definite.\")\n    \n    self.mu = np.asarray(mu)\n    self.cov = np.asarray(cov)\n    dim = self.mu.shape[0]\n\n    assert self.mu.shape == (dim,)\n    assert self.cov.shape == (dim, dim)\n\n  @property\n  def dim(self) -&gt; int:\n    return self.mu.shape[0]\n\n  def sample(\n    self,\n    rng: np.random.Generator,\n    size: int = 1,\n  ) -&gt; np.ndarray:\n    return rng.multivariate_normal(\n      mean=self.mu, cov=self.cov, size=size,\n    )\n\n  def logpdf(self, y: Float[Array, \" dim\"]) -&gt; float:\n    return stats.multivariate_normal.logpdf(\n      y,\n      mean=self.mu,\n      cov=self.cov,\n      allow_singular=False,\n    )\n\n\ndef _contruct_projection_matrix(\n  n: int,\n  indices: Int[Array, \" k\"],\n) -&gt; Int[Array, \"k n\"]:\n  indices = np.asarray(indices, dtype=int)  \n\n  # Output dimension\n  k = indices.shape[0]\n  if np.unique(indices).shape[0] != k:\n    raise ValueError(\"Indices should be unique.\")\n\n  inp = np.arange(n, dtype=int)\n  \n  ret = np.asarray(inp[None, :] == indices[:, None], dtype=int)\n  assert ret.shape == (k, n)\n  return ret\n\n\ndef select(\n  dist: MultivariateNormal,\n  indices: Int[Array, \" k\"],\n) -&gt; MultivariateNormal:\n  proj = np.asarray(\n    _contruct_projection_matrix(\n      n=dist.dim,\n      indices=indices,\n    ),\n    dtype=float,\n  )\n  \n  new_mu = np.einsum(\"oi,i -&gt; o\", proj, dist.mu)\n  new_cov = np.einsum(\"oi,iI,OI -&gt; oO\", proj, dist.cov, proj)\n  \n  return MultivariateNormal(\n    mu=new_mu,\n    cov=new_cov,\n  )\n\ndef _regression_coefs(\n  sigma12: Float[Array, \"Q K\"],\n  sigma22: Float[Array, \"K K\"],\n) -&gt; Float[Array, \"Q K\"]:\n  return spla.solve(sigma22, sigma12.T).T\n\ndef _condition_gaussian(\n  dist: MultivariateNormal,\n  m: int,\n  y: Float[Array, \" vals\"]\n) -&gt; MultivariateNormal:\n  assert y.shape[0] == dist.dim - m\n\n  mu1 = dist.mu[:m]\n  mu2 = dist.mu[m:]\n  sigma11 = dist.cov[:m, :m]\n  sigma12 = dist.cov[:m, m:]\n  sigma22 = dist.cov[m:, m:]\n\n  reg = _regression_coefs(\n    sigma12=sigma12, sigma22=sigma22,\n  )\n\n  mu_ = mu1 + reg @ (y - mu2)\n  sigma_ = sigma11 - reg @ sigma12.T\n\n  return MultivariateNormal(\n    mu=mu_, cov=sigma_,\n  )\n\ndef condition(\n  dist: MultivariateNormal,\n  query: Int[Array, \" Q\"],\n  key: Int[Array, \" K\"],\n  values: Float[Array, \" K\"],\n) -&gt; MultivariateNormal:\n  q, k = query.shape[0], key.shape[0]\n  assert values.shape == (k,), \"Values have wrong shape\"\n\n  total_index = np.concatenate((query, key))\n\n  if np.unique(total_index).shape[0] != k + q:\n    raise ValueError(\"Indices must be unique.\")\n  if np.min(total_index) &lt; 0 or np.max(total_index) &gt;= dist.dim:\n    raise ValueError(\"Indices must be from the set 0, 1, ..., dim-1.\")\n\n  ordered_dist = select(dist, indices=total_index)\n\n  if k == 0:\n    return ordered_dist\n  else:\n    return _condition_gaussian(\n      dist=ordered_dist,\n      m=q,\n      y=values,\n    )\n\n\nNow we can do conditioning. For example, imagine that we have \\(\\mu = 0\\) and we know \\(\\Sigma\\): \\(Y_1\\) correlates with \\(Y_3\\), and \\(Y_2\\) anticorrelates with \\(Y_4\\) and \\(Y_5\\) doesn’t correlate with anything else. We measure \\(Y_1\\) and \\(Y_2\\), so we can use this correlation structure to impute \\(Y_3\\), \\(Y_4\\) and \\(Y_5\\).\nFirst, let’s plot the covariance matrix and the samples:\n\n\nCode\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nplt.style.use(\"dark_background\")\n\nmixing = np.asarray([\n  [1.5, -0.7, 1.5, 0, 0],\n  [0, 1, 0, -1, 0],\n  [0, 0, 0, 0, 1],\n], dtype=float)\n\ncov = np.einsum(\"io,iO-&gt;oO\", mixing, mixing) + 0.1 * np.eye(5)\n\nfig, axs = plt.subplots(1, 2, figsize=(4.5, 2), dpi=200)\nticklabels = [f\"$Y_{i}$\" for i in range(1, 5+1)]\n\nax = axs[0]\nsns.heatmap(cov, ax=ax, xticklabels=ticklabels, yticklabels=ticklabels, vmin=-1.5, vmax=1.5, center=0, square=True, annot=True, fmt=\".1f\")\n\ndist = MultivariateNormal(mu=np.zeros(5), cov=cov)\nrng = np.random.default_rng(42)\nsamples = dist.sample(rng, size=10)\n\nax = axs[1]\nfor sample in samples:\n  x_ax = np.arange(1, 6)\n  ax.plot(x_ax, sample, alpha=0.5, c=\"C1\")\n  ax.scatter(x_ax, sample, alpha=0.5, c=\"C1\", s=4)\n\nax.spines[['top', 'right']].set_visible(False)\nax.set_xlim(0.9, 5.1)\nax.set_xticks(x_ax, ticklabels)\n\nfig.tight_layout()\n\n\n\n\n\nImagine now that we observed \\(Y_1=1.5\\) and \\(Y_2=1\\). We expect that \\(Y_3\\) should move upwards (the posterior should be shifted so that most of the mass is above \\(0\\)), \\(Y_4\\) to go downwards and \\(Y_5\\) to stay as it was. Let’s plot covariance matrix and draws from the conditional posterior \\(P(Y_3, Y_4, Y_5\\mid Y_1=1.5, Y_2=1)\\):\n\n\nCode\ny_obs = np.asarray([1.5, 1])\ncond = condition(\n  dist=dist,\n  query=np.asarray([2, 3, 4]),\n  key=np.asarray([0, 1]),\n  values=y_obs,\n)\n\nfig, axs = plt.subplots(1, 2, figsize=(4.5, 2), dpi=200)\n\nax = axs[0]\nsns.heatmap(\n  cond.cov,\n  ax=ax,\n  xticklabels=ticklabels[2:],\n  yticklabels=ticklabels[2:],\n  vmin=-1.5,\n  vmax=1.5,\n  center=0,\n  annot=True,\n  square=True,\n)\n\nax = axs[1]\nax.spines[['top', 'right']].set_visible(False)\nax.set_xlim(0.9, 5.1)\nax.set_xticks(x_ax)\n\nax.scatter([1, 2], y_obs, c=\"maroon\", s=4)\n\nsamples = cond.sample(rng, size=10)\nfor sample in samples:\n  ax.plot([3, 4, 5], sample, alpha=0.5, c=\"C1\")\n  ax.scatter([3, 4, 5], sample, alpha=0.5, c=\"C1\", s=4)\n\nax.set_xticks(x_ax, ticklabels)\n\nfig.tight_layout()\n\n\n\n\n\nWe see that there is slight anticorrelation between \\(Y_3\\) and \\(Y_4\\): by sampling from the conditional distribution we obtain a coherent sample. This is different than drawing independent samples from \\(P(Y_3 \\mid Y_1=y_1, Y_2=y_2)\\) and \\(P(Y_4\\mid Y_1=y_1, Y_2=y_2)\\). Perhaps it’ll be easier to visualise it on a scatter plot:\n\n\nCode\nfig, axs = plt.subplots(1, 2, figsize=(4.5, 3), dpi=200, sharex=True, sharey=True)\nsamples = cond.sample(rng, size=15_000)\n\nax = axs[0]\nax.scatter(samples[:, 0], samples[:, 1], s=2, alpha=0.01)\nax.set_title(\"Joint sample\")\nax.set_xlabel(r\"$Y_3$\")\nax.set_ylabel(r\"$Y_4$\")\n\nax = axs[1]\nsamples2 = cond.sample(rng, size=samples.shape[0])\nax.scatter(samples[:, 0], samples2[:, 1], s=2, alpha=0.01)\nax.set_title(\"Independent\")\nax.set_xlabel(r\"$Y_3$\")\nax.set_ylabel(r\"$Y_4$\")\n\ncorr = cond.cov[0, 1] / np.sqrt(cond.cov[0, 0] * cond.cov[1, 1])\nfig.suptitle(r\"$\\text{Corr}(Y_3, Y_4) = $\" + f\"{corr:.2f}\")\nfig.tight_layout()\n\nfor ax in axs:\n  ax.spines[['top', 'right']].set_visible(False)\n\n\n\n\n\nOk, it’s hard to see, but visible – the (negative) correlation is just quite weak in this case.\nLet’s do the last visualisation before we move to Gaussian processes. As mentioned, the magical thing is the access to the whole posterior distribution \\(P(Y_3, Y_4, Y_5 \\mid Y_1=y_1, Y_2=y_2)\\): we can evaluate arbitrary probabilities and sample consistent vectors from this distribution. We can visualise samples, but sometimes a simpler summary statistic would be useful. Each of the distributions \\(P(Y_i \\mid Y_1=y_1, Y_2=y_2)\\) is one-dimensional Gaussian, so we can plot its mean and standard deviation. Or, even better, let’s plot \\(\\mu_i\\pm 2\\sigma_i\\) to see where approximately 95% of probability lies.\nWe’ll plot these regions both before and after conditioning:\n\n\nCode\nfig, axs = plt.subplots(1, 2, figsize=(4, 1.5), dpi=170, sharex=True, sharey=True)\n\n# Before conditioning\nax = axs[0]\n\nax.plot(np.arange(1, 1+5), np.zeros(5), linestyle=\"--\", c=\"gray\", alpha=0.5, linewidth=0.8)\n\nax.errorbar(\n  np.arange(1, 1+5),\n  dist.mu,\n  yerr=2 * np.sqrt(np.diagonal(dist.cov)),\n  fmt=\"o\",\n)\n\n# After conditioning\nax = axs[1]\nax.plot(np.arange(1, 1+5), np.zeros(5), linestyle=\"--\", c=\"gray\", alpha=0.5, linewidth=0.8)\nax.scatter([1, 2], y_obs, c=\"maroon\", s=4)\nax.errorbar(\n  [3, 4, 5],\n  cond.mu,\n  yerr=2 * np.sqrt(np.diagonal(cond.cov)),\n  fmt=\"o\",\n)\n\nfor ax in axs:\n  ax.spines[['top', 'right']].set_visible(False)\n  ax.set_xlim(0.8, 5.3)\n  ax.set_xticks(x_ax, ticklabels)\n\nfig.tight_layout()\n\n\n\n\n\nAs mentioned, these plots don’t really allow us to look at correlations between different variables, but they are still useful: we can easily see that the posterior of \\(Y_3\\) moved upwards and \\(Y_4\\) moved downwards! Variable \\(Y_5\\), which is independent of \\((Y_1, Y_2, Y_3, Y_4)\\), doesn’t change: if we want to know it, we just have to measure it."
  },
  {
    "objectID": "posts/conditioning-multivariate-normal.html#gaussian-processes",
    "href": "posts/conditioning-multivariate-normal.html#gaussian-processes",
    "title": "Starting (on finite domains) with Gaussian processes",
    "section": "Gaussian processes",
    "text": "Gaussian processes\nFor \\(\\mathcal X = \\{1, \\dotsc, n\\}\\) we considered an indexed collection of random variables \\(\\{Y_x\\}_{x\\in \\mathcal X}\\). Let’s call it a stochastic process.\nThis stochastic process has the property that the joint distribution over all variables was multivariate normal. From that we could deduce that distributions \\(P(Y_{x_1}, \\dotsc, Y_{x_m})\\) were again multivariate normal, what in turn allowed us to do prediction via conditioning (which resulted, again, in multivariate normal distributions).\nLet’s move beyond a finite dimension: take \\(\\mathcal X=\\mathbb R\\) and consider a stochastic process \\(\\{Y_x\\}_{x\\in \\mathcal X}\\). We will say that it’s a Gaussian process if for every finite set \\(\\{x_1, \\dotsc, x_m\\}\\subseteq \\mathcal X\\) the joint distribution \\(P(Y_{x_1}, \\dotsc, Y_{x_m})\\) is multivariate normal. More generally, we can take other domains \\(\\mathcal X\\) (e.g., \\(\\mathbb R^n\\)) and speak of Gaussian random fields.\nIn either case, the trick is that we never work with infinitely many random variables at once: for example, if we observe values \\(y_1, \\dotsc, y_k\\) at locations \\(x_1, \\dotsc, x_k\\) and we want to predict the values at points \\(x'_1, \\dotsc, x'_q\\), we will construct the joint multivariate normal distribution \\(P(Y_{x_1}, \\dotsc, Y_{x_m}, Y_{x'_1}, \\dotsc, Y_{x'_q})\\) and condition on observed values to get the conditional distribution \\(P(Y_{x'_1}, \\dotsc, Y_{x'_q} \\mid Y_{x_1} = y_1, \\dotsc, Y_{x_k}=y_k)\\).\nNow the questions is: how can we define a consistent stochastic process with these great properties? When \\(\\mathcal X\\) was finite, we could just define the joint probability distribution over all variables via mean and covariance. But now \\(\\mathcal X\\) is not finite!\nConsider therefore two functions, giving the mean and covariance: \\(m \\colon \\mathcal X\\to \\mathbb R\\) and \\(k\\colon \\mathcal X\\to \\mathcal X\\to \\mathbb R^+\\). The premise is to build multivariate normal distributions \\(P(Y_{x_1}, \\dotsc, Y_{x_m})\\) by using the mean vector \\(\\mu_i = m(x_i)\\) and covariance matrix \\(\\Sigma_{ij} = k(x_i, x_j)\\).\nFirst of all, we see that not all covariance functions are suitable: we want covariance matrices to be symmetric and positive-definite, so we should use positive-definite kernels.\nSecondly, we don’t know if these probability distributions can be coherently glued to a stochastic process. The answer to this problem is provided by Daniell-Kolmogorov extension theorem, which says when a family of probability distributions can be coherently glued yielding a stochastic process. In this case parameterising covariances via \\(\\Sigma_{ij}=k(x_i, x_j)\\) has the properties mentioned in the theorem. On the other hand, parameterising precision matrices via \\(k(x_i, x_j)\\) doesn’t generally yield a coherent stochastic process.\nThere are many important technical details, which I should mention here. Instead, I’ll refer to a great introduction to Gaussian processes at Dan Simpson’s blog, and implement something."
  },
  {
    "objectID": "posts/conditioning-multivariate-normal.html#modelling-functions",
    "href": "posts/conditioning-multivariate-normal.html#modelling-functions",
    "title": "Starting (on finite domains) with Gaussian processes",
    "section": "Modelling functions",
    "text": "Modelling functions\nThere are many libraries for working with Gaussian processes, including GPJax, GPyTorch and GPy. We will however just use the code developed above, plus some simple covariance functions.\nOur task will be the following: we are given some function on the interval \\((0, 1)\\). We observe some values \\(M=\\{(x_1, y_1), \\dotsc, (x_k, y_k)\\}\\) inside the intervals \\((0, u)\\) and \\((1-u, 1)\\) and we want to predict the function behaviour in the interval \\((u, 1-u)\\), from which we do not have any data.\n\n\nCode\nimport matplotlib.pyplot as plt\nimport dataclasses\n\n@dataclasses.dataclass\nclass Task:\n  xs_all: Float[Array, \" points\"]\n  ys_all: Float[Array, \" points\"]\n  xs_obs: Float[Array, \" key\"]\n  ys_obs: Float[Array, \" key\"]\n  xs_query: Float[Array, \" query\"]\n\ndef create_task(\n  f,\n  thresh: float = 0.25,\n  k_2: int = 5,\n  n_query: int = 101,\n) -&gt; Task:\n  assert 0.02 &lt; thresh &lt; 0.98, \"Threshold should be in (0.02, 0.98)\"\n\n  xs_all = np.linspace(0.01, 1)\n  \n  xs_obs = np.concatenate((np.linspace(0.01, thresh, k_2), np.linspace(1 - thresh, 0.99, k_2)))\n  xs_query = np.linspace(thresh + 0.01, 0.99 - thresh, n_query)\n\n  return Task(\n    xs_all=xs_all,\n    ys_all=f(xs_all),\n    xs_obs=xs_obs,\n    ys_obs=f(xs_obs),\n    xs_query=xs_query,\n  )\n\ndef plot_task(ax: plt.Axes, task: Task):\n  ax.plot(\n    task.xs_all, task.ys_all, linestyle=\"--\", c=\"maroon\", linewidth=1.0, alpha=0.8\n  )\n  ax.scatter(task.xs_obs, task.ys_obs, s=8, c=\"maroon\")\n  ax.spines[['top', 'right']].set_visible(False)\n  ax.set_xlim(-0.02, 1.02)\n\n\nfig, ax = plt.subplots(figsize=(2.2, 1.4), dpi=200)\n\ntask1 = create_task(lambda x: np.cos(2*np.pi * x))\nplot_task(ax, task1)\nfig.tight_layout()\n\n\n\n\n\nWe can approach this problem in two ways: first, we can impute missing values evaluated at some points.\nFor example, we can define a grid over \\((u, 1-u)\\) with \\(q\\) query points \\(x'_1, \\dotsc, x'_q\\) and sample from the conditional distribution \\(P(Y_{x'_1}, \\dotsc, Y_{x'_q} \\mid Y_{x_1}=y_1, \\dotsc, Y_{x_k}=y_k)\\) several times. This is one good way of plotting, showing us the behaviour of the whole sample at once.\nAnother way of plotting, which we also have already seen, is to take a single point \\(x'\\) and look at the normal distribution \\(P(Y_{x'} \\mid Y_{x_1}=y_1, \\dotsc, Y_{x_k}=y_k)\\), summarized by the mean and standard deviation: we can plot \\(\\mu(x') \\pm 2\\sigma(x')\\) as a function of \\(x'\\) (similarly to the finite-dimensional case). This approach doesn’t allow us to look at joint behaviour at different locations, but is quite convenient to summarise uncertainty at a single specific point. For example, this may be informative enough to determine a good location of the next sample to collect in Bayesian optimisation framework (unless one wants to consider multiple points).\nLet’s implement an example kernel and plot predictions in both ways:\n\n\nCode\ndef kernel(x, x_):\n  return np.exp(-20 * np.square(x-x_))\n\nfig, axs = plt.subplots(1, 2, figsize=(2*3, 2), dpi=120, sharex=True, sharey=True)\n\nfor ax in axs:\n  plot_task(ax, task1)\n\nxs_eval = np.concatenate((task1.xs_query, task1.xs_obs))\n\ncov = kernel(xs_eval[:, None], xs_eval[None, :]) + 1e-6 * np.eye(len(xs_eval))\ndist = MultivariateNormal(np.zeros_like(xs_eval), cov)\n\ncond = condition(\n  dist=dist,\n  query=np.arange(len(task1.xs_query)),\n  key=np.arange(len(task1.xs_query), len(xs_eval)),\n  values=task1.ys_obs\n)\n\nrng = np.random.default_rng(1)\nsamples = cond.sample(rng, size=30)\n\nax = axs[0]\nfor sample in samples:\n  ax.plot(task1.xs_query, sample, alpha=0.8, color=\"C1\", linewidth=0.1)\n\n\nax = axs[1]\nax.plot(task1.xs_query, cond.mu, c=\"C1\", linestyle=\":\")\nuncert = 2 * np.sqrt(np.diagonal(cond.cov))\nax.fill_between(task1.xs_query, cond.mu - uncert, cond.mu + uncert, color=\"C1\", alpha=0.2)\n\nfig.tight_layout()\n\n\n\n\n\nNice! I’ve been thinking about showing how different kernels result in differing predictions. But this post is already a bit too long, so I may write another one on this topic. In any case, there’s a Kernel Cookbook created by David Duvenaud."
  },
  {
    "objectID": "posts/conditioning-multivariate-normal.html#appendix",
    "href": "posts/conditioning-multivariate-normal.html#appendix",
    "title": "Starting (on finite domains) with Gaussian processes",
    "section": "Appendix",
    "text": "Appendix\n\nMatrix of regression coefficients\nLet’s take a quick look at the matrix of regression coefficients, \\(\\Sigma_{12}\\Sigma_{22}^{-1}\\).\nWe could implement it via inversion, but there is a better solution.\nNamely, note that if \\(X=\\Sigma_{12} \\Sigma_{22}^{-1}\\), then\n\\[\n  \\Sigma_{22} X^T = \\Sigma_{22} \\Sigma_{22}^{-1} \\Sigma_{12}^T = \\Sigma_{12}^T\n\\]\nHence, \\(X^T\\) is a solution to a matrix equation, which we can implement using scipy.linalg.solve. This is considered a better practice as it increases the numerical precision and can be faster (which is visible for large matrices; for small matrices the solution using matrix inversion was often faster).\n\n\nCode\nimport time\nimport numpy as np\nimport numpy.linalg as npla\nimport scipy.linalg as spla\n\n\ndef calc_inv(sigma_12, sigma_22):\n  return sigma_12 @ npla.inv(sigma_22)\n\n\ndef calc_solve(sigma_12, sigma_22):\n  return spla.solve(sigma_22, sigma_12.T).T\n\nrng = np.random.default_rng(42)\n\nn_examples = 3\nsize = 20\nm = 10\n\n_coefs = rng.normal(size=(n_examples, size, size))\nsigmas = np.einsum(\"kab,kac-&gt;kbc\", _coefs, _coefs)\nfor sigma in sigmas:\n  assert np.min(npla.eigvals(sigma)) &gt; 0\n\n  sigma_12 = sigma[:m, m:]\n  sigma_22 = sigma[m:, m:]\n\n  sol1 = calc_inv(sigma_12, sigma_22)\n  sol2 = calc_solve(sigma_12, sigma_22)\n\n  assert np.allclose(sol1, sol2), \"Solutions differ.\""
  },
  {
    "objectID": "posts/conditioning-multivariate-normal.html#footnotes",
    "href": "posts/conditioning-multivariate-normal.html#footnotes",
    "title": "Starting (on finite domains) with Gaussian processes",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nFor every non-zero \\(x\\in \\mathbb R^n\\) we have \\(x^T\\Sigma x &gt; 0\\), where the inequality is strict. As \\(\\Sigma\\) is a real symmetric matrix, one of the versions of the spectral theorem yields a decomposition \\(\\Sigma = R^TDR\\) for a diagonal matrix \\(D\\) and orthogonal \\(R\\). Hence, equivalently, we all eigenvalues have to be positive. See this link for more discussion.↩︎\nI like to call them “query”, “keys” and “values” vectors, which makes the language a bit more similar to transformers. From that we just need one conditioning operation:↩︎\nWhy is it called in this manner? What are the slopes of \\(\\mu'\\) change, when we vary observed values \\(y\\)?↩︎"
  },
  {
    "objectID": "posts/no-free-lunch-in-research.html",
    "href": "posts/no-free-lunch-in-research.html",
    "title": "The no free lunch theorem for scientific research",
    "section": "",
    "text": "Yesterday David and I went for pizza after work. As typical for our conversations, we spent quite some time discussing applied statistics and machine learning, and reached our usual conclusion that logistic regression is a wonderful model in so many problems.\nHowever, finding logistic regression or other “simple” methods in research papers can be quite hard, as we tend to look for methodological novelty. As Kristin Lennox nicely summarized, “you don’t get a lot of points for doing really good statistics on really important problems, if these statistics were invented in 1950s”. (In particular, Cynthia Rudin investigated how much applied research goes into the most presitigious machine learning conferences).\nThis is one of the reasons for the phenomenon which everybody in the field knows too well: from time to time you take a paper claiming state-of-the-art performance (“They are 0.02% better on CIFAR-10 than others! Let’s apply it to my problem”), and then find out that the method requires heavy hyperparameter tuning and hours of playing with the brittleness that makes the method impossible to use in practice. And, what’s even worse, the performance isn’t that different from a simple baseline.\nSimilarly, there are voices from many statisticians raising the issue that several of the grandiose results, which often involve solving important problems with the state-of-the-art methodology, may be simply invalid.\nTo summarize, a perfect paper should use novel methodology, aim at solving an important problem, and be correct (which should go without saying). The no free lunch of scientific research says that you can pick two out of three, at most.\nThis “theorem” is not very serious and is, of course, not universal – there exist great papers, but achieving all three goals in one manuscript is very hard to execute and they are exceptions, not the standard. Additionally, I don’t want to dichotomise here: methodological novelty, problem importance and correctness have many facets and subtleties (and may also be hard to assess upfront!), so it’s better to think about the level of each of these traits desired in a study.\nAs a first-order approximation, I found myself usually doing research on either novel methodology (illustrated on toy problems, without direct applications) or working on problems, which I find practical and important, but which require standard and well-trusted tools (at least as a starting point and a baseline).\nOn a more positive note, some novel (as of today) methods will have become standard and well-trusted tools in the coming years, with a lot practical impact to come. And practical problems often lead to improvement on existing methods or asking fundamental questions (see Pasteur’s quadrant). And they usually are much harder to solve, than it seems at the beginning! Let’s finish with a quote from Andrew Gelman: “Whenever I have an applied project, I’m always trying to do the stupidest possible thing, that will allow me to get out of the project alive. Unfortunately, the stupidest possible thing, that could possibly work, always seems to be a little more complicated, than the most complicated thing I already know how to do.”"
  },
  {
    "objectID": "posts/getting-started-with-Bayesian-statistics.html",
    "href": "posts/getting-started-with-Bayesian-statistics.html",
    "title": "Getting started with Bayesian statistics",
    "section": "",
    "text": "Getting started\n\nTake any two examples from PyMC Gallery. For example, GLM and hierarchical partial pooling. Implement them in a Jupyter notebook. Rather than copying and running the code, type it on your own and think what it is supposed to do.\nRead Michael Betancourt’s Inferring gravity from data. Reproduce it in PyMC — the data are available on GitHub.\nIf you would like a lecture series to watch, there’s Richard McElreath’s Statistical rethinking.\n\n\n\nConsult regularly\n\nLearning Bayesian Statistics is a truly excellent podcast hosted by Alexandre Andorra. Many leading statisticians, including Frank Harrell, Jessica Hullman, Kevin Murphy and Aki Vehtari. I find it the best place to learn about various perspectives on modelling techniques and important problems people are working on. I also enjoy listening to Data and Science with Glen Wright Colopy, which covers a wide range of topics and featured many prominent statisticians, such as Deborah Mayo, Chris Holmes, Andrew Gelman and David Dunson.\nAndrew Gelman’s blog and Frank Harrell’s blog.\nAnd, of course, whenever I open Bayesian data analysis, Probabilistic machine learning and Michael Betancourt’s notes, I learn something new.\n\n\n\nHandbooks\n\nGeneral references, covering many topics\n\nBayesian Data Analysis from Andrew Gelman et al.\nBiostatistics for Biomedical Research written by Frank Harrell.\nMichael Betancourt’s notes.\nProbabilistic Machine Learning written by Kevin Murphy.\n\n\n\nPrincipled modelling workflow\n\nThe Bayesian workflow manuscript.\nMichael Betancourt’s workflow description.\nKris Sankaran and Susan Holmes wrote a great paper Generative models: an interdisciplinary perspective.\nThere’s David Blei’s Build, compute, critique, repeat paper.\n\n\n\nInference methods\n\nVariational inference: a review for statisticians.\nHandbook of Markov chain Monte Carlo (unfortunately, this book is not freely available).\nAn introduction to sequential Monte Carlo (unfortunately, this book is not freely available).\n\n\n\n\nGreat to watch\nIf you are in mood for something as engaging as a TV series episode, but more informative, take a look at these talks:\n\nDavid Dunson’s Debunking the hype.\nKristin Lennox’s Everything wrong with statistics.\nAndrew Gelman’s Solve all your statistics problems using p-values."
  },
  {
    "objectID": "posts/dirichlet-process.html",
    "href": "posts/dirichlet-process.html",
    "title": "The Dirichlet process",
    "section": "",
    "text": "In this post we will quickly review different constructions of the Dirichlet process, following Teh et al. (2006) and Gelman et al. (2013, chap. 23)."
  },
  {
    "objectID": "posts/dirichlet-process.html#finite-dimensional-dirichlet-prior",
    "href": "posts/dirichlet-process.html#finite-dimensional-dirichlet-prior",
    "title": "The Dirichlet process",
    "section": "Finite-dimensional Dirichlet prior",
    "text": "Finite-dimensional Dirichlet prior\nConsider the simplest Gaussian mixture model: there are several normal distributions with unit variance \\(\\mathcal N(\\mu_k, 1)\\) for \\(k\\in \\{1, \\dotsc, K\\}\\) and mixture proportions vector \\(\\pi = (\\pi_1, \\dotsc, \\pi_K)\\) with \\(\\pi_k\\ge 0\\) and \\(\\sum_k \\pi_k=1\\).\nA convenient prior for \\(\\pi\\) is the Dirichlet distribution. We put some \\(F_0\\) prior on the parameters \\(\\{\\mu_k\\}\\) of the model, so the generative process looks like: \\[\\begin{align*}\n  \\pi \\mid \\alpha &\\sim \\mathrm{Dirichlet}(\\alpha_1, \\dotsc, \\alpha_K)\\\\\n  \\mu_k \\mid F_0 &\\sim F_0, & k=1, \\dotsc, K\\\\\n  Z_n \\mid \\pi &\\sim \\mathrm{Categorical}(\\pi_1, \\dotsc, \\pi_K), & n=1, \\dotsc, N\\\\\n  X_n\\mid Z_n=z_n, \\{\\mu_k\\} &\\sim \\mathcal N(\\mu_{z_n}, 1),\\quad & n=1, \\dotsc, N.\n\\end{align*}\\]\n\nAnother point of view\nRather than using individual random variables \\(Z_n\\) and a shared set of parameters \\(\\{\\mu_k\\}\\) we could reparametrize the model to use individual means \\(\\tilde \\mu_n = \\mu_{Z_n}\\). In other words, we could consider a probability measure with atoms ${_k}$ given by \\[F = \\sum_{k=1}^K \\pi_k \\delta_{\\mu_k}.\\]\nIf we only know the Dirichlet weights vector \\((\\alpha_1, \\dotsc, \\alpha_K)\\) and the base measure \\(F_0\\) we can think of \\(F\\) as of a random probability measure generated according to \\[\\begin{align*}\n  \\pi \\mid \\alpha &\\sim \\mathrm{Dirichlet}(\\alpha_1, \\dotsc, \\alpha_K)\\\\\n  \\mu_k &\\sim F_0, \\quad k = 1, \\dotsc, K\\\\\n  F &:= \\sum_{k=1}^K \\pi_k \\delta_{\\mu_k}.\n\\end{align*}\\]\nThen sampling individual data points amounts to the following model with \\(n=1, \\dotsc, N\\): \\[\\begin{align*}\n  F\\mid \\alpha, F_0 &\\sim \\text{the procedure above}\\\\\n  \\theta_n \\mid F &\\sim F, \\\\\n  X_n\\mid \\theta_n &\\sim \\mathcal N(\\theta_n, 1).\n\\end{align*}\\]\nNote that the values of \\(\\theta_n\\) come from the set \\(\\{\\mu_1, \\dotsc, \\mu_K\\}\\) as \\(F\\) is atomic."
  },
  {
    "objectID": "posts/dirichlet-process.html#dirichlet-process-prior",
    "href": "posts/dirichlet-process.html#dirichlet-process-prior",
    "title": "The Dirichlet process",
    "section": "Dirichlet process prior",
    "text": "Dirichlet process prior\n\nStick-breaking construction\nWith the following example in mind we will pass now to a general distribution \\(F_0\\) defined over some infinite space \\(\\mathcal M\\) (which can be \\(\\mathbb R\\) as above) and a single positive parameter \\(\\alpha &gt; 0\\).\nWe will generate a random measure \\(F\\) from \\(F_0\\) using the construction known as the Dirichlet process.\nSample for \\(k=1, 2, \\dotsc\\) \\[\\begin{align*}\nv_k \\mid \\alpha &\\sim \\mathrm{Beta}(1, \\alpha)\\\\\n\\mu_k \\mid F_0 &\\sim F_0\n\\end{align*}\\] and define \\[\\begin{align*}\n  p_1 &= v_1\\\\\n  p_k &= v_k \\prod_{i=1}^{k-1} (1-v_k) \\quad \\text{for } k\\ge 2,\\\\\n  F &= \\sum_{k=1}^\\infty p_k \\delta_{\\mu_k}\n\\end{align*}\\]\nWith probability 1 it holds that \\[\\sum_{k=1}^\\infty  p_k = 1,\\] i.e., \\((p_k)\\) is a valid proportions vector.\nWe say that the distribution \\(F\\) was drawn from the Dirichlet process: \\[F \\sim \\mathrm{DP}(\\alpha, F_0).\\]\n\n\nInfinite limit\nThe atomic distributions generated with finite-dimensional proportions \\((\\pi_k)_{k=1, 2, \\dotsc, K}\\) and infinite sequence of weights \\((p_k)_{k=1, 2, \\dotsc, \\infty}\\) look optically similar. There is a close relation between these two generative processes.\nConsider a random measure \\(F^K\\) defined using a symmetric Dirichlet distribution: \\[\\begin{align*}\n  \\pi^K \\mid \\alpha &\\sim \\mathrm{Dirichlet}(\\alpha/K, \\cdots, \\alpha/K)\\\\\n  \\mu^K_k \\mid F_0 &\\sim F_0\\\\\n  F^K &= \\sum_{k=1}^K \\pi^K_k\\delta_{\\mu^K_k}\n\\end{align*}\\]\nNow if \\(F^{\\infty} \\sim \\mathrm{DP}(\\alpha, F_0)\\) and \\(u\\) is any measurable function integrable with respect to \\(F_0\\), then the sequence of random variables \\[ \\int_{\\mathcal M} u\\, \\mathrm{d} F^{K} \\] converges in distribution (that is, weakly) to \\[ \\int_{\\mathcal M} u\\, \\mathrm{d} F^{\\infty}.\\]\n\nWhere the difference really is\nWe see that \\((p_k)\\) looks deceptively similar as \\((\\pi_k^K)\\) for large \\(K\\). There are some differences, though. First of all, \\((p_k)\\) is infinite and the number of atoms appearing in the analysis of a particular data set is implicitly controlled by the number of data points. If \\(F_0\\) is non-atomic,one can expect \\(O(\\alpha\\log N)\\) atoms in a data set with \\(N\\) points. In the finite-dimensional case more than \\(K\\) clusters are impossible.\nHowever, for \\(K\\gg N\\) it’s natural to expect that several entries from \\((\\pi^K_k)\\) should be matching several entries of \\((p_k)\\). However, the intuition that \\(p_1 = \\pi_1^K\\), \\(p_2 = \\pi_2^K\\), … is wrong. In the stick-breaking construction of the Dirichlet process we expect the first few entries to have the most of the mass, while in the finite-dimensional case the Dirichlet prior is symmetric — we don’t know which weights \\(\\pi_k^K\\) will have vanishing mass.\nAlthough it seems obvious I spent quite some time trying to understand why the stick-breaking sampling procedure from the Dirichlet distribution gives different results!\nThe stick-breaking sampling procedure for the \\(\\mathrm{Dirichlet}(\\alpha/K, \\dotsc, \\alpha/K)\\) distribution works as follows: \\[\\begin{align*}\n  u_k &\\sim \\mathrm{Beta}( \\alpha/K, \\alpha\\cdot (1-k/K) )\\\\\n  \\pi_1 &= u_1\\\\\n  \\pi_k &= u_k \\prod_{j &lt; k} (1-u_k), \\quad k = 2, \\dotsc, K-1\\\\\n  \\pi_K &= 1 - (\\pi_1 + \\dotsc + \\pi_{K-1})\n\\end{align*}\\]\nwhich for \\(k \\ll K\\) corresponds to sampling from (approximately) \\(\\mathrm{Beta}(\\alpha/K, \\alpha)\\), rather than \\(\\mathrm{Beta}(1, \\alpha)\\).\nPitman (1996) describes size-biased permutations, which perhaps can be used to establish link between \\((\\pi_k)\\) for large \\(K\\) and \\((p_k)\\), but I haven’t understood it yet.\n\n\n\nDefining property\nWe have seen in what sense the Dirichlet process prior can be thought as of an infinite-dimensional generalization of the Dirichlet prior. However, there is another link.\nRecall that the defining property of a Gaussian process is that it is a continuous-time stochastic process \\(\\{X_t\\}_{t\\in [0, 1]}\\) such that for every finite set of indices \\(t_1, t_2, \\dotsc, t_m\\) random vector \\((X_{t_1}, \\dotsc, X_{t_m})\\) is distributed according to multivariate normal distribution. (In particular every \\(X_t\\) is a normal random variable). While this defining property is not sufficient without a proof of existence (e.g., an explicit construction), it is useful in many calculations involving them.\nWe will now give the defining property of the Dirichlet process. Take a probability measure \\(F_0\\) over \\(\\mathcal M\\) and the concentration parameter \\(\\alpha &gt; 0\\). We say that \\(\\mathrm{DP}(\\alpha, F_0)\\) is a Dirichlet process if every sample \\(F\\sim \\mathrm{DP}(\\alpha, F_0)\\) is a probability measure over \\(\\mathcal M\\) such that for every partition \\(A_1, \\cdots, A_m\\) of \\(\\mathcal M\\) the following holds: \\[ \\left( F(A_1), \\dotsc, F(A_K) \\right) \\sim \\mathrm{Dirichlet}\\big(\\alpha F_0(A_1), \\dotsc, \\alpha F_0(A_K) \\big) \\]\nIn particular if \\(A\\subseteq \\mathcal X\\) is any measurable subset, then we can use the partition \\(\\{A, \\mathcal M\\setminus A\\}\\) to get \\[ F(A) \\sim \\mathrm{Beta}\\big( \\alpha F_0(A), \\alpha(1-F_0(A)) \\big),\\] so that \\[\\mathbb E[ F(A) ] = F_0(A)\\] and \\[\\mathrm{Var}[F(A)] = \\frac{ F_0(A)\\cdot (1-F_0(A)) }{1+\\alpha}\\]\nHence, each draw \\(F\\) is centered around \\(F_0\\) and the variance is small for large parameter values \\(\\alpha\\).\n\n\nPólya urn scheme\nFinally, we give an interpretation in terms of Pólya urn scheme.\nAbove we considered the sampling process from the finite-dimensional Dirichlet distribution: \\[\\begin{align*}\n  F\\mid \\alpha, F_0 &\\sim \\text{construct atomic measure},\\\\\n  \\theta_n \\mid F &\\sim F,\n\\end{align*}\\] where each of the \\(\\theta_n\\) was actually some atom of the distribution \\(\\mu_k\\).\nThis interpretation is also easy to understand when the atomic measure \\(F\\) is drawn from the Dirichlet process using the stick-breaking construction.\nConsider now a sampling procedure of \\(\\theta_n\\) where we do not have direct access to \\(F\\), but only to the distribution \\(F_0\\), concentration parameter \\(\\alpha &gt; 0\\) and previous draws \\(\\theta_1, \\dotsc, \\theta_{n-1}\\). It holds that \\[\\theta_n \\mid \\alpha, F_0, \\theta_1, \\dotsc, \\theta_{n-1} \\sim \\frac{\\alpha}{ (n-1) + \\alpha }F_0 + \\sum_{u=1}^{n-1} \\frac{1}{(n-1)+\\alpha}\\delta_{ \\theta_n }.\\]\nIf \\(\\alpha\\) is a positive integer we can interpret this sampling procedure as follows: we want to draw the \\(n\\)th ball and we have an urn with \\(\\alpha\\) transparent balls and \\(n-1\\) balls of different colors. We draw a random ball. If it is transparent, we use \\(G_0\\) to sample a colored ball from \\(F_0\\), note it down, and put it to the urn.\nThis also suggests a clustering property: if there is a color \\(\\mu_k\\) such that there are already \\(m_k\\) balls inside the urn (i.e., \\(m_k\\) is the number of indices \\(1\\le i\\le n-1\\) such that \\(\\theta_i = \\mu_k\\)), then we have a larger chance to draw a ball of this color: \\[\\theta_n \\mid \\alpha, F_0, \\theta_1, \\dotsc, \\theta_{n-1} \\sim \\frac{\\alpha}{(n-1) + \\alpha}F_0 + \\sum_{k} \\frac{m_k}{ (n-1) + \\alpha } \\delta_{ \\mu_k }.\\]\nWe also see that for the concentration parameter \\(\\alpha \\gg n\\) this sampling procedure approximates independent sampling from \\(F_0\\).\n\nAsymptotic number of clusters\nThe above formulation can be used to argue why the number of clusters grows as \\(O(\\alpha\\log n)\\) if \\(F_0\\) is non-atomic. Define \\(D_1 = 1\\) and for \\(n\\ge 1\\) \\[D_n = \\begin{cases} 1 &\\text{ if } \\theta_n \\notin \\{\\theta_1, \\dotsc, \\theta_{n-1}\\}\\\\\n0 &\\text{otherwise}\\end{cases}\\] From the above construction we know the probability of drawing a new atom, so \\[\\mathbb E[D_n] = \\alpha / (\\alpha + n-1)\\] The number of distinct atoms in \\(F\\) is then \\[\\mathbb E[C_n] = \\mathbb E[D_1 + \\dotsc + D_n] = \\alpha \\sum_{i=1}^n \\frac{1}{\\alpha + n - 1}.\\] We recognise that this sum is similar to the harmonic series and (this can be proven formally) also grows as \\(O(\\log n)\\), so that \\(\\mathbb E[C_n] = O(\\alpha\\log n)\\). To provide a more precise result: \\[\\lim_{n\\to\\infty}\\frac{ \\mathbb E[C_n] }{\\alpha \\log n} = 1.\\] For this and related results consult these notes.\n\n\n\nChinese restaurant process\nThe procedure above is also closely related to the Chinese restaurant process, where the metaphor is that there are \\(K\\) occupied tables (where a dish \\(\\mu_k\\) is served) and there are \\(m_k\\) people sitting around the \\(k\\)th table. When a new customer enters the restaurant, they can either join an existing table (with probability proportional to \\(m_k\\)) or start a new (\\((K+1)\\)th) table with probability proportional to \\(\\alpha\\), where a new dish \\(\\mu_{K+1}\\sim F_0\\) will be served."
  },
  {
    "objectID": "posts/dirichlet-process.html#afterword",
    "href": "posts/dirichlet-process.html#afterword",
    "title": "The Dirichlet process",
    "section": "Afterword",
    "text": "Afterword\nThe Dirichlet process is a useful construction, which can be used as a nonparametric prior in clustering problems — instead of specifying a fixed number of clusters one can specify the growth rate via \\(\\alpha\\).\nIn practice the results (including the number of inferred clusters in a particular data set) need to be treated with caution: the clusters found in the data set do not need to have the “real world” meaning (or perhaps “clusters” are a wrong abstration at all, with heterogeneity attributable e.g., to some continuous covariates which could be measured). Careful validation is often needed, epsecially that these models may be non-robust to misspecification (see this paper on coarsening) or the inferences may be hard to do (see this overview of their intrinsic non-identifiability).\nAnyway, although difficult, clustering can provide useful information about a given problem, so we often need do it. For example, take a look at this application of Chinese restaurant process to the clustering of single-cell DNA profiles."
  },
  {
    "objectID": "posts/beta-bernoulli.html",
    "href": "posts/beta-bernoulli.html",
    "title": "Beta-Bernoulli distribution",
    "section": "",
    "text": "I have already demonstrated that I don’t know how to properly toss a coin. Let’s do that again."
  },
  {
    "objectID": "posts/beta-bernoulli.html#famous-bernoulli-and-the-company",
    "href": "posts/beta-bernoulli.html#famous-bernoulli-and-the-company",
    "title": "Beta-Bernoulli distribution",
    "section": "Famous Bernoulli and the company",
    "text": "Famous Bernoulli and the company\n\nCounting the distributions\nBefore we go any further: how many distributions can give outcomes from the set \\(\\{0, 1\\}\\)?\nIn other words, we have a measurable space \\(\\{0, 1\\}\\) (such that all four subsets are measurable) and we would like to know how many probability measures exist on this space. We have \\(P(\\varnothing) = 0\\) and \\(P(\\{0, 1\\}) = 1\\) straight from the definition of a probability measure. As we need to have \\(P(\\{0\\}) + P(\\{1\\}) = 1\\) we see that that there is a bijection between these probability measures and numbers from the set \\([0, 1]\\), given by \\(P_b(\\{1\\}) = b\\) for any bias \\(b\\in [0, 1]\\). This distribution is called \\(\\mathrm{Bernoulli}(b)\\) and it’s easy to prove that if \\(X\\sim \\mathrm{Bernoulli}(b)\\), then \\[\n\\mathbb E[X] = P(X=1) = b.\n\\]\nHence, the first moment fully determines any distribution on \\(\\{0, 1\\}\\).\nDerivation of variance is quite elegant, once one notices that if \\(X\\sim \\mathrm{Bernoulli}(b)\\), then also \\(X^2\\sim \\mathrm{Bernoulli}(b)\\), because it has to be some Bernoulli and \\(P(X^2=1) = P(X=1)\\). Then: \\[\n\\mathbb V[X] = \\mathbb E[X^2] - \\mathbb E[X]^2 = b - b^2 = b(1-b).\n\\]\nBy considering both cases, we see that for every outcome \\(x\\in \\{0, 1\\}\\), the likelihood is given by \\[\n\\mathrm{Bernoulli}(x\\mid b) = b^x(1-b)^{1-x}.\n\\]\n\n\nA few coins\nThis characterization of distributions over \\(\\{0, 1\\}\\) is very powerful.\nConsider the following problem: we have \\(K\\) coins with biases \\(b_1, \\dotsc, b_K\\) and we throw a loaded dice which can give \\(K\\) different outcomes, each with probability \\(d_1, \\dotsc, d_K\\) (which, of course, sum up to \\(1\\)) to decide which coin we will toss. What is the outcome of this distribution? It’s, of course, a coin toss outcome, that is a number from the set \\(\\{0, 1\\}\\). Hence, this has to be some Bernoulli distribution. Which one? Bernoulli distributions are fully determined by their expectations and the expectation in this case is given by a weighted average \\(\\bar b = b_1 d_1 + \\cdots + b_K d_K\\). In other words, we can replace a loaded dice and \\(K\\) biased coins with just a single biased coin.\nTo have more equations, the first procedure corresponds to \\[\\begin{align*}\nD &\\sim \\mathrm{Categorical}(d_1, \\dotsc, d_K)\\\\\nX \\mid D &\\sim \\mathrm{Bernoulli}(b_D)\n\\end{align*}\\] and the second one to \\[\nY \\sim \\mathrm{Bernoulli}(\\bar b),\n\\] with \\(\\bar b = b_1d_1 + \\cdots + b_Kd_K\\).\nBoth of these distributions are the same, i.e., \\(\\mathrm{law}\\, X = \\mathrm{law}\\, Y\\).\nIn particular, the likelihood has to be the same, proving an equality \\[\n\\sum_{k=1}^K d_k\\, b_k^x(1-b_k)^{1-x} = \\bar b^x(1-\\bar b)^{1-x}\n\\] for every \\(x\\in \\{0, 1\\}\\).\n\n\nEven more coins\nEven more interestingly, consider infinitely many coins with different biases, which are chosen according to the beta distribution. Once the coin is chosen, we toss it: \\[\\begin{align*}\nB &\\sim \\mathrm{Beta}(\\alpha, \\beta)\\\\\nX\\mid B &\\sim \\mathrm{Bernoulli}(B)\n\\end{align*}\n\\]\nThis is a continuous mixture, which we could perhaps call \\(\\mathrm{BetaBernoulli}(\\alpha, \\beta)\\)… if it wasn’t just a plain Bernoulli distribution with bias \\[\n\\mathbb E[X] = \\mathbb E[\\mathbb E[X\\mid B]] = \\mathbb E[B] = \\frac{\\alpha}{\\alpha + \\beta}.\n\\]\n\n\nNoisy communication channel\nLet’s consider an example involving plain Bernoulli distributions and a noisy communication channel.\nLet \\(X\\sim \\mathrm{Bernoulli}(b)\\) be an input variable. The binary output, \\(Y\\), is a noisy version of \\(X\\), with \\(\\alpha\\) controlling the false positive rate and \\(\\beta\\) the false negative rate: \\[\nP(Y = 1 \\mid X=1) = 1-\\beta, \\quad P(Y=1\\mid X=0) = \\alpha.\n\\]\nWe can write this model as: \\[\\begin{align*}\nX &\\sim \\mathrm{Bernoulli}(b)\\\\\nY \\mid X &\\sim \\mathrm{Bernoulli}( X(1-\\beta) + (1-X) \\alpha)\n\\end{align*}\n\\]\nIn fact, we have already seen this example: we can treat \\(X\\) as a special case of a loaded dice, indexing a finite mixture with just two components. Hence, the marginal distribution of \\(Y\\) is \\[\nY \\sim \\mathrm{Bernoulli}(b(1-\\beta) + (1-b) \\alpha).\n\\]\n\n\nTossing multiple coins\nWe know that a single coin toss characterises all probability distributions on the set \\(\\{0, 1\\}\\). However, once we consider \\(N\\ge 2\\) coin tosses, yielding outcomes in the set \\(\\{0, 1, 2, \\dotsc, N\\}\\), many different distributions will appear.\nWe mentioned some of these distributions in this post, but just for completeness at the end there is a list of standard distributions.\nSimilarly, if one is interested in modelling binary vectors, which are from the set \\(\\{0, 1\\}\\times \\{0, 1\\} \\cdots \\{0, 1\\}\\), many different distributions will appear. Let’s analyse an example below."
  },
  {
    "objectID": "posts/beta-bernoulli.html#two-deceptively-similar-distributions",
    "href": "posts/beta-bernoulli.html#two-deceptively-similar-distributions",
    "title": "Beta-Bernoulli distribution",
    "section": "Two deceptively similar distributions",
    "text": "Two deceptively similar distributions\n\nDenoising problem\nWe have a fixed bit \\(T \\in \\{0, 1\\}\\) and we observe its noisy measurements, with false negatives rate \\(\\beta\\) and false positive rate \\(\\alpha\\) (recall this section). We will write \\(c_0 = \\alpha\\) and \\(c_1 = 1-\\beta\\) and put some prior on \\(T\\): \\[\\begin{align*}\n  \\theta &\\sim \\mathrm{Beta}(\\alpha, \\beta)\\\\\n  T\\mid \\theta &\\sim \\mathrm{Bernoulli}(\\theta)\\\\\n  X_n\\mid T &\\sim \\mathrm{Bernoulli}( c_0(1-T) + c_1T) \\text{ for } n = 1, \\cdots, N\n\\end{align*}\n\\]\nLet’s use shorthand notation \\(\\mathbf{X} = (X_1, \\dotsc, X_N)\\) and note that the likelihood is: \\[\\begin{align*}\n  P(\\mathbf{X} \\mid T) &= \\prod_{n=1}^N P(X_n \\mid b(T) ) \\\\\n  &= \\prod_{n=1}^N b(T) ^{X_n} \\left(1-b(T)\\right)^{1-X_n} \\\\\n  &= b(T)^S \\left(1-b(T)\\right)^{N-S}\n\\end{align*}\n\\]\nwhere \\(b(T) = c_0(1-T) + c_1T\\) and \\(S = X_1 + \\cdots + X_N\\).\nAfter reading the discussion above, there is a natural question: why is \\(\\theta\\) introduced at all? For a simple denoising question, i.e., finding \\(P(T\\mid \\mathbf{X}) \\propto P(\\mathbf{X} \\mid T) P(T)\\) this parameter is not needed at all: \\(P(T)\\) is just a Bernoulli variable, with bias parameter \\(\\bar \\theta = \\alpha/(\\alpha + \\beta)\\). Then, \\[\n  P(T=1\\mid \\mathbf{X}) = \\frac{ c_1^S (1-c_1)^{N-S} \\cdot \\bar \\theta }{ c_1^S(1-c_1)^{N-S} \\cdot \\bar \\theta + c_0^S(1-c_0)^{N-S}\\cdot (1-\\bar \\theta) }.\n\\]\nLet’s quickly implement this formula and see how the posterior looks like if we start with an unbiased coin (i.e., \\(\\bar \\theta=0.5\\)), observe \\(S=N-1\\) successes and we vary noise level \\(\\alpha = \\beta = c_0 = 1 - c_1\\):\n\n\nCode\nfrom typing import Callable, NamedTuple\n\nimport jax\nimport jax.numpy as jnp\n\nimport numpyro\nimport numpyro.distributions as dist\nimport matplotlib.pyplot as plt\nplt.style.use(\"dark_background\")\n\n\ndef _binomial_loglikelihood(n: int, s: int, bias: float) -&gt; float:\n  return s * jnp.log(bias) + (n - s) * jnp.log1p(-bias)\n\n\nclass LikelihoodArgs(NamedTuple):\n  n: int  # Number of throws\n  s: int  # Number of successes, 0 &lt;= s &lt;= n\n  c0: float  # Probability of observing success if T = 0 (false positive rate) \n  c1: float  # Probability of observing success if T = 1 (true positive rate)\n\n\nclass BetaPriorArgs(NamedTuple):\n  alpha: float\n  beta: float\n\n  @property\n  def mean(self) -&gt; float:\n    return self.alpha / (self.alpha + self.beta)\n\n\ndef posterior_t(\n  bias: float,\n  like: LikelihoodArgs,\n) -&gt; float:\n  \"\"\"Calculates P(T | data),\n  where the prior is given by T ~ Bernoulli(bias).\"\"\"\n  n, s, c0, c1 = like.n, like.s, like.c0, like.c1\n  log_0 = _binomial_loglikelihood(n=n, s=s, bias=c0) + jnp.log1p(-bias)\n  log_1 = _binomial_loglikelihood(n=n, s=s, bias=c1) + jnp.log(bias)\n\n  return jax.nn.softmax(jnp.array([log_0, log_1]))\n\nfig, axs = plt.subplots(\n  1, 3, dpi=120, figsize=(8, 2.4), sharex=True, sharey=True,\n)\n\nfor ax in axs:\n  ax.spines[['top', 'right']].set_visible(False)\n  ax.set_xticks([0, 1])\n  ax.set_xlim([-0.5, 1.5])\n  ax.set_ylim([-0.1, 1.1])\n  ax.set_yticks([0, 0.25, 0.5, 0.75, 1])\n\n\nfor ax, n in zip(axs, [1, 3, 5]):\n  ax.set_title(f\"$N = {n}$, $S = {n-1}$\")\n  for i, noise in enumerate([0.01, 0.1, 0.25, 0.45]):\n    like = LikelihoodArgs(\n      n=n,\n      s=n - 1,\n      c0=noise,\n      c1=1-noise,\n    )\n    posterior = posterior_t(bias=0.5, like=like)\n\n    ax.plot([0, 1], posterior, label=f\"{noise:.2f}\", c=f\"C{i}\")\n    ax.scatter([0, 1], posterior, c=f\"C{i}\", s=4)\n  \nax = axs[-1]\nax.legend(frameon=False, bbox_to_anchor=(1.05, 1))\nfig.tight_layout()\n\n\n\n\n\nThis formula is elegant: we can use it to solve the denoising problem, i.e., infer the value of the \\(T\\) variable. Why did we introduce the \\(\\theta\\) variable with its own prior? We may be interested not only in inferring a missing coin throw, but also in learning about the unknown bias \\(\\theta\\) of the coin from this experiment.\nFor example, if \\(T\\) was directly observed, we would have \\(P(\\theta \\mid T) = \\mathrm{Beta}(\\theta \\mid \\alpha + T, \\beta + (1-T) )\\):\n\n\nCode\nfig, axs = plt.subplots(1, 3, figsize=(5, 2.3), dpi=200, sharex=True, sharey=True)\n\nfor ax in axs:\n  ax.spines[['top', 'right']].set_visible(False)\n  ax.set_xlim([-0.02, 1.02])\n\nbins = jnp.linspace(0, 1, 31)\n\nkey = jax.random.PRNGKey(1)\nkey, *subkeys = jax.random.split(key, 4)\n\nax = axs[0]\nax.set_title(\"Prior\\nBeta(1, 1)\")\nsamples = dist.Beta(1, 1).sample(subkeys[0], sample_shape=(10_000,))\nax.hist(samples, bins=bins, density=True)\n\nax = axs[1]\nax.set_title(\"Posterior for $T=0$\\nBeta(1, 2)\")\nsamples = dist.Beta(1, 2).sample(subkeys[0], sample_shape=(10_000,))\nax.hist(samples, bins=bins, density=True)\n\nax = axs[2]\nax.set_title(\"Posterior for $T=1$\\nBeta(2, 1)\")\nsamples = dist.Beta(2, 1).sample(subkeys[0], sample_shape=(10_000,))\nax.hist(samples, bins=bins, density=True)\n\nfig.tight_layout()\n\n\n\n\n\nAs \\(T\\) is not directly observed, we have to look at \\(P(\\theta \\mid \\mathbf{X})\\): \\[\\begin{align*}\n  P(\\theta \\mid \\mathbf{X}) &= \\sum_{t} P(\\theta, T=t \\mid \\mathbf{X}) \\\\\n  &= \\sum_{t} P(\\theta \\mid T=t,  \\mathbf{X})P(T=t\\mid \\mathbf{X})\\\\\n  &= \\sum_{t} P(\\theta \\mid T=t) P(T=t\\mid \\mathbf{X}),\n\\end{align*}\n\\] which is therefore a mixture of \\(\\mathrm{Beta}(\\alpha+1, \\beta)\\) and \\(\\mathrm{Beta}(\\alpha, \\beta + 1)\\) distributions:\n\n\nCode\n_noise = 0.2\n\ndef create_posterior_theta(\n  prior: BetaPriorArgs,\n  like: LikelihoodArgs,\n):\n  mixing = dist.Categorical(\n    probs=posterior_t(\n      bias=prior.mean, like=like\n  ))\n  return dist.MixtureSameFamily(\n    mixing_distribution=mixing,\n    component_distribution=dist.Beta(\n      # alpha\n      concentration1=jnp.array([prior.alpha, prior.alpha + 1]),\n      # beta\n      concentration0=jnp.array([prior.beta + 1, prior.beta]),\n    )\n  )\n\nfig, axs = plt.subplots(1, 3, figsize=(5, 2.3), dpi=200, sharex=True, sharey=True)\n\nfor ax in axs:\n  ax.spines[['top', 'right']].set_visible(False)\n  ax.set_xlim([-0.02, 1.02])\n\nkey, *subkeys = jax.random.split(key, 4)\n\nax = axs[0]\nax.set_title(\"$N=S=1$\")\nd = create_posterior_theta(\n  prior=BetaPriorArgs(alpha=1.0, beta=1.0),\n  like=LikelihoodArgs(\n    n=1, s=1,\n    c0=_noise, c1=1-_noise,\n  )\n)\nsamples = d.sample(subkeys[0], sample_shape=(10_000,))\nax.hist(samples, bins=bins, density=True)\n\nax = axs[1]\nax.set_title(\"$N=S=2$\")\nd = create_posterior_theta(\n  prior=BetaPriorArgs(alpha=1.0, beta=1.0),\n  like=LikelihoodArgs(\n    n=2, s=2,\n    c0=_noise, c1=1-_noise,\n  )\n)\nsamples = d.sample(subkeys[0], sample_shape=(10_000,))\nax.hist(samples, bins=bins, density=True)\n\nax = axs[2]\nax.set_title(\"$N=S=100$\")\nd = create_posterior_theta(\n  prior=BetaPriorArgs(alpha=1.0, beta=1.0),\n  like=LikelihoodArgs(\n    n=100, s=100,\n    c0=_noise, c1=1-_noise,\n  )\n)\nsamples = d.sample(subkeys[0], sample_shape=(10_000,))\nax.hist(samples, bins=bins, density=True)\n\nfig.tight_layout()\n\n\n\n\n\nImportantly, we see that the posterior on \\(\\theta\\) won’t shrink even for large \\(N\\): increasing \\(N\\) affects how well we can determine the outcome of the throw \\(T\\). But even if we observe \\(T\\) perfectly, it’s only one throw, so it doesn’t give too much information on the bias \\(\\theta\\).\nLet’s now look at this problem also from a bit different perspective. If we want to update \\(\\theta\\) directly from the observed data \\(\\mathbf{X}\\), we can marginalise \\(T\\) out in the likelihood: \\[\\begin{align*}\n  P(\\mathbf{X} \\mid \\theta ) &= \\sum_{t} P(\\mathbf{X} \\mid T=t) P(T=t\\mid \\theta) \\\\\n  &= \\theta P(\\mathbf{X} \\mid T=1) + (1-\\theta) P( \\mathbf{X}\\mid T=0) \\\\\n  &= \\theta c_1^{S}(1-c_1)^{N-S} + (1-\\theta) c_0^S(1-c_0)^{N-S}.\n\\end{align*}\n\\]\nWe have now only one continuous variable and we could use Hamiltonian Monte Carlo to sample from the distribution \\(P(\\theta \\mid \\mathbf{X})\\). We have already determined it analytically, as a mixture of beta distributions, but let’s quickly compare the results:\n\n\nCode\nfrom numpyro.infer import MCMC, NUTS\n\n\ndef create_model(like: LikelihoodArgs, prior: BetaPriorArgs):\n  theta = numpyro.sample(\"theta\", \n    dist.Beta(concentration1=prior.alpha, concentration0=prior.beta)\n  )\n  log1 = _binomial_loglikelihood(n=like.n, s=like.s, bias=like.c1) + jnp.log(theta)\n  log0 = _binomial_loglikelihood(n=like.n, s=like.s, bias=like.c0) + jnp.log1p(-theta)\n  numpyro.factor(\"loglikelihood\", jnp.logaddexp(log1, log0))\n\n\nprior_args = BetaPriorArgs(\n  alpha=1.0,\n  beta=1.0,\n)\n\nlike_args = LikelihoodArgs(\n  n=3,\n  s=2,\n  c0=0.1,\n  c1=0.9,\n)\n\nfig, axs = plt.subplots(1, 3, figsize=(5, 2.3), dpi=200, sharex=True, sharey=True)\nfor ax in axs:\n  ax.spines[['top', 'right']].set_visible(False)\n  ax.set_xlim([-0.02, 1.02])\n\nkey, *subkeys = jax.random.split(key, 4)\n\n\nax = axs[0]\nax.set_title(\"$P(\\\\theta \\\\mid T=1)$\")\nsamples = dist.Beta(2, 1).sample(subkeys[0], sample_shape=(100_000,))\nax.hist(samples, bins=bins, density=True, color=\"C2\")\n\nax = axs[1]\nax.set_title(\"$P(\\\\theta\\\\mid \\\\mathbf{X})$\\n(explicit)\")\nd = create_posterior_theta(\n  like=like_args,\n  prior=prior_args,\n)\nsamples = d.sample(subkeys[1], sample_shape=(100_000,))\nax.hist(samples, bins=bins, density=True)\n\nax = axs[2]\nax.set_title(\"$P(\\\\theta\\\\mid \\\\mathbf{X})$\\n(HMC)\")\n\nnuts_kernel = NUTS(create_model)\nmcmc = MCMC(nuts_kernel, num_warmup=1000, num_samples=10_000, num_chains=2)\nmcmc.run(subkeys[2], like=like_args, prior=prior_args)\nsamples = mcmc.get_samples()[\"theta\"]\nax.hist(samples, bins=bins, density=True)\n\n\n/tmp/ipykernel_47585/1934137946.py:51: UserWarning: There are not enough devices to run parallel chains: expected 2 but got 1. Chains will be drawn sequentially. If you are running MCMC in CPU, consider using `numpyro.set_host_device_count(2)` at the beginning of your program. You can double-check how many devices are available in your system using `jax.local_device_count()`.\n  mcmc = MCMC(nuts_kernel, num_warmup=1000, num_samples=10_000, num_chains=2)\n  0%|          | 0/11000 [00:00&lt;?, ?it/s]warmup:   0%|          | 1/11000 [00:00&lt;3:01:55,  1.01it/s, 3 steps of size 1.40e+01. acc. prob=0.98]warmup:   6%|▌         | 642/11000 [00:01&lt;00:12, 807.37it/s, 1 steps of size 1.58e+00. acc. prob=0.79]sample:  12%|█▏        | 1287/11000 [00:01&lt;00:05, 1666.83it/s, 3 steps of size 9.13e-01. acc. prob=0.91]sample:  18%|█▊        | 1936/11000 [00:01&lt;00:03, 2528.66it/s, 1 steps of size 9.13e-01. acc. prob=0.91]sample:  24%|██▎       | 2585/11000 [00:01&lt;00:02, 3333.24it/s, 3 steps of size 9.13e-01. acc. prob=0.91]sample:  29%|██▉       | 3233/11000 [00:01&lt;00:01, 4040.97it/s, 7 steps of size 9.13e-01. acc. prob=0.92]sample:  35%|███▌      | 3873/11000 [00:01&lt;00:01, 4614.79it/s, 1 steps of size 9.13e-01. acc. prob=0.91]sample:  41%|████      | 4517/11000 [00:01&lt;00:01, 5084.11it/s, 1 steps of size 9.13e-01. acc. prob=0.91]sample:  47%|████▋     | 5158/11000 [00:01&lt;00:01, 5439.79it/s, 1 steps of size 9.13e-01. acc. prob=0.91]sample:  53%|█████▎    | 5800/11000 [00:01&lt;00:00, 5709.75it/s, 3 steps of size 9.13e-01. acc. prob=0.91]sample:  59%|█████▊    | 6445/11000 [00:01&lt;00:00, 5917.71it/s, 1 steps of size 9.13e-01. acc. prob=0.91]sample:  64%|██████▍   | 7088/11000 [00:02&lt;00:00, 6063.36it/s, 3 steps of size 9.13e-01. acc. prob=0.91]sample:  70%|███████   | 7733/11000 [00:02&lt;00:00, 6175.46it/s, 1 steps of size 9.13e-01. acc. prob=0.91]sample:  76%|███████▌  | 8376/11000 [00:02&lt;00:00, 6249.90it/s, 3 steps of size 9.13e-01. acc. prob=0.91]sample:  82%|████████▏ | 9018/11000 [00:02&lt;00:00, 6297.99it/s, 3 steps of size 9.13e-01. acc. prob=0.91]sample:  88%|████████▊ | 9660/11000 [00:02&lt;00:00, 6267.00it/s, 3 steps of size 9.13e-01. acc. prob=0.91]sample:  94%|█████████▎| 10295/11000 [00:02&lt;00:00, 6258.64it/s, 7 steps of size 9.13e-01. acc. prob=0.91]sample:  99%|█████████▉| 10927/11000 [00:02&lt;00:00, 6256.46it/s, 3 steps of size 9.13e-01. acc. prob=0.91]sample: 100%|██████████| 11000/11000 [00:02&lt;00:00, 4055.54it/s, 1 steps of size 9.13e-01. acc. prob=0.91]\n  0%|          | 0/11000 [00:00&lt;?, ?it/s]warmup:   6%|▌         | 653/11000 [00:00&lt;00:01, 6526.47it/s, 1 steps of size 1.27e+00. acc. prob=0.79]sample:  12%|█▏        | 1307/11000 [00:00&lt;00:01, 6531.20it/s, 3 steps of size 1.01e+00. acc. prob=0.89]sample:  18%|█▊        | 1961/11000 [00:00&lt;00:01, 6530.24it/s, 3 steps of size 1.01e+00. acc. prob=0.89]sample:  24%|██▍       | 2624/11000 [00:00&lt;00:01, 6566.90it/s, 3 steps of size 1.01e+00. acc. prob=0.89]sample:  30%|██▉       | 3281/11000 [00:00&lt;00:01, 6525.99it/s, 1 steps of size 1.01e+00. acc. prob=0.89]sample:  36%|███▌      | 3934/11000 [00:00&lt;00:01, 6506.50it/s, 1 steps of size 1.01e+00. acc. prob=0.89]sample:  42%|████▏     | 4587/11000 [00:00&lt;00:00, 6511.28it/s, 3 steps of size 1.01e+00. acc. prob=0.89]sample:  48%|████▊     | 5240/11000 [00:00&lt;00:00, 6516.41it/s, 3 steps of size 1.01e+00. acc. prob=0.89]sample:  54%|█████▎    | 5892/11000 [00:00&lt;00:00, 6501.81it/s, 3 steps of size 1.01e+00. acc. prob=0.89]sample:  60%|█████▉    | 6552/11000 [00:01&lt;00:00, 6529.73it/s, 3 steps of size 1.01e+00. acc. prob=0.89]sample:  66%|██████▌   | 7212/11000 [00:01&lt;00:00, 6549.27it/s, 1 steps of size 1.01e+00. acc. prob=0.89]sample:  72%|███████▏  | 7874/11000 [00:01&lt;00:00, 6568.25it/s, 1 steps of size 1.01e+00. acc. prob=0.89]sample:  78%|███████▊  | 8531/11000 [00:01&lt;00:00, 6539.46it/s, 1 steps of size 1.01e+00. acc. prob=0.89]sample:  84%|████████▎ | 9196/11000 [00:01&lt;00:00, 6570.76it/s, 1 steps of size 1.01e+00. acc. prob=0.89]sample:  90%|████████▉ | 9854/11000 [00:01&lt;00:00, 6532.52it/s, 3 steps of size 1.01e+00. acc. prob=0.89]sample:  96%|█████████▌| 10517/11000 [00:01&lt;00:00, 6559.21it/s, 3 steps of size 1.01e+00. acc. prob=0.89]sample: 100%|██████████| 11000/11000 [00:01&lt;00:00, 6538.11it/s, 3 steps of size 1.01e+00. acc. prob=0.89]\n\n\n(array([0.24749999, 0.28499999, 0.31799995, 0.39600002, 0.43200003,\n        0.5084998 , 0.55500003, 0.64650004, 0.63000004, 0.77400005,\n        0.81750005, 0.79799933, 0.86250005, 0.86850005, 1.00800096,\n        1.08899909, 1.08749909, 1.2570012 , 1.22699898, 1.25250119,\n        1.26899894, 1.34400128, 1.36049886, 1.4324988 , 1.53300146,\n        1.48199876, 1.48800142, 1.60799866, 1.74450166, 1.6785016 ]),\n array([0.        , 0.03333334, 0.06666667, 0.10000001, 0.13333334,\n        0.16666667, 0.20000002, 0.23333335, 0.26666668, 0.30000001,\n        0.33333334, 0.36666667, 0.40000004, 0.43333337, 0.4666667 ,\n        0.5       , 0.53333336, 0.56666672, 0.60000002, 0.63333338,\n        0.66666669, 0.70000005, 0.73333335, 0.76666671, 0.80000007,\n        0.83333337, 0.86666673, 0.90000004, 0.9333334 , 0.9666667 ,\n        1.        ]),\n &lt;BarContainer object of 30 artists&gt;)\n\n\n\n\n\nHMC agrees well with the mixture of beta distributions. We see that \\(P(\\theta \\mid \\mathbf{X})\\) is slightly more diffuse compared with the case when \\(T\\) is directly observed.\n\n\nA bit different model\nIn the model above for any single observation \\(X_n\\) we had \\[\nP(X_n\\mid \\theta) = \\theta c_1^{X_n}(1-c_1)^{1-X_n} + (1-\\theta) c_0^{X_n}(1-c_0)^{1-X_n}.\n\\]\nHowever, the joint likelihood was given by\n\\[\\begin{align*}\nP(\\mathbf{X} \\mid \\theta) &= \\theta \\prod_{n} c_1^{X_n}(1-c_1)^{1-X_n} + (1-\\theta) \\prod_{n} c_0^{X_n}(1-c_0)^{1-X_n} \\\\\n&= \\theta c_1^S (1-c_1)^{N-S} + (1-\\theta)c_0^S(1-c_0)^S,\n\\end{align*}\n\\]\nwhich is not the product of \\(\\prod_n P(X_n\\mid \\theta)\\), because all variables \\(X_n\\) were noisy observations of a single coin toss outcome \\(T\\).\nLet’s now consider a deceptively similar model: \\[\\begin{align*}\n  \\phi &\\sim \\mathrm{Beta}(\\alpha, \\beta)\\\\\n  U_n \\mid \\phi &\\sim \\mathrm{Bernoulli}(\\phi) \\\\\n  Y_n \\mid U_n &\\sim \\mathrm{Bernoulli}(c_0(1-U_n) + c_1U_n)\n\\end{align*}\n\\]\nIn this case for each observation \\(Y_n\\) we have a new coin toss \\(U_n\\).\nWe have \\(U_n\\sim \\mathrm{Bernoulli}(\\alpha/(\\alpha + \\beta))\\), so that \\(\\mathrm{law}\\, U_n = \\mathrm{law}\\, T\\). Similarly, \\(P(Y_n \\mid \\phi)\\) and \\(P(X_n \\mid \\theta)\\) will be very similar: \\[\\begin{align*}\nP(Y_n \\mid \\phi) &= \\sum_{u} P( Y_n \\mid U_n=u ) P(U_n=u \\mid \\phi) \\\\\n&= \\phi c_1^{Y_n}(1-c_1)^{1-Y_n} + (1-\\phi) c_0^{Y_n} (1-c_1)^{1-Y_n}.\n\\end{align*}\n\\]\nWe see that for \\(X_n = Y_n\\) and \\(\\theta = \\phi\\) the expressions are exactly the same.\nFor \\(N=1\\) there is no real difference between these two models. However, for \\(N\\ge 2\\) a difference appears, because throws \\(U_n\\) are independent and we have \\[\nP(\\mathbf{Y} \\mid \\phi) = \\prod_{n} \\left( \\phi c_1^{Y_n}(1-c_1)^{1-Y_n} + (1-\\phi) c_0^{Y_n} (1-c_1)^{1-Y_n} \\right).\n\\]\nThis is substantially different from \\(P(\\mathbf{X}\\mid \\theta)\\).\nPerhaps the following perspective is useful: the new model, with variables \\(U_n\\) marginalised out, corresponds to the following:\n\\[\\begin{align*}\n  \\phi &\\sim \\mathrm{Beta}(\\alpha, \\beta)\\\\\n  Y_n \\mid \\phi &\\sim \\mathrm{Bernoulli}(c_0(1-\\phi) + c_1\\phi)\n\\end{align*}\n\\]\nwhich is different from the model with a shared latent variable \\(T\\):\n\\[\\begin{align*}\n  \\theta &\\sim \\mathrm{Beta}(\\alpha, \\beta)\\\\\n  T\\mid \\theta &\\sim \\mathrm{Bernoulli}(\\theta)\\\\\n  X_n \\mid T &\\sim \\mathrm{Bernoulli}(c_0(1-T) + c_1T)\n\\end{align*}\n\\]\nHence, although the likelihood functions agree for any single observation, i.e., for every \\(x\\in \\{0, 1\\}\\), the likelihood functions \\(P(X_n=x\\mid \\theta)\\) and \\(P(Y_n=x\\mid \\phi)\\) are the same, the likelihood functions constructed using all observed variables, \\(P(\\mathbf{X}\\mid \\theta)\\) and \\(P(\\mathbf{Y}\\mid \\phi)\\), are usually very different.\nAlso, the posteriors \\(P(\\theta \\mid \\mathbf{X})\\) and \\(P(\\phi \\mid \\mathbf{Y})\\) will differ: \\(\\phi\\) treats each outcome \\(Y_n\\) independently, so that the posterior can shrink quickly if \\(N\\) is large.\nCompare this with the posterior on \\(\\theta\\), which assumes that all \\(X_n\\) are noisy versions of a single throw \\(T\\), so it knows that there’s not that much information about \\(\\theta\\) even if \\(N\\) is very large: the posterior will always be a mixture of \\(\\mathrm{Beta}(\\alpha+1, \\beta)\\) and \\(\\mathrm{Beta}(\\alpha, \\beta+1)\\). In particular, if \\(\\alpha = \\beta = 1\\) the posterior on \\(\\theta\\) will still be very diffuse.\nFor example, consider \\(T = 1\\). We toss the coin \\(N=10\\) times, but due to the noise \\(c_0 = 1-c_1 = 0.1\\) we observed \\(S=8\\). We have the following \\(P(T=1\\mid \\mathbf{X})\\):\n\n\nCode\nprior = BetaPriorArgs(1.0, 1.0)\nlike = LikelihoodArgs(\n  n=10,\n  s=8,\n  c0=0.1,\n  c1=0.9,\n)\n\nposterior_t_value = posterior_t(\n  bias=prior.mean,\n  like=like,\n)\n\nprint(f\"P(T=1 | X) = {posterior_t_value[1]:.7f}\")\n\n\nP(T=1 | X) = 0.9999981\n\n\nHow would the posteriors on \\(\\theta\\) and \\(\\phi\\) look like? We will plot the above value as a dashed line:\n\n\nCode\ndef new_model(prior: BetaPriorArgs, like: LikelihoodArgs):\n  phi = numpyro.sample(\"phi\", dist.Beta(prior.alpha, prior.beta))\n\n  bias = like.c0 * (1 - phi) + like.c1 * phi\n  numpyro.factor(\"loglikelihood\",\n    _binomial_loglikelihood(n=like.n, s=like.s, bias=bias)\n  )\n\n\nfig, axs = plt.subplots(1, 3, figsize=(5, 2.3), dpi=200, sharex=True, sharey=True)\nfor ax in axs:\n  ax.spines[['top', 'right']].set_visible(False)\n  ax.set_xlim([-0.02, 1.02])\n  ax.axvline(posterior_t_value[1], c=\"white\", linestyle=\"--\", linewidth=2)\n\nkey, *subkeys = jax.random.split(key, 4)\n\nax = axs[0]\nax.set_title(\"$P(\\\\theta \\\\mid T=1)$\")\nsamples = dist.Beta(2, 1).sample(subkeys[0], sample_shape=(100_000,))\nax.hist(samples, bins=bins, density=True, color=\"C2\")\n\nax = axs[1]\nax.set_title(\"$P(\\\\theta\\\\mid \\\\mathbf{X}=\\\\mathbf{x})$\")\nd = create_posterior_theta(\n  like=like_args,\n  prior=prior_args,\n)\nsamples = d.sample(subkeys[1], sample_shape=(100_000,))\nax.hist(samples, bins=bins, density=True)\n\nax = axs[2]\nax.set_title(\"$P(\\\\phi\\\\mid \\\\mathbf{Y}=\\\\mathbf{x})$\")\n\nnuts_kernel = NUTS(new_model)\nmcmc = MCMC(nuts_kernel, num_warmup=1000, num_samples=5_000, num_chains=2)\nmcmc.run(subkeys[2], like=like, prior=prior)\nsamples = mcmc.get_samples()[\"phi\"]\nax.hist(samples, bins=bins, density=True, color=\"C4\")\n\nfig.tight_layout()\n\n\n/tmp/ipykernel_47585/3633385242.py:36: UserWarning: There are not enough devices to run parallel chains: expected 2 but got 1. Chains will be drawn sequentially. If you are running MCMC in CPU, consider using `numpyro.set_host_device_count(2)` at the beginning of your program. You can double-check how many devices are available in your system using `jax.local_device_count()`.\n  mcmc = MCMC(nuts_kernel, num_warmup=1000, num_samples=5_000, num_chains=2)\n  0%|          | 0/6000 [00:00&lt;?, ?it/s]warmup:   0%|          | 1/6000 [00:00&lt;1:33:23,  1.07it/s, 3 steps of size 8.10e+00. acc. prob=0.68]warmup:  11%|█         | 643/6000 [00:01&lt;00:06, 852.33it/s, 1 steps of size 2.66e+00. acc. prob=0.79]sample:  21%|██        | 1274/6000 [00:01&lt;00:02, 1721.39it/s, 3 steps of size 7.26e-01. acc. prob=0.93]sample:  32%|███▏      | 1910/6000 [00:01&lt;00:01, 2580.47it/s, 3 steps of size 7.26e-01. acc. prob=0.92]sample:  42%|████▏     | 2549/6000 [00:01&lt;00:01, 3377.24it/s, 3 steps of size 7.26e-01. acc. prob=0.92]sample:  53%|█████▎    | 3204/6000 [00:01&lt;00:00, 4106.89it/s, 1 steps of size 7.26e-01. acc. prob=0.92]sample:  64%|██████▍   | 3857/6000 [00:01&lt;00:00, 4705.36it/s, 3 steps of size 7.26e-01. acc. prob=0.92]sample:  75%|███████▌  | 4508/6000 [00:01&lt;00:00, 5175.87it/s, 7 steps of size 7.26e-01. acc. prob=0.92]sample:  86%|████████▌ | 5162/6000 [00:01&lt;00:00, 5546.17it/s, 1 steps of size 7.26e-01. acc. prob=0.92]sample:  97%|█████████▋| 5800/6000 [00:01&lt;00:00, 5642.21it/s, 3 steps of size 7.26e-01. acc. prob=0.92]sample: 100%|██████████| 6000/6000 [00:01&lt;00:00, 3200.78it/s, 3 steps of size 7.26e-01. acc. prob=0.92]\n  0%|          | 0/6000 [00:00&lt;?, ?it/s]warmup:  11%|█         | 666/6000 [00:00&lt;00:00, 6652.36it/s, 3 steps of size 1.40e+00. acc. prob=0.79]sample:  22%|██▏       | 1342/6000 [00:00&lt;00:00, 6710.71it/s, 3 steps of size 7.83e-01. acc. prob=0.92]sample:  34%|███▎      | 2016/6000 [00:00&lt;00:00, 6720.75it/s, 7 steps of size 7.83e-01. acc. prob=0.92]sample:  45%|████▍     | 2689/6000 [00:00&lt;00:00, 6703.26it/s, 3 steps of size 7.83e-01. acc. prob=0.92]sample:  56%|█████▌    | 3360/6000 [00:00&lt;00:00, 6687.27it/s, 3 steps of size 7.83e-01. acc. prob=0.92]sample:  67%|██████▋   | 4030/6000 [00:00&lt;00:00, 6689.24it/s, 3 steps of size 7.83e-01. acc. prob=0.92]sample:  78%|███████▊  | 4699/6000 [00:00&lt;00:00, 6670.51it/s, 1 steps of size 7.83e-01. acc. prob=0.92]sample:  90%|████████▉ | 5374/6000 [00:00&lt;00:00, 6694.52it/s, 3 steps of size 7.83e-01. acc. prob=0.92]sample: 100%|██████████| 6000/6000 [00:00&lt;00:00, 6705.25it/s, 1 steps of size 7.83e-01. acc. prob=0.92]\n\n\n\n\n\nAs we expected, the posterior on \\(\\phi\\) is much more precise than posterior on \\(\\theta\\). It also is shifted towards the value \\(T=1\\), so (in this case) it behaves as sort of an approximation to \\(T\\)."
  },
  {
    "objectID": "posts/beta-bernoulli.html#which-model-is-better-for-denoising",
    "href": "posts/beta-bernoulli.html#which-model-is-better-for-denoising",
    "title": "Beta-Bernoulli distribution",
    "section": "Which model is better for denoising?",
    "text": "Which model is better for denoising?\nBoth models actually answer different questions: the first model tries to estimate \\(T\\), a single coin toss, and slightly updates the information about this coin bias, \\(\\theta\\).\nThe second model assumes independent coin tosses, where the bias is controlled by \\(\\phi\\). As such, it can quickly shrink the posterior on \\(\\phi\\). Moreover, it can be used to answer the question to impute individual coin tosses, \\(P(\\mathbf{U} \\mid \\mathbf{Y})\\).\nLet’s think what could happen if we fitted each model to the data generated from the other one: this is working with misspecified models (in the \\(\\mathcal M\\)-complete setting).\nConsider a setting where we have a lot of data, \\(N\\gg 1\\) and the false positive and false negative rates are small, with \\(c_0 \\ll c_1\\). If the data come from the second model, with individual variables \\(U_n\\sim \\mathrm{Bernoulli}(\\phi)\\), and we have \\(Y_n\\approx U_n\\), then the posterior on \\(T\\) will have most mass on the maximum likelihood solution: either \\(0\\) (which should happen for \\(\\phi \\ll 0.5\\)) or \\(1\\) (for \\(\\phi \\gg 0.5\\)). This model will be underfitting and the predictive distribution from this model will be quite bad: new \\(Y_{N+1}\\) would again be an approximation to \\(U_{N+1}\\), which is sampled from \\(\\mathrm{Bernoulli}(\\phi)\\), but the model would just return a noisy version of the inferred \\(T\\).\nOn the other hand, if we have a lot of data from the first model (with a single \\(T\\) variable) and we fit the second model, the posterior on \\(\\phi\\) may have most of the mass near \\(0\\) or \\(1\\), depending on the true value of \\(T\\). Hence, although \\(U_n\\sim \\mathrm{Bernoulli}(\\phi)\\) are sampled independently, once \\(\\phi\\) is near the true value of \\(T\\) (\\(0\\) or \\(1\\)), they can all be approximately equal to \\(T\\).\nSo, when there is a lot of data, noting where most of the mass of \\(\\phi\\) lies can be a good approximation to the maximum likelihood of \\(T\\).\nOf course, these \\(N\\gg 1\\) settings only tell what happens when we have a lot of data and we didn’t discuss the uncertainty: can we use \\(\\phi\\) to get well-calibrated uncertainty on \\(T\\)?\nI generally expect that the \\(\\phi\\) model can be a bit better in terms of handling slight misspecification, but doing inference directly on \\(T\\) will provide better results in terms of uncertainty quantification for small \\(N\\). But this is just a hypothesis: extensive simulations are not for today."
  },
  {
    "objectID": "posts/beta-bernoulli.html#appendix",
    "href": "posts/beta-bernoulli.html#appendix",
    "title": "Beta-Bernoulli distribution",
    "section": "Appendix",
    "text": "Appendix\n\nList of distributions\n\nBinomial distribution\nThe simplest choice: we have a coin with bias \\(b\\) and we toss it \\(N\\) times: \\[\\begin{align*}\n  X_n &\\sim \\mathrm{Bernoulli}(b) \\text{ for } n = 1, \\dotsc, N\\\\\n  S &= X_1 + \\cdots + X_N\n\\end{align*}\n\\]\nThen, we have \\(S \\sim \\mathrm{Binomial}(N, b)\\). As the individual throws are independent, it’s easy to prove that \\[\n\\mathbb E[S] = Nb, \\quad \\mathbb V[S] = Nb(1-b).\n\\]\n\n\nFinite mixture of binomial distributions\nAs above, consider \\(K\\) coins with biases \\(b_1, \\dotsc, b_K\\) and a dice used to choose the coin which will be tossed. This is a finite mixture of binomial distributions: \\[\\begin{align*}\n  D &\\sim \\mathrm{Categorical}(d_1, \\dotsc, d_K)\\\\\n  S \\mid D &\\sim \\mathrm{Binomial}(N, b_D)\n\\end{align*}\n\\]\nIn this case the expectation is exactly what one can expect: \\[\n\\mathbb E[S] = \\sum_{k=1}^K d_k \\cdot Nb_k = N\\bar b,\n\\] where \\(\\bar b = d_1b_1 + \\cdots + d_Kb_K\\).\nHowever, the formula for variance is more complex: from the law of total variance:\n\\[\\begin{align*}\n\\mathbb V[S] &= \\mathbb E[\\mathbb V[S\\mid B]] + \\mathbb V[ \\mathbb E[S\\mid B] ] \\\\\n&= \\sum_{k=1}^K d_k N b_k(1-b_k) + \\mathbb V[NB(1-B)]\n\\end{align*}\n\\]\n\n\nBeta-binomial distribution\nSimilarly as here, we can consider an infinite collection of coins, chosen from the beta distribution. Once we pick a coin, we toss it \\(N\\) times:\n\\[\\begin{align*}\n  B &\\sim \\mathrm{Beta}(\\alpha, \\beta)\\\\\n  S \\mid B &\\sim \\mathrm{Binomial}(N, B)\n\\end{align*}\n\\]\nThe marginal distribution is called the beta-binomial distribution: \\[\nS \\sim \\mathrm{BetaBinomial}(N, \\alpha, \\beta).\n\\]\nIt’s easy to prove that \\[\n\\mathbb E[S] = N \\frac{\\alpha}{\\alpha + \\beta},\n\\] but I don’t know an easy derivation of the formula for the variance: \\[\n\\mathbb V[S] = Nb(1-b)\\cdot (1 + (N-1)\\rho),\n\\] where \\(b=\\alpha/(\\alpha + \\beta)\\) and \\(\\rho=1/(1 + \\alpha + \\beta)\\).\nHence, choosing the coin first incurs additional variance (compared to the binomial distribution).\n\n\nPoisson binomial distribution\nThat was quite a few examples. Let’s do one more: the Poisson binomial distribution, because it is fun.\nIn this case one has \\(N\\) coins with biases \\(b_1, \\dotsc, b_N\\) and tosses each of them exactly once: \\[\\begin{align*}\n  X_n &\\sim \\mathrm{Bernoulli}(b_n) \\text{ for } n=1, \\dotsc, N\\\\\n  S &= X_1 + \\cdots + X_N.\n\\end{align*}\n\\]\nWe see that if all biases are equal, this reduces to the binomial distribution. However, this one is more flexible, as the expectation and variance are given now by \\[\n  \\mathbb E[S] = \\sum_{n=1}^N b_n, \\quad \\mathbb  V[S] = \\sum_{n=1}^N b_n(1-b_n).\n\\]\n\n\n\nBeta-Bernoulli sparsity magic\nConsider the following prior on coefficients in a linear model: \\[\\begin{align*}\n  \\gamma &\\sim \\mathrm{Beta}(\\alpha, \\beta)\\\\\n  \\theta_k \\mid \\gamma &\\sim \\gamma\\, Q_0 + (1-\\gamma)\\, Q_1 \\text{ for } k = 1, \\dotsc, K\n\\end{align*}\n\\]\nwhere \\(Q_1\\) is e.g., a \\(\\mathrm{Normal}(0, 10^2)\\) distribution corresponding to “slab” component and \\(Q_0\\), e.g., \\(\\mathrm{Normal}\\left(0, 0.01^2\\right)\\) is the “spike” component.\nIntuitively, we expect that fraction \\(\\gamma\\) of the parameters will be shrunk to \\(0\\) by the spike component \\(Q_0\\) and the rest (the \\(1-\\gamma\\) fraction) of the parameters will be actually used to predict values.\nMichael Betancourt wrote an amazing tutorial in which he introduces local latent variables, \\(\\lambda_k\\), individually controlling whether \\(\\theta_k\\) should be shrunk or not:\n\\[\\begin{align*}\n  \\lambda_k &\\sim \\mathrm{Beta}(\\alpha, \\beta) \\text{ for } k = 1, \\dotsc, K\\\\\n  \\theta_k \\mid \\lambda_k &\\sim \\lambda_k \\, Q_0 + (1-\\lambda_k)\\, Q_1 \\text{ for } k = 1, \\dotsc, K.\n\\end{align*}\n\\]\nUsing small letters for PDFs, we can marginalise variables \\(\\lambda_k\\) as follows: \\[\n  p(\\mathbf{\\theta}) = \\prod_{k=1}^K p(\\theta_k) = \\prod_{k=1}^K \\left( \\int p(\\theta_k \\mid \\lambda_k) \\, \\mathrm{d}P(\\lambda_k) \\right)\n\\] and \\[\\begin{align*}\n  p(\\theta_k) &= \\int p(\\theta_k \\mid \\lambda_k) \\, \\mathrm{d}P(\\lambda_k) \\\\\n  &= q_0(\\theta_k) \\int \\lambda_k\\, \\mathrm{Beta}(\\lambda_k \\mid \\alpha, \\beta) \\, \\mathrm{d}\\lambda_k + q_1(\\theta_k) \\int (1-\\lambda_k)\\, \\mathrm{Beta}(\\lambda_k \\mid \\alpha, \\beta)\\, \\mathrm{d} \\lambda_k \\\\\n  &= q_0(\\theta_k) \\frac{\\alpha}{\\alpha + \\beta} + q_1(\\theta_k) \\left( 1 - \\frac{\\alpha}{\\alpha + \\beta} \\right),\n\\end{align*}\n\\] so that \\[\n  \\theta_k \\sim \\gamma\\, Q_0 + (1-\\gamma)\\, Q_1,\n\\]\nwhere \\(\\gamma = \\alpha / (\\alpha + \\beta)\\).\nBeta-Bernoulli distribution offers the following perspective: draw latent indicator variables \\(T_k \\mid \\lambda_k \\sim \\mathrm{Bernoulli}(\\lambda_k)\\), so that \\(\\theta_k \\mid T_k \\sim T_k\\, Q_0 + (1-T_k) \\, Q_1\\).\nWe recognise that \\(T_k \\sim \\mathrm{BetaBernoulli}(\\alpha, \\beta)\\) which is just \\(\\mathrm{Bernoulli}(\\gamma)\\) for \\(\\gamma =\\alpha/(\\alpha+\\beta)\\). By integrating out \\(T_k\\) variables (which is just trivial summation!) we have \\[\n  \\theta_k \\sim \\gamma\\, Q_0 + (1-\\gamma)\\, Q_1.\n\\]"
  },
  {
    "objectID": "posts/triangle-distributions.html",
    "href": "posts/triangle-distributions.html",
    "title": "Two distributions on a triangle",
    "section": "",
    "text": "Frederic, Alex and I have been discussing some experiments related to our work on mutual information estimators and Frederic suggested to look at one distribution. I misunderstood what he meant, but this mistake turned out to be quite an interesting object.\nSo let’s take a look at two distributions defined over a triangle \\[T = \\{ (x, y)\\in (0, 1)\\times (0, 1) \\mid y &lt; x \\}\\] and calculate their mutual information."
  },
  {
    "objectID": "posts/triangle-distributions.html#uniform-joint",
    "href": "posts/triangle-distributions.html#uniform-joint",
    "title": "Two distributions on a triangle",
    "section": "Uniform joint",
    "text": "Uniform joint\nConsider a probability distribution with constant probability density function (PDF) of the joint distribution: \\[p_{XY}(x, y) = 2 \\cdot \\mathbf{1}[y&lt;x].\\]\nWe have \\[p_X(x) = \\int\\limits_0^x p_{XY}(x, y)\\, \\mathrm{d}y = 2x\\] and \\[ p_Y(y) = \\int\\limits_0^1 p_{XY}(x, y) \\mathbf{1}[y &lt; x]  \\, \\mathrm{d}x = \\int\\limits_y^1 p_{XY}(x, y) \\, \\mathrm{d}x = 2(1-y).\\]\nHence, pointwise mutual information is given by \\[ i(x, y) = \\log \\frac{ p_{XY}(x, y) }{p_X(x) \\, p_Y(y) } = \\log \\frac{1}{2x(1-y)}\\] and mutual information is\n\\[I(X; Y) = \\int\\limits_0^1 \\mathrm{d}x \\int\\limits_x^1 i(x, y)\\, p_{XY}(x, y) \\mathrm{d}y = 1-\\log 2 \\approx 0.307.\\]\nFinally, let’s visualise this distribution to numerically validate the formulae above:\n\n\nCode\nfrom typing import Protocol\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nplt.style.use(\"dark_background\")\n\n\nclass Distribution(Protocol):\n  def sample(self, rng, n_samples: int) -&gt; np.ndarray:\n    pass\n\n  def p_xy(self, x: np.ndarray, y: np.ndarray) -&gt; np.ndarray:\n    pass\n\n  def p_x(self, x: np.ndarray) -&gt; np.ndarray:\n    pass\n\n  def p_y(self, y: np.ndarray) -&gt; np.ndarray:\n    pass\n\n  def pmi(self, x: np.ndarray, y: np.ndarray) -&gt; np.ndarray:\n    pass\n\n  @property\n  def mi(self) -&gt; float:\n    pass\n\n\nclass UniformJoint(Distribution):\n  def sample(self, rng, n_samples):\n    samples = rng.uniform(low=1e-9, size=(3 * n_samples, 2))\n    samples = np.asarray(list(filter(lambda point: point[1] &lt; point[0], samples)))\n    if len(samples) &lt; n_samples:\n      samples = self.sample(rng, n_samples)\n    \n    assert len(samples) &gt;= n_samples\n    return samples[:n_samples, ...]\n\n  def p_xy(self, x, y):\n    return np.where(y &lt; x, 2.0, 0.0)\n\n  def p_x(self, x):\n    return 2*x\n\n  def p_y(self, y):\n    return 2*(1-y)\n\n  def pmi(self, x, y):\n    return np.where(y &lt; x, -np.log(2*x*(1-y)), np.nan)\n\n  @property\n  def mi(self):\n    return 0.307\n\n\ndef visualise_dist(\n  rng,\n  dist: Distribution,\n  n_samples: int = 15_000,\n) -&gt; plt.Figure:\n  fig, axs = plt.subplots(2, 3, figsize=(3*2.2, 2*2.2))\n\n  samples = dist.sample(rng, n_samples=n_samples)\n\n  t_axis = np.linspace(1e-9, 1 - 1e-9, 51)\n\n  X, Y = np.meshgrid(t_axis, t_axis)\n\n  # Visualise joint probability\n  ax = axs[0, 0]\n  ax.scatter(samples[:, 0], samples[:, 1], rasterized=True, alpha=0.3, s=0.2, marker=\".\")\n  ax.set_xlim(0, 1)\n  ax.set_ylim(0, 1)\n  ax.set_title(\"Samples from $P_{XY}$\")\n  ax.set_xlabel(\"$x$\")\n  ax.set_ylabel(\"$y$\")\n\n  ax = axs[1, 0]\n  ax.imshow(dist.p_xy(X, Y), origin=\"lower\", extent=[0, 1, 0, 1], cmap=\"magma\")\n  ax.set_title(\"PDF $p_{XY}$\")\n  ax.set_xlabel(\"$x$\")\n  ax.set_ylabel(\"$y$\")\n\n  # Visualise marginal distributions\n  ax = axs[0, 1]\n  ax.set_xlim(0, 1)\n  ax.hist(samples[:, 0], bins=np.linspace(0, 1, 51), density=True, alpha=0.2, rasterized=True)\n  ax.plot(t_axis, dist.p_x(t_axis))\n  ax.set_xlabel(\"$x$\")\n  ax.set_title(\"PDF $p_X$\")\n\n  ax = axs[1, 1]\n  ax.set_xlim(0, 1)\n  ax.hist(samples[:, 1], bins=np.linspace(0, 1, 51), density=True, alpha=0.2, rasterized=True)\n  t_axis = np.linspace(0, 1, 51)\n  ax.plot(t_axis, dist.p_y(t_axis))\n  ax.set_xlabel(\"$y$\")\n  ax.set_title(\"PDF $p_Y$\")\n\n  # Visualise PMI\n  ax = axs[0, 2]\n  ax.set_xlim(0, 1)\n  ax.set_ylim(0, 1)\n  ax.imshow(dist.pmi(X, Y), origin=\"lower\", extent=[0, 1, 0, 1], cmap=\"magma\")\n  ax.set_title(\"PMI\")\n  ax.set_xlabel(\"$x$\")\n  ax.set_ylabel(\"$y$\")\n\n  ax = axs[1, 2]\n  pmi_profile = dist.pmi(samples[:, 0], samples[:, 1])\n  mi = np.mean(pmi_profile)\n  ax.set_title(f\"PMI histogram. MI={dist.mi:.2f}\")  \n  ax.axvline(mi, color=\"navy\", linewidth=1)\n  ax.axvline(dist.mi, color=\"salmon\", linewidth=1, linestyle=\"--\")\n  ax.hist(pmi_profile, bins=np.linspace(-2, 5, 21), density=True)\n  ax.set_xlabel(\"PMI value\")\n\n  return fig\n\nrng = np.random.default_rng(42)\ndist = UniformJoint()\n\nfig = visualise_dist(rng, dist)\nfig.tight_layout()"
  },
  {
    "objectID": "posts/triangle-distributions.html#uniform-margin",
    "href": "posts/triangle-distributions.html#uniform-margin",
    "title": "Two distributions on a triangle",
    "section": "Uniform margin",
    "text": "Uniform margin\nThe above distribution is interesting, but when I heard about the distribution over the triangle, I actually had the following generative model in mind: \\[\\begin{align*}\n  X &\\sim \\mathrm{Uniform}(0, 1),\\\\\n  Y \\mid X=x &\\sim \\mathrm{Uniform}(0, x).\n\\end{align*}\\]\nWe have \\(p_X(x) = 1\\) and therefore \\[p_{XY}(x, y) = p_{Y\\mid X}(y\\mid x) = \\frac{1}{x}\\,\\mathbf{1}[y &lt; x].\\]\nAgain, this distribution is defined on the triangle \\(T\\), although now the joint is not uniform.\nWe have \\[ p_Y(y) = \\int\\limits_y^1  \\frac{1}{x} \\, \\mathrm{d}x = -\\log y\\] and \\[i(x, y) = \\log \\frac{1}{-x \\log y} = -\\log \\big(x\\cdot (-\\log y)\\big )\n= - \\left(\\log(x) + \\log(-\\log y) \\right) = -\\log x - \\log(-\\log y).\\] This expression suggests that if \\(p_Y(y)\\) were uniform on \\((0, 1)\\) (but it is not), the pointwise mutual information \\(i(x, Y)\\) would be distributed according to Gumbel distribution.\nThe mutual information \\[\n  I(X; Y) = -\\int\\limits_0^1 \\mathrm{d}y \\int\\limits_y^1 \\frac{ \\log x + \\log(-\\log y)}{x} \\, \\mathrm{d}x = \\frac{1}{2} \\int\\limits_0^1 \\log y \\cdot \\log \\left(y \\log ^2 y\\right) \\, \\mathrm{d}y = \\gamma \\approx 0.577\n\\] is in this case the Euler–Mascheroni constant. I don’t know how to do this integral, but both Mathematica and Wolfram Alpha seem to be quite confident in it.\nPerhaps it shouldn’t be too surprising as \\(\\gamma\\) can appears in expressions involving mean of the Gumbel distribution. However, I’d like to understand this connection better.\nPerhaps another time; let’s finish this post with another visualisation:\n\n\nCode\nclass UniformMargin(Distribution):\n  def sample(self, rng, n_samples: int) -&gt; np.ndarray:\n    x = rng.uniform(size=(n_samples,))\n    y = rng.uniform(high=x)\n    return np.hstack([x.reshape((-1, 1)), y.reshape((-1, 1))])\n\n  def p_xy(self, x: np.ndarray, y: np.ndarray) -&gt; np.ndarray:\n    return np.where(y &lt; x, np.reciprocal(x), np.nan)\n\n  def p_x(self, x: np.ndarray) -&gt; np.ndarray:\n    return np.full_like(x, fill_value=1.0)\n\n  def p_y(self, y: np.ndarray) -&gt; np.ndarray:\n    return -np.log(y)\n\n  def pmi(self, x: np.ndarray, y: np.ndarray) -&gt; np.ndarray:\n    return np.where(y &lt; x, -np.log(-x * np.log(y)), np.nan)\n\n  @property\n  def mi(self):\n    return 0.577\n\n\nrng = np.random.default_rng(42)\ndist = UniformMargin()\n\nfig = visualise_dist(rng, dist)\nfig.tight_layout()\n\n\n/tmp/ipykernel_95941/2727834072.py:14: RuntimeWarning: divide by zero encountered in log\n  return -np.log(y)"
  },
  {
    "objectID": "posts/expectations-student-mentorship.html",
    "href": "posts/expectations-student-mentorship.html",
    "title": "Student mentorship: expectations document",
    "section": "",
    "text": "Welcome! This document is supposed to explain my general mentoring style and act as a skeleton around we can build the collaboration and mentorship rules.\nPlease, note:"
  },
  {
    "objectID": "posts/expectations-student-mentorship.html#mission-statement",
    "href": "posts/expectations-student-mentorship.html#mission-statement",
    "title": "Student mentorship: expectations document",
    "section": "Mission statement",
    "text": "Mission statement\nWhen I advise on a project I try to keep the following in mind:\n\nI want you to learn and become a better researcher and engineer at the end of the project.\nIt’s more important that we understand each other and are happy with the mentorship, rather than we get an additional feature."
  },
  {
    "objectID": "posts/expectations-student-mentorship.html#expectations",
    "href": "posts/expectations-student-mentorship.html#expectations",
    "title": "Student mentorship: expectations document",
    "section": "Expectations",
    "text": "Expectations\n\nWhat you can expect from me\n\nI’ll find regular time to meet with you and advise on the steps which may be worth taking. While I will be more “hands-on” and have more precise ideas when you start, I want you to become an independent thinker with a good knowledge on the topic – it’s also likely that you’ll know more about the topic than me by the end of your project!\nI’ll advise you on your code and writing, to make sure that your skills improve.\n\nI’ll keep an open mindset to your comments and suggestions. If you encounter any issues, let me know and we’ll work together on resolving them.\n\n\n\nWhat I’d like to expect from you\n\nHonesty. If something doesn’t work for you (e.g., the expectations and the workload are too high), I said something ridiculously wrong, or the experiments fail, let’s discuss. I’m still learning both how to be a good mentor and a good scientist.\nConforming to use good research and coding practices. We will work on open-source projects and I expect you to write good code (with documentation and tests) and run reproducible experiments. Developing these skills takes time and we will work together to make sure that your research and programming skills are improving.\nTaking the ownership of conforming to the university rules. You should remind me when your thesis is due three months before submitting it, so we can discuss the outline, and send me the first draft three weeks before the deadline, so I can review it.\n\n\n\nConflict resolution\nIn case of a conflict with an academic or a student, please contact me and we will work together to resolve the conflict. If you feel that you do not want me to be involved (e.g., the conflict is between you and me), I encourage you to contact my mentor, Professor Niko Beerenwinkel or ETH’s Ombudspersons.\n\n\nMisc\nI’m not an established researcher in the field (and I don’t have a PhD!). Apart from the fact that I may be wrong in different aspects (happy to learn!), the reference letters written by me are unlikely to be accepted e.g., if you apply for a PhD. If you need a reference letter at the end of the project, I’d suggest to ask Professor Niko Beerenwinkel (and CC me) whether he could provide one."
  },
  {
    "objectID": "posts/expectations-student-mentorship.html#general-research-advice",
    "href": "posts/expectations-student-mentorship.html#general-research-advice",
    "title": "Student mentorship: expectations document",
    "section": "General research advice",
    "text": "General research advice\nAlthough I will supply you with an initial reading list tailored to your project, I’d like to share below some general advice on research, knowledge work, and learning. (Remember – if you see that some of these do not work you, feel free to replace them with better practices. I’d also be grateful if you could share them with me, so I can update this document).\n\nResearch notes and journal\nI’d strongly encourage you to book some time at the start and the end of every working day to work on your research notes and write your observations in a journal.\nThis serves multple purposes: - I believe it will help you to improve your understanding of the domain. - At some point you will need to write your thesis. You will see that it is much easier to edit a series of connected research notes into a first draft, rather than starting with an empty page. - By practicing this over the duration of the project, you will end up with a skill which is useful regardless whether you decide to move into industry or stay in academia.\nTo start writing research notes, read an Andy Matuschak’s note or watch Martin Adams’ video. Popular software includes Obsidian and Zettlr (and you can use them for the journal as well).\nFor your research journal, you may find this blog post useful. Journal can also be helpful to track your feelings and attitude towards the project, so we can adjust the workload or troubleshoot the process – see this post.\n\n\nReading scientific literature\n\nI’d suggest to read this Andy Matuschak’s note and this short P.N. Edwards’ article.\nThis is also a skill which takes time to master, so I’d suggest to practice it regularly and go back and refresh the principles of effective reading.\nYou will see that there is always too much literature to read than the time permits, so it’s critical to think what you want to learn from a paper.\n\nAre there specific questions I want to have the answer to by reading this paper? (It’s always good to read papers with several questions in mind.)\nIs this some maths or statistics which is crucial to deeply understand for the project? If so, several hours may be required and there is nothing to be done.\nIs this a paper which main conclusion can be quickly understood just by looking at one figure and the abstract or conclusions?\nIs this a paper which is potentially useful if problem X arises? If so, it’s probably good to say in a research problem on topic X that this paper may be useful to deeply understand it then.\n\nI like to use Connected Papers to find papers related to the paper of interest. Another strategy is to use Google Scholar to find papers which cited the paper of interest or see what the superstars are doing.\nSpeaking of superstars, Twitter has become a place where new research results are often announced and have short “tl;dr” threads. I would suggest to create a recurrent task (e.g., half an hour every two weeks) and check what the superstars have been doing. (Note that the temptation to procrastinate can be huge. This is why I recommend to set only a specific time to check it.)\n\n\n\nFinding a sustainable working style\n\nAs Bastian Rieck advises, it’s crucial that you find a sustainable workflow. Working on a project over a few months is a long time and “it’s rather a marathon than a sprint”.\nI’m interested in seeing that your expertise grows and that work you produce is of good quality, rather than in counting the hours you put into the work:\n\nIf you think that my expectations are unrealistic, just talk to me – we don’t need to rush and the scope of the thesis can always be adjusted to be more realistic.\nMake sure that you prioritize your mental health and well-being.\nPlease, please, please, no work on weekends and holidays.\n\nYou will see that different people have different working styles. This is fine – they are also working on different projects, have different backgrounds, and have different goals. Don’t compare yourself with them and embrace your way of working as well as theirs.\n\n\n\nProgramming\n\nWe will use Git version control and GitHub. Please, make sure that you have an account and send me your username, so I can add you to the project.\nIf you had not worked with Git before, I recommend (a) Creating a “sandbox” repository and playing with different commands. R. Dudler’s “The simple guide” is a nice way to get started.\nLearning good software practices is like learning a new language – working with a dictionary won’t make one proficient in one day, but using it regularly can help to avoid common errors. I recommend Google Style Guide and (to know what should be avoided) Python anti-patterns.\n\n\n\nMisc\n\nIf you want to become a researcher, R. Hamming’s “You and your research” talk is a classic.\nPatrick Kidger and Andrej Karpathy also wrote on this topic."
  },
  {
    "objectID": "posts/expectations-student-mentorship.html#references",
    "href": "posts/expectations-student-mentorship.html#references",
    "title": "Student mentorship: expectations document",
    "section": "References",
    "text": "References\nI used the following resources to draft the document above. However, all the mistakes (scientific, mentoring, grammar) are to blame on myself.\n\nK.S. Masters and P.K. Kreeger’s “Ten Simple Rules” article\nYinghzhen Li’s blogpost\nThe document issued by Niko Beerenwinkel to his PhD students."
  },
  {
    "objectID": "posts/regression-to-the-mean-bias-rejoinder.html",
    "href": "posts/regression-to-the-mean-bias-rejoinder.html",
    "title": "Regression to the mean and biased predictions",
    "section": "",
    "text": "In September 2008 Bayesian Analysis, vol. 3, issue 3 featured a wonderful discussion between five statistics superstars: Andrew Gelman, José M. Bernardo, Joseph B. Kadane, Stephen Senn and Larry Wasserman.\nThe discussion is a great read on foundations of Bayesian statistics (and it’s open access!), but we will not summarise it today. Instead, let’s focus on an example from Andrew Gelman’s Rejoinder on regression to the mean and unbiased predictions."
  },
  {
    "objectID": "posts/regression-to-the-mean-bias-rejoinder.html#the-example",
    "href": "posts/regression-to-the-mean-bias-rejoinder.html#the-example",
    "title": "Regression to the mean and biased predictions",
    "section": "The example",
    "text": "The example\nAndrew Gelman considers a problem in which one tries to estimate the height of adult daughter, \\(Y\\), from the height of her mother, \\(X\\). Consider an artificial scenario, where we have millions of data points, which we can use to estimate the joint probability distribution \\((X, Y)\\) and it turns out to be bivariate normal1 of the following form: \\[\n\\begin{pmatrix} X\\\\Y \\end{pmatrix} \\sim \\mathcal N\\left(\\begin{pmatrix}\\mu\\\\\\mu\\end{pmatrix}, \\sigma^2\\begin{pmatrix}  1 & 0.5 \\\\ 0.5 & 1\\end{pmatrix}  \\right)\n\\] with known \\(\\mu\\) and \\(\\sigma\\), say, \\(\\mu=160\\) and \\(\\sigma=10\\) in centimeters.\nThe marginal distributions on both \\(X\\) and \\(Y\\) are the same: \\(\\mathcal N(\\mu, \\sigma^2)\\).\nLet’s plot all of them:\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nplt.style.use(\"dark_background\")\n\ndef generate_data(\n    rng,\n    mu: float,\n    sigma: float,\n    n_points: int,\n) -&gt; np.ndarray:\n    var = np.square(sigma)\n    return rng.multivariate_normal(\n        mean=(mu, mu),\n        cov=var * np.asarray([\n            [1.0, 0.5],\n            [0.5, 1.0],\n        ]),\n        size=n_points,\n    )\n\n\nrng = np.random.default_rng(42)\n\nmu = 160\nsigma = 10\n\ndata = generate_data(rng, mu=mu, sigma=sigma, n_points=5_000)\n\nfig, axs = plt.subplots(1, 2, figsize=(4.5, 2.2), dpi=170, sharex=True)\n\nax = axs[0]\nax.scatter(data[:, 0], data[:, 1], c=\"C0\", s=1, alpha=0.1, rasterized=True)\nax.set_title(\"Joint\")\nax.set_xlabel(\"$X$\")\nax.set_ylabel(\"$Y$\")\n\nlim = (mu - 4*sigma, mu + 4*sigma)\n\nax.set_xlim(*lim)\nax.set_ylim(*lim)\n\nax = axs[1]\nax.hist(data[:, 0], color=\"C1\", bins=np.linspace(*lim, 21), rasterized=True, density=True)\nax.set_title(\"Marginal\")\nax.set_xlim(*lim)\nax.set_xlabel(\"$X$\")\nax.set_ylabel(\"PDF\")\n\nfor ax in axs:\n  ax.spines[['top', 'right']].set_visible(False)\n\nfig.tight_layout()\n\n\n\n\n\nThe conditional distributions are also normal and take very similar form: \\[\nY\\mid X=x \\sim \\mathcal N\\big( \\mu + 0.5(x-\\mu), 0.75\\sigma^2\\big),\n\\] and \\[\nX\\mid Y=y \\sim \\mathcal N\\big( \\mu + 0.5(y-\\mu), 0.75\\sigma^2\\big).\n\\]\nLet’s plot the conditional distribution in the fist panel. We will plot conditional distributions using mean \\(\\mathbb E[Y\\mid X=x]\\) and the intervals representing one, two, and three standard deviations from it.\nThen, in the second panel we will overlay it with points and the line \\(y=x\\) (dashed red line).\n\n\nCode\nfig, axs = plt.subplots(1, 2, figsize=(4.5, 2.2), dpi=170, sharex=True, sharey=True)\n\nx_ax = np.linspace(*lim, 51)\nmean_pred = mu + 0.5 * (x_ax-mu)\n\ndef plot_conditional(ax):\n    ax.set_xlim(*lim)\n    ax.set_ylim(*lim)\n\n    for n_sigma in [1, 2, 3]:\n        band = n_sigma * np.sqrt(0.75) * sigma\n        ax.fill_between(\n            x_ax,\n            mean_pred - band,\n            mean_pred + band,\n            alpha=0.1,\n            color=\"yellow\",\n            edgecolor=None,\n        )\n\n    ax.plot(\n        x_ax,\n        mean_pred,\n        c=\"white\",\n    )\n\nax = axs[0]\nplot_conditional(ax)\nax.set_xlabel(\"$X$\")\nax.set_ylabel(r\"$Y\\mid X$\")\n\nax = axs[1]\nplot_conditional(ax)\nax.set_xlabel(\"$X$\")\nax.set_ylabel(\"$Y$\")\nax.scatter(data[:, 0], data[:, 1], c=\"C0\", s=1, alpha=0.2, rasterized=True)\nax.plot(x_ax, x_ax, c=\"maroon\", linestyle=\"--\")\n\nfor ax in axs:\n  ax.spines[['top', 'right']].set_visible(False)\n\nfig.tight_layout()\n\n\n\n\n\nThe \\(y=x\\) has greater slope than \\(\\mathbb E[Y\\mid X=x]\\) (namely, 1 is greater than 0.5), which is the usual regression to the mean."
  },
  {
    "objectID": "posts/regression-to-the-mean-bias-rejoinder.html#biased-and-unbiased-estimators",
    "href": "posts/regression-to-the-mean-bias-rejoinder.html#biased-and-unbiased-estimators",
    "title": "Regression to the mean and biased predictions",
    "section": "Biased and unbiased estimators",
    "text": "Biased and unbiased estimators\nWe have seen that we could do probabilistic prediction, returning the whole conditional distribution \\(P(Y\\mid X=x)\\). Imagine however that a single point is required as the answer. We can take \\(\\mathbb E[Y\\mid X=x]\\) as one estimator (solid line in the previous plot). More generally, for every number \\(f\\in \\mathbb R\\) define an estimator \\[\n\\hat Y_f = \\mu + f\\cdot (X-\\mu).\n\\]\nWe have the following:\n\nFor \\(f=0\\), we have \\(\\hat Y_0 = \\mu\\) is constantly predicting the mean.\nFor \\(f=0.5\\), \\(\\hat Y_{0.5} = 0.5(X + \\mu)\\) is the regression to the mean we have seen above.\nFor \\(f=1\\), \\(\\hat Y_1 = X\\) returns the input, which we also have seen above.\n\nLet’s take a look at the bias and variance2 of these estimators.\nWe have \\[\n(X\\mid Y=y) \\sim \\mathcal N\\big(\\mu + 0.5(y-\\mu), 0.75\\sigma^2\\big)\n\\] and \\(\\hat Y_f = f\\cdot X + \\mu(1-f)\\), meaning that \\[\n\\mathbb{V}[ \\hat Y_f \\mid Y=y ] = f^2 \\cdot \\mathbb{V}[X \\mid Y=y] = 0.75 f^2\\sigma^2\n\\] and \\[\n\\begin{align*}\\mathbb E[ \\hat Y_f \\mid Y=y ] &= \\mu(1-f) + f\\cdot \\mathbb E[X \\mid Y=y ]\\\\ &= \\mu(1-f) + f\\cdot ( \\mu + 0.5(y-\\mu) ) \\\\ &=\\mu-\\mu f + f\\mu + 0.5 f\\cdot (y-\\mu) \\\\\n&= \\mu + 0.5f\\cdot (y-\\mu) \\\\ &= 0.5 f\\cdot y +\\mu(1-0.5f)\\end{align*}\n\\]\nHence, for \\(f=0.5\\) we have \\[\n\\mathbb E[\\hat Y_{0.5}\\mid Y=y] = 0.25y + 0.75\\mu,\n\\] which is biased towards the mean.\nTo have an unbiased estimate, consider \\(f=2\\): \\[\n\\mathbb E[\\hat Y_2\\mid Y=y] = y,\n\\] which however has the form \\((\\hat Y_2 \\mid X=x) = \\mu + 2(x-\\mu)\\), which amplifies the measured distance from the mean!\n\nVisualisations\nLet’s spend a minute designing the plots and then visualise the estimators.\nThe raw data are visualised by plotting \\(Y\\) and \\(X\\). We can add the lines \\(\\hat Y_f\\mid X=x\\) to them, to add some information on how \\(\\hat Y_f\\) (which is a deterministic function of \\(X\\)) varies, and what the predictions are.\n\n\nCode\ndef yhat(x: np.ndarray, f: float) -&gt; np.ndarray:\n    return mu + f * (x - mu)\n\nfig, ax = plt.subplots(figsize=(3, 3), dpi=150)\n\n\nax.scatter(data[:, 0], data[:, 1], c=\"C0\", s=1, alpha=0.2, rasterized=True)\n\nfs = [0, 0.5, 1, 2]\ncolors = [\"orange\", \"white\", \"maroon\", \"purple\"]\n\nfor f, col in zip(fs, colors):\n    ax.plot(\n        x_ax,\n        yhat(x_ax, f),\n        color=col,\n        label=f\"$f=${f:.1f}\",\n    )\n\nax.set_xlabel(\"$X$\")\nax.set_ylabel(\"$Y$\")\nax.set_xlim(*lim)\nax.set_ylim(*lim)\nax.spines[['top', 'right']].set_visible(False)\nax.legend(frameon=False)\nfig.tight_layout()\n\n\n\n\n\nThen, we can also look at the plot of \\(\\hat Y_f\\) and \\(Y\\). This will be a good illustration showing what bias and variance of these estimators actually mean. We will add a dashed “diagonal” line, \\(\\hat y_f = y\\), to each of these plots.\n\n\nCode\nfig, axs = plt.subplots(1, len(fs), figsize=(2 * len(fs), 2), dpi=150, sharex=True, sharey=True)\n\nfor f, ax in zip(fs, axs):\n    x, y = data[:, 0], data[:, 1]\n    y_ = yhat(x, f)\n    ax.scatter(y, y_, c=\"C2\", s=1, alpha=0.2, rasterized=True)\n    ax.plot(\n        x_ax, x_ax\n    )\n    ax.set_xlim(*lim)\n    ax.set_ylim(np.min(y_), np.max(y_))\n    ax.set_title(f\"$f=${f:.1f}\")\n    ax.set_xlabel(\"$Y$\")\n    ax.set_ylabel(f\"$\\hat Y_f$\")\n\nfor ax in axs:\n    ax.spines[['top', 'right']].set_visible(False)\nfig.tight_layout()\n\n\n/tmp/ipykernel_107538/2941644956.py:11: UserWarning: Attempting to set identical low and high ylims makes transformation singular; automatically expanding.\n  ax.set_ylim(np.min(y_), np.max(y_))\n\n\n\n\n\nThe bias can be seen in the following manner: for each value \\(Y\\), the values of \\(\\hat Y\\) corresponding to that \\(Y\\) should be distributed in such a way that the mean lies on the line. This actually will be easier to see once we plot the difference \\(\\hat Y_f - \\hat Y\\) and \\(Y\\).\n\n\nCode\nfig, axs = plt.subplots(1, len(fs), figsize=(2 * len(fs), 2), dpi=150, sharex=True, sharey=True)\n\nfor f, ax in zip(fs, axs):\n    x, y = data[:, 0], data[:, 1]\n    y_ = yhat(x, f)\n    ax.scatter(y, y_ - y, c=\"C2\", s=1, alpha=0.2, rasterized=True)\n    ax.plot(\n        x_ax, np.zeros_like(x_ax)\n    )\n    ax.set_xlim(*lim)\n    ax.set_title(f\"$f=${f:.1f}\")\n    ax.set_xlabel(\"$Y$\")\n    ax.set_ylabel(f\"$\\hat Y_f - Y$\")\n\nfor ax in axs:\n    ax.spines[['top', 'right']].set_visible(False)\nfig.tight_layout()\n\n\n\n\n\nWe also see what how variance manifests: for \\(Y\\approx 160\\), we have quite a range of corresponding \\(\\hat Y_2\\). The estimator \\(\\hat Y_{0.5}\\) has clear bias for \\(Y\\) far from the mean, but for most of the data points (which lie close the mean in this case) the prediction is reasonable.\nIn fact, let’s plot \\(\\mathbb E[|\\hat Y_f -y | \\mid y]\\) and \\(\\sqrt{\\mathbb E[(\\hat Y_f-y)^2\\mid y]}\\) as a function of \\(y\\):\n\n\nCode\nn_y_hat_samples = 10_000\ny_ax = np.linspace(mu - 4 * sigma, mu + 4 * sigma, 51)\n\n# Shape (n_y, n_y_hat_samples)\nx_samples = mu + 0.5 * (y_ax[:, None] - mu) + rng.normal(loc=0, scale=np.sqrt(0.75) * sigma, size=(y_ax.shape[0], n_y_hat_samples))\n\nfig, axs = plt.subplots(1, 2, figsize=(2.5*2+1, 2.5), dpi=150, sharex=True, sharey=True)\n\nax = axs[0]\nfor f in fs:\n    preds = yhat(x_samples, f)\n    loss = np.mean(np.abs(preds - y_ax[:, None]), axis=1)\n    ax.plot(y_ax, loss, label=f\"$f=${f:.1f}\")\n\n# ax.legend(frameon=False)\nax.set_xlabel(\"$y$\")\nax.set_ylabel(r\"$E[|\\hat Y_f-y| \\mid y ]$\")\n\nax = axs[1]\nfor f in fs:\n    preds = yhat(x_ax, f)\n    loss = np.sqrt(np.mean(np.square(preds - y_ax[:, None]), axis=1))\n    ax.plot(y_ax, loss, label=f\"$f=${f:.1f}\")\n\nax.legend(frameon=False, bbox_to_anchor=(1.05, 1.0))\nax.set_xlabel(\"$y$\")\nax.set_ylabel(r\"$\\sqrt{E[(\\hat Y_f-y)^2 \\mid y ]}$\")\n\nfor ax in axs:\n    ax.spines[['top', 'right']].set_visible(False)\nfig.tight_layout()\n\n\n\n\n\n\n\nTwisting the problem\nThe above plots condition on unobserved parameter \\(y\\). Let’s think what happens when we observe the value \\(X\\) and we want to know how far our estimate \\(\\hat Y_f\\) is from the unobserved value \\(Y\\). We can do a variant of one plots we have seen previously, where we put \\(X\\) on the horizontal axis and \\(\\hat Y_f-Y\\) on the vertical one:\n\n\nCode\nfig, axs = plt.subplots(1, len(fs), figsize=(2 * len(fs), 2), dpi=150, sharex=True, sharey=True)\n\nfor f, ax in zip(fs, axs):\n    x, y = data[:, 0], data[:, 1]\n    y_ = yhat(x, f)\n    ax.scatter(x, y_ - y, c=\"C2\", s=1, alpha=0.2, rasterized=True)\n    ax.plot(\n        x_ax, np.zeros_like(x_ax)\n    )\n    ax.set_xlim(*lim)\n    ax.set_title(f\"$f=${f:.1f}\")\n    ax.set_xlabel(\"$X$\")\n    ax.set_ylabel(f\"$\\hat Y_f - Y$\")\n\nfor ax in axs:\n    ax.spines[['top', 'right']].set_visible(False)\nfig.tight_layout()\n\n\n\n\n\nWe see that for all values of \\(X\\), the regression to the mean estimator, \\(\\hat Y_{0.5}\\), works well. Let’s also take a look at the following losses, measuring how wrong our predictions will be on average for a given value of \\(X\\): \\[\\begin{align*}\n\\ell_1(x) &= \\mathbb E\\left[\\left| Y_f - Y \\right| \\mid X=x \\right], \\\\\n\\ell_2(x) &= \\sqrt{\\mathbb E\\left[\\left( \\hat Y_f - Y \\right)^2 \\mid X=x \\right]}.\n\\end{align*}\n\\]\n\n\nCode\nn_y_samples = 10_000\n\n# Shape (n_x, n_y_samples)\ny_samples = mu + 0.5 * (x_ax[:, None] - mu) + rng.normal(loc=0, scale=np.sqrt(0.75) * sigma, size=(x_ax.shape[0], n_y_samples))\n\nfig, axs = plt.subplots(1, 2, figsize=(2.5*2 + 1, 2.5), dpi=150, sharex=True, sharey=True)\n\nax = axs[0]\nfor f in fs:\n    preds = yhat(x_ax, f)\n    loss = np.mean(np.abs(y_samples - preds[:, None]), axis=1)\n    ax.plot(x_ax, loss, label=f\"$f=${f:.1f}\")\n\nax.set_xlabel(\"$x$\")\nax.set_ylabel(r\"$\\ell_1$\")\n\nax = axs[1]\nfor f in fs:\n    preds = yhat(x_ax, f)\n    loss = np.sqrt(np.mean(np.square(y_samples - preds[:, None]), axis=1))\n    ax.plot(x_ax, loss, label=f\"$f=${f:.1f}\")\n\nax.set_xlabel(\"$x$\")\nax.set_ylabel(r\"$\\ell_2$\")\n\nax.legend(frameon=False, bbox_to_anchor=(1.05, 1.0))\n\nfor ax in axs:\n    ax.spines[['top', 'right']].set_visible(False)\nfig.tight_layout()\n\n\n\n\n\nEven for values of \\(X\\) quite far from the mean, some bias helps! Overall, we see that regression to the mean is a sensible strategy in this case.\nAlso, using \\(f=1\\) (i.e., \\(\\hat Y_f=X\\)) performs as well as \\(f=0\\) (constant prediction \\(\\mu\\)). More precisely, observe that \\[\n\\left(Y - \\hat Y_0 \\mid X=x\\right) = (Y-\\mu \\mid X=x) \\sim \\mathcal N\\left( 0.5(x-\\mu), 0.75 \\sigma^2\\right)\n\\] and \\[\n\\left(Y - \\hat Y_1 \\mid X=x\\right) = (Y-X \\mid X=x) \\sim \\mathcal N\\left( 0.5(\\mu - x) , 0.75\\sigma^2 \\right)\n\\]\nand the absolute value (or squaring) makes the losses exactly equal. This is also visible in the plot representing \\(X\\) and \\(Y_f - Y\\).\nAs a final note, one can notice that for every \\(x\\), both \\(\\ell_1\\) and \\(\\ell_2\\) are minimised for \\(f=0.5\\) using the following argument: \\(f=0.5\\) predicts the mean of the conditional distribution \\(Y\\mid X=x\\). The sum of squares (in this case we have actually expectation, but let’s not worry about averaging) is minimised for the mean. Similarly, median optimises the sum of absolute deviations, and each of the normal distributions representing \\(Y-\\hat Y_f \\mid X=x\\) has mean equal to median."
  },
  {
    "objectID": "posts/regression-to-the-mean-bias-rejoinder.html#digression-simple-linear-regression-and-pca",
    "href": "posts/regression-to-the-mean-bias-rejoinder.html#digression-simple-linear-regression-and-pca",
    "title": "Regression to the mean and biased predictions",
    "section": "Digression: simple linear regression and PCA",
    "text": "Digression: simple linear regression and PCA\nThis section perhaps may be distracting and shouldn’t really be a part of this post, but I couldn’t resist: I still very much like this digression and I’m grateful for the opportunity to figure it out together with David.\n\nSimple linear regression\nWhat would happen if we fitted simple linear regression, \\(y=a+bx\\)? As we assume that we have a very large sample size, sample covariance is pretty much the same as the population covariance. Hence, the slope is given by \\[\nb = \\mathrm{Cov}(X, Y)/\\mathbb{V}[X] = 0.5\\sigma^2/\\sigma^2 = 0.5\n\\]\nand the intercept is \\[\n    a = \\mathbb E[Y] - b\\cdot \\mathbb E[X] = \\mu - 0.5\\mu = 0.5\\mu,\n\\] so that the line is \\(y = 0.5(\\mu + x)\\) and corresponds to the regression to the mean estimator \\(\\hat Y_{0.5}\\). It shouldn’t be surprising: simple linear regression minimises the overall squared error. For each value \\(x\\) of \\(X\\) we actually know that it should be \\(\\mathbb E[Y\\mid X=x]\\), which is exactly \\(\\hat Y_{0.5}\\).\n\n\nPrincipal component analysis\nWhat is the first principal component? For PCA we center the data, so that the principal component will be passing through \\((\\mu, \\mu)\\). To find the slope, we need to find the eigenvector corresponding to the largest value of the covariance matrix of the data. We don’t really need to worry about the positive \\(\\sigma^2\\) factor, so let’s find the eigenvector of the matrix \\[\n\\begin{pmatrix}\n    1 & 0.5\\\\\n    0.5 & 1\n\\end{pmatrix}.\n\\]\nThe largest eigenvalue is \\(1.5\\) with an eigenvector \\((1, 1)\\) (and the other eigenvalue is \\(0.5\\) with eigenvector, of course orthogonal, \\((-1, 1)\\)). Hence, the line describing the principal component is given by \\((x-\\mu, y-\\mu) = t (1, 1)\\), where \\(t\\in \\mathbb R\\), which is the same line as \\(y=x\\). We see that this is essentially the \\(\\hat Y_1\\) estimator."
  },
  {
    "objectID": "posts/regression-to-the-mean-bias-rejoinder.html#footnotes",
    "href": "posts/regression-to-the-mean-bias-rejoinder.html#footnotes",
    "title": "Regression to the mean and biased predictions",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nYes, height can’t be negative. But let’s ignore that: it’s a toy, but still informative, problem.↩︎\nIt would be a wasted opportunity to not mention this wonderful joke. The whole lecture is a true gem.↩︎"
  },
  {
    "objectID": "posts/histograms-vs-density-estimation.html",
    "href": "posts/histograms-vs-density-estimation.html",
    "title": "Histograms or kernel density estimators?",
    "section": "",
    "text": "I have recently seen Michael Betancourt’s talk in which he explains why kernel density estimators can be misleading when visualising samples and points to his wonderful case study which includes comparison between histograms and kernel density estimators, as well as many other things.\nI recommend reading this case study in depth; in this blog post we will only try to reproduce the example with kernel density estimators in Python."
  },
  {
    "objectID": "posts/histograms-vs-density-estimation.html#problem-setup",
    "href": "posts/histograms-vs-density-estimation.html#problem-setup",
    "title": "Histograms or kernel density estimators?",
    "section": "Problem setup",
    "text": "Problem setup\nWe will start with a Gaussian mixture with two components and draw the exact probability density function (PDF) as well as a histogram with a very large sample size.\n\n\nCode\nimport matplotlib.pyplot as plt\nimport numpy as np \nfrom scipy import stats\n\nplt.style.use(\"dark_background\")\n\nclass GaussianMixture:\n  def __init__(self, proportions, mus, sigmas) -&gt; None:\n    proportions = np.asarray(proportions)\n    self.proportions = proportions / proportions.sum()\n    assert np.min(self.proportions) &gt; 0\n\n    self.mus = np.asarray(mus)\n    self.sigmas = np.asarray(sigmas)\n\n    n = len(self.proportions)\n    self.n_classes = n\n    assert self.proportions.shape == (n,)\n    assert self.mus.shape == (n,)\n    assert self.sigmas.shape == (n,)\n\n  def sample(self, rng, n: int) -&gt; np.ndarray:\n    z = rng.choice(\n      self.n_classes,\n      p=self.proportions,\n      replace=True,\n      size=n,\n    )\n    return self.mus[z] + self.sigmas[z] * rng.normal(size=n)\n\n  def pdf(self, x):\n    ret = 0\n    for k in range(self.n_classes):\n      ret += self.proportions[k] * stats.norm.pdf(x, loc=self.mus[k], scale=self.sigmas[k])\n    return ret\n\nmixture = GaussianMixture(\n  proportions=[2, 1],\n  mus=[-2, 2],\n  sigmas=[1, 1],\n)\n\nrng = np.random.default_rng(32)\n\nlarge_data = mixture.sample(rng, 100_000)\n\nx_axis = np.linspace(np.min(large_data), np.max(large_data), 101)\npdf_values = mixture.pdf(x_axis)\n\nfig, ax = plt.subplots(figsize=(3, 2), dpi=100)\n\nax.hist(large_data, bins=150, density=True, histtype=\"stepfilled\", alpha=0.5, color=\"C0\")\nax.plot(x_axis, pdf_values, c=\"C2\", linestyle=\"--\")\n\nax.set_title(\"Probability density function\\nand histogram with large sample size\")\n\n\nText(0.5, 1.0, 'Probability density function\\nand histogram with large sample size')\n\n\n\n\n\nGreat, histogram with large sample size agreed well with the exact PDF!"
  },
  {
    "objectID": "posts/histograms-vs-density-estimation.html#plain-old-histograms",
    "href": "posts/histograms-vs-density-estimation.html#plain-old-histograms",
    "title": "Histograms or kernel density estimators?",
    "section": "Plain old histograms",
    "text": "Plain old histograms\nLet’s now move to a more challenging problem: we have only a moderate sample size available, say 100 points.\n\n\nCode\ndata = mixture.sample(rng, 100)\n\nfig, axs = plt.subplots(5, 1, figsize=(3.2, 3*5), dpi=100)\nbin_sizes = (3, 5, 10, 20, 50)\n\nfor bins, ax in zip(bin_sizes, axs):\n  ax.hist(data, bins=bins, density=True, histtype=\"stepfilled\", alpha=0.5, color=\"C0\")\n  ax.plot(x_axis, pdf_values, c=\"C2\", linestyle=\"--\")\n\n  ax.set_title(f\"{bins} bins\")\n\nfig.tight_layout()\n\n\n\n\n\nWe see that too few bins (three, but nobody will actually choose this number for 100 data points) we don’t see two modes and that for more than 20 and 50 bins the histogram looks quite noisy. Both 5 and 10 bins would make a sensible choice in this problem."
  },
  {
    "objectID": "posts/histograms-vs-density-estimation.html#kernel-density-estimators",
    "href": "posts/histograms-vs-density-estimation.html#kernel-density-estimators",
    "title": "Histograms or kernel density estimators?",
    "section": "Kernel density estimators",
    "text": "Kernel density estimators\nNow it’s the time for kernel density estimators. We will use several kernel families and several different bandwidths:\n\n\nCode\nfrom sklearn.neighbors import KernelDensity\n\n\nkernels = [\"gaussian\", \"tophat\", \"cosine\"]\nbandwidths = [0.1, 1.0, 3.0, \"scott\", \"silverman\"]\n\nfig, axs = plt.subplots(\n  len(kernels),\n  len(bandwidths),\n  figsize=(12, 8),\n  dpi=130,\n)\n\nfor i, kernel in enumerate(kernels):\n  axs[i, 0].set_ylabel(f\"Kernel: {kernel}\")\n  for j, bandwidth in enumerate(bandwidths):\n    ax = axs[i, j]\n\n    kde = KernelDensity(bandwidth=bandwidth, kernel=kernel)\n    kde.fit(data[:, None])\n\n    kde_pdf = np.exp(kde.score_samples(x_axis[:, None]))\n\n    ax.plot(x_axis, pdf_values, c=\"C2\", linestyle=\"--\")\n    ax.fill_between(x_axis, 0.0, kde_pdf, color=\"C0\", alpha=0.5)\n\n\nfor j, bandwidth in enumerate(bandwidths):\n  axs[0, j].set_title(f\"Bandwidth: {bandwidth}\")\n\nfig.tight_layout()\n\n\n\n\n\nI see the point now! Apart from the small bandwidth case (0.1 and sometimes Silverman) the issues with KDE plots are hard to diagnose. Moreover, conclusions from different plots are different: is the distribution multimodal? If so, how many modes are there? What are the “probability masses” of each modes? Observing only one of these plots can lead to wrong conclusions."
  },
  {
    "objectID": "posts/histograms-vs-density-estimation.html#links",
    "href": "posts/histograms-vs-density-estimation.html#links",
    "title": "Histograms or kernel density estimators?",
    "section": "Links",
    "text": "Links\n\nWhat’s wrong with a kernel density: a blog post by Andrew Gelman, explaining why he prefers histograms over kernel density plots.\nMichael Betancourt’s case study, which also discusses histograms with error bars."
  },
  {
    "objectID": "posts/em-gibbs-quantification.html",
    "href": "posts/em-gibbs-quantification.html",
    "title": "Expectation-maximization and Gibbs sampling in quantification",
    "section": "",
    "text": "Consider an unlabeled image data set \\(x_1, \\dotsc, x_N\\). We know that each image in this data set corresponds to a unique class (e.g., a cat or a dog) \\(y\\in \\{1, \\dotsc, L\\}\\) and we would like to estimate how many images \\(x_i\\) belong to each class. This problem is known as quantification and there exist numerous approaches to this problem, employing an auxiliary data set. Albert Ziegler and I were interested in additionally quantifying uncertainty1 around such estimates (see Ziegler and Czyż 2023) by building a generative model on summary statistic and performing Bayesian inference.\nWe got a very good question from the reviewer: if we compare our method to point estimates produced by an expectation-maximization algorithm (Saerens, Latinne, and Decaestecker 2001) and we are interested in uncertainty quantification, why don’t we upgrade this method to a Gibbs sampler?\nI like this question, because it’s very natural to ask, yet I overlooked the possibility of doing it. As Richard McElreath explains here, Hamiltonian Markov chain Monte Carlo is usually the preferred way of sampling, but let’s see how exactly the expectation-maximization algorithm works in this case and how to adapt it to a Gibbs sampler."
  },
  {
    "objectID": "posts/em-gibbs-quantification.html#modelling-assumptions",
    "href": "posts/em-gibbs-quantification.html#modelling-assumptions",
    "title": "Expectation-maximization and Gibbs sampling in quantification",
    "section": "Modelling assumptions",
    "text": "Modelling assumptions\nThe model is very similar to the one used in clustering problems: for each object we have an observed random variable \\(X_i\\) (with its realization being the image \\(x_i\\)) and a latent random variable \\(Y_i\\), which is valued in the set of labels \\(\\{1, \\dotsc, L\\}\\).\nAdditionally, there’s a latent vector \\(\\pi = (\\pi_1, \\dotsc, \\pi_L)\\) with non-negative entries, such that \\(\\pi_1 + \\cdots + \\pi_L = 1\\). In other words, vector \\(\\pi\\) is the proportion vector of interest.\nWe can visualise the assumed dependencies in the following graphical model:\n\n\nCode\nimport daft\nimport matplotlib.pyplot as plt\nplt.style.use(\"dark_background\")\n\n# Instantiate a PGM object\npgm = daft.PGM(dpi=200)\n\ndef add_node(id: str, name: str, x: float, y: float, observed: bool = False):\n  if observed:\n    params={\"facecolor\": \"grey\"}\n  else:\n    params={\"edgecolor\": \"w\"}\n  pgm.add_node(id, name, x, y, plot_params=params)\n\ndef add_edge(start: str, end: str):\n  pgm.add_edge(start, end, plot_params={\"edgecolor\": \"w\", \"facecolor\": \"w\"})\n\ndef add_plate(coords, label: str, shift: float):\n  pgm.add_plate(coords, label=label, shift=shift, rect_params={\"edgecolor\": \"w\"})\n\nadd_node(\"pi\", \"$\\\\pi$\", 0, 1)\nadd_node(\"pi\", \"$\\\\pi$\", 0, 1, )\nadd_node(\"Y_i\", r\"$Y_i$\", 2, 1)\nadd_node(\"X_i\", r\"$X_i$\", 4, 1, observed=True)\n\n# Add edges\nadd_edge(\"pi\", \"Y_i\")\nadd_edge(\"Y_i\", \"X_i\")\n\n# Add a plate\nadd_plate([1.5, 0.5, 3, 1.5], label=r\"$i = 1, \\ldots, N$\", shift=-0.1)\n\n# Render and show the PGM\npgm.render()\nplt.show()\n\n\n\n\n\nAs \\(\\pi\\) is simplex-valued, it’s convenient to model it with a Dirichlet prior. Then, \\(Y_i\\mid \\pi \\sim \\mathrm{Categorical}(\\pi)\\). Finally, we assume that each class \\(y\\) has a corresponding distribution \\(D_y\\) from which the image is sampled. In other words, \\(X_i\\mid Y_i=y \\sim D_y\\).\nIn case we know all distributions \\(D_y\\), this is quite a simple problem: we can marginalise the latent variables \\(Y_i\\) obtaining \\[\nP(\\{X_i=x_i\\} \\mid \\pi) = \\prod_{i=1}^N \\big( \\pi_1 D_1(x_i) + \\cdots + \\pi_L D_L(x_i) \\big)\n\\] which in turn can be used to infer \\(\\pi\\) using Hamiltonian Markov chain Monte Carlo algorithms. In fact, a variant of this approach, employing maximum likelihood estimate, rather than Bayesian inference, was proposed by Peters and Coberly (1976) as early as in 1976!"
  },
  {
    "objectID": "posts/em-gibbs-quantification.html#why-expectation-maximization",
    "href": "posts/em-gibbs-quantification.html#why-expectation-maximization",
    "title": "Expectation-maximization and Gibbs sampling in quantification",
    "section": "Why expectation-maximization?",
    "text": "Why expectation-maximization?\nHowever, learning well-calibrated generative models \\(D_y\\) may be very hard task. Saerens, Latinne, and Decaestecker (2001) instead propose to learn a well-calibrated probabilistic classifier \\(P(Y \\mid X, \\pi^{(0)})\\) on an auxiliary population.\nThe assumption on the auxiliary population is the following: the conditional probability distributions \\(D_y = P(X\\mid Y=y)\\) have to be the same. The only thing that can differ is the proportion vector \\(\\pi_0\\), assumed to be known. This assumption is called prior probability shift or label shift and is rather strong, but also quite hard to avoid: if arbitrary distribution shifts are avoided, it’s not possible to generalize from one distribution to another! Finding suitable ways how to weaken the prior probability shift is therefore an interesting research problem on its own.\nNote that if we have a well-calibrated classifier \\(P(Y\\mid X, \\pi^{(0)})\\), we also have an access to a distribution \\(P(Y\\mid X, \\pi)\\). Namely, note that \\[\\begin{align*}\nP(Y=y\\mid X=x, \\pi) &\\propto P(Y=y, X=x \\mid \\pi) \\\\\n&= P(X=x \\mid Y=y, \\pi) P(Y=y\\mid \\pi) \\\\\n&= P(X=x \\mid Y=y)\\, \\pi_y,\n\\end{align*}\n\\] where the proportionality constant does not depend on \\(y\\). Analogously, \\[\nP(Y=y\\mid X=x, \\pi^{(0)}) \\propto P(X=x\\mid Y=y)\\, \\pi^{(0)}_y,\n\\] where the key observation is that for both distributions we assume that the conditional distribution \\(P(X=x\\mid Y=y)\\) is the same. Now we can take the ratio of both expressions and obtain \\[\nP(Y=y\\mid X=x, \\pi) \\propto P(Y=y\\mid X=x, \\pi^{(0)}) \\frac{ \\pi_y }{\\pi^{(0)}_y},\n\\] where the proportionality does not depend on \\(y\\). Hence, we can calculate unnormalized probabilities in this manner and then normalize them, so that they sum up to \\(1\\).\nTo summarize, we have the access to:\n\nWell-calibrated probability \\(P(Y=y\\mid X=x, \\pi)\\);\nThe prior probability \\(P(\\pi)\\);\nThe probability \\(P(Y_i=y \\mid \\pi) = \\pi_y\\);\n\nand we want to do inference on the posterior \\(P(\\pi \\mid \\{X_i\\})\\)."
  },
  {
    "objectID": "posts/em-gibbs-quantification.html#expectation-maximization",
    "href": "posts/em-gibbs-quantification.html#expectation-maximization",
    "title": "Expectation-maximization and Gibbs sampling in quantification",
    "section": "Expectation-maximization",
    "text": "Expectation-maximization\nExpectation-maximization is an iterative algorithm trying to find a stationary point of the log-posterior \\[\\begin{align*}\n\\log P(\\pi \\mid \\{X_i=x_i\\}) &= P(\\pi) + \\log P(\\{X_i = x_i\\} \\mid \\pi) \\\\\n&= P(\\pi) + \\sum_{i=1}^N \\log P(X_i=x_i\\mid \\pi).\n\\end{align*}\n\\]\nIn particular, by running the optimization procedure several times, we can hope to find the maximum a posteriori estimate (or the maximum likelihood estimate, when the uniform distribution over the simplex is used as \\(P(\\pi)\\)). Interestingly, this optimization procedure will not assume that we can compute \\(\\log P(X_i=x_i\\mid \\pi)\\), using instead quantities available to us.\nAssume that at the current iteration the proportion vector is \\(\\pi^{(t)}\\). Then, \\[\\begin{align*}\n\\log P(X_i = x_i\\mid \\pi) &= \\log \\sum_{y=1}^L P(X_i = x_i, Y_i = y\\mid \\pi) \\\\\n&= \\log \\sum_{y=1}^L P(Y_i=y \\mid \\pi^{(t)}, X_i = x_i ) \\frac{ P(X_i=x_i, Y_i=y \\mid \\pi) }{P(Y_i=y \\mid \\pi^{(t)}, X_i=x_i)} \\\\\n&\\ge \\sum_{y=1}^L P(Y_i=y\\mid \\pi^{(t)}, X_i=x_i) \\log \\frac{P(X_i=x_i, Y_i=y \\mid \\pi)}{P(Y_i=y \\mid \\pi^{(t)}, X_i=x_i)}\n\\end{align*}\n\\]\nwhere the inequality follows from Jensen’s inequality for concave functions2.\nWe can now bound the loglikelihood by \\[\\begin{align*}\n\\log P(\\{X_i = x_i \\}\\mid \\pi) &= \\sum_{i=1}^N \\log P(X_i=x_i\\mid \\pi) \\\\\n&\\ge \\sum_{i=1}^N \\sum_{y=1}^L P(Y_i=y\\mid \\pi^{(t)}, X_i=x_i) \\log \\frac{P(X_i=x_i, Y_i=y \\mid \\pi)}{P(Y_i=y \\mid \\pi^{(t)}, X_i=x_i)}.\n\\end{align*}\n\\]\nNow let \\[\nQ(\\pi, \\pi^{(t)}) = \\log P(\\pi) + \\sum_{i=1}^N \\sum_{y=1}^L P(Y_i=y\\mid \\pi^{(t)}, X_i=x_i) \\log \\frac{P(X_i=x_i, Y_i=y \\mid \\pi)}{P(Y_i=y \\mid \\pi^{(t)}, X_i=x_i)},\n\\] which is a lower bound on the log-posterior. We will define the value \\(\\pi^{(t+1)}\\) by optimizing this lower bound: \\[\n\\pi^{(t+1)} := \\mathrm{argmax}_\\pi Q(\\pi, \\pi^{(t)}).\n\\]\nLet’s define auxiliary quantities \\(\\xi_{iy} = P(Y_i=y \\mid \\pi^{(t)}, X_i=x_i)\\), which can be calculated using the probabilistic classifier, as outlined above. This is called the expectation step (although we are actually calculating just probabilities, rather than more general expectations). In the new notation we have \\[\nQ(\\pi, \\pi^{(t)}) = \\log P(\\pi) + \\sum_{i=1}^N\\sum_{y=1}^L \\left(\\xi_{iy} \\log P(X_i=x_i, Y_i=y\\mid \\pi) - \\xi_{iy} \\log \\xi_{iy}\\right)\n\\]\nThe term \\(\\xi_{iy}\\log \\xi_{iy}\\) does not depend on \\(\\pi\\), so we don’t have to include it in the optimization. Writing \\(\\log P(X_i = x_i, Y_i=y\\mid \\pi) = \\log D_y(x_i) + \\log \\pi_y\\) we see that it suffices to optimize for \\(\\pi\\) the expression \\[\n\\log P(\\pi) + \\sum_{i=1}^N\\sum_{y=1}^L \\xi_{iy}\\left( \\log \\pi_y + \\log D_y(x_i) \\right).\n\\] Even better: not only \\(\\xi_{iy}\\) does not depend on \\(\\pi\\), but also \\(\\log D_y(x_i)\\)! Hence, we can drop from the optimization the terms requiring the generative models and we are left only with the easy to calculate quantities: \\[\n\\log P(\\pi) + \\sum_{i=1}^N\\sum_{y=1}^L \\xi_{iy} \\log \\pi_y.\n\\]\nLet’s use the prior \\(P(\\pi) = \\mathrm{Dirichlet}(\\pi \\mid \\alpha_1, \\dotsc, \\alpha_L)\\), so that \\(\\log P(\\pi) = \\mathrm{const.} + \\sum_{y=1}^L (\\alpha_y-1)\\log \\pi_y\\). Hence, we are interested in optimising \\[\n\\sum_{y=1}^L \\left((\\alpha_y-1) + \\sum_{i=1}^N \\xi_{iy} \\right)\\log \\pi_y.\n\\]\nWrite \\(A_y = \\alpha_y - 1 + \\sum_{i=1}^N\\xi_{iy}\\). We have to optimize the expression \\[\n\\sum_{y=1}^L A_y\\log \\pi_y\n\\] under a constraint \\(\\pi_1 + \\cdots + \\pi_L = 1\\).\nSaerens, Latinne, and Decaestecker (2001) use Lagrange multipliers, but we will use the first \\(L-1\\) coordinates to parameterise the simplex and write \\(\\pi_L = 1 - (\\pi_1 + \\cdots + \\pi_{L-1})\\). In this case, if we differentiate with respect to \\(\\pi_l\\), we obtain \\[\n\\frac{A_l}{\\pi_l} + \\frac{A_L}{\\pi_L} \\cdot (-1) = 0,\n\\]\nwhich in turn gives that \\(\\pi_y = k A_y\\) for some constant \\(k &gt; 0\\). We have \\[\n\\sum_{y=1}^L A_y = \\sum_{y=1}^L \\alpha_y - L + \\sum_{i=1}^N\\sum_{y=1}^L \\xi_{iy} = \\sum_{y=1}^L \\alpha_y - L + N.\n\\] Hence, \\[\n\\pi_y = \\frac{1}{(\\alpha_1 + \\cdots + \\alpha_L) + N - L}\\left( \\alpha_y-1 + \\sum_{i=1}^N \\xi_{iy} \\right),\n\\] which is taken as the next \\(\\pi^{(t+1)}\\).\nAs a minor observation, note that for a uniform prior over the simplex (i.e., all \\(\\alpha_y = 1\\)) we have \\[\n\\pi^{(t+1)}_y = \\frac 1N\\sum_{i=1}^N P(Y_i=y_i \\mid X_i=x_i, \\pi^{(t)} ).\n\\] Once we have converged to a fixed point and we have \\(\\pi^{(t)} = \\pi^{(t+1)}\\), it very much looks like \\[\nP(Y) = \\frac 1N\\sum_{i=1}^N P(Y_i \\mid X_i, \\pi) \\approx \\mathbb E_{X \\sim \\pi_1 D_1 + \\dotsc + \\pi_L D_L}[ P(Y\\mid X) ]\n\\] when \\(N\\) is large."
  },
  {
    "objectID": "posts/em-gibbs-quantification.html#gibbs-sampler",
    "href": "posts/em-gibbs-quantification.html#gibbs-sampler",
    "title": "Expectation-maximization and Gibbs sampling in quantification",
    "section": "Gibbs sampler",
    "text": "Gibbs sampler\nFinally, let’s think how to implement a Gibbs sampler for this problem. Compared to the expectation-maximization this will be easy.\nTo solve the quantification problem we have to sample from the posterior distribution \\(P(\\pi \\mid \\{X_i\\})\\). Instead, let’s sample from a high-dimensional distribution \\(P(\\pi, \\{Y_i\\} \\mid \\{X_i\\})\\) — once we have samples of the form \\((\\pi, \\{Y_i\\})\\) we can simply forget about the \\(Y_i\\) values.\nThis is computationally a harder problem (we have many more variables to sample), however each sampling step will be very convenient. We will alternatively sample from \\[\n\\pi \\sim P(\\pi \\mid \\{X_i, Y_i\\})\n\\] and \\[\n\\{Y_i\\} \\sim P(\\{Y_i\\} \\mid \\{X_i\\}, \\pi).\n\\]\nThe first step is easy: \\(P(\\pi \\mid \\{X_i, Y_i\\}) = P(\\pi\\mid \\{Y_i\\})\\) which (assuming a Dirichlet prior) is a Dirichlet distribution. Namely, if \\(P(\\pi) = \\mathrm{Dirichlet}(\\alpha_1, \\dotsc, \\alpha_L)\\), then \\[\nP(\\pi\\mid \\{Y_i=y_i\\}) = \\mathrm{Dirichlet}\\left( \\alpha_1 + \\sum_{i=1}^N \\mathbf{1}[y_i = 1], \\dotsc, \\alpha_L + \\sum_{i=1}^N \\mathbf{1}[y_i=L] \\right).\n\\]\nLet’s think how to sample \\(\\{Y_i\\} \\sim P(\\{Y_i\\} \\mid \\{X_i\\}, \\pi)\\). This is a high-dimensional distribution, so let’s… use Gibbs sampling. Namely, we can iteratively sample \\[\nY_k \\sim P(Y_k \\mid \\{Y_1, \\dotsc, Y_{k-1}, Y_{k+1}, \\dotsc, Y_L\\}, \\{X_i\\}, \\pi).\n\\]\nThanks to the particular structure of this model, this is equivalent to sampling from \\[\nY_k \\sim P(Y_k \\mid X_k, \\pi) = \\mathrm{Categorical}(\\xi_{k1}, \\dotsc, \\xi_{kL}),\n\\] where \\(\\xi_{ky} = P(Y_k = y\\mid X_k = x_k, \\pi)\\) is obtained by recalibrating the given classifier."
  },
  {
    "objectID": "posts/em-gibbs-quantification.html#summary",
    "href": "posts/em-gibbs-quantification.html#summary",
    "title": "Expectation-maximization and Gibbs sampling in quantification",
    "section": "Summary",
    "text": "Summary\nTo sum up, the reviewer was right: it’s very simple to upgrade the inference scheme in this model from a point estimate to a sample from the posterior!\nI however haven’t run simulations to know how well this sampler works in practice: I expect that this approach could suffer from:\n\nProblems from not-so-well-calibrated probabilistic classifier.\nEach iteration of the algorithm (whether expectation-maximization or a Gibbs sampler) requires passing through all \\(N\\) examples.\nAs there are \\(N\\) latent variables sampled, the convergence may perhaps be slow.\n\nIt’d be interesting to see how problematic these points are in practice (perhaps not at all!)"
  },
  {
    "objectID": "posts/em-gibbs-quantification.html#appendix-numerical-implementation-in-jax",
    "href": "posts/em-gibbs-quantification.html#appendix-numerical-implementation-in-jax",
    "title": "Expectation-maximization and Gibbs sampling in quantification",
    "section": "Appendix: numerical implementation in JAX",
    "text": "Appendix: numerical implementation in JAX\nAs these algorithms are so simple, let’s quickly implement them in JAX. We will consider two Gaussian densities \\(D_1 = \\mathcal N(0, 1^2)\\) and \\(D_2 = \\mathcal N(\\mu, 1^2)\\). Let’s generate some data:\n\n\nCode\nimport jax\nimport jax.numpy as jnp\nfrom jax import random\nfrom jaxtyping import Array, Float, Int\nfrom jax.scipy.special import logsumexp\n\nn_cases: Int[Array, \" classes\"] = jnp.asarray([10, 40], dtype=int)\nmus: Float[Array, \" classes\"] = jnp.asarray([0.0, 1.0])\n\nkey = random.PRNGKey(42)\nkey, *subkeys = random.split(key, len(n_cases) + 1)\n\nxs: Float[Array, \" points\"] = jnp.concatenate(tuple(\n  mu + random.normal(subkey, shape=(n,))\n  for subkey, n, mu in zip(subkeys, n_cases, mus)\n))\n\nn_classes: int = len(n_cases)\nn_points: int = len(xs)\n\n\nAn NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n\n\nNow we need a probabilistic classifier. We will assume that it was calibrated on population with proportion \\(\\pi^{(0)} = (0.4, 0.6)\\).\n\n\nCode\n_normalizer: float = 0.5 * jnp.log(2 * jnp.pi)\n\ndef log_p(x, mu: float) -&gt; float:\n  \"\"\"Log-density N(x | mu, 1^2).\"\"\"\n  return -0.5 * jnp.square(x - mu) - _normalizer\n\n\n# Auxiliary matrix log P(X | Y)\n_log_p_x_y: Float[Array, \"points classes\"] = jnp.stack(tuple(log_p(xs, mu) for mu in mus)).T\nassert _log_p_x_y.shape == (n_points, n_classes), f\"Shape mismatch: {_log_p_x_y.shape}.\"\n\nlog_pi0: Float[Array, \" classes\"] = jnp.log(jnp.asarray([0.4, 0.6]))\n\n# Matrix representing log P(Y | X) for labeled population\nlog_p_y_x: Float[Array, \"points classes\"] = _log_p_x_y + log_pi0[None, :]\n# ... currently it's unnormalized, so we have to normalize it\n\ndef normalize_logprobs(log_ps: Float[Array, \"points classes\"]) -&gt; Float[Array, \"points classes\"]:\n  log_const = logsumexp(log_ps, keepdims=True, axis=-1)\n  return log_ps - log_const\n\nlog_p_y_x = normalize_logprobs(log_p_y_x)\n\n# Let's quickly check if it works\nsums = jnp.sum(jnp.exp(log_p_y_x), axis=1)\nassert sums.shape == (n_points,)\nassert jnp.min(sums) &gt; 0.999, f\"Minimum: {jnp.min(sums)}.\"\nassert jnp.max(sums) &lt; 1.001, f\"Maximum: {jnp.max(sums)}.\"\n\n\n\nExpectation-maximization algorithm\nIt’s time to implement expectation-maximization.\n\n\nCode\ndef expectation_maximization(\n  log_p_y_x: Float[Array, \"points classes\"],\n  log_pi0: Float[Array, \" classes\"],\n  log_start: None | Float[Array, \" classes\"] = None,\n  alpha: Float[Array, \" classes\"] | None = None,\n  n_iterations: int = 10_000,\n) -&gt; Float[Array, \" classes\"]:\n  \"\"\"Runs the expectation-maximization algorithm.\n\n  Args:\n    log_p_y_x: array log P(Y | X) for the calibrated population\n    log_pi0: array log P(Y) for the calibrated population\n    log_start: starting point. If not provided, `log_pi0` will be used\n    alpha: concentration parameters for the Dirichlet prior.\n      If not provided, the uniform prior will be used\n    n_iterations: number of iterations to run the algorithm for\n  \"\"\"\n  if log_start is None:\n    log_start = log_pi0\n  if alpha is None:\n    alpha = jnp.ones_like(log_pi0)\n\n  def iteration(_, log_pi: Float[Array, \" classes\"]) -&gt; Float[Array, \" classes\"]:\n    # Calculate log xi[n, y]\n    log_ps = normalize_logprobs(log_p_y_x + log_pi[None, :] - log_pi0[None, :])\n    # Sum xi[n, y] over n. We use the logsumexp, as we have log xi[n, y]\n    summed = jnp.exp(logsumexp(log_ps, axis=0, keepdims=False))\n    # The term inside the bracket (numerator)\n    numerator = summed + alpha - 1.0\n    # Denominator\n    denominator = jnp.sum(alpha) + log_p_y_x.shape[0] - log_p_y_x.shape[1]\n    return jnp.log(numerator / denominator)\n\n  return jax.lax.fori_loop(\n    0, n_iterations, iteration, log_start\n  )\n\nlog_estimated = expectation_maximization(\n  log_p_y_x=log_p_y_x,\n  log_pi0=log_pi0,\n  n_iterations=1000,\n  # Let's use slight shrinkage towards more uniform solutions\n  alpha=2.0 * jnp.ones_like(log_pi0),\n)\nestimated = jnp.exp(log_estimated)\nprint(f\"Estimated: {estimated}\")\nprint(f\"Actual:    {n_cases / n_cases.sum()}\")\n\n\nEstimated: [0.16425547 0.83574456]\nActual:    [0.2 0.8]\n\n\n\n\nGibbs sampler\nExpectation-maximization returns only a point estimate. We’ll explore the region around the posterior mode with a Gibbs sampler.\n\n\nCode\ndef gibbs_sampler(\n  key: random.PRNGKeyArray,\n  log_p_y_x: Float[Array, \"points classes\"],\n  log_pi0: Float[Array, \" classes\"],\n  log_start: None | Float[Array, \" classes\"] = None,\n  alpha: Float[Array, \" classes\"] | None = None,\n  n_warmup: int = 1_000,\n  n_samples: int = 1_000,\n) -&gt; Float[Array, \"n_samples classes\"]:\n  if log_start is None:\n    log_start = log_pi0\n  if alpha is None:\n    alpha = jnp.ones_like(log_pi0)\n\n  def iteration(\n    log_ps: Float[Array, \" classes\"],\n    key: random.PRNGKeyArray,\n  ) -&gt; tuple[Float[Array, \" classes\"], Float[Array, \" classes\"]]:\n    key, subkey1, subkey2 = random.split(key, 3)\n\n    ys = random.categorical(\n      subkey1,\n      log_ps[None, :] + log_p_y_x - log_pi0[None, :],\n      axis=-1,\n    )\n    counts = jnp.bincount(ys, length=log_pi0.shape[0])\n\n    new_log_pi = jnp.log(\n      random.dirichlet(subkey2, alpha + counts)\n    )\n\n    return new_log_pi, new_log_pi\n\n  _, samples = jax.lax.scan(\n    iteration,\n    log_start,\n    random.split(key, n_warmup + n_samples),\n  )\n  return samples[n_warmup:, :]\n\nkey, subkey = random.split(key)\nsamples = gibbs_sampler(\n  key=subkey,\n  log_p_y_x=log_p_y_x,\n  log_pi0=log_pi0,\n  # Let's use slight shrinkage towards more uniform solutions\n  alpha=2.0 * jnp.ones_like(log_pi0),\n  # Use EM point as a starting point\n  log_start=log_estimated,\n  n_samples=5_000,\n)\nsamples = jnp.exp(samples)\n\nprint(f\"Mean:   {jnp.mean(samples, axis=0)}\")\nprint(f\"Std:    {jnp.std(samples, axis=0)}\")\nprint(f\"Actual: {n_cases / n_cases.sum()}\")\n\n\n/tmp/ipykernel_38981/3942328187.py:2: DeprecationWarning: jax.random.PRNGKeyArray is deprecated. Use jax.Array for annotations, and jax.dtypes.issubdtype(arr.dtype, jax.dtypes.prng_key) for runtime detection of typed prng keys.\n  key: random.PRNGKeyArray,\n/tmp/ipykernel_38981/3942328187.py:17: DeprecationWarning: jax.random.PRNGKeyArray is deprecated. Use jax.Array for annotations, and jax.dtypes.issubdtype(arr.dtype, jax.dtypes.prng_key) for runtime detection of typed prng keys.\n  key: random.PRNGKeyArray,\n\n\nMean:   [0.20171012 0.79828984]\nStd:    [0.09912279 0.09912279]\nActual: [0.2 0.8]\n\n\nLet’s visualise the posterior samples, together with the expectation-maximization solution and the ground truth:\n\n\nCode\nimport matplotlib.pyplot as plt\nplt.style.use(\"dark_background\")\n\nfig, ax = plt.subplots(dpi=150)\n\nbins = jnp.linspace(0, 1, 40)\n\nfor y in range(n_classes):\n  color = f\"C{y+1}\"\n  ax.hist(samples[:, y], bins=bins, density=True, histtype=\"step\", color=color)\n  ax.axvline(n_cases[y] / n_cases.sum(), color=color, linewidth=3)\n  ax.axvline(estimated[y], color=color, linestyle=\"--\")\n\nax.set_title(\"Posterior distribution\")\nax.set_ylabel(\"Posterior density\")\nax.set_xlabel(\"Component value\")\nax.spines[[\"top\", \"right\"]].set_visible(False)\nfig.tight_layout()"
  },
  {
    "objectID": "posts/em-gibbs-quantification.html#footnotes",
    "href": "posts/em-gibbs-quantification.html#footnotes",
    "title": "Expectation-maximization and Gibbs sampling in quantification",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nLet’s call this problem “quantification of uncertainty in quantification problems”.↩︎\nIt’s good to remember: \\(\\log \\mathbb E[A] \\ge \\mathbb E[\\log A]\\).↩︎"
  },
  {
    "objectID": "posts/board-games-monte-carlo.html",
    "href": "posts/board-games-monte-carlo.html",
    "title": "Understanding board games with Monte Carlo simulations",
    "section": "",
    "text": "I like playing board games, but I never remember the probabilities of different interesting events. Let’s code a very simple Monte Carlo simulation to evaluate probabilities used in them, so I can revisit to this website and use it to (maybe eventually) win."
  },
  {
    "objectID": "posts/board-games-monte-carlo.html#fight-or-flight",
    "href": "posts/board-games-monte-carlo.html#fight-or-flight",
    "title": "Understanding board games with Monte Carlo simulations",
    "section": "Fight or flight?",
    "text": "Fight or flight?\nIn the rare days when I find time to play Runebound, I find myself in situations fighting monsters and trying to decide whether I should try to fight them or escape. I know a monster’s strength (high), I know my strength (low), but I don’t know how likely it is that the difference can be compensated by throwing two ten-sided dice.\nLet’s estimate the chances of getting at least \\(X\\) points due to the dice throw.\n\n\nCode\nimport numpy as np\n\nn_simulations: int = 100_000\ndice: int = 10\n\nrng = np.random.default_rng(42)\noccurrences = np.zeros(2 * dice + 1, dtype=float)\n\nthrows = rng.integers(1, dice, endpoint=True, size=(n_simulations, 2))\ntotal = throws.sum(axis=1)\n\nfor t in total:\n    occurrences[:t+1] += 1\n\noccurrences /= n_simulations\n\nfor i, p in enumerate(occurrences):\n    if i &lt; 1:\n        continue\n    print(f\"{i}: {100*p:.1f}%\")\n\n\n1: 100.0%\n2: 100.0%\n3: 99.0%\n4: 97.0%\n5: 94.0%\n6: 90.1%\n7: 85.2%\n8: 79.2%\n9: 72.2%\n10: 64.1%\n11: 55.1%\n12: 45.2%\n13: 36.0%\n14: 28.0%\n15: 21.1%\n16: 15.1%\n17: 10.0%\n18: 6.0%\n19: 3.0%\n20: 1.0%\n\n\nIn this case it’s also very easy to actually calculate the probabilities without Monte Carlo simulation:\n\n\nCode\nprobabilities = np.zeros(2*dice + 1, dtype=float)\n\nfor result1 in range(1, dice + 1):\n    for result2 in range(1, dice + 1):\n        total = result1 + result2\n        probabilities[:total + 1] += 1/dice**2\n\nfor i, p in enumerate(occurrences):\n    if i &lt; 1:\n        continue\n    print(f\"{i}: {100*p:.1f}%\")\n\n\n1: 100.0%\n2: 100.0%\n3: 99.0%\n4: 97.0%\n5: 94.0%\n6: 90.1%\n7: 85.2%\n8: 79.2%\n9: 72.2%\n10: 64.1%\n11: 55.1%\n12: 45.2%\n13: 36.0%\n14: 28.0%\n15: 21.1%\n16: 15.1%\n17: 10.0%\n18: 6.0%\n19: 3.0%\n20: 1.0%\n\n\nThe exact solution requires \\(O(K^2)\\) operations, where one uses two dice with \\(K\\) sides1. For a larger number of dice this solution may not be as tractable, so Monte Carlo approximations may shine."
  },
  {
    "objectID": "posts/board-games-monte-carlo.html#where-should-my-cheese-be",
    "href": "posts/board-games-monte-carlo.html#where-should-my-cheese-be",
    "title": "Understanding board games with Monte Carlo simulations",
    "section": "Where should my cheese be?",
    "text": "Where should my cheese be?\nIn Cashflow one way to win the end-game is to quickly get to the tile with a cheese-shaped token. As this token can be placed in advance, I was wondering what the optimal location of it should be.\nIf I put the token on the first tile, I need to throw exactly one in my first throw or I will need to travel across the whole board to close the loop and have another chance (or try to win the game in another way).\nLet’s use Monte Carlo simulation to estimate where I should put the token so I can win in at most five moves:\n\n\nCode\nimport numpy as np \n\nN_SIMULATIONS: int = 100_000\nN_THROWS: int = 5\nDICE: int = 6  # Number of sides on the dice\nrng = np.random.default_rng(101)\n\nvisitations = np.zeros(N_THROWS * DICE + 1)\n\nfor simulation in range(N_SIMULATIONS):\n    position = 0\n    for throw_index in range(N_THROWS):\n        result = rng.integers(1, DICE, endpoint=True)\n        position += result\n        visitations[position] += 1\n\nfor i in range(N_THROWS * DICE + 1):\n    percentage = 100 * visitations[i] / N_SIMULATIONS\n    print(f\"{i}: {percentage:.1f}\")\n\n\n0: 0.0\n1: 16.5\n2: 19.3\n3: 22.8\n4: 26.4\n5: 30.8\n6: 36.2\n7: 25.2\n8: 26.8\n9: 28.1\n10: 28.6\n11: 28.4\n12: 27.9\n13: 25.8\n14: 25.1\n15: 24.0\n16: 21.8\n17: 19.6\n18: 16.5\n19: 13.9\n20: 11.2\n21: 8.5\n22: 6.2\n23: 4.3\n24: 2.7\n25: 1.6\n26: 0.9\n27: 0.5\n28: 0.2\n29: 0.1\n30: 0.0\n\n\nAgain, we could do this in the exact fashion — for example, for 30 we know that the probability is exactly \\(6^{-5}\\approx 0.013\\%\\), but it’s quite clear that the sixth tile gives decent chances of winning in the first few moves."
  },
  {
    "objectID": "posts/board-games-monte-carlo.html#footnotes",
    "href": "posts/board-games-monte-carlo.html#footnotes",
    "title": "Understanding board games with Monte Carlo simulations",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe implemented solution works in \\(O(K^3)\\) due to the probabilities[:total + 1] operation. If the performance did really matter here, we could store the occurrences and then calculate cumulative sums only once in the end.↩︎"
  },
  {
    "objectID": "posts/kernel-regression-transformer.html",
    "href": "posts/kernel-regression-transformer.html",
    "title": "From kernel regression to the transformer",
    "section": "",
    "text": "I remember that when we read Attention is all you need at a journal club back in 2020, I did not really understand what attention was1.\nFortunately for me, Transformer dissection paper and Cosma Shalizi’s post on the topic appeared, which show the connection between attention and kernel regression. This point of view was exactly what I needed! I like this so much that when I explain attention to other people, I always start from kernel regression."
  },
  {
    "objectID": "posts/kernel-regression-transformer.html#kernel-regression",
    "href": "posts/kernel-regression-transformer.html#kernel-regression",
    "title": "From kernel regression to the transformer",
    "section": "Kernel regression",
    "text": "Kernel regression\nLet’s start with kernel regression as independently proposed by Nadaraya and Watson sixty years ago. We will generate some data with heteroskedastic noise, \\(y = f(x) + n(x)\\epsilon\\) where \\(\\epsilon \\sim \\mathcal N(0, 1)\\), \\(f(x)\\) is the expected value \\(\\mathbb E[y\\mid x]\\) and function \\(n(x)\\) makes the noise heteroskedastic.\nWe’ll plot the observed data points as well as \\(f(x) + 2 n(x)\\) and \\(f(x) - 2n(x)\\) as is often done.\n\n\nCode\nfrom functools import partial\n\nimport jax\nimport jax.numpy as jnp\nfrom jax import random\n\nimport equinox as eqx\nfrom jaxtyping import Float, Array\n\nimport matplotlib.pyplot as plt\nplt.style.use(\"dark_background\")\n\nVector = Float[Array, \" n_points\"]\n\ndef f(X: Vector) -&gt; Vector:\n    return 0.5 * jnp.sin(X) - 1 * jnp.sin(3 * X) + 0.2 * jnp.square(X)\n\ndef n(X: Vector) -&gt; Vector:\n    return 0.2 + 0.05 * jnp.abs(X)\n\nn_points: int = 150\n\nkey = random.PRNGKey(2024)\nkey, subkey = random.split(key)\nX = jnp.linspace(-3, 3, n_points)\nY = f(X) + n(X) * random.normal(subkey, shape=X.shape)\n\nfig, ax = plt.subplots(figsize=(4, 3), dpi=150)\nX_ax = jnp.linspace(-3, 3, 201)\nax.fill_between(\n    X_ax, f(X_ax)- 2 * n(X_ax), f(X_ax) + 2 * n(X_ax), alpha=0.4, color=\"maroon\"\n)\nax.plot(X_ax, f(X_ax), color=\"maroon\", alpha=0.8)\nax.scatter(X, Y, color=\"white\", s=5, alpha=1.0)\nax.set_xlabel(\"$X$\")\nax.set_ylabel(\"$Y$\")\n\n\nAn NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n\n\nText(0, 0.5, '$Y$')\n\n\n\n\n\nWe will be interested in finding \\(f(x)\\) via the weighted average: \\[\n\\hat f(x) = \\sum_{i=1}^n y_i\\, w_i(x)\n\\]\nwhere \\(w_i(x)\\) is the weight of the \\(i\\)-th data point used to estimate the value at \\(x\\). To make it a weighted average, we will ensure that \\(w_1(x) + \\cdots + w_n(x) = 1\\). In case where \\(w_i(x) = 1/n\\) we obtain just constant prediction, equal to the sample average over \\(y_i\\).\nMore generally, consider a positive function \\(K\\colon \\mathcal X \\times \\mathcal X \\to \\mathbb R^+\\) which measures similarity between two data points: we want \\(K(x, x')\\) to attain the largest possible value and for \\(x'\\) very far from \\(x\\) we want to have \\(K(x, x')\\) to be small. For such a function we can form a set of weights via \\[\nw_i(x) = \\frac{K(x, x_i)}{\\sum_{j=1}^n K(x, x_j)}.\n\\]\nLet’s restrict our attention for now to Gaussian kernels, \\(K(x, x'; \\ell) = \\exp \\left(\\left( \\frac{x-x'}{\\ell} \\right)^2 \\right)\\) with lengthscale \\(\\ell\\) and visualise the predictions for different lengthscales. As kernels are parameterised functions, we will use Equinox:\n\n\nCode\nclass GaussianKernel(eqx.Module):\n    _log_lengthscale: float\n\n    def __init__(self, lengthscale: float) -&gt; None:\n        assert lengthscale &gt; 0, \"Lengthscale should be positive.\"\n        self._log_lengthscale = jnp.log(lengthscale)\n\n    @property\n    def lengthscale(self) -&gt; float:\n        return jnp.exp(self._log_lengthscale)\n\n    def __call__(self, x: float, x_: float) -&gt; float:\n        return jnp.exp(-jnp.square((x-x_) / self.lengthscale))\n\n\n    def predict(self, X_test: Float[Array, \" n_test\"], X_obs: Vector, Y_obs: Vector) -&gt; Float[Array, \" n_test\"]:\n        kernel = self\n        def predict_one(x: float) -&gt; float:\n            ks = jax.vmap(partial(kernel, x))(X_obs)\n            ws = ks / (jnp.sum(ks) + 1e-16)\n            return jnp.sum(Y_obs * ws)\n        return jax.vmap(predict_one)(X_test)    \n\n\nkernels = {lengthscale: GaussianKernel(lengthscale) for lengthscale in [3.0, 0.5, 0.25, 0.05]} \n\n\nfig, axs = plt.subplots(2, 2, figsize=(2*4, 2*3), dpi=150, sharex=True, sharey=True)\n\nfor (lengthscale, k), ax in zip(kernels.items(), axs.ravel()):\n    pred = k.predict(X_ax, X_obs=X, Y_obs=Y)\n    ax.set_title(f\"$\\\\ell = {lengthscale}$\")\n    ax.plot(X_ax, f(X_ax), color=\"maroon\", alpha=0.8)\n    ax.plot(X_ax, pred, color=\"orangered\", alpha=0.8)\n    ax.scatter(X, Y, color=\"white\", s=5, alpha=0.8)\n    ax.set_xlabel(\"$X$\")\n    ax.set_ylabel(\"$Y$\")\n\nfig.tight_layout()\n\n\n\n\n\nIt seems that \\(\\ell=3.0\\) results in underfitted, almost constant, predictions, and \\(\\ell=0.05\\) arguably overfits, resulting in predictions changing a bit too quickly. Generally, it seems that \\(\\ell \\approx 0.25\\) is a reasonable choice.\n\nMasked training\nLet’s now think how we could find \\(\\ell\\) algorithmically (and when the true mean curve is not available for comparison!).\nFor example, we could use something like the leave-one-out cross-validation:\n\nHold out a data point \\((x_i, y_i)\\);\nFit the kernel regression with lengthscale \\(\\ell\\) to the data \\((x_1, y_1), \\dotsc, (x_{i-1}, y_{i-1}), (x_{i+1}, y_{i+1}), \\dotsc, (x_n, y_n)\\);\nPredict \\(y_i\\) from \\(x_i\\) given the kernel regression.\n\nLooking at different values \\(\\ell\\) and varying the index \\(i\\) of the hold-out data point may be a reasonable training procedure. Note however that if we use standard squared loss, this will have a drawback that points which are further from the mean (due to heteroskedasticity) will be treated similarly to the data points where the noise is small. We could try to reweight them, but we won’t do that and implement a vanilla variant.\nIn fact, we will try several variants of this approach, allowing to hold out more data points than \\(1\\). In terms of probabilistic interpretation this is even worse: apart from problems with interpreting square loss due to heteroskedasticity, now we are also predicting values at several locations at once, effectively assuming that they are independent, given the observed data. In a way, this is similar to the BERT training. XLNet considers different permutations, being closer to an orderless autoregressive model. Anyway, BERT had impressive performance, so let’s try different variants here:\n\n\nCode\nimport optax\n\n\ndef train(\n    key,\n    model: eqx.Module,\n    X: Vector,\n    Y: Vector,\n    learning_rate: float = 0.2,\n    hold_out_size: int = 1,\n    n_steps: int = 100,\n    print_every: int = 100\n) -&gt; eqx.Module:\n    assert n_steps &gt; 1\n    if print_every is None:\n        print_every = n_steps + 100\n    assert print_every &gt; 0\n    assert learning_rate &gt; 0\n\n    assert X.shape[0] == Y.shape[0]\n    n_total = X.shape[0]\n\n    def split_data(key):\n        \"\"\"Splits the data into training and test.\"\"\"\n        indices = random.permutation(key, jnp.arange(n_total))\n        ind_test = indices[:hold_out_size]\n        ind_obs = indices[hold_out_size:]\n\n        return X[ind_obs], Y[ind_obs], X[ind_test], Y[ind_test]\n\n    @jax.jit\n    def step(\n        model: eqx.Module,\n        opt_state,\n        X_obs: Vector,\n        Y_obs: Vector,\n        X_test: Float[Array, \" n_test\"],\n        Y_test: Float[Array, \" n_test\"],\n    ):\n        def loss_fn(model):\n            preds = model.predict(\n                X_test=X_test,\n                X_obs=X_obs,\n                Y_obs=Y_obs,\n            )\n            return jnp.mean(jnp.square(preds - Y_test))\n\n        loss, grads = jax.value_and_grad(loss_fn)(model)\n        updates, opt_state = optimizer.update(grads, opt_state, model)\n        model = optax.apply_updates(model, updates)\n        return model, opt_state, loss\n\n    optimizer = optax.adam(learning_rate=learning_rate)\n    opt_state = optimizer.init(model)\n\n    losses = []\n\n    for n_step in range(1, n_steps + 1):\n        key, subkey = random.split(key)\n        \n        X_obs, Y_obs, X_test, Y_test = split_data(subkey)\n\n        model, opt_state, loss = step(\n            model,\n            opt_state=opt_state,\n            X_obs=X_obs,\n            Y_obs=Y_obs,\n            X_test=X_test,\n            Y_test=Y_test\n        )\n\n        losses.append(loss)\n\n        if n_step % print_every == 0:\n            avg_loss = jnp.mean(jnp.asarray(losses[-20:]))\n            print(f\"Step: {n_step}\")\n            print(f\"Loss: {avg_loss:.2f}\")\n            print(\"-\" * 14)\n\n    return model\n\n\nfig, axs = plt.subplots(2, 2, figsize=(2*4, 2*3), dpi=150, sharex=True, sharey=True)\n\nfor holdout, ax in zip([1, 10, n_points // 2, int(0.8 * n_points)], axs.ravel()):\n    key, subkey = random.split(key)\n    \n    model = train(\n        key=subkey,\n        model=GaussianKernel(lengthscale=1.0),\n        X=X,\n        Y=Y,\n        print_every=None,\n        hold_out_size=holdout,\n        n_steps=100,\n    )\n    pred = model.predict(X_ax, X_obs=X, Y_obs=Y)\n    ax.set_title(f\"Hold-out={holdout}, $\\ell$={model.lengthscale:.2f}\")\n    ax.plot(X_ax, f(X_ax), color=\"maroon\", alpha=0.8)\n    ax.plot(X_ax, pred, color=\"orangered\", alpha=0.8)\n    ax.scatter(X, Y, color=\"white\", s=5, alpha=0.8)\n    ax.set_xlabel(\"$X$\")\n    ax.set_ylabel(\"$Y$\")\n\nfig.tight_layout()\n\n\n\n\n\nHey, this worked pretty well!\n\n\nMulti-headed kernel regression\nAt this point we’ll introduce yet another modification; later we’ll see that it’s analogous to multi-head attention. Consider a model with \\(H\\) “heads”. Each head will be a kernel with a potentially different lengthscale \\(\\ell_h\\). In this manner, we will allow different heads to capture information at a different lengthscale. Finally, we will combine the predictions using auxiliary parameters \\(u_1, \\dotsc, u_H\\): \\[\n\\hat f(x) = \\sum_{h=1}^H u_h\\, \\hat f_h(x) = \\sum_{h=1}^H u_h\\, \\sum_{i=1}^n y_i \\frac{ K(x, x_i; \\ell_h) }{ \\sum_{j=1}^n K(x, x_j; \\ell_h) }.\n\\]\nLet’s implement it quickly in Equinox:\n\n\nCode\nclass MultiheadGaussianKernel(eqx.Module):\n    kernels: list[GaussianKernel]\n    weights: jax.Array\n\n    def __init__(self, n_heads: int) -&gt; None:\n        assert n_heads &gt; 0\n\n        self.weights = jnp.full(shape=(n_heads,), fill_value=1 / n_heads)\n        self.kernels = [\n            GaussianKernel(lengthscale=l)\n            for l in jnp.linspace(0.1, 3, n_heads)\n        ]\n\n    @property\n    def lengthscale(self) -&gt; list[float]:\n        return [k.lengthscale for k in self.kernels]\n\n    def predict(self, X_test: Float[Array, \" n_test\"], X_obs: Vector, Y_obs: Vector) -&gt; Float[Array, \" n_test\"]:\n        # Shape (kernels, n_test)\n        preds = jnp.stack([k.predict(X_test=X_test, X_obs=X_obs, Y_obs=Y_obs) for k in self.kernels])\n        return jnp.einsum(\"kn,k-&gt;n\", preds, self.weights)\n\nfig, axs = plt.subplots(2, 2, figsize=(2*4, 2*3), dpi=150, sharex=True, sharey=True)\n\nfor n_heads, ax in zip([1, 2, 4, 8], axs.ravel()):\n    key, subkey = random.split(key)\n    \n    model = train(\n        key=subkey,\n        model=MultiheadGaussianKernel(n_heads=n_heads),\n        X=X,\n        Y=Y,\n        print_every=None,\n        hold_out_size=1,\n        n_steps=1_000,\n    )\n    pred = model.predict(X_ax, X_obs=X, Y_obs=Y)\n    ax.set_title(f\"Heads={n_heads}\") # $\\ell$={model.lengthscale:.2f}\")\n    ax.plot(X_ax, f(X_ax), color=\"maroon\", alpha=0.8)\n    ax.plot(X_ax, pred, color=\"orangered\", alpha=0.8)\n    ax.scatter(X, Y, color=\"white\", s=5, alpha=0.8)\n    ax.set_xlabel(\"$X$\")\n    ax.set_ylabel(\"$Y$\")\n\n    u_h_str = \", \".join([f\"{w:.2f}\" for w in model.weights])\n    l_h_str = \", \".join([f\"{k.lengthscale:.2f}\" for k in model.kernels])\n\n    print(f\"Number of heads: {n_heads}\")\n    print(f\"  Combination:  {u_h_str}\")\n    print(f\"  Lengthscales: {l_h_str}\")\n\nfig.tight_layout()\n\n\nNumber of heads: 1\n  Combination:  1.09\n  Lengthscales: 0.16\nNumber of heads: 2\n  Combination:  0.99, -0.10\n  Lengthscales: 0.08, 0.02\nNumber of heads: 4\n  Combination:  1.09, -0.36, -0.24, 0.10\n  Lengthscales: 0.16, 2.95, 2.18, 11.60\nNumber of heads: 8\n  Combination:  0.90, 0.02, 0.86, 0.95, -0.45, -0.53, -0.51, -0.22\n  Lengthscales: 0.08, 9.49, 20.43, 25.16, 210.44, 33.52, 34.75, 82.24\n\n\n\n\n\nWe see that coefficients \\(u_h\\) are not constrained to be positive and they do not have to sum up to 1: we allow an arbitrary linear combination of predictions, rather than a weighted sum. Note also that many heads allow for larger flexibility, although on such a small data set this can arguably result in some amount of overfitting."
  },
  {
    "objectID": "posts/kernel-regression-transformer.html#attention",
    "href": "posts/kernel-regression-transformer.html#attention",
    "title": "From kernel regression to the transformer",
    "section": "Attention",
    "text": "Attention\nRecall the equation \\[\n\\hat f(x) = \\sum_{i=1}^n y_i\\, \\frac{K(x, x_i; \\theta)}{ \\sum_{j=1}^n K(x, x_j; \\theta)},\n\\] where there kernel \\(K\\) is now parameterised by \\(\\theta\\). As we want the kernel to give positive values, let’s write \\[\nK(x, x'; \\theta) = \\exp s_\\theta(x, x')\n\\] for some function \\(s_\\theta\\). Hence, we can write \\[\n\\hat f(x) = \\sum_{i=1}^n y_i \\, \\mathrm{softmax}( s_\\theta(x, x_j)_{j = 1, \\dotsc, n} ).\n\\] The usual approach is to use \\(\\theta = (W^{(q)}, W^{(k)})\\) for matrices mapping from \\(\\mathcal X\\) to some space \\(\\mathbb R^{d_\\text{qk}}\\) and use a scalar product \\[\ns_\\theta(x, x') = \\frac{\\left\\langle W^{(q)}x, W^{(k)}x'\\right\\rangle}{\\sqrt{d_\\text{qk}}} = \\frac{ x^T \\left(W^{(q)}\\right)^T W^{(k)}x'}{\\sqrt{d_\\text{qk}}},\n\\] where the denominator takes various forms and is usually used to ensure that the values are properly normalized and the gradients can propagate through the softmax layer well.\nNow consider another modification. We will write \\(y_i = W^{(v)}x_i\\) for some matrix \\(W^{(v)}\\) mapping from \\(\\mathcal X\\) to some space \\(\\mathbb R^{d_\\text{v}}\\). (One can think that it’s a restriction when it comes to the regression (as we are not using values \\(y_i\\) as provided), but it’s not really a big issue: it just suffices to relabel as “point \\(x_i\\)” a tuple \\((x_i, y_i)\\) and redefine the introduced parameter matrices, so that they first project on the required component.)\nIn this case, we obtain a function \\[\nx\\mapsto \\sum_{i=1}^n W^{(v)}x_i \\, \\mathrm{softmax}\\left(  \\frac{ x^T \\left(W^{(q)}\\right)^T W^{(k)}x_i}{\\sqrt{d_\\text{qk}}}  \\right).\n\\]\nIf we apply this formula to each \\(x\\) from the sequence \\((x_1, \\dotsc, x_n) \\in \\mathcal X^n\\), we obtain a new sequence \\((x_1', \\dotsc, x'_n) \\in \\left(\\mathbb R^{d_\\text{v}}\\right)^n\\). This is exactly the self-attention layer used in transformers. How to obtain multi-head attention? Similarly as in multi-head kernel regression, we will introduce \\(H\\) different “heads” with individual parameters \\(W^{(k)}_h, W^{(q)}_h, W^{(v)}_h\\). Hence, for each data point \\(x\\) in the original sequence, we have \\(H\\) vectors in \\(\\mathbb R^{d_\\text{v}}\\) given by \\[\nx\\mapsto \\sum_{i=1}^n W^{(v)}_hx_i \\, \\mathrm{softmax}\\left(  \\frac{ x^T \\left(W^{(q)}_h\\right)^T W^{(k)}_h x_i}{\\sqrt{d_\\text{qk}}}  \\right) \\in \\mathbb R^{d_\\text{v}}.\n\\]\nIf we want to obtain a mapping into some vector space \\(\\mathcal Y\\), we can now introduce matrices \\(U_h\\colon \\mathbb R^{d_\\text{v}}\\to \\mathcal Y\\), so that in the end we have \\[\nx\\mapsto \\sum_{h=1}^H U_h \\sum_{i=1}^n W^{(v)}_hx_i \\, \\mathrm{softmax}\\left(  \\frac{ x^T \\left(W^{(q)}_h\\right)^T W^{(k)}_h x_i}{\\sqrt{d_\\text{qk}}}  \\right) \\in \\mathcal Y.\n\\]\nTo summarize, multi-head attention maps a sequence \\((x_1, \\dotsc, x_n)\\in \\mathcal X^n\\) to a sequence in \\(\\mathcal Y^n\\) and is parameterised by \\(H\\) tuples of matrices \\((W^{(q)}_h, W^{(k)}_h, W^{(v)}_h, U_h)\\), where index \\(h\\) corresponds to the attention head.\nConveniently, Equinox implements multi-head attention, from which I took dimension annotations."
  },
  {
    "objectID": "posts/kernel-regression-transformer.html#footnotes",
    "href": "posts/kernel-regression-transformer.html#footnotes",
    "title": "From kernel regression to the transformer",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe authors had to explain self-attention, its multi-head variant, the transformer architecture with encoder and decoder block, and positional encoding. All in a short conference paper, so it may indeed appear quite dense in ideas.↩︎"
  },
  {
    "objectID": "posts/clustering-exponential-family-softmax.html",
    "href": "posts/clustering-exponential-family-softmax.html",
    "title": "Softmax, mixtures and exponential families",
    "section": "",
    "text": "Recently, Carl kindly explained his variational classification paper to me. In particular, he recalled the following sentence: “the softmax layer can be interpreted as applying Bayes’ rule (…), assuming that the variables follow exponential family class-conditional distributions”.\nI very much like this observation (as well as the paper), but I did not understand at all why this was true: isn’t that too powerful? Let’s try to rewrite it, so I understand it better: consider a space of features \\(\\mathcal X\\) and a space of labels \\(\\mathcal Y = \\{1, 2, \\dotsc, L\\}\\).\nWe want the conditional distributions \\(P(X\\mid Y=y)\\) to have PDFs with respect to some nice reference measure \\(\\mu\\) on \\(\\mathcal X\\) and we will assume that these PDFs are positive everywhere. For example, (non-singular) multivariate normal and Student distributions have this property on \\(\\mathbb R^n\\) (but truncated normal distributions generally do not).\nThen, we can write \\[\np_{X\\mid Y}(x\\mid y) = \\exp(\\log p_{X\\mid Y}(x\\mid y)) = f(x)\\cdot \\exp\\!\\big( \\langle \\eta_y, T(x) \\rangle  \\big),\n\\]\nwhere \\(T(x) = \\big(\\log p_{X\\mid Y}(x\\mid y) \\big)_{y=1,\\dotsc, L} \\in \\mathbb R^L\\) is called the sufficient statistic; \\(\\eta_y \\in \\mathbb R^L\\) are the \\(y\\)-th standard basis vectors in \\(\\mathbb R^L\\) (i.e., the one-hot encoding) forming the natural parameters, and \\(f(x)=1\\) is there just to make the formula look more familiar: it turns out that if the conditional distributions are fully supported, then they have to form1 an exponential family!\nSo, in a way, whenever we have positive densities, we need to have an exponential family. We can transpose this statement using the quote from the beginning: whenever we have positive probabilities, we need to have softmax! Namely, \\[\\begin{align*}\n  p_{Y\\mid X}(y\\mid x) &= \\frac{ p_{X\\mid Y}(x\\mid y)\\, p_Y(y) }{ \\sum_{y'} p_{X\\mid Y}(x\\mid y')\\, p_Y(y') } \\\\\n  &= \\mathrm{softmax}( \\log p_{X\\mid Y}(x\\mid \\bullet) + \\log p_Y(\\bullet) ).\n\\end{align*}\n\\]"
  },
  {
    "objectID": "posts/clustering-exponential-family-softmax.html#footnotes",
    "href": "posts/clustering-exponential-family-softmax.html#footnotes",
    "title": "Softmax, mixtures and exponential families",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIt may seem that we are missing the log-partition function, \\(A_y\\), but this is indeed the case: \\[\\begin{align*}\nA_y &= \\log \\int_{\\mathcal X} f(x) \\exp(\\langle \\eta_y, T(x)\\rangle) \\, \\mathrm{d}\\mu(x) \\\\\n&= \\log \\int_{\\mathcal X} p_{X\\mid Y}(x\\mid y) \\, \\mathrm{d} \\mu(x) \\\\\n&= \\log 1 = 0.\n\\end{align*}\n\\]↩︎"
  },
  {
    "objectID": "posts/determinant-multilinear.html",
    "href": "posts/determinant-multilinear.html",
    "title": "Matrix determinant from the exterior algebra viewpoint",
    "section": "",
    "text": "In every linear algebra course matrix determinant is a must. Often it is introduced in the following form:\nThe definition above has a lot of advantages, but it also has an important drawback — the “why” of this construction is hidden and appears only later in a long list of its properties.\nWe’ll take an alternative viewpoint, which I have learned from Darling (1994, chap. 1), and is based around the exterior algebra."
  },
  {
    "objectID": "posts/determinant-multilinear.html#motivational-examples",
    "href": "posts/determinant-multilinear.html#motivational-examples",
    "title": "Matrix determinant from the exterior algebra viewpoint",
    "section": "Motivational examples",
    "text": "Motivational examples\nConsider \\(V=\\mathbb R^3\\). For vectors \\(v\\) and \\(w\\) we can define their vector product \\(v\\times w\\) with the following properties:\n\nBilinearity: \\((\\lambda v+v')\\times w = \\lambda (v\\times w) + v'\\times w\\) and \\(v\\times (\\lambda w+w') = \\lambda (v\\times w) + v\\times w'\\).\nAntisymmetry: \\(v\\times w = -w\\times v\\).\n\nGeometrically we can think of it as of a signed area of the parallelepiped spanned by \\(v\\) and \\(w\\).\nFor three vectors \\(v, w, u\\) we can form signed volume: \\[\\langle v, w, u\\rangle = v\\cdot (w\\times u),\\] which has similar properties:\n\nTrilinearity: \\(\\langle \\lambda v+v', w, u \\rangle = \\lambda \\langle v, w, u \\rangle + \\langle v', w, u\\rangle\\) (and similarly in \\(w\\) and \\(u\\) arguments).\nAntisymmetry: when we swap any two arguments the sign changes, e.g., \\(\\langle v, w, u\\rangle = -\\langle w, v, u\\rangle = \\langle w, u, v\\rangle = -\\langle u, w, v\\rangle\\).\n\nExterior algebra will be a generalisation of the above construction beyond the three-dimensional space \\(V=\\mathbb R^3\\)."
  },
  {
    "objectID": "posts/determinant-multilinear.html#exterior-algebra",
    "href": "posts/determinant-multilinear.html#exterior-algebra",
    "title": "Matrix determinant from the exterior algebra viewpoint",
    "section": "Exterior algebra",
    "text": "Exterior algebra\nLet’s start with the natural definition:\n\nDefinition 2 (Antisymmetric multilinear function) Let \\(V\\) and \\(U\\) be vector spaces and \\(f\\colon V\\times V \\times \\cdots \\times V \\to U\\) be a function. We will say that it is multilinear if for all \\(i = 1, 2, \\dotsc, n\\) it holds that \\[\nf(v_1, v_2, \\dotsc, \\lambda v_i + v_i', v_{i+1}, \\dotsc, v_n) = \\lambda f(v_1, \\dotsc, v_i, \\dotsc, v_n) + f(v_1, \\dotsc, v_i', \\dotsc, v_n).\n\\] We will say that it is antisymmetric if it changes the sign whenever we swap any two arguments: \\[\nf(v_1, \\dotsc, v_i, \\dotsc, v_j, \\dotsc, v_n) = -f(v_1, \\dotsc, v_j, \\dotsc, v_i, \\dotsc, v_n).\n\\]\n\nAs we have seen above both \\((v, w)\\mapsto v\\times w\\) and \\((v, w, u)\\mapsto v\\cdot (w\\times u)\\) are antisymmetric multilinear functions.\nNote that for every \\(\\sigma\\in S_n\\) it holds that \\[\nf(v_1, \\dotsc, v_n) = \\mathrm{sgn}\\,\\sigma \\, f(v_{\\sigma(1)}, \\dotsc, v_{\\sigma(n)})\n\\] as \\(\\mathrm{sgn}\\,\\sigma\\) counts transpositions modulo 2.\n\nExercise 1 Let \\(f\\colon V\\times V\\to U\\) be multilinear. Show that the following are equivalent:\n\n\\(f\\) is antisymmetric, i.e., \\(f(v, w) = -f(w, v)\\) for every \\(v, w \\in V\\).\n\\(f\\) is alternating, i.e., \\(f(v, v) = 0\\) for every \\(v\\in V\\).\n\nGeneralise to multilinear mappings \\(f\\colon V\\times V \\times \\cdots\\times V\\to U\\).\n\n\n\n\n\n\nHint\n\n\n\n\n\nExpand \\(f(v+w, v+w)\\) using multilinearity.\n\n\n\n\nNow we are ready to construct (a particular) exterior algebra.\n\nDefinition 3 (Second exterior power) Let \\(V\\) be a vector space. Its second exterior power \\(\\bigwedge^2 V\\) we be the vector space of expressions \\[\n\\lambda_1 v_1\\wedge w_1 + \\cdots + \\lambda_n v_n\\wedge w_n\n\\] with the following rules:\n\nThe wedge \\(\\wedge\\) operator is bilinear, i.e., \\((\\lambda v+v')\\wedge w = \\lambda v\\wedge w + v'\\wedge w\\) and \\(v\\wedge (\\lambda w+w') = \\lambda v\\wedge w + v\\wedge w'\\).\n\\(\\wedge\\) is antisymmetric, i.e., \\(v\\wedge w = -w\\wedge v\\) (or, equivalently, \\(v\\wedge v=0\\)).\nIf \\(e_1, \\dotsc, e_n\\) is a basis of \\(V\\), then \\[\\begin{align*}\n    &e_1\\wedge e_2, e_1\\wedge e_3, \\dotsc, e_1\\wedge e_n, \\\\\n    &e_2\\wedge e_3, \\dotsc, e_2\\wedge e_n\\\\\n    &\\qquad\\vdots\\\\\n    &e_{n-1}\\wedge e_n\n    \\end{align*}\n    \\] is a basis of \\(\\bigwedge^2 V\\).\n\n\nNote that \\(v\\wedge w\\) has the interpretation of a signed area of the parallelepiped spanned by \\(v\\) and \\(w\\). Such parallelepipeds can be formally added and there is a resemblance between the wedge product and the vector product in \\(\\mathbb R^3\\).\nWe just need to prove that such a space actually exists (this construction can be skipped at the first reading): similarly to the tensor space, build the free vector space on the set \\(V\\times V\\). Now quotient it by expressions like \\((v, v)\\), \\((\\lambda v, w) - (v, \\lambda w)\\), \\((v+v', w) - (v, w) - (v', w)\\) and \\((v, w+w') - (v, w) - (v, w')\\).\nThen define \\(v\\wedge w\\) to be the equivalence class \\([(v, w)]\\).\n\n\n\n\n\n\nNote\n\n\n\n\n\nIf we had introduced the determinant by other means, we could construct the exterior algebra \\(\\bigwedge^k V\\) also as the space of antisymmetric multilinear functions \\(V^*\\times V^*\\to \\mathbb R\\) (where \\(V^*\\) is the dual space) by\n\\[\n(v\\wedge w)(\\alpha, \\beta) := \\det \\begin{pmatrix}  \\alpha(v_1) & \\alpha(v_2) \\\\ \\beta(v_1) & \\beta(v_2) \\end{pmatrix}\n\\]\n\n\n\nAnalogously we can construct:\n\nDefinition 4 (Exterior power) Let \\(V\\) be a vector space. We define \\(\\bigwedge^0 V = \\mathbb R\\), \\(\\bigwedge^1 V = V\\) and for \\(k\\ge 2\\) its \\(k\\)th exterior power \\(\\bigwedge^k V\\) as the vector space of expressions \\[\n\\lambda_1 a_1\\wedge a_2\\wedge \\cdots\\wedge a_k + \\cdots + \\lambda_n v_1\\wedge v_2 \\wedge \\cdots\\wedge v_k\n\\] such that the wedge operator \\(\\wedge\\) is multilinear and antisymmetric (alternating) and that if \\(e_1, \\dotsc, e_n\\) is a basis of \\(V\\), then the set \\[\n\\{ e_{i_1}\\wedge e_{i_2}\\wedge \\cdots \\wedge e_{i_k}\\mid i_1 &lt; i_2 &lt; \\cdots &lt; i_k \\}\n\\]\nis a basis of \\(\\bigwedge^k V\\).\n\n\nExercise 2 Show that if \\(\\dim V = n\\), then \\(\\dim \\bigwedge^k V = \\binom{n}{k}\\). (And that in particular for \\(k &gt; n\\) we have \\(\\bigwedge^k V = 0\\), the trivial vector space).\n\nThe introduced space can be used to convert between antisymmetric multilinear and linear functions by the means of the universal property:\n\nTheorem 1 (Universal property) Let \\(f\\colon V\\times V \\cdots\\times V\\to U\\) be an antisymmetric multilinear function. Then, there exists a unique linear mapping \\(\\tilde f\\colon \\bigwedge^k V\\to U\\) such that for every set of vectors \\(v_1, \\dotsc, v_k\\) \\[\nf(v_1, \\dotsc, v_k) = \\tilde f(v_1\\wedge \\dotsc \\wedge v_k).\n\\]\n\n\nProof. (Can be skipped at the first reading.)\nAs \\(f\\) is multlilinear, its values are determined by the values on the tuples \\((e_{i_1}, \\dotsc, e_{i_k})\\), where \\(\\{e_1, \\dotsc, e_n\\}\\) is a basis of \\(V\\).\nWe can use antisymmetry to show that by “sorting out” the elements such that \\(i_1 \\le i_2\\cdots \\le i_k\\) and defining \\(\\tilde f(e_{i_1} \\wedge \\dotsc, \\wedge e_{i_k}) = f(e_{i_1}, \\dotsc, e_{i_k})\\) we obtain a well-defined mapping. Linearity is easy to proof.\nNow the uniqueness is proven by observing that antisymmetry and multilinearity uniquely prescribe the values at the basis elements of \\(\\bigwedge^k V\\).\n\nIts importance is the following: to show that a linear map \\(\\bigwedge^k V\\to U\\) is well-defined, one can construct a multilinear antisymmetric map \\(V\\times V\\times \\cdots \\times V\\to U\\)."
  },
  {
    "objectID": "posts/determinant-multilinear.html#determinants",
    "href": "posts/determinant-multilinear.html#determinants",
    "title": "Matrix determinant from the exterior algebra viewpoint",
    "section": "Determinants",
    "text": "Determinants\nFinally, we can define the determinant. Note that if \\(\\dim V = n\\), then \\(\\dim \\bigwedge^n V = 1\\).\n\nDefinition 5 (Determinant) Let \\(n=\\dim V\\) and \\(A\\colon V\\to V\\) be a linear mapping. We consider the mapping \\[\n(v_1, \\dotsc, v_n) \\mapsto (Av_1) \\wedge \\cdots \\wedge (Av_n).\n\\]\nAs it is antisymmetric and multilinear, we know that it induces a unique linear mapping \\(\\bigwedge^n V\\to \\bigwedge^n V\\).\nBecause \\(\\bigwedge^n V\\) is one-dimensional, this mapping must be multiplication by a number. Namely, we define the determinant \\(\\det A\\) to be the number such that for every set of vectors \\(v_1, \\dotsc, v_n\\) \\[\nAv_1 \\wedge \\cdots \\wedge Av_n = \\det A\\, (v_1\\wedge \\cdots \\wedge v_n).\n\\]\n\nIn other words, determinant measures the volume stretch of the parallelepiped spanned by the vectors after they are transformed by the mapping.\nI like this geometric intuition, especially that it is clear that determinant depends only on the linear map, rather than a particular matrix representation — it is independent on the chosen basis.\nWe can now show a number of lemmata.\n\nProposition 1 If \\(\\mathrm{id}_V\\colon V\\to V\\) is the identity mapping, then \\(\\det \\mathrm{id}_V = 1\\).\n\n\nProof. Obvious from the definition! Similarly, it’s clear that \\(\\det \\left(\\lambda\\cdot \\mathrm{id}_V\\right) = \\lambda^{\\dim V}\\).\n\n\nProposition 2 For every two mappings \\(A, B\\colon V\\to V\\) it holds that \\(\\det (B\\circ A) = \\det B\\cdot \\det A\\).\n\n\nProof. For every set of vectors we have \\[\n\\begin{align*}\n\\det (B\\circ A) \\, v_1\\wedge \\cdots \\wedge v_n &= (BAv_1) \\wedge \\cdots \\wedge (BAv_n) \\\\\n&= B(Av_1) \\wedge \\cdots \\wedge B(Av_n) \\\\\n&= \\det B \\, (Av_1) \\wedge \\cdots \\wedge (Av_n) \\\\\n&= \\det B\\cdot \\det A\\, v_1\\wedge \\cdots\\wedge v_n.\n\\end{align*}\n\\]\n\n\nProposition 3 (Only invertible matrices have non-zero determinants) A mapping is an isomorphism if and only if it has non-zero determinant.\n\n\nProof. If the mapping is invertible, then \\(A\\circ A^{-1} = \\mathrm{id}\\) and we have \\(\\det A \\cdot \\det A^{-1} = 1\\), so its determinant must be non-zero.\nNow assume that the mapping is non-invertible. This means that there exists a non-zero vector \\(k\\in \\ker A\\) such that \\(Ak=0\\). Let’s complete \\(k\\) to a basis \\(k, e_1, \\dotsc, e_{n-1}\\). Then \\[\n\\det A\\, k\\wedge e_1\\wedge \\cdots\\wedge e_{n-1} = (Ak) \\wedge \\cdots \\wedge (Ae_{n-1}) = 0,\n\\] which means that \\(\\det A=0\\) as \\(\\{k\\wedge e_1\\wedge \\dotsc \\wedge e_{n-1}\\}\\) is a basis of \\(\\bigwedge^n V\\).\n\nLet’s now connect the usual definition of the determinant to the one coming from exterior algebra:\n\nProposition 4 (Recovering the standard expression) Let \\(e_1, \\dotsc, e_n\\) be a basis of \\(V\\) and \\((A^{i}_j)\\) be the matrix of coordinates, i.e., \\[\nAe_k = \\sum_i A^{i}_k e_i.\n\\] Then the determinant \\(\\det A\\) can be calculated as \\[\n\\det A = \\sum_{\\sigma\\in S_n} \\mathrm{sgn}\\,\\sigma \\, A^{\\sigma(1)}_1 A^{\\sigma(2)}_2 \\dotsc A^{\\sigma(n)}_n.\n\\]\n\n\nProof. Observe that \\[\\begin{align*}\n\\det A e_1 \\wedge \\cdots \\wedge e_n &= Ae_1 \\wedge \\cdots \\wedge Ae_n\\\\\n&= \\left( \\sum_{i_1} A^{i_1}_1 e_{i_1} \\right) \\wedge \\cdots \\wedge \\left( \\sum_{i_n} A^{i_n}_n e_{i_n} \\right)\\\\\n&= \\sum_{i_1, \\dotsc, i_n} A^{i_1}A^{i_2}\\cdots A^{i_n} \\, e_{i_1} \\wedge \\cdots \\wedge e_{i_n}.\n\\end{align*}\\]\nNow we see that repeated indices give zero contribution to this sum, so we can only consider the indices which are permutations of \\(1, 2, \\dotsc, n\\). We also see that \\(e_{i_1} \\wedge \\cdots \\wedge e_{i_n}\\) can be then written as \\(\\pm 1\\, e_1\\wedge \\dotsc \\wedge e_n\\), where the sign is the number of required transpositions, that is the sign of the permutation. This ends the proof.\n\nGoing just a bit further into exterior algebra we can also show that matrix transposition does not change the determinant.\nTo represent matrix transposition, we will use the dual mapping: if \\(A\\colon V\\to V\\) there is the dual mapping \\(A^*\\colon V^*\\to V^*\\), given as \\[\n  (A^*\\omega)(v) := \\omega(Av).\n\\]\nWe can therefore build the \\(n\\)th exterior power of \\(V^*\\): \\(\\bigwedge^n (V^*)\\) and consider the determinant \\(\\det A^*\\).\nWe will formally show that\n\nProposition 5 (Determinant of the transpose) Let \\(A\\colon V\\to V\\) be a linear map and \\(A^*\\colon V^*\\to V^*\\) be its dual. Then \\[\n\\det A^* = \\det A.\n\\]\n\n\nProof. To do this we will need an isomorphism \\[\n\\iota \\colon {\\bigwedge}^n (V^*) \\to \\left({\\bigwedge}^n V\\right)^*\n\\] given on basis elements by \\[\n\\iota( \\omega^1 \\wedge \\cdots \\wedge \\omega^n ) (v_1\\wedge \\cdots \\wedge v_n) = \\det \\big(\\omega^i(v_j) \\big)_{i, j = 1, \\cdots, n},\n\\] where on the right side we use any already known formula for the determinant. It is easy to show that this mapping is well-defined and linear, as it descends from a multilinear alternating mapping.\nHaving this, the proof becomes straightforward calculation: \\[\n\\begin{align*}\n  \\det A^* \\iota\\left(  \\omega^1\\wedge \\cdots\\wedge \\omega^n  \\right)(v_1\\wedge \\cdots\\wedge v_n ) &=\n  \\iota\\bigg( \\det A^* \\, \\omega^1\\wedge \\cdots\\wedge \\omega^n  \\bigg)(v_1\\wedge \\cdots\\wedge v_n ) \\\\\n  &=\\iota\\bigg( A^*\\omega^1 \\wedge \\cdots\\wedge A^*\\omega^n  \\bigg )(v_1\\wedge \\cdots\\wedge v_n) \\\\\n  &= \\det \\bigg((A^*\\omega^i)(v_j)\\bigg) = \\det \\bigg( \\omega^i(Av_j ) \\bigg) \\\\\n  &= \\iota\\left(\\omega^1\\wedge \\cdots\\wedge \\omega^n \\right)(Av_1\\wedge\\cdots\\wedge Av_n) \\\\\n  &= \\iota\\left(\\omega^1\\wedge \\cdots\\wedge \\omega^n \\right)(\\det A\\, v_1\\wedge\\cdots\\wedge v_n) \\\\\n  &= \\det A~ \\iota\\left(\\omega^1\\wedge \\cdots\\wedge \\omega^n\\right)(v_1\\wedge\\cdots\\wedge v_n)\n\\end{align*}\n\\]\n\nEstablishing such isomorphisms is quite a nice technique, which also can be used to prove\n\nProposition 6 (Determinant of a block-diagonal matrix) Let \\(A\\colon V\\to V\\) and \\(B\\colon W\\to W\\) be two linear mappings and \\(A\\oplus B\\colon V\\oplus W\\to V\\oplus W\\) be the mapping given by \\[\n(A\\oplus B)(v, w) = (Av, Bw).\n\\]\nThen \\(\\det (A\\oplus B) = \\det A\\cdot \\det B\\).\n\n\nProof. We will use this approach: there exists an isomorphism \\[\n{\\bigwedge}^p (V\\oplus W) \\simeq \\bigoplus_k {\\bigwedge}^k V \\otimes {\\bigwedge}^{p-k} W,\n\\] so if we take \\(n=\\dim V\\) and \\(m=\\dim W\\) and note that \\(\\bigwedge^{p} V = 0\\) for \\(p &gt; n\\) (and similarly for \\(W\\)) we have \\[\n\\iota\\colon {\\bigwedge}^{n+m} (V\\oplus W) \\simeq {\\bigwedge}^n V\\otimes {\\bigwedge}^m W.\n\\] If \\(i\\colon V\\to V\\oplus W\\) and \\(j\\colon W\\to V\\oplus W\\) are the two “canonical” inclusions, this isomorphism is given as \\[\n\\iota\\big( iv_1 \\wedge \\cdots \\wedge iv_n\\wedge jw_1 \\wedge \\cdots \\wedge jw_m \\big) = (v_1\\wedge \\cdots\\wedge v_n) \\otimes (w_1\\wedge\\cdots\\wedge w_m).\n\\] Now we calculate: \\[\\begin{align*}\n(A\\oplus B)( iv_1 \\wedge \\cdots \\wedge iv_n\\wedge jw_1 \\wedge \\cdots \\wedge jw_m ) &=\niAv_1 \\wedge \\cdots \\wedge iAv_n \\wedge jBw_1\\wedge\\cdots\\wedge jBw_m \\\\\n&= \\iota^{-1}\\big( Av_1\\wedge \\cdots\\wedge Av_n \\otimes Bw_1\\wedge\\cdots\\wedge Bw_m   \\big) \\\\\n&= \\iota^{-1}\\big(\\det A\\cdot \\det B\\, v_1\\wedge \\cdots \\wedge v_n \\otimes w_1\\wedge\\cdots\\wedge w_m) \\\\\n&= \\det A\\cdot \\det B \\, \\iota^{-1}\\big( v_1\\wedge \\cdots \\wedge v_n \\otimes w_1\\wedge\\cdots\\wedge w_m \\big)\\\\\n&= \\det A\\cdot \\det B \\, iv_1\\wedge \\cdots \\wedge iv_n \\wedge jw_1\\wedge\\cdots\\wedge jw_m.\n\\end{align*}\n\\]\n\n\nProposition 7 (Determinant of an upper-triangular matrix) Let \\(A\\colon V\\to V\\) be a linear mapping and \\(e_1, \\dotsc, e_n\\) be a basis of \\(V\\) such that matrix \\((A^i_j)\\) is upper-triangular, that is \\[\n\\begin{align*}\n  Ae_1 &= A^1_1 e_1\\\\\n  Ae_2 &= A^1_2 e_1 + A^2_2 e_2\\\\\n  &\\vdots\\\\\n  Ae_n &= A^1_n e_1 + A^2_ne_2 + \\dotsc + A^n_n e_n\n\\end{align*}\n\\] Then \\[\n\\det A = \\prod_{i=1}^n A^i_i.\n\\]\n\nOnce proven, this result can also be used for lower-triangular matrices due to Proposition 5.\n\nProof. Recall that whenever there is \\(i_j=i_k\\), then \\(e_{i_1}\\wedge \\cdots\\wedge e_{i_n} = 0\\). Hence, there is only one term that may be non-zero: \\[\nAe_1\\wedge Ae_2 \\wedge \\cdots \\wedge Ae_n = A^1_1 e_1 \\wedge \\cdots \\wedge A^n_n e_n = \\prod_{i=1}^n A^i_i\\, e_1\\wedge \\cdots\\wedge e_n.\n\\]"
  },
  {
    "objectID": "posts/determinant-multilinear.html#acknowledgements",
    "href": "posts/determinant-multilinear.html#acknowledgements",
    "title": "Matrix determinant from the exterior algebra viewpoint",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nI would like to thank Adam Klukowski for helpful editing suggestions."
  },
  {
    "objectID": "posts/almost-binomial-markov-chain.html",
    "href": "posts/almost-binomial-markov-chain.html",
    "title": "An almost binomial Markov chain",
    "section": "",
    "text": "Recall that \\(Y\\sim \\mathrm{Bernoulli}(p)\\) if \\(Y\\) can attain values from the set \\(\\{0, 1\\}\\) with probability \\(P(Y=1) = p\\) and \\(P(Y=0) = 1-p\\). It’s easy to see that:\n\nFor every \\(k\\ge 1\\) the random variables \\(Y^k\\) and \\(Y\\) are equal.\nThe expected value is \\(\\mathbb E[Y] = p\\).\nThe variance is \\(\\mathbb{V}[Y]=\\mathbb E[Y^2]-\\mathbb E[Y]^2 = p-p^2 = p(1-p).\\) From AM-GM we see that \\(\\mathbb{V}[Y] \\le (1/2)^2=1/4\\).\n\nNow, if we consider independent and identically distributed variables \\(Y_1\\sim \\mathrm{Bernoulli}(p)\\), \\(Y_2\\sim \\mathrm{Bernoulli}(p)\\), …, \\(Y_n\\sim \\mathrm{Bernoulli}(p)\\), we can define a new variable \\(N_n = Y_1 + \\cdots + Y_n\\) and an average \\[ \\bar Y^{(n)} = \\frac{N_n}{n}.\\]\nThe random variable \\(N_n\\) is distributed according to the binomial distribution and it’s easy to calculate the mean \\(\\mathbb E[N_n] = np\\) and variance \\(\\mathbb V[N_n] = np(1-p)\\). Consequently, \\(\\mathbb E[ \\bar Y^{(n)} ] = p\\) and \\(\\mathbb V[\\bar Y^{(n)}] = np(1-p)/n^2 = p(1-p)/n \\le 1/4n\\).\nHence, we see that if we want to estimate \\(p\\), then \\(\\bar Y^{(n)}\\) is a reasonable estimator to use, and we can control its variance by choosing appropriate \\(n\\).\nThinking about very large \\(n\\), recall that the strong law of large numbers guarantees that \\[\nP\\left( \\lim\\limits_{n\\to \\infty} \\bar Y^{(n)} = p \\right) = 1.\n\\]"
  },
  {
    "objectID": "posts/almost-binomial-markov-chain.html#a-bernoulli-random-variable",
    "href": "posts/almost-binomial-markov-chain.html#a-bernoulli-random-variable",
    "title": "An almost binomial Markov chain",
    "section": "",
    "text": "Recall that \\(Y\\sim \\mathrm{Bernoulli}(p)\\) if \\(Y\\) can attain values from the set \\(\\{0, 1\\}\\) with probability \\(P(Y=1) = p\\) and \\(P(Y=0) = 1-p\\). It’s easy to see that:\n\nFor every \\(k\\ge 1\\) the random variables \\(Y^k\\) and \\(Y\\) are equal.\nThe expected value is \\(\\mathbb E[Y] = p\\).\nThe variance is \\(\\mathbb{V}[Y]=\\mathbb E[Y^2]-\\mathbb E[Y]^2 = p-p^2 = p(1-p).\\) From AM-GM we see that \\(\\mathbb{V}[Y] \\le (1/2)^2=1/4\\).\n\nNow, if we consider independent and identically distributed variables \\(Y_1\\sim \\mathrm{Bernoulli}(p)\\), \\(Y_2\\sim \\mathrm{Bernoulli}(p)\\), …, \\(Y_n\\sim \\mathrm{Bernoulli}(p)\\), we can define a new variable \\(N_n = Y_1 + \\cdots + Y_n\\) and an average \\[ \\bar Y^{(n)} = \\frac{N_n}{n}.\\]\nThe random variable \\(N_n\\) is distributed according to the binomial distribution and it’s easy to calculate the mean \\(\\mathbb E[N_n] = np\\) and variance \\(\\mathbb V[N_n] = np(1-p)\\). Consequently, \\(\\mathbb E[ \\bar Y^{(n)} ] = p\\) and \\(\\mathbb V[\\bar Y^{(n)}] = np(1-p)/n^2 = p(1-p)/n \\le 1/4n\\).\nHence, we see that if we want to estimate \\(p\\), then \\(\\bar Y^{(n)}\\) is a reasonable estimator to use, and we can control its variance by choosing appropriate \\(n\\).\nThinking about very large \\(n\\), recall that the strong law of large numbers guarantees that \\[\nP\\left( \\lim\\limits_{n\\to \\infty} \\bar Y^{(n)} = p \\right) = 1.\n\\]"
  },
  {
    "objectID": "posts/almost-binomial-markov-chain.html#a-bit-lazy-coin-tossing",
    "href": "posts/almost-binomial-markov-chain.html#a-bit-lazy-coin-tossing",
    "title": "An almost binomial Markov chain",
    "section": "A bit lazy coin tossing",
    "text": "A bit lazy coin tossing\nAbove we defined a sequence of independent Bernoulli variables. Let’s introduce some dependency between them: define \\[\nY_1\\sim \\mathrm{Bernoulli}(p)\n\\] and, for \\(n\\ge 1\\), \\[\nY_{n+1}\\mid Y_n \\sim w\\,\\delta_{Y_n} +(1-w)\\, \\mathrm{Bernoulli}(p).\n\\]\nHence, to draw \\(Y_1\\) we simply toss a coin, but to draw \\(Y_2\\) we can be lazy with probability \\(w\\) and use the sampled value \\(Y_1\\) or, with probability \\(1-w\\), actually do the hard work of tossing a coin again.\nLet’s think about the marginal distributions, i.e., we observe only the \\(n\\)th coin toss. As \\(Y_n\\) takes values in \\(\\{0, 1\\}\\), it has to be distributed according to some Bernoulli distribution.\nOf course, we have \\(\\mathbb E[Y_1] = p\\), but what is \\(\\mathbb E[Y_2]\\)? Using the law of total expectation we have \\[\n\\mathbb E[Y_2] = \\mathbb E[ \\mathbb E[Y_2\\mid Y_1] ] = \\mathbb E[ w Y_1 + (1-w)p ] = p.\n\\] Interesting! Even if we have large \\(w\\), e.g., \\(w=0.9\\), we will still see \\(Y_2=1\\) with original probability \\(p\\). More generally, we can prove by induction that that \\(\\mathbb E[Y_n] = p\\) for all \\(n\\ge 1\\).\nTo calculate the variance, we could try the law of total variance, but there is a simpler way: from the above observations we see that all the variables are distributed as \\(Y_n\\sim \\mathrm{Bernoulli}(p)\\) (so they are identically distributed, but not independent for \\(w&gt;0\\)) and the variance has to be \\(\\mathbb V[Y_n] = p(1-p)\\).\nLet’s now introduce variables \\(N_n = Y_1 + \\cdots + Y_n\\) and \\(\\bar Y^{(n)}=N_n/n\\). As expectation is a linear operator, we know that \\(\\mathbb E[N_n] = np\\) and \\(\\mathbb E[\\bar Y^{(n)}]=p\\), but how exactly are these variables distributed? Or, at least, can we say anything about their variance?\nIt’s instructive to see what happens for \\(w=1\\): intuitively, we only tossed the coin once, and then just “copied” the result \\(n\\) times, so that the sample size used to estimate \\(\\bar Y^{(n)}\\) is still one.\nMore formally, with probability 1 we have \\(Y_1 = Y_2 = \\cdots = Y_n\\), so that \\(N_n = nY_1\\) and \\(\\mathbb V[N_n] = n^2p(1-p)\\). Then, also with probability 1, we also have \\(\\bar Y^{(n)}=Y_1\\) and \\(\\mathbb V[\\bar Y^{(n)}]=p(1-p)\\).\nMore generally, we have \\[\n\\mathbb V[N_n] = \\sum_{i=1}^n \\mathbb V[Y_i] + \\sum_{i\\neq j} \\mathrm{cov}[Y_i, Y_j]\n\\] and we can suspect that the covariance terms will be non-negative, usually incurring larger variance than a corresponding binomial distribution (obtained from independent draws). Let’s prove that.\n\nMarkov chain\nWe will be interested in covariance terms \\[\\begin{align*}\n\\mathrm{cov}(Y_i, Y_{i+k}) &= \\mathbb E[Y_i\\cdot Y_{i+k}] - \\mathbb E[Y_i]\\cdot \\mathbb E[Y_{i+k}] \\\\\n&= P(Y_i=1, Y_{i+k}=1)-p^2 \\\\\n&= P(Y_i=1)P( Y_{i+k}=1\\mid Y_i=1) -p^2 \\\\\n&= p\\cdot P(Y_{i+k}=1 \\mid Y_i=1) - p^2.\n\\end{align*}\n\\]\nTo calculate the probability \\(P(Y_{i+k}=1\\mid Y_i=1)\\) we need an observation: the sampling procedure defines a Markov chain with the transition matrix \\[\nT = \\begin{pmatrix}\n    P(0\\to 0) & P(0 \\to 1)\\\\\n    P(1\\to 0) & P(1\\to 1)\n\\end{pmatrix}\n= \\begin{pmatrix}\n    w+(1-w)(1-p) & p(1-w)\\\\\n    (1-w)(1-p) & w + p(1-w)\n\\end{pmatrix}.\n\\]\nBy induction and a handy identity \\((1-x)(1+x+\\cdots + x^{k-1}) = 1-x^{k}\\) one can prove that \\[\nT^k = \\begin{pmatrix}\n    1-p(1-w^k) & p(1-w^k)\\\\\n    (1-p)(1-w^k) & p+w^k(1-p),\n\\end{pmatrix}\n\\] from which we can conveniently read \\[\nP(Y_{i+k}=1\\mid Y_i=1) = p+w^k(1-p)\n\\] and \\[\\mathrm{cov}(Y_i, Y_{i+k}) = w^k\\cdot p(1-p).\\]\nGreat, these terms are always non-negative! Let’s do a quick check: for \\(w=0\\) the covariance terms vanish, resulting in \\(\\mathbb V[N_n]=np(1-p) + 0\\) and for \\(w=1\\) we have \\(\\mathbb V[N_n] = np(1-p) + n(n-1)p(1-p)=n^2p(1-p)\\).\nFor \\(w\\neq 1\\) we can use the same identity as before to get \\[\\begin{align*}\n    \\mathbb V[N_n] &= p(1-p)\\cdot \\left(n+\\sum_{i=1}^n \\sum_{k=1}^{n-i} w^k \\right) \\\\\n    &= p(1-p)\\left( n+ \\frac{2 w \\left(w^n-n w+n-1\\right)}{(w-1)^2} \\right)\n\\end{align*}\n\\]\nLet’s numerically check whether this formula seems right:\n\n\nCode\nfrom functools import partial\n\nimport jax\nimport jax.numpy as jnp\nfrom jax import random, lax\n\n@partial(jax.jit, static_argnames=[\"n\"])\ndef simulate_markov_chain(key, n: int, p: float, w: float) -&gt; jnp.ndarray:\n    keys = random.split(key, n)\n\n    def step(i, y):\n        key_w, key_p = random.split(keys[i])\n        y_prev = y[i-1]\n        mixture_sample = random.bernoulli(key_w, w)\n        y_next = jnp.where(mixture_sample, y_prev, random.bernoulli(key_p, p))\n        y = y.at[i].set(y_next)\n        return y\n    \n    y_init = jnp.zeros(n, dtype=jnp.int32)\n    y_init = y_init.at[0].set(random.bernoulli(keys[0], p))\n\n    y_final = lax.fori_loop(1, n, step, y_init)\n    return y_final\n\ndef simulate_correlated_binomial(key, n: int, p: float, w: float) -&gt; int: \n    return simulate_markov_chain(key=key, n=n, p=p, w=w).sum()\n\n@partial(jax.jit, static_argnames=[\"n\", \"n_samples\"])\ndef sample_correlated_binomial(key, n: int, p: float, w: float, n_samples: int = 1_000_000) -&gt; jnp.ndarray:\n    keys = random.split(key, n_samples)\n    return jax.vmap(partial(simulate_correlated_binomial, n=n, p=p, w=w))(keys)\n\ndef variance_correlated_binomial(n: int, p: float, w: float) -&gt; float:\n    factor = n**2\n    if w &lt; 1.0:\n        factor = n + ( 2 * w * (-1 + n - n * w + w**n)) / (-1 + w)**2\n    return p*(1-p) * factor\n\nkey = random.PRNGKey(2024-1-19)\n\ntest_cases = [\n    (10, 0.5, 0.5),\n    (10, 0.3, 0.8),\n    (10, 0.2, 0.1),\n    (5, 0.4, 0.3),\n    (20, 0.8, 0.7),\n]\n\nfor n, p, w in test_cases:\n    key, subkey = random.split(key)\n    approx = jnp.var(sample_correlated_binomial(subkey, n, p, w))\n    exact = variance_correlated_binomial(n, p, w)\n\n    print(f\"Variance (appr.): {approx:.2f}\")\n    print(f\"Variance (exact): {exact:.2f}\")\n    print(\"-\"*23)\n\n\nAn NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n\n\nVariance (appr.): 6.50\nVariance (exact): 6.50\n-----------------------\nVariance (appr.): 11.40\nVariance (exact): 11.40\n-----------------------\nVariance (appr.): 1.92\nVariance (exact): 1.92\n-----------------------\nVariance (appr.): 1.93\nVariance (exact): 1.94\n-----------------------\nVariance (appr.): 15.66\nVariance (exact): 15.65\n-----------------------"
  },
  {
    "objectID": "posts/almost-binomial-markov-chain.html#markov-chain-monte-carlo",
    "href": "posts/almost-binomial-markov-chain.html#markov-chain-monte-carlo",
    "title": "An almost binomial Markov chain",
    "section": "Markov chain Monte Carlo",
    "text": "Markov chain Monte Carlo\nRecall that when the samples are independent, we can estimate \\(p\\) via \\(\\bar Y^{(n)}\\) which is an unbiased estimator, i.e., \\(\\mathbb E[\\bar Y^{(n)}] = p\\) and its variance is \\(\\mathbb V[\\bar Y^{(n)}]=p(1-p)/n\\le 1/4n\\).\nWhen we passed to a Markov chain introducing parameter \\(w\\), we also found out that \\(\\mathbb E[\\bar Y^{(n)}]=p\\). Moreover, for \\(w&lt;1\\) (i.e., there’s some genuine sampling, rather than copying the first result) the variance of \\(N_n\\) also grows as \\(\\mathcal O(n + w^n)=\\mathcal O(n)\\), so that \\(\\mathbb V[\\bar Y^{(n)}] =\\mathcal O(1/n)\\), so that for a large \\(n\\) the estimator \\(\\bar Y^{(n)}\\) can be a reliable estimator for \\(p\\). However, note that in the variance there’s a term \\(1/(1-w)^2\\), so that for \\(w\\) close to \\(1\\) one may have to use very, very, very large \\(n\\) to make sure that the variance is small enough.\nThis Markov chain is in fact connected to Markov chain Monte Carlo samplers, used to sample from a given distribution.\nThe Markov chain \\(Y_2, Y_3, \\dotsc\\) has transition matrix \\(T\\) and initial distribution of \\(Y_1\\) (namely \\(\\mathrm{Bernoulli}(p)\\)). For \\(0 &lt; p &lt; 1\\) and \\(w &lt; 1\\) the transition matrix has positive entries, which implies that this Markov chain is ergodic (both irreducibility and aperiodicity are trivially satisfied in this case; more generally quasi-positivity, i.e., \\(T^k\\) has positive entries for some \\(k\\ge 1\\), is equivalent to ergodicity).\nWe can deduce two things. First, there’s a unique stationary distribution of this Markov chain. It can be found by solving the equations for the eigenvector \\(T^{t}\\pi=\\pi\\); in this case \\(\\pi=(1-p, p)\\) (what a surprise!), meaning that the stationary distribution is \\(\\mathrm{Bernoulli}(p)\\).\nSecondly, we can use the ergodic theorem. The ergodic theorem states that in this case for every function1 \\(f\\colon \\{0, 1\\}\\to \\mathbb R\\) it holds that \\[\nP\\left(\\lim\\limits_{n\\to\\infty} \\frac{1}{n}\\sum_{i=1}^n f(Y_i) = \\mathbb E[f] \\right) = 1\n\\] where the expectation \\(\\mathbb E[f]\\) is taken with respect to \\(\\pi\\).\nNote that for \\(f(x) = x\\) we find out that with probability \\(1\\) it holds that \\(\\lim\\limits_{n\\to \\infty} \\bar Y^{(n)} = p\\).\nPerhaps it’s worth commenting on why the stationary distribution is \\(\\mathrm{Bernoulli}(p)\\). Consider any distribution \\(\\mathcal D\\) and a Markov chain \\[\nY_{n+1} \\mid Y_n \\sim w\\, \\delta_{Y_{n}} + (1-w)\\, \\mathcal D\n\\] for \\(w &lt; 1\\). Intuitively, this Markov chain will either jump to a new location with the right probability, or stay at a current point by some additional time. This additional time depends only on \\(w\\), so that on average, at each point we spend the same time. Hence, it should not affect time averages over very, very, very long sequences. (However, as we have seen, large \\(w\\) may imply large autocorrelation in the Markov chain and the chain would have to be extremely long to yield acceptable variance).\nI think it should not be hard to formalize and prove the above observation, but it’s not for today. This review could be useful for investigating this further."
  },
  {
    "objectID": "posts/almost-binomial-markov-chain.html#how-does-it-differ-from-beta-binomial",
    "href": "posts/almost-binomial-markov-chain.html#how-does-it-differ-from-beta-binomial",
    "title": "An almost binomial Markov chain",
    "section": "How does it differ from beta-binomial?",
    "text": "How does it differ from beta-binomial?\nRecall that a beta-binomial distribution generates samples as follows:\n\nDraw \\(b\\sim \\mathrm{Beta}(\\alpha, \\beta)\\);\nThen, draw \\(M \\mid b \\sim \\mathrm{Binomial}(n, b)\\).\n\nHence, first a random coin is selected from a set of coins with different biases, and then it’s tossed \\(n\\) times. This distribution has two degrees of freedom: \\(\\alpha\\) and \\(\\beta\\), and allows one a more flexible control over both the mean and the variance. The mean is given by \\[\n\\mathbb E[M] = n\\frac{\\alpha}{\\alpha + \\beta},\n\\] so if we write \\(p = \\alpha/(\\alpha + \\beta)\\), we match the mean of a “corresponding” binomial distribution. The variance is given by \\[\n\\mathbb V[M] = np(1-p)\\left(1 + \\frac{n-1}{\\alpha + \\beta + 1} \\right),\n\\] so that for \\(n \\ge 2\\) we will see a larger variance than for a binomial distribution with corresponding mean.\nWe see that this variance is quadratic in \\(n\\), which is different from the formula for the variance of the almost binomial Markov chain. Nevertheless, we can ask ourselves a question whether beta-binomial can be a good approximation to the distribution studied before.\nThis intuition may be formalized in many ways, e.g., as minimization of statistical discrepancy measures, including total variation, various Wasserstain distances or \\(f\\)-divergences. Instead, we will just match mean and variance.\nSo, of course, we will take \\(p=\\alpha/(\\alpha + \\beta)\\) and additionally solve for \\(\\mathbb V[M] = V\\). The solution is then given by \\[\\begin{align*}\n\\alpha &= pR,\\\\\n\\beta &= (1-p)R,\\\\\nR &= \\frac{n^2 p(1-p)-V}{V - n p(1-p)}.\n\\end{align*}\n\\]\nNow it’s coding time! We could use TensorFlow Probability on JAX to sample from beta-binomial distribution, but we will resort to core JAX.\n\n\nCode\nimport matplotlib.pyplot as plt \nplt.style.use(\"dark_background\")\n\ndef find_alpha_beta(n: int, p: float, variance: float) -&gt; tuple[float, float]:\n    num = n**2 * p * (1-p) - variance\n    den = variance - n * p * (1-p)\n    r = num / den\n\n    if r &lt;= 0 or p &lt;= 0 or p &gt;= 1:\n        raise ValueError(\"Input results in non-positive alpha or beta\")\n\n    return p*r, (1-p) * r\n\n@partial(jax.jit, static_argnames=[\"n\"])\ndef _sample_beta_binomial(key, n: int, alpha: float, beta: float) -&gt; int:\n    key_p, key_b = random.split(key)\n    p = random.beta(key_p, a=alpha, b=beta)\n    ber = random.bernoulli(key_b, p=p, shape=(n,))\n    return jnp.sum(ber)\n\n@partial(jax.jit, static_argnames=[\"n\", \"n_samples\"])\ndef sample_beta_binomial(key, n: int, alpha: float, beta: float, n_samples: int = 1_000_000) -&gt; jnp.ndarray:\n    keys = random.split(key, n_samples)\n    return jax.vmap(partial(_sample_beta_binomial, n=n, alpha=alpha, beta=beta))(keys)\n\n\ndef plot_compare(key, ax: plt.Axes, n: int, p: float, w: float, n_samples: int = 1_000_000, n_bins: int | None = None) -&gt; None:\n    variance = variance_correlated_binomial(n=n, p=p, w=w)\n    alpha, beta = find_alpha_beta(n=n, p=p, variance=variance)\n\n    key1, key2 = random.split(key)\n    sample_corr = sample_correlated_binomial(key1, n=n, p=p, w=w, n_samples=n_samples)\n    sample_betabin = sample_beta_binomial(key2, n=n, alpha=alpha, beta=beta, n_samples=n_samples)\n\n    if n_bins is None:\n        bins = jnp.arange(-0.5, n + 1.5, 1)\n    else:\n        bins = jnp.linspace(-0.1, n + 0.1, n_bins)\n\n    ax.hist(\n        sample_corr, bins=bins, density=True, rasterized=True,\n        color=\"yellow\",\n        label=\"Markov chain\",\n        histtype=\"step\",\n    )\n    ax.hist(\n        sample_betabin, bins=bins, density=True, rasterized=True,\n        color=\"orange\",\n        label=\"Beta-binomial\",\n        histtype=\"step\",\n        linestyle=\"--\"\n    )\n    ax.spines[[\"top\", \"right\"]].set_visible(False)\n    ax.set_xlabel(\"Number of heads\")\n    ax.set_ylabel(\"Probability\")\n\n\nfig, _axs = plt.subplots(2, 2, dpi=250)\naxs = _axs.ravel()\n\nkey, *keys = random.split(key, 1 + len(axs))\n\nparams = [\n    # (n, p, w, n_bins)\n    (10, 0.5, 0.9, None),\n    (10, 0.3, 0.2, None),\n    (100, 0.7, 0.98, 41),\n    (100, 0.3, 0.6, 41),\n]\nassert len(params) == len(axs)\n\nfor key, ax, (n, p, w, n_bins) in zip(keys, axs, params):\n    plot_compare(\n        key=key,\n        ax=ax,\n        n=n,\n        p=p,\n        w=w,\n        n_samples=1_000_000,\n        n_bins=n_bins,\n)\naxs[0].legend(frameon=False)\nfig.tight_layout()"
  },
  {
    "objectID": "posts/almost-binomial-markov-chain.html#footnotes",
    "href": "posts/almost-binomial-markov-chain.html#footnotes",
    "title": "An almost binomial Markov chain",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nUsually one has to add that the function is bounded. But we are working with a finite domain \\(\\{0, 1\\}\\), so literally every function is bounded.↩︎"
  },
  {
    "objectID": "posts/setting-up-repository.html",
    "href": "posts/setting-up-repository.html",
    "title": "Setting up a new project repository",
    "section": "",
    "text": "First, a disclaimer: I am not an expert and what I write below is likely to be suboptimal in many ways. On the other hand, in academia we rarely have access to an experienced team of software engineers, DevOps, testers and documentation writers, so perhaps some of the tricks below will be useful.\nSecondly, setting up a new scientific project is generally much more complex than setting up a repository: discussing the project idea with the academic adviser, writing a good research plan, checking with an ethics committee, applying for a grant…\nWe will not cover any of these here, assuming that you have completed the necessary steps and you and your collaborators just want to start coding."
  },
  {
    "objectID": "posts/setting-up-repository.html#on-collaboration-and-mentorship",
    "href": "posts/setting-up-repository.html#on-collaboration-and-mentorship",
    "title": "Setting up a new project repository",
    "section": "On collaboration and mentorship",
    "text": "On collaboration and mentorship\nSometimes you will be the sole developer of the project. More frequently, you will be a member of a team. This includes mentoring students, which includes taking more responsibility, but we will discuss some of these aspects below.\nI think it is important to set the expectations on all sides involved. In the meeting it’s worth to discuss the following:\n\nFinding the time for a regular meeting. (Or, if a regular time slot is impossible, scheduling the next meeting.)\nDoes the whole team adhere to some best coding practices?\nWhat are the views on authorship of the subsequent publication(s)?\n\nHow is it going to be reflected in the expected contributions and time committed to the project?\nI don’t like politics, but this topic will eventually appear, so it’s good to discuss it early.\n\n\nRegarding deciding on coding practices, we often use the following:\n\nWe work within Git feature branch workflow.\nWe try to submit small pull requests and have pull request reviewed happening as soon as possible.\nWe put ideas for next changes as GitHub issues.\nWhen we want to discuss large code changes, sometimes we submit a PR just with function stubs and docstrings. This PR is not really going to be merged, but we can discuss proposed changes using reviewing capabilities, before the time is spent on implementing them.\nWe often do pair programming, which is great.\nWe have continuous integration set using GitHub Actions, which forces us to use consistent style (sometimes to write enough tests and documentation).\n\n\nStudent mentorship\nSpeaking of mentoring students, I have a separate document on this topic. I think it is good to discuss expectations as early as possible and remember that part of our job is mentoring. Of course, it’s great that the students can contribute to the project, but in the end, they should learn useful skills via mentorship.\nSpeaking of coding aspects of mentorship:\n\nSchedule enough time in the calendar to the student at least once a week.\nDiscuss not only the science, but also other issues they may be facing. Isn’t the workload too high?\nDo detailed code reviews (and pair programming, if possible), so they can learn software development from you.\nFor complicated changes, sometimes it may help to write code stubs. I.e., design interfaces, write most important function names (together with type annotations and docstrings) and ask the student to study them and fill in the missing bits.\n\nDesigning right abstractions is hard and it may take a lot of yor time.\nBut remember that this skill is hard even for experienced software developers, and many students haven’t had yet enough experience to handle these problems.\n\n\nTry to find balance with being hand-on and hands-off.\n\nI find it quite difficult and this balance is going to depend on the project difficulty and the student’s needs.\nGenerally, being hands-on is important, so that they feel supported. Mentorship is a key part of our job.\nOn the other hand, micromanagement is bad.\nEven if somebody is not not micromanaging, sometimes being too available can harm the student.\n\nThey need to know how to use available resources and work independently. (Whether they stay in academia or move to industry.)\nDiscuss with the students that they are expected to check obvious resources (like Wikipedia, Google, ChatGPT, project documentation…) before reaching out.\nIn some teams there is something like a formal “15-minute rule” and “1-hour rule”, but I think it’s better to rely on discussing the expectations early.\nIn some cases, answering the questions with a delay of a few hours seemed to reinforce the students to search for the answers on their own. I’m not convinced about this method, though, and would prefer to rely on clear communication of expectations."
  },
  {
    "objectID": "posts/setting-up-repository.html#setting-up-a-repository",
    "href": "posts/setting-up-repository.html#setting-up-a-repository",
    "title": "Setting up a new project repository",
    "section": "Setting up a repository",
    "text": "Setting up a repository\nGreat: the research plan tells the team what they need to do, expectations are clear, but we still haven’t set up the repository.\nI recommend taking studying how Equinox is set up, which is the model solution.\n\nSee .github/workflows to see how GitHub Actions are set.\nNote that pre-commit is used and configured.\nProject manager is used. Equinox uses Hatch, I usually go for Poetry.\n\nMy personal steps to creating a new repository are the following:\n\nCreate a new Micromamba environment. (There are many other alternatives to virtual environments, though.)\nInitialise the project. I use Poetry and the src layout, i.e., poetry new --src project_name.\nCreate a GitHub repository and commit the changes.\nSet up tests using PyTest.\n\nYou can create a simple function and one unit test.\n\nSet up pre-commit checks.\n\nFor linting and formatting, I use Ruff. (Alternatives include Black, flake8 and isort.)\nFor type checking I usually go for Pyright. (Alternatives include MyPy and PyType.)\n\nSet up documentation using Material for Mkdocs.\nMake sure that GitHub Actions are configured and properly run. (As a starting point, consult the configurations of Equinox or pMHN.)\nSet up pull request merging criteria on GitHub (depending on what has been agreed with the team):\n\nNobody can commit to main branch without a pull request.\nPre-commit checks and unit tests run properly.\nThe code is reviewed and conversations are resolved.\nThe branch has all changes from main (disputable).\nThe pull request is squashed into one commit on main, rather than merged (disputable).\n\n\nRegarding the data science workflow (which often includes developing some Python package, but also running experiments and generating the figures):\n\nWe usually develop Python code in src/ and use test/ and docs/ for unit tests for its documentation.\nTo run experiments, we use Snakemake workflows in the workflows/ directory.\n\nEnsuring reproducibility is important.\nMicromamba works very nicely with Snakemake.\nAlternatives include NextFlow (I personally liked Snakemake more, though).\n\nKedro sounds interesting. At the moment of writing this post I haven’t had much experience with it, but I’m planning to try it."
  },
  {
    "objectID": "posts/small-power.html",
    "href": "posts/small-power.html",
    "title": "From \\(t\\)-test to “This is what ‘power=0.06’ looks like”",
    "section": "",
    "text": "Andrew Gelman wrote an amazing blogpost, where he argues that with large noise-to-signal ratio (low power studies) statistically significant results:\nThis is especially troublesome because as there is a tradition of publishing only statistically significant results1, many of the reported effects will have the wrong sign or be seriously exaggerated.\nBut we’ll take a look at these phenomena later. First, let’s review what the \\(t\\)-test, confidence intervals, and statistical power are."
  },
  {
    "objectID": "posts/small-power.html#a-quick-overview-of-t-test",
    "href": "posts/small-power.html#a-quick-overview-of-t-test",
    "title": "From \\(t\\)-test to “This is what ‘power=0.06’ looks like”",
    "section": "A quick overview of \\(t\\)-test",
    "text": "A quick overview of \\(t\\)-test\nRecall that if we have access to i.i.d. random variables\n\\[\nX_1, \\dotsc, X_n \\sim \\mathcal N(\\mu, \\sigma^2),\n\\] we introduce sample mean \\[\nM = \\frac{X_1 + \\cdots + X_n}{n}\n\\] and sample standard deviation: \\[\nS = \\sqrt{ \\frac{1}{n-1} \\sum_{i=1}^n \\left(X_i - M\\right)^2}.\n\\]\nIt follows that \\[\nM\\sim \\mathcal N(\\mu, \\sigma^2/n)\n\\] and \\[\nS^2\\cdot (n-1)/\\sigma^2 \\sim \\chi^2_{n-1}\n\\] are independent. We can construct a pivot \\[\nT_\\mu = \\frac{M-\\mu}{S / \\sqrt{n}},\n\\] which is distributed according to Student’s \\(t\\) distribution2 with \\(n-1\\) degrees of freedom, \\(t_{n-1}\\).\nChoose a number \\(0 &lt; \\alpha &lt; 1\\) and write \\(u\\) for the solution to the equation \\[\nP(T_\\mu \\in (-u, u)) = 1-\\alpha.\n\\]\nThe \\(t\\) distribution is continuous and symmetric around \\(0\\), so that \\[\n\\mathrm{CDF}(-u) = 1 - \\mathrm{CDF}(u)\n\\] and \\[\nP(T_\\mu \\in (-u, u)) = \\mathrm{CDF}(u) - \\mathrm{CDF}(-u) = 2\\cdot \\mathrm{CDF}(u) - 1.\n\\]\nFrom this we have \\[\n\\mathrm{CDF}(u) = 1 - \\alpha / 2.\n\\]\nHence, we have \\[\nP(\\mu \\in ( M - \\delta, M + \\delta )) = 1 - \\alpha,\n\\] where \\[\n\\delta = u \\cdot\\frac{S}{\\sqrt{n}}\n\\] is the half-width of the confidence interval.\nIn this way we have constructed a confidence interval with coverage \\(1-\\alpha\\). Note that \\(u\\) coresponds to the \\((1-\\alpha/2)\\)th quantile. For example, if we want coverage of \\(95\\%\\), we take \\(\\alpha=5\\%\\) and \\(u\\) will be the 97.5th percentile.\n\nHypothesis testing\nConsider a hypothesis \\(H_0\\colon \\mu = 0\\).\nWe will reject \\(H_0\\) if \\(0\\) is outside of the confidence interval defined above. Note that \\[\nP( 0 \\in (M-\\delta, M + \\delta)  \\mid H_0 ) = 1-\\alpha\n\\] and \\[\nP(0 \\not\\in (M-\\delta, M + \\delta) \\mid H_0 ) = 1 - (1-\\alpha) = \\alpha,\n\\] meaning that such a test will have false discovery rate \\(\\alpha\\).\n\n\nInterlude: \\(p\\)-values\nThe above test is often presented in terms of \\(p\\)-values. Define the \\(t\\)-statistic \\[\nT_0 = \\frac{M}{S/\\sqrt{n}},\n\\] which does not require the knowledge of a true \\(\\mu\\). Note that if \\(H_0\\) is true, then \\(T_0\\) will be distributed according to \\(t_{n-1}\\) distribution. (If \\(H_0\\) is false, then it won’t be. We will take a closer look at this case when we discuss power).\nWe have \\(T_0 \\in (-u, u)\\) if and only if \\(0\\) is inside the confidence interval defined above: we can reject \\(H_0\\) whenever \\(|T_0| \\ge u\\). As the procedure hasn’t really changed, we also have \\[\nP(T_0 \\in (-u, u) \\mid H_0 ) = P( |T_0| &lt; u \\mid H_0 ) = 1-\\alpha\n\\] and this test again has false discovery rate \\(\\alpha\\).\nNow define \\[\n\\Pi = 2\\cdot (1 - \\mathrm{CDF}( |T_0| ) ) = 2\\cdot \\mathrm{CDF}(-|T_0|).\n\\] Note that we have\n\\[\\begin{align*}\n  \\Pi &lt; \\alpha &\\Leftrightarrow 2\\cdot (1-\\mathrm{CDF}(|T_0|)) &lt; 2\\cdot (1-\\mathrm{CDF}(u)) \\\\\n  &\\Leftrightarrow 1- \\mathrm{CDF}(|T_0|) &lt; 1 - \\mathrm{CDF}(u) \\\\\n  &\\Leftrightarrow \\mathrm{CDF}(-|T_0|) &lt; \\mathrm{CDF}(-u)\\\\\n  &\\Leftrightarrow -|T_0| &lt; -u \\\\\n  &\\Leftrightarrow |T_0| &gt; u,\n\\end{align*}\n\\]\nso that we can compare the \\(p\\)-value \\(\\Pi\\) against false discovery rate \\(\\alpha\\).\nBoth comparisons are equivalent and equally hard to compute: the hard part of comparing \\(|T_0|\\) against \\(u\\) is calculation of \\(u\\) from \\(\\alpha\\) (which requires the access to the CDF). The hard part of comparing the \\(p\\)-value \\(\\Pi\\) against \\(\\alpha\\) is calculation of \\(\\Pi\\) from \\(|T_0|\\) (which requires the access to the CDF). I should also add that, as we will see below, the CDF is actually easy to access in Python.\nLet’s also observe that we have \\[\nP( \\Pi \\le \\alpha \\mid H_0 ) = P( |T_0| \\ge u \\mid H_0 ) = \\alpha,\n\\] meaning that if the null hypothesis is true, then \\(\\Pi\\) has the uniform distribution over the interval \\((0, 1)\\).\n\n\nSummary\n\nChoose a coverage level \\(1-\\alpha\\).\nCollect a sample of size \\(n\\).\nCalculate \\(u\\) as using \\(\\mathrm{CDF}(u)=1-\\alpha/2\\), or equivalently, \\(\\alpha = 2\\cdot ( 1 - \\mathrm{CDF}(u) )\\), where we use the CDF of the Student distribution with \\(n-1\\) degrees of freedom.\nCalculate sample mean and sample standard deviation, \\(M\\) and \\(S\\).\nConstruct the confidence interval as \\(M\\pm uS/\\sqrt{n}\\).\nTo test for \\(H_0\\colon \\mu = 0\\) check if \\(\\mu_0\\) is outside of the confidence interval (to reject \\(H_0\\)).\nAlternatively, construct \\(T_0 = M\\sqrt{n}/S\\) and compare \\(|T_0| &gt; u\\) (to reject \\(H_0\\)).\nAlternatively, construct \\(\\Pi = 2\\cdot (1-\\mathrm{CDF}(|T_0|))\\) and check whether \\(\\Pi &lt; \\alpha\\) (to reject \\(H_0\\)).\n\n\n\nA bit of code\nLet’s implement the above formulae using SciPy’s \\(t\\) distribution to keep things as related to the formulae above as possible. Any discrepancies will suggest an error in the derivations or a bug in the code.\n\n\nCode\nimport numpy as np\n\nfrom scipy import stats\n\ndef alpha_to_u(alpha: float, nobs: int) -&gt; float:\n  if np.min(alpha) &lt;= 0 or np.max(alpha) &gt;= 1:\n    raise ValueError(\"Alpha has to be inside (0, 1).\")\n  return stats.t.ppf(1 - alpha / 2, df=nobs - 1)\n\n\ndef u_to_alpha(u: float, nobs: int) -&gt; float:\n  if np.min(u) &lt;= 0:\n    raise ValueError(\"u has to be positive\")\n  return 2 * (1 - stats.t.cdf(u, df=nobs - 1))\n\n# Let's test whether the functions seem to be compatible\nfor alpha in [0.01, 0.05, 0.1, 0.99]:\n  for nobs in [5, 10]:\n    u = alpha_to_u(alpha, nobs=nobs)\n    a = u_to_alpha(u, nobs=nobs)\n\n    if abs(a - alpha) &gt; 1e-4:\n      raise ValueError(f\"Discrepancy for {nobs=} {alpha=}\")\n\n\ndef calculate_t(xs: np.ndarray):\n  # Sample mean and sample standard deviation\n  n = len(xs)\n  m = np.mean(xs)\n  s = np.std(xs, ddof=1)\n\n  # Calculate the t value assuming the null hypothesis mu = 0\n  t = m / (s / np.sqrt(n))\n  return t\n\ndef calculate_p_value_from_t(t: float, nobs: int) -&gt; float:\n  return 2 * (1 - stats.t.cdf(np.abs(t), df=nobs-1))\n\ndef calculate_p_value_from_data(xs: np.ndarray) -&gt; float:\n  n = len(xs)\n  t = calculate_t(xs)\n  return calculate_p_value_from_t(t=t, nobs=n)\n\ndef calculate_ci_delta_from_params(\n  s: float,\n  nobs: int,\n  alpha: float,\n) -&gt; tuple[float, float]:\n  u = alpha_to_u(alpha, nobs=nobs)\n  delta = u * s / np.sqrt(nobs)\n  return delta\n\ndef calculate_ci_delta_from_data(xs: np.ndarray, alpha: float) -&gt; float:\n  m = np.mean(xs)\n  s = np.std(xs, ddof=1)\n  delta = calculate_ci_delta_from_params(s=s, nobs=len(xs), alpha=alpha)\n  return delta\n\ndef calculate_confidence_interval_from_data(\n  xs: np.ndarray,\n  alpha: float,\n) -&gt; tuple[float, float]:\n  delta = calculate_ci_delta_from_data(xs, alpha=alpha)\n  return (m-delta, m+delta)\n\n\nWe have three equivalent tests for rejecting \\(H_0\\). Let’s see how they perform on several samples. We’ll simulate \\(N\\) times a fresh data set \\(X_1, \\dotsc, X_n\\) from \\(\\mathcal N\\left(0, \\sigma^2\\right)\\) and calculate the confidence interval, the \\(T_0\\) statistic and the \\(p\\)-value for each of these. We will order the samples by increasing \\(p\\)-value (equivalently, with decreasing \\(|T_0|\\) statistic), to make dependencies in the plot easier to see.\nAdditionally, we will choose relatively large \\(\\alpha\\) and mark the regions such that the test does not reject \\(H_0\\). In terms of confidence intervals there is no such region, as each sample which has its own confidence interval, which can contain \\(0\\) or not.\n\n\nCode\nimport matplotlib.pyplot as plt\nplt.style.use(\"dark_background\")\n\nrng = np.random.default_rng(42)\n\nnsimul = 100\nnobs = 5\n\nsamples = rng.normal(loc=0, scale=1.0, size=(nsimul, nobs))\np_values = np.asarray([calculate_p_value_from_data(x) for x in samples])\nindex = np.argsort(p_values)\n\n\nalpha: float = 0.1\nu_thresh = alpha_to_u(alpha, nobs=nobs)\n\nsamples = samples[index, :]\np_values = p_values[index]\nt_stats = np.asarray([calculate_t(x) for x in samples])\nsample_means = np.mean(samples, axis=1)\nci_deltas = np.asarray([calculate_ci_delta_from_data(x, alpha=alpha) for x in samples])\n\nx_axis = np.arange(1, nsimul + 1)\nfig, axs = plt.subplots(3, 1, figsize=(3, 4), dpi=150, sharex=True)\n\n# P-values plot\nax = axs[0]\nax.fill_between(x_axis, alpha, 0, color=\"lime\", alpha=0.2)\nax.plot(x_axis, np.linspace(0, 1, num=len(x_axis)), linestyle=\"--\", linewidth=0.5, color=\"white\")\nax.scatter(x_axis, p_values, c=\"yellow\", s=1)\nax.set_xlim(-0.5, nsimul + 0.5)\nax.set_ylabel(\"$p$-value\")\n\nax.axvline(alpha * nsimul + 0.5, color=\"maroon\", linestyle=\"--\", linewidth=0.5)\n\n# T statistics\nax = axs[1]\nax.fill_between(x_axis, -u_thresh, u_thresh, color=\"lime\", alpha=0.2)\nax.scatter(x_axis, t_stats, c=\"yellow\", s=1)\nax.plot(x_axis, np.zeros_like(x_axis), c=\"white\", linestyle=\"--\", linewidth=0.5)\nax.set_ylabel(\"$t$ statistic\")\n\nax.axvline(alpha * nsimul + 0.5, color=\"maroon\", linestyle=\"--\", linewidth=0.5)\n\n# Confidence intervals\nax = axs[2]\nax.set_xlabel(\"Sample index\")\nax.plot(x_axis, np.zeros_like(x_axis), c=\"white\", linestyle=\"--\", linewidth=0.5)\nax.errorbar(x_axis, sample_means, yerr=ci_deltas, fmt=\"o\", markersize=1, linewidth=0.5, c=\"yellow\")\nax.set_ylabel(\"Conf. int.\")\n\nax.axvline(alpha * nsimul + 0.5, color=\"maroon\", linestyle=\"--\", linewidth=0.5)\n\nfor ax in axs:\n  ax.spines[['top', 'right']].set_visible(False)\n\nfig.tight_layout()\n\n\n\n\n\nLet’s also quickly verify that, indeed, the distribution of \\(p\\)-values is uniform over \\((0, 1)\\) (which, in a way, can be already deduced from the plot with ordered \\(p\\) values) and that the distribution of the \\(t\\) statistic is indeed \\(t_{n-1}\\).\n\n\nCode\nfig, axs = plt.subplots(1, 3, figsize=(5, 2), dpi=150)\n\nax = axs[0]\nax.set_title(\"CDF ($p$-value)\")\nx_ax = np.linspace(0, 1, 5)\nax.plot(x_ax, x_ax, color=\"white\", linestyle=\"--\", linewidth=1)\nax.ecdf(p_values, c=\"maroon\")\n\nax = axs[1]\nax.set_title(\"CDF ($t$-stat.)\")\nx_ax = np.linspace(-3.5, 3.5, 101)\nax.plot(x_ax, stats.t.cdf(x_ax, df=nobs-1), color=\"white\", linestyle=\"--\", linewidth=1)\nax.ecdf(t_stats, c=\"maroon\")\n\nax = axs[2]\nax.set_title(\"CDF (mean)\")\nx_ax = np.linspace(-1.2, 1.2, 101)\nax.plot(x_ax, stats.norm.cdf(x_ax, scale=1/np.sqrt(nobs)), color=\"white\", linestyle=\"--\", linewidth=1)\nax.ecdf(sample_means, c=\"maroon\")\n\nfor ax in axs:\n  ax.spines[['top', 'right']].set_visible(False)\n\nfig.tight_layout()"
  },
  {
    "objectID": "posts/small-power.html#statistical-power",
    "href": "posts/small-power.html#statistical-power",
    "title": "From \\(t\\)-test to “This is what ‘power=0.06’ looks like”",
    "section": "Statistical power",
    "text": "Statistical power\nAbove we have seen a procedure used to reject the null hypothesis \\(H_0\\colon \\mu = 0\\) either by constructing the confidence interval or defining the variable \\(T_0\\) with the property that \\(P( |T_0| &gt; u \\mid H_0) = \\alpha.\\)\nConsider now the data coming from a distribution with any other \\(\\mu\\), i.e., we will not condition on \\(H_0\\) anymore. To make this explicit in the notation, we will condition on \\(H_\\mu\\), rather than \\(H_0\\).\nHow does the distribution of \\(T_0\\) look like now? Recall that have independent variables \\[\n\\frac{M-\\mu}{\\sigma/\\sqrt{n}} \\sim \\mathcal N(0, 1)\n\\] and \\[\nS^2\\cdot (n-1)/\\sigma^2 \\sim \\chi^2_{n-1}\n\\] so that we have, of course, \\[\nT_\\mu = \\frac{M-\\mu}{S/\\sqrt{n}} =  \n\\frac{ \\frac{M-\\mu}{\\sigma/\\sqrt{n}}}{\\sqrt{ \\frac{ S^2\\cdot (n-1)/\\sigma^2 }{n-1} }}\n\\sim t_{n-1}.\n\\] For \\(T_0\\) we have \\[\nT_0 = \\frac{ \\frac{M-\\mu}{\\sigma/\\sqrt{n}} + \\frac{\\mu}{\\sigma/\\sqrt{n}}}{\\sqrt{ \\frac{ S^2\\cdot (n-1)/\\sigma^2 }{n-1} }}\n\\] which is distributed according to the noncentral \\(t\\) distribution with noncentrality parameter \\(\\theta = \\mu\\sqrt{n} / \\sigma\\). Note that this distribution is generally asymmetric and different from the location-scale generalisation of the (central) \\(t\\) distribution. Let’s write \\(F_{n-1, \\theta}\\) for the CDF of this function. We will reject the null hypothesis \\(H_0\\) with probability \\[\\begin{align*}\nP(|T_0| &gt; u \\mid H_\\mu) &= P(T_0 &gt; u \\mid H_\\mu ) + P(T_0 &lt; -u \\mid H_\\mu) \\\\\n&= 1 - P(T_0 &lt; u \\mid H_\\mu) + P(T_0 &lt; -u \\mid H_\\mu) \\\\\n&= 1 - F_{n-1,\\theta}(u) + F_{n-1,\\theta}(-u),\n\\end{align*}\n\\]\nwhich gives us statistical power of the test.\nWe see that power depends on chosen \\(\\alpha\\) (as it controls \\(u\\), the value we compare against the \\(|T_0|\\) statistic), on \\(n\\) (as it controls both the number of degrees of freedom in the CDF \\(F_{n-1,\\theta}\\) and the noncentrality parameter \\(\\theta\\)) and on the effect size, by which we understand the standardized mean difference, i.e., \\(\\mu/\\sigma\\).\n\nAnother bit of code\nLet’s implement power calculation. In fact, statsmodels has very convenient utilities for power calculation, so in practice implementing it is never needed:\n\n\nCode\nfrom statsmodels.stats.power import TTestPower\n\ndef calculate_power(\n  effect_size: float,\n  nobs: int,\n  alpha: float,\n) -&gt; float:\n  theta = np.sqrt(nobs) * effect_size\n  u = alpha_to_u(alpha, nobs=nobs)\n\n  def cdf(x: float) -&gt; float:\n    return stats.nct.cdf(x, df=nobs-1, nc=theta)\n  \n  return 1 - cdf(u) + cdf(-u)\n\nfor nobs in [5, 10]:\n  for alpha in [0.01, 0.05, 0.1]:\n    for effect_size in [0, 0.1, 1.0, 3.0]:\n      power = calculate_power(\n        effect_size=effect_size, nobs=nobs, alpha=alpha,\n      )\n      power_ = TTestPower().power(\n        effect_size=effect_size, nobs=nobs, alpha=alpha, alternative=\"two-sided\")\n      \n      if abs(power - power_) &gt; 0.001:\n        raise ValueError(f\"For {nobs=} {alpha=} {effect_size=} we noted discrepancy {power} != {power_}\")\n\n\nLet’s quickly check how power depends on the effect size in the following setting. We collect \\(X_1, \\dotsc, X_n\\sim \\mathcal N(\\mu, 1^2)\\), so that standardized mean difference is \\(\\mu = \\mu/1\\) and we will use standard \\(\\alpha = 5\\%\\).\n\n\nCode\nfig, axs = plt.subplots(1, 3, figsize=(7, 3), dpi=150, sharey=True)\n\nax = axs[0]\nmus = np.linspace(0, 1.5)\nax.plot(\n  mus, calculate_power(effect_size=mus, nobs=10, alpha=0.05)\n)\nax.set_xlabel(r\"$\\mu/\\sigma$\")\nax.set_ylabel(\"Power\")\n\nax = axs[1]\nns = np.arange(5, 505)\nfor eff in [0.1, 0.5, 1.0]:\n  ax.plot(\n    ns, calculate_power(effect_size=eff, nobs=ns, alpha=0.05),\n    label=f\"$\\mu/\\sigma={eff:.1f}$\"\n  )\nax.set_xscale(\"log\")\nn_labels = np.asarray([5, 10, 20, 100, 500])\nax.set_xticks(n_labels, n_labels)\nax.set_xlabel(\"$n$\")\n\nax = axs[2]\nalphas = np.linspace(0.01, 0.99, 99)\nfor eff in [0.1, 0.5, 1.0]:\n  ax.plot(\n    alphas, calculate_power(effect_size=eff, nobs=5, alpha=alphas),\n    label=f\"$\\mu/\\sigma={eff:.1f}$\"\n  )\nax.legend(frameon=False)\nax.set_xlabel(r\"$\\alpha$\")\n\nfor ax in axs:\n  ax.spines[['top', 'right']].set_visible(False)\n\nfig.tight_layout()\n\n\n/home/pawel/micromamba/envs/data-science/lib/python3.10/site-packages/scipy/stats/_continuous_distns.py:7313: RuntimeWarning: invalid value encountered in _nct_cdf\n  return np.clip(_boost._nct_cdf(x, df, nc), 0, 1)"
  },
  {
    "objectID": "posts/small-power.html#consequences-of-low-power",
    "href": "posts/small-power.html#consequences-of-low-power",
    "title": "From \\(t\\)-test to “This is what ‘power=0.06’ looks like”",
    "section": "Consequences of low power",
    "text": "Consequences of low power\nConsider a study with \\(\\sigma=1\\), \\(\\mu=0.1\\) and \\(n=10\\). At \\(\\alpha = 5\\%\\) we have power of:\n\n\nCode\npower = calculate_power(effect_size=0.1, nobs=10, alpha=0.05)\nif abs(power - 0.06) &gt; 0.005:\n  raise ValueError(f\"We want power to be around 6%\")\nprint(f\"Power: {100 * power:.2f}%\")\n\n\nPower: 5.93%\n\n\nwhich is similar to the value used in Andrew Gelman’s post. Let’s simulate a lot of data sets and confirm that the power of the test is indeed around 6%:\n\n\nCode\nnsimul = 200_000\nnobs = 10\nalpha = 0.05\n\ntrue_mean = 0.1\n\nsamples = rng.normal(loc=true_mean, scale=1.0, size=(nsimul, nobs))\np_values = np.asarray([calculate_p_value_from_data(x) for x in samples])\n\nprint(f\"Power from simulation: {100 * np.mean(p_values &lt; alpha):.2f}%\")\n\n\nPower from simulation: 5.97%\n\n\n(As a side note, I already regret not implementing \\(p\\)-value calculation in JAX – I can’t use vmap!)\n\n\nCode\nfig, axs = plt.subplots(1, 2, figsize=(5, 2), dpi=150, sharex=True)\n\nbins = np.linspace(-1.5, 1.5, 31)\n\nax = axs[0]\nsample_means = np.mean(samples, axis=1)\nax.hist(\n  sample_means,\n  bins=bins,\n  histtype=\"step\",\n)\nax.axvline(true_mean)\nax.set_title(\"Histogram of sample means\")\nax.set_xlabel(\"Sample mean\")\nax.set_ylabel(\"PDF\")\n\nax = axs[1]\nx_ax = np.linspace(-1.5, 1.5, 51)\nax.plot(\n  x_ax, stats.norm.cdf(x_ax, loc=0.1, scale=1/np.sqrt(nobs))\n)\nax.ecdf(sample_means)\nax.set_xlabel(\"Sample mean\")\nax.set_ylabel(\"CDF\")\nax.set_title(\"Empirical CDF\")\n\nfor ax in axs:\n  ax.spines[['top', 'right']].set_visible(False)\n\nfig.tight_layout()\n\n\n\n\n\nWe see that sample mean, represented by the random variable \\(M\\), is indeed distributed according to \\(\\mathcal N\\left(0.1, \\left(1/\\sqrt{10}\\right)^2\\right)\\). The standard error of the mean, \\(1/\\sqrt{10}\\approx 0.31\\) is three times larger than the population mean \\(\\mu=0.1\\), so we have a lot of noise here.\nLet’s see what happens if a “statistically significant” result is somehow obtained.\n\n\nCode\nstat_signif_samples = samples[p_values &lt; alpha, :]\n\nfig, ax = plt.subplots(figsize=(2, 2), dpi=150)\n\nsample_means = np.mean(stat_signif_samples, axis=1)\nax.hist(\n  sample_means,\n  bins=bins,\n  histtype=\"step\",\n)\nax.axvline(true_mean)\nax.set_title(\"Stat. signif. results\")\nax.set_xlabel(\"Sample mean\")\nax.set_ylabel(\"PDF\")\n\n\nax.spines[['top', 'right']].set_visible(False)\nfig.tight_layout()\n\n\n\n\n\nConditioning only on statistically significant results, let’s take a look at:\n\nhow many of them have sample mean of wrong sign,\nhow many of them have sample mean seriously exaggerated (e.g., at least 5 times),\n\nsimilarly as Andrew Gelman did in his blog post:\n\n\nCode\nfrac_wrong_sign = np.mean(sample_means &lt; 0)\nprint(f\"Wrong sign: {100*frac_wrong_sign:.1f}%\")\n\nfrac_exaggerated = np.mean(sample_means &gt;= 5 * true_mean)\nprint(f\"Exaggerated (5x): {100 * frac_exaggerated:.1f}%\")\n\n\nWrong sign: 20.8%\nExaggerated (5x): 69.5%\n\n\nOh, that’s not good! We see that a statistically significant result (which itself has only 6% occurrence probability, if the study is executed properly), will have about 20% chance of being of a wrong sign and around 2/3 chance of being quite exaggerated.\nActually, Andrew Gelman looked at results exaggerated 9 times. Let’s make a plot summarizing these probabilities:\n\n\nCode\nfig, ax = plt.subplots(figsize=(2, 2), dpi=150)\n\nratio_exag = np.linspace(1, 11, 31)\nfrac_exag = [np.mean(sample_means &gt;= r * true_mean) for r in ratio_exag]\n\nax.plot(\n  ratio_exag,\n  frac_exag,\n)\nax.set_ylim(0, 1)\nax.set_xlabel(\"Exag. factor\")\nax.set_ylabel(\"Probability\")\nxticks = [1, 3, 5, 7, 9]\nax.set_xticks(xticks, xticks)\n\nax.spines[['top', 'right']].set_visible(False)\nfig.tight_layout()\n\n\n\n\n\nFinally, there is a great plot by Art Owen showing how confidence intervals of statistically significant results look like when power is low. Let’s quickly reproduce it:\n\n\nCode\nci_deltas = np.asarray([calculate_ci_delta_from_data(x, alpha=alpha) for x in stat_signif_samples])\n\nn_to_plot = min(50, len(stat_signif_samples))\n\nx_axis = np.arange(1, n_to_plot + 1)\nfig, ax = plt.subplots(1, figsize=(3, 2), dpi=150)\n\nax.set_xlabel(\"Sample index\")\n\nax.plot(x_axis, np.zeros_like(x_axis), c=\"white\", linestyle=\"--\", linewidth=0.5)\nax.plot(x_axis, np.zeros_like(x_axis) + true_mean, c=\"maroon\", linestyle=\"-\", linewidth=0.5)\n\nax.errorbar(x_axis, sample_means[:n_to_plot], yerr=ci_deltas[:n_to_plot], fmt=\"o\", markersize=1, linewidth=0.5, c=\"yellow\")\nax.set_ylabel(\"Conf. int.\")\n\nax.spines[['top', 'right']].set_visible(False)\nfig.tight_layout()\n\n\n\n\n\nOf course, the confidence intervals cannot contain \\(0\\) (otherwise the results wouldn’t have been statistically significant). How many of them contain \\(\\mu=0.1\\)? In case we look at all the results (including the majority of nonsignificant results), \\(1-\\alpha=95\\%\\) of confidence intervals contains the true value. However, once we restrict our attention to only significant results, this coverage drops to\n\n\nCode\nfrac_contain = np.mean(\n  (sample_means - ci_deltas &lt; true_mean) \n  & (true_mean &lt; sample_means + ci_deltas)\n)\nprint(f\"Coverage: {100 * frac_contain:.1f}%\")\n\n\nCoverage: 37.2%"
  },
  {
    "objectID": "posts/small-power.html#summary-1",
    "href": "posts/small-power.html#summary-1",
    "title": "From \\(t\\)-test to “This is what ‘power=0.06’ looks like”",
    "section": "Summary",
    "text": "Summary\nLow-powered studies have, of course, high probability of not rejecting \\(H_0\\) when the alternative is true. But even when \\(H_0\\) is rejected, estimated mean will be often of wring sign or exaggerated. This indeed shows that principled experimental design and power analysis are crucial to execute before any data collection!"
  },
  {
    "objectID": "posts/small-power.html#footnotes",
    "href": "posts/small-power.html#footnotes",
    "title": "From \\(t\\)-test to “This is what ‘power=0.06’ looks like”",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nA practice, which can result in p-hacking and HARKing. On a related manner see this post on negative results and the paper on “the garden of forking paths”.↩︎\nStudent’s \\(t\\) distribution with \\(k\\) degrees of freedom arises as the distribution of the variable \\(T = A/\\sqrt{B / k}\\), where \\(A\\sim \\mathcal N(0, 1)\\) and \\(B\\sim \\chi^2_k\\) are independent. In this case, \\(\\frac{M-\\mu}{\\sigma/\\sqrt{n}}\\sim N(0, 1)\\) and \\(S^2\\cdot (n-1)/ \\sigma^2 \\sim \\chi^2_{n-1}\\) are independent. Parameter \\(\\sigma\\) cancels out.↩︎"
  },
  {
    "objectID": "posts/on-beyond-normal.html",
    "href": "posts/on-beyond-normal.html",
    "title": "The mutual information saga",
    "section": "",
    "text": "Where and when should we start this story? Probably a good origin will be in the Laboratory of Modeling in Biology and Medicine in Warsaw, where Tomasz Lipniacki and Marek Kochańczyk decided to mentor two students who just completed high-school education, namely the younger versions of Frederic and myself.\nInformally speaking, we tried to model the MAPK pathway as a communication channel. Imagine that the cell is given some input \\(x\\in \\mathcal X\\) and we measure the response \\(y\\in \\mathcal Y\\). Once we vary \\(x\\) and we record different values \\(y\\) we may start observing some patterns – perhaps changes with \\(y\\) is somehow associated with changes in \\(x\\). To make this more formal, consider a random variable \\(X\\) representing the given inputs and another random variable \\(Y\\) representing the outputs. The mutual information \\(I(X; Y)\\) measures how dependent these variables are: \\(I(X; Y) = 0\\) if and only if \\(X\\) and \\(Y\\) are independent. Contrary to correlation, mutual information works for any kind of variables (continuous, discrete, of arbitrary dimension…) and can capture nonlinear dependencies.\nI enjoyed my time in the project and the provided mentorship very much! We wrote a paper, and – perhaps more importantly – I learned that information theory and biology are great fields to study!\nMany years have passed and I thought that perhaps it’s the time to become a mentor on my own. Fortunately, the very first Master’s student I supervised, Anej, was so bright and motivated that he wrote a great Master’s thesis despite having such an unexperienced supervisor as me! Anej was working on representation learning and extensively used mutual information estimators. But I had not done my homework: when I wrote the project proposal, I happily assumed that mutual information estimators have to work, if the space \\(\\mathcal X\\times \\mathcal Y\\) is of moderate dimension and the number of points is quite large. I was wrong. Different mutual information estimators seemed to give very different estimates and that was concerning."
  },
  {
    "objectID": "posts/on-beyond-normal.html#beyond-normal",
    "href": "posts/on-beyond-normal.html#beyond-normal",
    "title": "The mutual information saga",
    "section": "Beyond normal",
    "text": "Beyond normal\nI did the only rational thing in this situation: I ran screaming for help to two mutual information experts, Frederic and Alex. We started thinking:\n\nHow do we really know when mutual information estimators work? As mutual information is analytically known only for the simplest distributions, the estimators are evaluated usually on “simple” low-dimensional distributions (or moderate-dimensional multivariate normal distributions).\nIs it possible to construct more expressive distributions with known ground-truth mutual information?\nHow invariant are the estimators to diffeomorphisms? Namely, if \\(f\\) and \\(g\\) are diffeomorphisms, then \\(I(X; Y) = I(f(X); g(Y))\\). Do the numerical estimates have the same property?\n\nThe 1st and 2nd question are related. But so are 2nd and 3rd! Suppose that we can easily sample points \\((x_1, y_1), \\dotsc, (x_n, y_n)\\) from the joint distribution \\(P_{XY}\\). If \\(f\\colon \\mathcal X\\to \\mathcal X\\) and \\(g\\colon \\mathcal Y\\to \\mathcal Y\\) are diffeomorphisms, we can apply them to obtain a sample \\((f(x_1), g(y_1)), \\dotsc, (f(x_n), g(y_n))\\) from \\(P_{f(X)g(Y)}\\), which is a joint distribution between variables \\(f(X)\\) and \\(g(Y)\\). As we apply a diffeomorphism1, the mutual information does not change: \\(I(X; Y) = I(f(X); g(Y))\\).\nFrederic and I started programming2 different distributions and transformations, in the meantime learning , and after five months we had a ready manuscript titled Are mutual information estimators homeomorphism-invariant?, which shows that the 3rd question was the most important one for a lapsed differential geometer who is currently trying to be an imposter in the machine learning world me.\nWell, I was wrong: after the manuscript got rejected from ICML, we realized that the most important aspect of our work was actually using the transformed distributions to study the strengths and limitations of existing estimators3. We improved the experiments in the paper and changed the story to Beyond normal: on the evaluation of the mutual information estimators4, which was accepted to NeurIPS.\nOf course, we were very happy. But there were some important aspects that deserved to be studied a bit more…"
  },
  {
    "objectID": "posts/on-beyond-normal.html#here-comes-the-trouble",
    "href": "posts/on-beyond-normal.html#here-comes-the-trouble",
    "title": "The mutual information saga",
    "section": "Here comes the trouble",
    "text": "Here comes the trouble\n\nReally that expressive?\nOur distributions were only “beyond normal”, rather than “just amazing”: we suspected that one cannot construct all the interesting distributions.\nConsider \\(\\mathcal X = \\mathbb R^m\\), \\(\\mathcal Y = \\mathbb R^n\\) and a random vector \\(Z \\sim \\mathcal N(0, I_{m+n})\\). Normalizing flows guarantee that there exists a diffeomorphism \\(u: \\mathcal X\\times \\mathcal Y \\to \\mathcal X\\times \\mathcal Y\\) such that \\(P_{XY}\\) is well-approximated5 by the distribution of \\(u(Z)\\).\nHowever, the diffeomorphism \\(u\\) does not have to be of the form \\(f\\times g\\) (recall that \\((f\\times g)(x, y) = (f(x), g(y))\\)), which leaves the mutual information invariant. Thinking geometrically, the product group \\(\\mathrm{Diff}(\\mathcal X)\\times \\mathrm{Diff}(\\mathcal Y)\\) is usually a very small subgroup of \\(\\mathrm{Diff}(\\mathcal X\\times \\mathcal Y)\\), the group of all diffeomorphisms of \\(\\mathcal X\\times \\mathcal Y\\).\nWe had this intuition quite early, but we did not have a convincing counterexample that our distributions were not sufficient. However, Frederic started plotting histograms of pointwise mutual information, which let us formalize this intuition.\nConsider the pointwise mutual information: \\[ i_{XY}(x, y) = \\log \\frac{ p_{XY}(x, y) }{ p_X(x)\\, p_Y(y) },\\] where \\(p_{XY}\\) is the PDF of the joint distribution and \\(p_X\\) and \\(p_Y\\) are the PDFs of the marginal distributions. It is easy to prove that if \\(f\\) and \\(g\\) are diffeomorphisms and that \\(x'=f(x)\\) and \\(y'=g(y)\\), then6 \\[i_{XY}(x, y) = i_{f(X)g(Y)}(x', y').\\] From this it is easy to observe that the distribution of the random variable \\(i_{XY}(X; Y)\\) is the same as of \\(i_{f(X)g(Y)}(f(X); g(Y))\\). We termed it the pointwise mutual information profile, although it’s more than likely that people had already studied this before us.\nHence, diffeomorphisms leave invariant not only the mutual information: they leave invariant also the whole pointwise mutual information profile, what limits how expressive the distributions can be. We did not have yet a counterexample, but a strong feeling that it should exist: we just needed to find distributions with different profiles, but the same mutual information.\n\n\nModel-based mutual information estimation\nWe started the project with the idea that if \\((A, B)\\sim \\mathcal N(0, \\Sigma)\\), then \\(I(A; B)\\) is analytically known in terms of the covariance matrix \\(\\Sigma\\) and we can obtain more complicated dependencies between \\(X=f(A)\\) and \\(Y=f(B)\\) without changing the mutual information: \\(I(X; Y) = I(A; B)\\).\nAt the same time we asked the inverse question: if we have \\(X\\) and \\(Y\\), can we perhaps find a covariance matrix \\(\\Sigma\\) and a normalizing flow \\(f\\times g\\) such that \\((X, Y) = (f(A), g(B))\\) and \\((A, B)\\) are distributed according to the multivariate normal distribution with covariance matrix \\(\\Sigma\\)? If \\(f\\) and \\(g\\) are identity functions, this construction corresponds to the assumption that \\((X, Y)\\) are multivariate normal and calculating the mutual information via the estimation of the joint covariance matrix. A particular example of this approach is canonical correlation analysis, which worked remarkably well for multivariate normal distributions, providing more accurate estimates and requiring a lower number of samples available.\nHowever, as discussed above, generally we cannot expect that a normalizing flow of the form \\(f\\times g\\) will transform a distribution to a multivariate normal. So there is some potential for more explicit modelling of the joint distribution \\(P_{XY}\\), but we needed to make sure that it is expressive enough to cover some interesting cases.\n\n\nDo outliers break everything?\nThere’s no real data without real noise and we wanted to have distributions which can be used in practice. One source of noise are outliers, which sometimes can be attributed to errors in data collection or recording (e.g., the equipment was apparently switched off or some piece of experimental setup broke), and are well-known suspects when an estimator behaves badly. In Beyond normal we investigated heavy-tailed distributions (either by applying some transformations to multivariate normal distributions to make the tails heavier, or by using multivariate Student distributions), but we felt that it was not enough."
  },
  {
    "objectID": "posts/on-beyond-normal.html#the-mixtures-and-the-critics",
    "href": "posts/on-beyond-normal.html#the-mixtures-and-the-critics",
    "title": "The mutual information saga",
    "section": "The mixtures and the critics",
    "text": "The mixtures and the critics\nThe outliers here were the most concerning and I had the feeling that if \\(P_{XY}\\) is the distribution from which we want to sample and \\(P_{\\tilde X \\tilde Y}\\) is the “noise distribution” with \\(I(\\tilde X; \\tilde Y) = 0\\), then we could perhaps calculate the information contained in the mixture distribution: \\[P_{X'Y'} = (1-\\alpha)\\, P_{XY} + \\alpha \\, P_{\\tilde X \\tilde Y},\\] where \\(\\alpha \\ll 1\\) is the fraction of outliers.\nI spent quite some time with a pen and paper trying to calculate \\(I(X'; Y')\\) in terms of \\(I(X; Y)\\), but I could not really derive anything. Even proving the conjectured bound that \\(I(X'; Y')\\) should not exceed \\(I(X; Y)\\) was hard…\nAnd it’s actually good that I didn’t manage to prove it: this conjecture is false. When I asked Frederic about it, he immediately responded with: 1. An example of two distributions such that each of them encodes 0 bits, but their mixture encodes 1 bit. 2. An example of two distributions such that each of them encodes 1 bit, but their mixture encodes 0 bits.\nThis is disturbing. As Larry Wasserman said: I have decided that mixtures, like tequila, are inherently evil and should be avoided at all costs. Fortunately, when I was still struggling with trying to prove it, I recalled Frederic’s histograms, approximating the pointwise mutual information profile. For multivariate normal and Student distributions he sampled a lot of data points \\((x_1, y_1), \\dotsc, (x_n, y_n)\\) and then evaluated the pointwise mutual information \\(i_{XY}(x_i, y_i)\\) – which is easy to evaluate using \\(\\log p_{XY}\\), \\(\\log p_X\\) and \\(\\log p_Y\\) densities – to construct a histogram. The mean of this sample is the estimate of the mutual information \\(I(X; Y) = \\mathbb E_{(x, y)\\sim P_{XY} }[i_{XY}(x; y)]\\).\nThis works for multivariate normal distributions and Student distributions, so why wouldn’t it work for mixture distributions? In the end this is simply a Monte Carlo estimator: we only need to sample a lot of data points (and sampling from a mixture distribution is trivial if one can sample from the component distributions) and evaluate the pointwise mutual information (which can be calculated from the PDFs of the involved variables. The PDF of the mixture distribution can be evaluated using the PDFs of the components).\nHence, although we do not have an exact formula for \\(I(X; Y)\\) where \\(P_{XY}\\) is a mixture of multivariate normal or Student distributions, we have its Monte Carlo approximation. Now we can apply diffeomorphisms to this distribution obtaining more expressive distribution, having e.g., a normalizing flow applied to a Gaussian mixture or a mixture of normalizing flow, or mix all of these again…\nImplementation of a prototype in TensorFlow Probability took one day7 and after a month we had The mixtures and the neural critics8 ready.\nI very much like this concept because:\n\nThis construction can model outliers as a mixture of different distributions.\nWe can construct a mixture distribution with a different pointwise mutual information profile than the multivariate normal distribution. Due to the fact that Gaussian mixtures are very expressive, we can build a whole new family of distributions!\nWe also do not have to model \\(P_{XY}\\) as a multivariate normal distribution transformed by diffeomorphism \\(f\\times g\\) – we can use Gaussian (or Student) mixtures! We quickly built a prototype of model-based mutual information estimation with mixtures, which can provide uncertainty quantification on mutual information in the usual Bayesian manner.\n\nLet’s finish this post at the place where we started: in Beyond normal we could apply diffeomorphisms to transform continuous \\(X\\) and \\(Y\\) variables. The Monte Carlo estimator from The mixtures and the critics applies in exactly the same manner also to the case when \\(X\\) is discrete variable and \\(Y\\) is continuous, which is exactly the case we investigated many years ago!"
  },
  {
    "objectID": "posts/on-beyond-normal.html#footnotes",
    "href": "posts/on-beyond-normal.html#footnotes",
    "title": "The mutual information saga",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nMore precisely, if all the spaces involved are standard Borel, then \\(f\\colon \\mathcal X\\to \\mathcal X'\\) and \\(g\\colon \\mathcal Y\\to \\mathcal Y'\\) can be continuous injective mappings and e.g., increase the dimensionality of the space. See Theorem 2.1 here, which is a well-known fact, but it still took us quite some time to prove it. M.S. Pinsker’s Information and information stability of random variables and processes proved to be an invaluable resource.↩︎\nAs a byproduct we learned Snakemake, which transformed my approach to data science entirely. But this is a different story.↩︎\nAlso, Frederic figured out that the 3rd question (whether the estimators which are invariant to diffeomorphisms) has a trivial answer. If \\(\\mathcal M\\) is a connected smooth manifold of dimension at least 2, then for any two sets of distinct points \\(\\{a_1, \\dotsc, a_n\\}\\) and \\(\\{b_1, \\dotsc, b_n\\}\\) there exists a diffeomorphism \\(u\\colon \\mathcal M\\to \\mathcal M\\) such that \\(b_i = u(a_i)\\) (the proof of this fact can be e.g., found in P.W. Michor’s and Cornelia Vizman’s \\(n\\)-transitivity of certain diffeomorphisms groups). Hence, if \\(\\mathcal X\\) and \\(\\mathcal Y\\) fulfil the assumptions above, we can move a finite data set as we wish and the only invariant estimator has to return the same answer for any set of input data points.↩︎\nLet me state two obvious facts. First: “beyond” refers to the fact that we can transform multivariate normal distributions to obtain more expressive distributions. Second: although I intended to name the paper after a wonderful musical, Next to Normal, we worried that the title was copyrighted.↩︎\nImportant detail: “well-approximated” using one statistical distance may me “badly approximated” with respect to another one.↩︎\nIt’s tempting to say that pointwise mutual information transforms as a scalar under the transformations from the group \\(\\mathrm{Diff}(\\mathcal X)\\times \\mathrm{Diff}(\\mathcal Y)\\).↩︎\nActually, it took three weeks of me complaining that we couldn’t use multivariate Student distributions in TensorFlow Probability on JAX. Since I had learned that I was wrong, a few hours passed before we had the prototype and a couple of more days before it was refactored into a stable solution.↩︎\nYes, this is named after J.R.R. Tolkien’s The monsters and the critics. I’m very stubborn, so it’s great that Frederic and Alex are so patient!↩︎"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome!",
    "section": "",
    "text": "My name is Paweł Czyż and I am a data scientist interested in modelling complex biological data using techniques originating in probabilistic machine learning and applied Bayesian statistics.\nCurrently I am a Doctoral Fellow of the ETH AI Center working in Computational Biology Group and Computational Cancer Genomics Laboratory.\nPrior to starting my PhD I was an AI Resident at Microsoft Research Cambridge and studied mathematical physics at University of Oxford."
  },
  {
    "objectID": "publications/pmi-profiles.html",
    "href": "publications/pmi-profiles.html",
    "title": "On the properties and estimation of pointwise mutual information profiles",
    "section": "",
    "text": "@misc{pmi-profiles,\n      title={The Mixtures and the Neural Critics: On the Pointwise Mutual Information Profiles of Fine Distributions}, \n      author={Paweł Czyż and Frederic Grabowski and Julia E. Vogt and Niko Beerenwinkel and Alexander Marx},\n      year={2023},\n      eprint={2310.10240},\n      archivePrefix={arXiv},\n      primaryClass={stat.ML}\n}"
  },
  {
    "objectID": "publications/pmi-profiles.html#citation",
    "href": "publications/pmi-profiles.html#citation",
    "title": "On the properties and estimation of pointwise mutual information profiles",
    "section": "",
    "text": "@misc{pmi-profiles,\n      title={The Mixtures and the Neural Critics: On the Pointwise Mutual Information Profiles of Fine Distributions}, \n      author={Paweł Czyż and Frederic Grabowski and Julia E. Vogt and Niko Beerenwinkel and Alexander Marx},\n      year={2023},\n      eprint={2310.10240},\n      archivePrefix={arXiv},\n      primaryClass={stat.ML}\n}"
  },
  {
    "objectID": "publications/pmi-profiles.html#abstract",
    "href": "publications/pmi-profiles.html#abstract",
    "title": "On the properties and estimation of pointwise mutual information profiles",
    "section": "Abstract",
    "text": "Abstract\nMutual information quantifies the dependence between two random variables and remains invariant under diffeomorphisms. In this paper, we explore the pointwise mutual information profile, an extension of mutual information that maintains this invariance. We analytically describe the profiles of multivariate normal distributions and introduce the family of fine distributions, for which the profile can be accurately approximated using Monte Carlo methods. We then show how fine distributions can be used to study the limitations of existing mutual information estimators, investigate the behavior of neural critics used in variational estimators, and understand the effect of experimental outliers on mutual information estimation. Finally, we show how fine distributions can be used to obtain model-based Bayesian estimates of mutual information, suitable for problems with available domain expertise in which uncertainty quantification is necessary."
  },
  {
    "objectID": "publications/beyond-normal.html",
    "href": "publications/beyond-normal.html",
    "title": "Beyond normal: on the evaluation of mutual information estimators",
    "section": "",
    "text": "@inproceedings{beyond-normal-2023,\n  title={Beyond Normal: On the Evaluation of Mutual Information Estimators},\n  author={Paweł Czyż and Frederic Grabowski and Julia E. Vogt and Niko Beerenwinkel and Alexander Marx},\n  booktitle={Thirty-seventh Conference on Neural Information Processing Systems},\n  year={2023}\n}"
  },
  {
    "objectID": "publications/beyond-normal.html#citation",
    "href": "publications/beyond-normal.html#citation",
    "title": "Beyond normal: on the evaluation of mutual information estimators",
    "section": "",
    "text": "@inproceedings{beyond-normal-2023,\n  title={Beyond Normal: On the Evaluation of Mutual Information Estimators},\n  author={Paweł Czyż and Frederic Grabowski and Julia E. Vogt and Niko Beerenwinkel and Alexander Marx},\n  booktitle={Thirty-seventh Conference on Neural Information Processing Systems},\n  year={2023}\n}"
  },
  {
    "objectID": "publications/beyond-normal.html#abstract",
    "href": "publications/beyond-normal.html#abstract",
    "title": "Beyond normal: on the evaluation of mutual information estimators",
    "section": "Abstract",
    "text": "Abstract\nMutual information is a general statistical dependency measure which has found applications in representation learning, causality, domain generalization and computational biology. However, mutual information estimators are typically evaluated on simple families of probability distributions, namely multivariate normal distribution and selected distributions with one-dimensional random variables. In this paper, we show how to construct a diverse family of distributions with known ground-truth mutual information and propose a language-independent benchmarking platform for mutual information estimators. We discuss the general applicability and limitations of classical and neural estimators in settings involving high dimensions, sparse interactions, long-tailed distributions, and high mutual information. Finally, we provide guidelines for practitioners on how to select appropriate estimator adapted to the difficulty of problem considered and issues one needs to consider when applying an estimator to a new data set."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "These posts have evolved from my digital garden1 into a bit longer form. Their main role is to educate myself and they may be incomplete, written from a very subjective point of view, or contain errors. I’d be grateful for feedback on how to improve them!"
  },
  {
    "objectID": "blog.html#footnotes",
    "href": "blog.html#footnotes",
    "title": "Blog",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSee this link for a difference. I maintain my digital garden in Obsidian. Please, let me know if you know a good way of integrating Obsidian with Quarto!↩︎"
  },
  {
    "objectID": "posts/irrational-pi-and-e.html",
    "href": "posts/irrational-pi-and-e.html",
    "title": "Proofs that pi and e are irrational",
    "section": "",
    "text": "Timothy Chow wrote a great paper A well-motivated proof that pi is irrational, where he explains a proof of irrationality of \\(e\\) (attributed to Fourier) and a proof of irrationality of \\(\\pi\\), which originally appeared in the I. Niven’s brilliant one-page article.\nIn this post I’ll just recall both proofs, so I can remember them: original articles will be more informative."
  },
  {
    "objectID": "posts/irrational-pi-and-e.html#irrationality-of-e",
    "href": "posts/irrational-pi-and-e.html#irrationality-of-e",
    "title": "Proofs that pi and e are irrational",
    "section": "Irrationality of \\(e\\)",
    "text": "Irrationality of \\(e\\)\nSuppose that \\(e=a/b\\) for positive integers \\(a\\) and \\(b\\). We can write \\[\ne = \\sum_{n=0}^{\\infty} \\frac{1}{n!}\n\\]\nand reach a contradiction by studying an integer:\n\\[\n0 &lt; b!\\cdot e - b!\\sum_{n=0}^b \\frac{1}{n!} = \\sum_{k=1}^{\\infty} \\frac{b!}{(b+k)!} &lt; \\sum_{k=1}^{\\infty} \\frac{1}{(b+1)^k} = \\frac{1}{b} \\le 1.\n\\]\nHence, this integer would lie strictly between \\(0\\) and \\(1\\), what is impossible."
  },
  {
    "objectID": "posts/irrational-pi-and-e.html#irrationality-of-pi",
    "href": "posts/irrational-pi-and-e.html#irrationality-of-pi",
    "title": "Proofs that pi and e are irrational",
    "section": "Irrationality of \\(\\pi\\)",
    "text": "Irrationality of \\(\\pi\\)\nSuppose that \\(\\pi=a/b\\) for positive integers \\(a\\) and \\(b\\). Let \\(n\\) be be a positive integer and define \\[\nf(x)=\\frac{x^n(a-bx)^n}{n!}\n\\]\nThe main idea of I. Niven’s proof is to show that the definite integral \\[\nI := \\int \\limits_0^\\pi f(x) \\sin x\\, \\mathrm{d}x\n\\]\nis an integer. But for \\(0 &lt; x &lt; \\pi\\) there are bounds \\[\n0 &lt; f(x) \\sin x &lt; \\frac{\\pi^n\\cdot (a-0)^n}{n!} = \\frac{ (\\pi a)^n }{n!},\n\\]\nso that for sufficiently large \\(n\\) we have \\(0 &lt; I &lt; 1\\).\nHence, the whole difficulty lies in proving that \\(I \\in \\mathbb Z\\).\nDefine \\[\nF(x) = f(x) - f''(x) + f^{(4)}(x) - \\cdots + (-1)^n f^{(2n)}(x),\n\\]\nwhere \\(f^{(k)}\\) is the \\(k\\)-th derivative of \\(f\\). We have\n\\[\\begin{align*}\n\\frac{\\mathrm{d}}{\\mathrm{d} x}\\left( F'(x) \\sin x - F(x) \\cos x \\right) &= F''(x) \\sin x + F'(x) \\cos x - F'(x) \\cos x + F(x) \\sin x\\\\\n&= (F(x) + F''(x)) \\sin x = f(x) \\sin x,\n\\end{align*}\\] so that \\[\n\\int\\limits_0^\\pi f(x) \\sin x\\, \\mathrm{d}x = \\big[F'(x) \\sin x - F(x) \\cos x\\big]_0^\\pi = F(0) + F(\\pi).\n\\]\nWe can prove that \\(F(0) + F(\\pi)\\) is an integer by proving that all derivatives \\(f^{(k)}_n(0)\\) and \\(f^{(k)}_n(\\pi)\\) are integers.\nFirst, note that \\(f_n(x) = f_n(a/b - x) = f_n(\\pi - x)\\), so that \\(f^{(k)}_n(0) = \\pm f^{(k)}_n(\\pi)\\). To prove that \\(f^{(k)}_n(0)\\) is an integer, observe that each summand in \\(f_n\\) has a form \\(u x^{n+r} / n!\\), where \\(u\\) is an integer and \\(r \\ge 0\\). If we differentiate it \\(k\\) times and evaluate it at \\(x=0\\) we will either have \\(0\\) (for \\(k \\neq n+r\\)) or \\(u \\cdot (n+r)! / n!\\) for \\(k=n+r\\), which is an integer."
  },
  {
    "objectID": "posts/irrational-pi-and-e.html#links",
    "href": "posts/irrational-pi-and-e.html#links",
    "title": "Proofs that pi and e are irrational",
    "section": "Links",
    "text": "Links\n\nI. Niven, A simple proof that \\(\\pi\\) is irrational, 1947: a one-page proof that \\(\\pi\\) is irrational.\nKostya_I, Exposition of Niven’s proof, 2023: a short note connecting Niven’s proof to orthogonal polynomials.\nT. Chow, A well-motivated proof that pi is irrational, 2024: proof of irrationality of \\(\\pi\\) and \\(e^r\\) for nonzero rational \\(r\\)."
  },
  {
    "objectID": "posts/irrational-pi-and-e.html#appendix-important-theorems-of-transcendental-number-theory",
    "href": "posts/irrational-pi-and-e.html#appendix-important-theorems-of-transcendental-number-theory",
    "title": "Proofs that pi and e are irrational",
    "section": "Appendix: important theorems of transcendental number theory",
    "text": "Appendix: important theorems of transcendental number theory\nI often forget the names of major results of transcendental number theory. Let’s quickly recall them:\nLindemann–Weierstrass theorem: if \\(\\alpha_1, \\dotsc, \\alpha_n\\) are algebraic numbers linearly independent over \\(\\mathbb Q\\), then \\(e^{\\alpha_1}, \\dotsc, e^{\\alpha_n}\\) are algebraically independent over \\(\\mathbb Q\\)."
  },
  {
    "objectID": "posts/irrational-pi-and-e.html#appendix-some-theorems-of-transcendental-number-theory",
    "href": "posts/irrational-pi-and-e.html#appendix-some-theorems-of-transcendental-number-theory",
    "title": "Proofs that pi and e are irrational",
    "section": "Appendix: some theorems of transcendental number theory",
    "text": "Appendix: some theorems of transcendental number theory\nI often forget the names of major results of transcendental number theory. Let’s quickly recall them:\nLindemann–Weierstrass theorem: if \\(\\alpha_1, \\dotsc, \\alpha_n\\) are algebraic numbers linearly independent over \\(\\mathbb Q\\), then \\(e^{\\alpha_1}, \\dotsc, e^{\\alpha_n}\\) are algebraically independent over \\(\\mathbb Q\\).\nGelfond–Schneider theorem: let \\(a \\in \\mathbb C\\setminus \\{0, 1\\}\\) be a complex algebraic number. If \\(b \\in \\mathbb C\\setminus (\\mathbb Q\\times \\{0\\})\\) is an irrational complex algebraic number, then \\(a^b = \\exp(b\\log a)\\) is transendental (independently on the branches of the functions used)."
  },
  {
    "objectID": "posts/mixtures-and-admixtures.html",
    "href": "posts/mixtures-and-admixtures.html",
    "title": "On mixtures and admixtures",
    "section": "",
    "text": "Consider a binary genome vector \\(Y_{n\\bullet} = (Y_{n1}, \\dotsc, Y_{nG})\\) representing at which loci a mutation has appeared. One of the simplest models is to assume that mutations appear independently, with \\(\\theta_g = P(Y_{ng} = 1)\\) representing the probability of mutation occurring. In other words, \\[\\begin{align*}\nP(Y_{n\\bullet} = y_{n\\bullet} \\mid \\theta_\\bullet) &= \\prod_{g=1}^G P(Y_{ng}=y_{ng} \\mid \\theta_g ) \\\\\n&= \\prod_{g=1}^G \\theta_g^{Y_{ng}}(1-\\theta_g)^{1-Y_{ng}}.\n\\end{align*}\n\\]\nConsider the case where we observe \\(N\\) exchangeable genomes. We will assume that they are conditionally independent given the model parameters: \\[\nP(Y_{*\\bullet} = y_{*\\bullet}\\mid \\theta_\\bullet) = \\prod_{n=1}^N P(Y_{n\\bullet}=y_{n\\bullet}\\mid \\theta_\\bullet).\n\\]\nThere’s an obligatory probabilistic graphical model representing our assumptions:\nCode\nimport daft\nimport matplotlib.pyplot as plt\nplt.style.use(\"dark_background\")\n\nclass MyPGM:\n  def __init__(self, dpi: int = 200) -&gt; None:\n    assert dpi &gt; 1\n    self.dpi = dpi\n    self.pgm = daft.PGM(dpi=dpi)\n\n  def add_node(self, id: str, name: str, x: float, y: float, observed: bool = False):\n    if observed:\n      params={\"facecolor\": \"grey\"}\n    else:\n      params={\"edgecolor\": \"w\"}\n    self.pgm.add_node(id, name, x, y, plot_params=params)\n\n  def add_edge(self, start: str, end: str):\n    self.pgm.add_edge(start, end, plot_params={\"edgecolor\": \"w\", \"facecolor\": \"w\"})\n\n  def add_plate(\n    self,\n    coords,\n    label: str,\n    shift: float = 0.0,\n    label_offset: tuple[float, float] = (0.02, 0.02),\n  ):\n    \"\"\"\n    Args:\n      coords: [x_left, y_bottom, x_length, y_length]\n      label: label\n      shift: vertical shift\n    \"\"\"\n    label_offset = (label_offset[0] * self.dpi, label_offset[1] * self.dpi)\n    self.pgm.add_plate(coords, label=label, shift=shift, rect_params={\"edgecolor\": \"w\"}, label_offset=label_offset)\n\n  def plot(self):\n    self.pgm.render()\n    plt.show()\n\npgm = MyPGM()\n\npgm.add_node(\"D\", \"$\\\\mathcal{D}$\", 0, 1)\npgm.add_node(\"theta\", r\"$\\theta_g$\", 2, 1)\npgm.add_node(\"Y\", r\"$Y_{ng}$\", 4, 1, observed=True)\n\npgm.add_edge(\"D\", \"theta\")\npgm.add_edge(\"theta\", \"Y\")\n\npgm.add_plate([1.5, 0.5, 3, 1.5], label=r\"$g = 1, \\ldots, G$\", shift=-0.1)\npgm.add_plate([2.7, 0.55, 1.7, 1], label=r\"$n=1, \\ldots, N$\")\n\npgm.plot()\nwhere \\(\\mathcal D\\) is the prior over the \\(\\theta_\\bullet\\) vector, i.e., \\(\\theta_g \\mid \\mathcal D \\sim \\mathcal D\\) and \\(\\mathcal D\\) is supported on the interval \\((0, 1)\\). The simplest choice is to fix \\(\\mathcal D = \\mathrm{Uniform}(0, 1)\\), but this may be not flexible enough. Namely, different draws from the posterior on \\(\\theta\\) may look quite different than draws from \\(\\mathcal D\\). And this discrepancy would be easy to observe once \\(G\\) is large.\nHence, the simplest improvement to this model is to make \\(\\mathcal D\\) more flexible, i.e., treating it as a random probability measure with some prior on it. The simplest possibility is to use \\(\\mathcal D = \\mathrm{Beta}(\\alpha, \\beta)\\), where \\(\\alpha\\) and \\(\\beta\\) are given some gamma (say) priors, but more flexible models are possible (such as mixtures of beta distribution or a Dirichlet process).\nThis model has enough flexibility to model marginal the mutation occurrence probabilities \\(P(Y_{n g} = 1)\\): given enough samples \\(N\\), the posterior on \\(\\theta_g\\) should concentrate around the average \\(N^{-1}\\sum_{n=1}^N Y_{ng}\\) (and, provided that \\(\\mathcal D\\) is flexible enough, it may concentrate near the distribution of these averages).\nGreat: this model is flexible enough to describe well the mutation probabilities. However, it’s quite rigid when it comes to mutation exclusivity and cooccurrence: \\[\nP(Y_{n1}=1, Y_{n2} = 1) = P(Y_{n1} = 1) \\cdot P(Y_{n2} = 1) = \\theta_{1}\\theta_{2}.\n\\]\nWe generally expect that mutations in some genes could lead to synthetic lethality, so they would be exclusive. Also, the genes are ordered in chromosomes and copy number aberrations can lead to mutations being simultaneously observed in several genes (e.g., if they are in the fragment of the chromosome which is frequently lost).\nThe discrepancies between this “independent mutations” model and real data is generally easy to observe by plotting the correlation matrix between different genes. I also like looking at the the empirical distribution of the observed number of mutations in one sample, i.e., \\(\\sum_{g=1}^G Y_{ng}\\), as the model predictis a Poisson binomial distribution, while real data may show very different behaviour."
  },
  {
    "objectID": "posts/mixtures-and-admixtures.html#why-expectation-maximization",
    "href": "posts/mixtures-and-admixtures.html#why-expectation-maximization",
    "title": "Mixtures and admixtures",
    "section": "Why expectation-maximization?",
    "text": "Why expectation-maximization?\nHowever, learning well-calibrated generative models \\(D_y\\) may be very hard task. Saerens, Latinne, and Decaestecker (2001) instead propose to learn a well-calibrated probabilistic classifier \\(P(Y \\mid X, \\pi^{(0)})\\) on an auxiliary population.\nThe assumption on the auxiliary population is the following: the conditional probability distributions \\(D_y = P(X\\mid Y=y)\\) have to be the same. The only thing that can differ is the proportion vector \\(\\pi_0\\), assumed to be known. This assumption is called prior probability shift or label shift and is rather strong, but also quite hard to avoid: if arbitrary distribution shifts are avoided, it’s not possible to generalize from one distribution to another! Finding suitable ways how to weaken the prior probability shift is therefore an interesting research problem on its own.\nNote that if we have a well-calibrated classifier \\(P(Y\\mid X, \\pi^{(0)})\\), we also have an access to a distribution \\(P(Y\\mid X, \\pi)\\). Namely, note that \\[\\begin{align*}\nP(Y=y\\mid X=x, \\pi) &\\propto P(Y=y, X=x \\mid \\pi) \\\\\n&= P(X=x \\mid Y=y, \\pi) P(Y=y\\mid \\pi) \\\\\n&= P(X=x \\mid Y=y)\\, \\pi_y,\n\\end{align*}\n\\] where the proportionality constant does not depend on \\(y\\). Analogously, \\[\nP(Y=y\\mid X=x, \\pi^{(0)}) \\propto P(X=x\\mid Y=y)\\, \\pi^{(0)}_y,\n\\] where the key observation is that for both distributions we assume that the conditional distribution \\(P(X=x\\mid Y=y)\\) is the same. Now we can take the ratio of both expressions and obtain \\[\nP(Y=y\\mid X=x, \\pi) \\propto P(Y=y\\mid X=x, \\pi^{(0)}) \\frac{ \\pi_y }{\\pi^{(0)}_y},\n\\] where the proportionality does not depend on \\(y\\). Hence, we can calculate unnormalized probabilities in this manner and then normalize them, so that they sum up to \\(1\\).\nTo summarize, we have the access to:\n\nWell-calibrated probability \\(P(Y=y\\mid X=x, \\pi)\\);\nThe prior probability \\(P(\\pi)\\);\nThe probability \\(P(Y_i=y \\mid \\pi) = \\pi_y\\);\n\nand we want to do inference on the posterior \\(P(\\pi \\mid \\{X_i\\})\\)."
  },
  {
    "objectID": "posts/mixtures-and-admixtures.html#expectation-maximization",
    "href": "posts/mixtures-and-admixtures.html#expectation-maximization",
    "title": "Mixtures and admixtures",
    "section": "Expectation-maximization",
    "text": "Expectation-maximization\nExpectation-maximization is an iterative algorithm trying to find a stationary point of the log-posterior \\[\\begin{align*}\n\\log P(\\pi \\mid \\{X_i=x_i\\}) &= P(\\pi) + \\log P(\\{X_i = x_i\\} \\mid \\pi) \\\\\n&= P(\\pi) + \\sum_{i=1}^N \\log P(X_i=x_i\\mid \\pi).\n\\end{align*}\n\\]\nIn particular, by running the optimization procedure several times, we can hope to find the maximum a posteriori estimate (or the maximum likelihood estimate, when the uniform distribution over the simplex is used as \\(P(\\pi)\\)). Interestingly, this optimization procedure will not assume that we can compute \\(\\log P(X_i=x_i\\mid \\pi)\\), using instead quantities available to us.\nAssume that at the current iteration the proportion vector is \\(\\pi^{(t)}\\). Then, \\[\\begin{align*}\n\\log P(X_i = x_i\\mid \\pi) &= \\log \\sum_{y=1}^L P(X_i = x_i, Y_i = y\\mid \\pi) \\\\\n&= \\log \\sum_{y=1}^L P(Y_i=y \\mid \\pi^{(t)}, X_i = x_i ) \\frac{ P(X_i=x_i, Y_i=y \\mid \\pi) }{P(Y_i=y \\mid \\pi^{(t)}, X_i=x_i)} \\\\\n&\\ge \\sum_{y=1}^L P(Y_i=y\\mid \\pi^{(t)}, X_i=x_i) \\log \\frac{P(X_i=x_i, Y_i=y \\mid \\pi)}{P(Y_i=y \\mid \\pi^{(t)}, X_i=x_i)}\n\\end{align*}\n\\]\nwhere the inequality follows from Jensen’s inequality for concave functions1.\nWe can now bound the loglikelihood by \\[\\begin{align*}\n\\log P(\\{X_i = x_i \\}\\mid \\pi) &= \\sum_{i=1}^N \\log P(X_i=x_i\\mid \\pi) \\\\\n&\\ge \\sum_{i=1}^N \\sum_{y=1}^L P(Y_i=y\\mid \\pi^{(t)}, X_i=x_i) \\log \\frac{P(X_i=x_i, Y_i=y \\mid \\pi)}{P(Y_i=y \\mid \\pi^{(t)}, X_i=x_i)}.\n\\end{align*}\n\\]\nNow let \\[\nQ(\\pi, \\pi^{(t)}) = \\log P(\\pi) + \\sum_{i=1}^N \\sum_{y=1}^L P(Y_i=y\\mid \\pi^{(t)}, X_i=x_i) \\log \\frac{P(X_i=x_i, Y_i=y \\mid \\pi)}{P(Y_i=y \\mid \\pi^{(t)}, X_i=x_i)},\n\\] which is a lower bound on the log-posterior. We will define the value \\(\\pi^{(t+1)}\\) by optimizing this lower bound: \\[\n\\pi^{(t+1)} := \\mathrm{argmax}_\\pi Q(\\pi, \\pi^{(t)}).\n\\]\nLet’s define auxiliary quantities \\(\\xi_{iy} = P(Y_i=y \\mid \\pi^{(t)}, X_i=x_i)\\), which can be calculated using the probabilistic classifier, as outlined above. This is called the expectation step (although we are actually calculating just probabilities, rather than more general expectations). In the new notation we have \\[\nQ(\\pi, \\pi^{(t)}) = \\log P(\\pi) + \\sum_{i=1}^N\\sum_{y=1}^L \\left(\\xi_{iy} \\log P(X_i=x_i, Y_i=y\\mid \\pi) - \\xi_{iy} \\log \\xi_{iy}\\right)\n\\]\nThe term \\(\\xi_{iy}\\log \\xi_{iy}\\) does not depend on \\(\\pi\\), so we don’t have to include it in the optimization. Writing \\(\\log P(X_i = x_i, Y_i=y\\mid \\pi) = \\log D_y(x_i) + \\log \\pi_y\\) we see that it suffices to optimize for \\(\\pi\\) the expression \\[\n\\log P(\\pi) + \\sum_{i=1}^N\\sum_{y=1}^L \\xi_{iy}\\left( \\log \\pi_y + \\log D_y(x_i) \\right).\n\\] Even better: not only \\(\\xi_{iy}\\) does not depend on \\(\\pi\\), but also \\(\\log D_y(x_i)\\)! Hence, we can drop from the optimization the terms requiring the generative models and we are left only with the easy to calculate quantities: \\[\n\\log P(\\pi) + \\sum_{i=1}^N\\sum_{y=1}^L \\xi_{iy} \\log \\pi_y.\n\\]\nLet’s use the prior \\(P(\\pi) = \\mathrm{Dirichlet}(\\pi \\mid \\alpha_1, \\dotsc, \\alpha_L)\\), so that \\(\\log P(\\pi) = \\mathrm{const.} + \\sum_{y=1}^L (\\alpha_y-1)\\log \\pi_y\\). Hence, we are interested in optimising \\[\n\\sum_{y=1}^L \\left((\\alpha_y-1) + \\sum_{i=1}^N \\xi_{iy} \\right)\\log \\pi_y.\n\\]\nWrite \\(A_y = \\alpha_y - 1 + \\sum_{i=1}^N\\xi_{iy}\\). We have to optimize the expression \\[\n\\sum_{y=1}^L A_y\\log \\pi_y\n\\] under a constraint \\(\\pi_1 + \\cdots + \\pi_L = 1\\).\nSaerens, Latinne, and Decaestecker (2001) use Lagrange multipliers, but we will use the first \\(L-1\\) coordinates to parameterise the simplex and write \\(\\pi_L = 1 - (\\pi_1 + \\cdots + \\pi_{L-1})\\). In this case, if we differentiate with respect to \\(\\pi_l\\), we obtain \\[\n\\frac{A_l}{\\pi_l} + \\frac{A_L}{\\pi_L} \\cdot (-1) = 0,\n\\]\nwhich in turn gives that \\(\\pi_y = k A_y\\) for some constant \\(k &gt; 0\\). We have \\[\n\\sum_{y=1}^L A_y = \\sum_{y=1}^L \\alpha_y - L + \\sum_{i=1}^N\\sum_{y=1}^L \\xi_{iy} = \\sum_{y=1}^L \\alpha_y - L + N.\n\\] Hence, \\[\n\\pi_y = \\frac{1}{(\\alpha_1 + \\cdots + \\alpha_L) + N - L}\\left( \\alpha_y-1 + \\sum_{i=1}^N \\xi_{iy} \\right),\n\\] which is taken as the next \\(\\pi^{(t+1)}\\).\nAs a minor observation, note that for a uniform prior over the simplex (i.e., all \\(\\alpha_y = 1\\)) we have \\[\n\\pi^{(t+1)}_y = \\frac 1N\\sum_{i=1}^N P(Y_i=y_i \\mid X_i=x_i, \\pi^{(t)} ).\n\\] Once we have converged to a fixed point and we have \\(\\pi^{(t)} = \\pi^{(t+1)}\\), it very much looks like \\[\nP(Y) = \\frac 1N\\sum_{i=1}^N P(Y_i \\mid X_i, \\pi) \\approx \\mathbb E_{X \\sim \\pi_1 D_1 + \\dotsc + \\pi_L D_L}[ P(Y\\mid X) ]\n\\] when \\(N\\) is large."
  },
  {
    "objectID": "posts/mixtures-and-admixtures.html#gibbs-sampler",
    "href": "posts/mixtures-and-admixtures.html#gibbs-sampler",
    "title": "Mixtures and admixtures",
    "section": "Gibbs sampler",
    "text": "Gibbs sampler\nFinally, let’s think how to implement a Gibbs sampler for this problem. Compared to the expectation-maximization this will be easy.\nTo solve the quantification problem we have to sample from the posterior distribution \\(P(\\pi \\mid \\{X_i\\})\\). Instead, let’s sample from a high-dimensional distribution \\(P(\\pi, \\{Y_i\\} \\mid \\{X_i\\})\\) — once we have samples of the form \\((\\pi, \\{Y_i\\})\\) we can simply forget about the \\(Y_i\\) values.\nThis is computationally a harder problem (we have many more variables to sample), however each sampling step will be very convenient. We will alternatively sample from \\[\n\\pi \\sim P(\\pi \\mid \\{X_i, Y_i\\})\n\\] and \\[\n\\{Y_i\\} \\sim P(\\{Y_i\\} \\mid \\{X_i\\}, \\pi).\n\\]\nThe first step is easy: \\(P(\\pi \\mid \\{X_i, Y_i\\}) = P(\\pi\\mid \\{Y_i\\})\\) which (assuming a Dirichlet prior) is a Dirichlet distribution. Namely, if \\(P(\\pi) = \\mathrm{Dirichlet}(\\alpha_1, \\dotsc, \\alpha_L)\\), then \\[\nP(\\pi\\mid \\{Y_i=y_i\\}) = \\mathrm{Dirichlet}\\left( \\alpha_1 + \\sum_{i=1}^N \\mathbf{1}[y_i = 1], \\dotsc, \\alpha_L + \\sum_{i=1}^N \\mathbf{1}[y_i=L] \\right).\n\\]\nLet’s think how to sample \\(\\{Y_i\\} \\sim P(\\{Y_i\\} \\mid \\{X_i\\}, \\pi)\\). This is a high-dimensional distribution, so let’s… use Gibbs sampling. Namely, we can iteratively sample \\[\nY_k \\sim P(Y_k \\mid \\{Y_1, \\dotsc, Y_{k-1}, Y_{k+1}, \\dotsc, Y_L\\}, \\{X_i\\}, \\pi).\n\\]\nThanks to the particular structure of this model, this is equivalent to sampling from \\[\nY_k \\sim P(Y_k \\mid X_k, \\pi) = \\mathrm{Categorical}(\\xi_{k1}, \\dotsc, \\xi_{kL}),\n\\] where \\(\\xi_{ky} = P(Y_k = y\\mid X_k = x_k, \\pi)\\) is obtained by recalibrating the given classifier."
  },
  {
    "objectID": "posts/mixtures-and-admixtures.html#summary",
    "href": "posts/mixtures-and-admixtures.html#summary",
    "title": "Mixtures and admixtures",
    "section": "Summary",
    "text": "Summary\nTo sum up, the reviewer was right: it’s very simple to upgrade the inference scheme in this model from a point estimate to a sample from the posterior!\nI however haven’t run simulations to know how well this sampler works in practice: I expect that this approach could suffer from:\n\nProblems from not-so-well-calibrated probabilistic classifier.\nEach iteration of the algorithm (whether expectation-maximization or a Gibbs sampler) requires passing through all \\(N\\) examples.\nAs there are \\(N\\) latent variables sampled, the convergence may perhaps be slow.\n\nIt’d be interesting to see how problematic these points are in practice (perhaps not at all!)"
  },
  {
    "objectID": "posts/mixtures-and-admixtures.html#appendix-numerical-implementation-in-jax",
    "href": "posts/mixtures-and-admixtures.html#appendix-numerical-implementation-in-jax",
    "title": "Mixtures and admixtures",
    "section": "Appendix: numerical implementation in JAX",
    "text": "Appendix: numerical implementation in JAX\nAs these algorithms are so simple, let’s quickly implement them in JAX. We will consider two Gaussian densities \\(D_1 = \\mathcal N(0, 1^2)\\) and \\(D_2 = \\mathcal N(\\mu, 1^2)\\). Let’s generate some data:\n\n\nCode\nimport jax\nimport jax.numpy as jnp\nfrom jax import random\nfrom jaxtyping import Array, Float, Int\nfrom jax.scipy.special import logsumexp\n\nn_cases: Int[Array, \" classes\"] = jnp.asarray([10, 40], dtype=int)\nmus: Float[Array, \" classes\"] = jnp.asarray([0.0, 1.0])\n\nkey = random.PRNGKey(42)\nkey, *subkeys = random.split(key, len(n_cases) + 1)\n\nxs: Float[Array, \" points\"] = jnp.concatenate(tuple(\n  mu + random.normal(subkey, shape=(n,))\n  for subkey, n, mu in zip(subkeys, n_cases, mus)\n))\n\nn_classes: int = len(n_cases)\nn_points: int = len(xs)\n\n\nAn NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n\n\nNow we need a probabilistic classifier. We will assume that it was calibrated on population with proportion \\(\\pi^{(0)} = (0.4, 0.6)\\).\n\n\nCode\n_normalizer: float = 0.5 * jnp.log(2 * jnp.pi)\n\ndef log_p(x, mu: float) -&gt; float:\n  \"\"\"Log-density N(x | mu, 1^2).\"\"\"\n  return -0.5 * jnp.square(x - mu) - _normalizer\n\n\n# Auxiliary matrix log P(X | Y)\n_log_p_x_y: Float[Array, \"points classes\"] = jnp.stack(tuple(log_p(xs, mu) for mu in mus)).T\nassert _log_p_x_y.shape == (n_points, n_classes), f\"Shape mismatch: {_log_p_x_y.shape}.\"\n\nlog_pi0: Float[Array, \" classes\"] = jnp.log(jnp.asarray([0.4, 0.6]))\n\n# Matrix representing log P(Y | X) for labeled population\nlog_p_y_x: Float[Array, \"points classes\"] = _log_p_x_y + log_pi0[None, :]\n# ... currently it's unnormalized, so we have to normalize it\n\ndef normalize_logprobs(log_ps: Float[Array, \"points classes\"]) -&gt; Float[Array, \"points classes\"]:\n  log_const = logsumexp(log_ps, keepdims=True, axis=-1)\n  return log_ps - log_const\n\nlog_p_y_x = normalize_logprobs(log_p_y_x)\n\n# Let's quickly check if it works\nsums = jnp.sum(jnp.exp(log_p_y_x), axis=1)\nassert sums.shape == (n_points,)\nassert jnp.min(sums) &gt; 0.999, f\"Minimum: {jnp.min(sums)}.\"\nassert jnp.max(sums) &lt; 1.001, f\"Maximum: {jnp.max(sums)}.\"\n\n\n\nExpectation-maximization algorithm\nIt’s time to implement expectation-maximization.\n\n\nCode\ndef expectation_maximization(\n  log_p_y_x: Float[Array, \"points classes\"],\n  log_pi0: Float[Array, \" classes\"],\n  log_start: None | Float[Array, \" classes\"] = None,\n  alpha: Float[Array, \" classes\"] | None = None,\n  n_iterations: int = 10_000,\n) -&gt; Float[Array, \" classes\"]:\n  \"\"\"Runs the expectation-maximization algorithm.\n\n  Args:\n    log_p_y_x: array log P(Y | X) for the calibrated population\n    log_pi0: array log P(Y) for the calibrated population\n    log_start: starting point. If not provided, `log_pi0` will be used\n    alpha: concentration parameters for the Dirichlet prior.\n      If not provided, the uniform prior will be used\n    n_iterations: number of iterations to run the algorithm for\n  \"\"\"\n  if log_start is None:\n    log_start = log_pi0\n  if alpha is None:\n    alpha = jnp.ones_like(log_pi0)\n\n  def iteration(_, log_pi: Float[Array, \" classes\"]) -&gt; Float[Array, \" classes\"]:\n    # Calculate log xi[n, y]\n    log_ps = normalize_logprobs(log_p_y_x + log_pi[None, :] - log_pi0[None, :])\n    # Sum xi[n, y] over n. We use the logsumexp, as we have log xi[n, y]\n    summed = jnp.exp(logsumexp(log_ps, axis=0, keepdims=False))\n    # The term inside the bracket (numerator)\n    numerator = summed + alpha - 1.0\n    # Denominator\n    denominator = jnp.sum(alpha) + log_p_y_x.shape[0] - log_p_y_x.shape[1]\n    return jnp.log(numerator / denominator)\n\n  return jax.lax.fori_loop(\n    0, n_iterations, iteration, log_start\n  )\n\nlog_estimated = expectation_maximization(\n  log_p_y_x=log_p_y_x,\n  log_pi0=log_pi0,\n  n_iterations=1000,\n  # Let's use slight shrinkage towards more uniform solutions\n  alpha=2.0 * jnp.ones_like(log_pi0),\n)\nestimated = jnp.exp(log_estimated)\nprint(f\"Estimated: {estimated}\")\nprint(f\"Actual:    {n_cases / n_cases.sum()}\")\n\n\nEstimated: [0.16425547 0.83574456]\nActual:    [0.2 0.8]\n\n\n\n\nGibbs sampler\nExpectation-maximization returns only a point estimate. We’ll explore the region around the posterior mode with a Gibbs sampler.\n\n\nCode\ndef gibbs_sampler(\n  key: random.PRNGKeyArray,\n  log_p_y_x: Float[Array, \"points classes\"],\n  log_pi0: Float[Array, \" classes\"],\n  log_start: None | Float[Array, \" classes\"] = None,\n  alpha: Float[Array, \" classes\"] | None = None,\n  n_warmup: int = 1_000,\n  n_samples: int = 1_000,\n) -&gt; Float[Array, \"n_samples classes\"]:\n  if log_start is None:\n    log_start = log_pi0\n  if alpha is None:\n    alpha = jnp.ones_like(log_pi0)\n\n  def iteration(\n    log_ps: Float[Array, \" classes\"],\n    key: random.PRNGKeyArray,\n  ) -&gt; tuple[Float[Array, \" classes\"], Float[Array, \" classes\"]]:\n    key, subkey1, subkey2 = random.split(key, 3)\n\n    ys = random.categorical(\n      subkey1,\n      log_ps[None, :] + log_p_y_x - log_pi0[None, :],\n      axis=-1,\n    )\n    counts = jnp.bincount(ys, length=log_pi0.shape[0])\n\n    new_log_pi = jnp.log(\n      random.dirichlet(subkey2, alpha + counts)\n    )\n\n    return new_log_pi, new_log_pi\n\n  _, samples = jax.lax.scan(\n    iteration,\n    log_start,\n    random.split(key, n_warmup + n_samples),\n  )\n  return samples[n_warmup:, :]\n\nkey, subkey = random.split(key)\nsamples = gibbs_sampler(\n  key=subkey,\n  log_p_y_x=log_p_y_x,\n  log_pi0=log_pi0,\n  # Let's use slight shrinkage towards more uniform solutions\n  alpha=2.0 * jnp.ones_like(log_pi0),\n  # Use EM point as a starting point\n  log_start=log_estimated,\n  n_samples=5_000,\n)\nsamples = jnp.exp(samples)\n\nprint(f\"Mean:   {jnp.mean(samples, axis=0)}\")\nprint(f\"Std:    {jnp.std(samples, axis=0)}\")\nprint(f\"Actual: {n_cases / n_cases.sum()}\")\n\n\n/tmp/ipykernel_48154/3942328187.py:2: DeprecationWarning: jax.random.PRNGKeyArray is deprecated. Use jax.Array for annotations, and jax.dtypes.issubdtype(arr.dtype, jax.dtypes.prng_key) for runtime detection of typed prng keys.\n  key: random.PRNGKeyArray,\n/tmp/ipykernel_48154/3942328187.py:17: DeprecationWarning: jax.random.PRNGKeyArray is deprecated. Use jax.Array for annotations, and jax.dtypes.issubdtype(arr.dtype, jax.dtypes.prng_key) for runtime detection of typed prng keys.\n  key: random.PRNGKeyArray,\n\n\nMean:   [0.20171012 0.79828984]\nStd:    [0.09912279 0.09912279]\nActual: [0.2 0.8]\n\n\nLet’s visualise the posterior samples, together with the expectation-maximization solution and the ground truth:\n\n\nCode\nimport matplotlib.pyplot as plt\nplt.style.use(\"dark_background\")\n\nfig, ax = plt.subplots(dpi=150)\n\nbins = jnp.linspace(0, 1, 40)\n\nfor y in range(n_classes):\n  color = f\"C{y+1}\"\n  ax.hist(samples[:, y], bins=bins, density=True, histtype=\"step\", color=color)\n  ax.axvline(n_cases[y] / n_cases.sum(), color=color, linewidth=3)\n  ax.axvline(estimated[y], color=color, linestyle=\"--\")\n\nax.set_title(\"Posterior distribution\")\nax.set_ylabel(\"Posterior density\")\nax.set_xlabel(\"Component value\")\nax.spines[[\"top\", \"right\"]].set_visible(False)\nfig.tight_layout()"
  },
  {
    "objectID": "posts/mixtures-and-admixtures.html#footnotes",
    "href": "posts/mixtures-and-admixtures.html#footnotes",
    "title": "Mixtures and admixtures",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIt’s good to remember: \\(\\log \\mathbb E[A] \\ge \\mathbb E[\\log A]\\).↩︎"
  },
  {
    "objectID": "posts/mixtures-and-admixtures.html#the-mixtures",
    "href": "posts/mixtures-and-admixtures.html#the-mixtures",
    "title": "Mixtures and admixtures",
    "section": "The mixtures",
    "text": "The mixtures\nThe model above is not flexible enough to model co-occurrences and exclusivity between different mutations. The simplest way to improve it is to consider a mixture distribution: \\[\nP(Y_n \\mid \\{\\theta_{k\\bullet}\\}_{k=1, \\dotsc, K}, \\pi) = \\sum_{k=1}^K P(Y_{n\\bullet} \\mid \\theta_{k\\bullet} ) \\pi_k,\n\\]\nwhere \\(\\pi\\in \\Delta^{K-1}\\) is proportion vector over \\(K\\) classes.\nThis distribution could be biologically motivated as follows: imagine that we measure \\(G=3\\) genes \\(1,\\,2,\\,3\\) which lie very close together in the genome.\nIf there is no large deletion in this chromosome, they are independently mutated with probabilities \\(1\\%\\), \\(5\\%\\) and \\(10\\%\\), respectively. However, if there is a large deletion, we will notice that they are all gone. Hence, it would make sense to consider two different “populations” \\(K=2\\) and \\(\\theta_{1\\bullet} = (1\\%, 5\\%, 10\\%)\\) and \\(\\theta_{2\\bullet} = (100\\%, 100\\%, 100\\%)\\), where the “population” describes whether such a large deletion had place.\nIn principle, the mixtures can be used to model an arbitrary distribution over binary vectors: consider \\(K=2^G\\) and each \\(\\theta_{k\\bullet}\\) vector to represent a different string \\(0\\) and \\(1\\) digits. Hence, we see that the “populations” do not necessarily have biological meaning. (See also Chapter 22 of Bayesian Data Analysis or this Cosma Shalizi’s blog post explaining the overinterpretation issues in factor analysis).\nAnother issue with using \\(K=2^G\\) classes is that \\(2^G\\) is usually much larger than \\(N\\), so that finding a suitable set of parameters \\(\\{\\theta_{k\\bullet}\\}\\) may be tricky. We will generally prefer a smaller number of components, although using a Dirichlet process mixture model is possible.\nThe mixtures are generally very popular, even if it is not always immediately clear that a model is a mixture: for example, this model can be modelled as a mixture with \\(G+1\\) components for an appropriate prior constraining the \\(\\theta\\) parameters. Or the Bayesian pyramids with \\(S\\) latent binary traits correspond to a finite mixture with \\(K=2^S\\) components (and, again, the \\(\\theta\\) parameters have to be constrained; see Section 3.1)."
  },
  {
    "objectID": "posts/mixtures-and-admixtures.html#mixture-models",
    "href": "posts/mixtures-and-admixtures.html#mixture-models",
    "title": "On mixtures and admixtures",
    "section": "Mixture models",
    "text": "Mixture models\nThe model above is not flexible enough to model co-occurrences and exclusivity between different mutations.\nImagine that we sequence \\(G=3\\) genes which lie very close together.\nIf there is no large deletion in this chromosome, they are independently mutated with probabilities \\(1\\%\\), \\(5\\%\\) and \\(10\\%\\), respectively. However, if there is a large deletion, we will notice that they are all gone. Hence, it would make sense to consider two different “populations” with mutation probabilities \\(\\theta_{1\\bullet} = (1\\%, 5\\%, 10\\%)\\) and \\(\\theta_{2\\bullet} = (100\\%, 100\\%, 100\\%)\\), where the “population” describes whether such a large deletion had place. More generally, if we consider \\(K\\) “populations”, we can introduce sample-specific variables \\(Z_n \\in \\{1, 2, \\dotsc, K\\}\\) and the distributions \\[\nP(Y_{n\\bullet} \\mid Z_n, \\{\\theta_{k\\bullet}\\}_{k=1, \\dotsc, K}) = P(Y_{n\\bullet} \\mid \\theta_{Z_n\\bullet}).\n\\]\nWe will draw this model as\n\n\nCode\npgm = MyPGM()\n\npgm.add_node(\"pi\", r\"$\\pi$\", 0, 0)\npgm.add_node(\"Z\", \"$Z_n$\", 1, 0)\npgm.add_node(\"Y\", r\"$Y_{n\\bullet}$\", 2, 0, observed=True)\n\npgm.add_node(\"D\", r\"$\\mathcal{D}$\", 1, 1)\npgm.add_node(\"theta\", r\"$\\theta_{k\\bullet}$\", 2, 1)\n\npgm.add_edge(\"pi\", \"Z\")\npgm.add_edge(\"Z\", \"Y\")\n\npgm.add_edge(\"D\", \"theta\")\npgm.add_edge(\"theta\", \"Y\")\n\npgm.add_plate([0.5, -0.5, 2, 1], label=r\"$n=1, \\ldots, N$\", shift=-0.1)\npgm.add_plate([1.5, 0.6, 1, 1], label=r\"$k=1, \\ldots, K$\", label_offset=(0.01, 0.2))\n\npgm.plot()\n\n\n\n\n\nwhere \\(\\pi\\in \\Delta^{K-1}\\) is proportion vector over \\(K\\) “populations”.\nDue to the conditional independence structure in this model, we can also integrate out the \\(Z_n\\) variables to get \\[\nP(Y_n \\mid \\{\\theta_{k\\bullet}\\}_{k=1, \\dotsc, K}, \\pi) = \\sum_{k=1}^K \\pi_k \\, P(Y_{n\\bullet} \\mid \\theta_{k\\bullet} )\n\\]\nwhich graphically corresponds to\n\n\nCode\npgm = MyPGM()\n\npgm.add_node(\"pi\", r\"$\\pi$\", 0, 0)\npgm.add_node(\"Y\", r\"$Y_{n\\bullet}$\", 2, 0, observed=True)\n\npgm.add_node(\"D\", r\"$\\mathcal{D}$\", 1, 1)\npgm.add_node(\"theta\", r\"$\\theta_{k\\bullet}$\", 2, 1)\n\npgm.add_edge(\"pi\", \"Y\")\n\npgm.add_edge(\"D\", \"theta\")\npgm.add_edge(\"theta\", \"Y\")\n\npgm.add_plate([0.5, -0.5, 2, 1], label=r\"$n=1, \\ldots, N$\", shift=-0.1)\npgm.add_plate([1.5, 0.6, 1, 1], label=r\"$k=1, \\ldots, K$\", label_offset=(0.01, 0.2))\n\npgm.plot()\n\n\n\n\n\nThis integrated out representation is quite convenient for inference (see Stan User’s guide)."
  },
  {
    "objectID": "posts/mixtures-and-admixtures.html#how-expressive-are-finite-mixtures",
    "href": "posts/mixtures-and-admixtures.html#how-expressive-are-finite-mixtures",
    "title": "On mixtures and admixtures",
    "section": "How expressive are finite mixtures?",
    "text": "How expressive are finite mixtures?\nIn principle, the mixtures can be used to model an arbitrary distribution over binary vectors: consider \\(K=2^G\\) and each \\(\\theta_{k\\bullet}\\) vector to represent a different string \\(0\\) and \\(1\\) digits. Hence, we see that the “populations” do not necessarily have biological meaning. (See also Chapter 22 of Bayesian Data Analysis or this Cosma Shalizi’s blog post explaining the overinterpretation issues in factor analysis).\nAnother issue with using \\(K=2^G\\) classes is that \\(2^G\\) is usually much larger than \\(N\\), so that finding a suitable set of parameters \\(\\{\\theta_{k\\bullet}\\}\\) may be tricky. We will generally prefer a smaller number of components, although using a Dirichlet process mixture model is possible (which is quite funny, because in these models \\(K=\\infty\\) and there may be more occupied components than \\(2^G\\), which still result in the distribution which could be modelled with \\(2^G\\) clusters).\nThe mixtures (with a smaller number of components than \\(2^G\\)) are generally very popular, even if it is not always immediately clear that a model is a finite mixture: for example, this model can be modelled as a mixture with \\(K = G+1\\) components for an appropriate prior constraining the \\(\\theta\\) parameters.\nAnother example of such model are Bayesian pyramids: imagine that we sequence genes 1, 2, 3 on one chromosome and genes 4, 5, 6 on another chromosome, then we may consider the following idealised model employing \\(K=4\\) “populations”:\n\n\\(Z_n=1\\) means that neither chromosome is lost. Mutations in the genes arise independently.\n\\(Z_n=2\\) means that only the first chromosome is lost, i.e., genes 1, 2, 3 are mutated. The mutations at loci 4, 5, 6 can arise independently.\n\\(Z_n=3\\) means that the second chromosome is lost, i.e., genes 4, 5, 6 are mutated. The mutations at loci 1, 2, 3 can arise independently.\n\\(Z_n=4\\) means that both chromosomes are lost and we observe mutations in all six genes.\n\nThis model with four populations works. However, some of the parameters are tied together, as they correspond to the effects of deletion of two different chromosomes and using this structure may be important for the scalability and speed of posterior shrinkage: if we consider 10 different chromosomes, we don’t need to model \\(2^10\\) independent clusters. More on this parameter constraints is in Section 3.1 of the original paper. (And, if you are interested in the biological applications to tumor genotypes, I’ll be showing a poster at RECOMB 2024 on this topic! You can also take a look at the Jnotype package)."
  },
  {
    "objectID": "posts/mixtures-and-admixtures.html#admixture-models",
    "href": "posts/mixtures-and-admixtures.html#admixture-models",
    "title": "On mixtures and admixtures",
    "section": "Admixture models",
    "text": "Admixture models\nAbove we discussed that mixture models can be very expressive when they have many components (so sometimes the parameters are shared between different components).\nLet’s think about an admixture model, which can be treated as a simplified version of the STRUCTURE model. We again assume that we have some probability vectors \\(\\theta_{k\\bullet}\\) for \\(k=1, \\dotsc, K\\). However, in this case we will call them “topics” or distinct mutational mechanisms which operate in the following way. For each gene \\(Y_{ng}\\) there is a mechanism \\(T_{ng} \\in \\{1, \\dotsc, K\\}\\) which is used to generate the mutation: \\[\n  P(Y_{ng} =1 \\mid T_{ng}, \\{\\theta_{k\\bullet}\\}_{k=1, \\dotsc, K}) = \\theta_{T_{ng}g}.\n\\]\nThe mechanisms corresponding to different genes are drawn independently within each sample, i.e., we have a sample-specific proportion vector \\(\\pi_n\\in \\Delta^{K-1}\\) which is used to sample the topics: \\[\n  T_{ng} \\mid \\pi_n \\sim \\mathrm{Categorical}(\\pi_n).\n\\]\nLet’s draw the dependencies in this model:\n\n\nCode\npgm = MyPGM()\n\npgm.add_node(\"G\", r\"$\\mathcal{G}$\", 0, 1)\n\npgm.add_node(\"pi\", r\"$\\pi_n$\", 0, 0)\npgm.add_node(\"T\", \"$T_{ng}$\", 1, 0)\npgm.add_node(\"Y\", r\"$Y_{ng}$\", 2, 0, observed=True)\n\npgm.add_node(\"D\", r\"$\\mathcal{D}$\", 1, 1)\npgm.add_node(\"theta\", r\"$\\theta_{k\\bullet}$\", 2, 1)\n\npgm.add_edge(\"G\", \"pi\")\npgm.add_edge(\"pi\", \"T\")\npgm.add_edge(\"T\", \"Y\")\n\npgm.add_edge(\"D\", \"theta\")\npgm.add_edge(\"theta\", \"Y\")\n\npgm.add_plate([-0.5, -0.5, 3, 1], label=r\"$n=1, \\ldots, N$\", shift=-0.1)\npgm.add_plate([1.5, 0.6, 1, 1], label=r\"$k=1, \\ldots, K$\", label_offset=(0.01, 0.2))\n\npgm.add_plate([0.55, -0.45, 1.85, 0.8], label=r\"$g=1, \\ldots, G$\", label_offset=(0.25, 0.01))\n\n\npgm.plot()\n\n\n\n\n\nSimilarly as in the mixture models, we can integrate out the latent variables: \\[\\begin{align*}\n  P(Y_{n\\bullet} \\mid \\pi_n, \\{\\theta_{k\\bullet}\\}_{k=1, \\dotsc, K}) &= \\sum_{T_{n\\bullet}} P(Y_{n\\bullet} \\mid T_{n\\bullet}, \\{\\theta_{k\\bullet}\\}_{k=1, \\dotsc, K}) P(T_{n\\bullet} \\mid \\pi_n ) \\\\\n  &= \\sum_{T_{n\\bullet}} \\prod_{g=1}^G  P( Y_{ng} \\mid \\theta_{T_{ng}g}) P(T_{ng} \\mid \\pi_n) \\\\\n  &= \\prod_{g=1}^G \\left( \\sum_{k=1}^K P(Y_{ng}\\mid \\theta_{kg})\\, \\pi_{nk} \\right)\n\\end{align*}\n\\]\nWe see that if the proportions vector \\(\\pi_n\\) becomes one-hot vectors, then the admixture model reduces to the mixture model we have seen above. However, this model is more flexible.\nLet’s draw the version with local variables integrated out:\n\n\nCode\npgm = MyPGM()\n\npgm.add_node(\"G\", r\"$\\mathcal{G}$\", 0, 1)\n\npgm.add_node(\"pi\", r\"$\\pi_n$\", 0, 0)\npgm.add_node(\"Y\", r\"$Y_{ng}$\", 2, 0, observed=True)\n\npgm.add_node(\"D\", r\"$\\mathcal{D}$\", 1, 1)\npgm.add_node(\"theta\", r\"$\\theta_{k\\bullet}$\", 2, 1)\n\npgm.add_edge(\"G\", \"pi\")\npgm.add_edge(\"pi\", \"Y\")\n\npgm.add_edge(\"D\", \"theta\")\npgm.add_edge(\"theta\", \"Y\")\n\npgm.add_plate([-0.5, -0.5, 3, 1], label=r\"$n=1, \\ldots, N$\", shift=-0.1)\npgm.add_plate([1.5, 0.6, 1, 1], label=r\"$k=1, \\ldots, K$\", label_offset=(0.01, 0.2))\n\npgm.add_plate([0.55, -0.45, 1.85, 0.8], label=r\"$g=1, \\ldots, G$\", label_offset=(0.25, 0.01))\n\n\npgm.plot()\n\n\n\n\n\nThe issue in this model is that we have \\(O(KG)\\) parameters in the \\(\\theta\\) matrix (unless some parameter sharing is used) and additionally we have \\(O(NK)\\) parameters of the proportion vectors. Hence, the inference in this model may be trickier to apply."
  },
  {
    "objectID": "posts/mixtures-and-admixtures.html#how-do-the-samples-look-like",
    "href": "posts/mixtures-and-admixtures.html#how-do-the-samples-look-like",
    "title": "On mixtures and admixtures",
    "section": "How do the samples look like?",
    "text": "How do the samples look like?\nLet’s take \\(G=10\\) and \\(K=3\\) and simulate some \\(\\theta\\) matrix:\n\n\nCode\nimport numpy as np\nimport seaborn as sns\n\nG = 10\nK = 3\n\nrng = np.random.default_rng(2024)\ntheta = np.zeros((K, G))\nfor k in range(K):\n  theta[k, :] = rng.beta(1, 5 + k**2, size=(G,))\n  theta[k, min(3*k, G):min(3*k+3,10)] = 0.9\n# rng.uniform(size=(K, G))\n\ntheta[0, :] = np.sort(theta[0, :])\ntheta[-1, :] = np.sort(theta[1, :])[::-1]\n\nfig, ax = plt.subplots(figsize=(4, 2), dpi=250)\nsns.heatmap(theta, vmin=0, vmax=1, cmap=\"Greys_r\", ax=ax, xticklabels=False, yticklabels=False, square=True, cbar=False)\nax.set_xlabel(\"Genes\")\nax.set_ylabel(\"Populations\")\nplt.show()\n\n\n\n\n\nLet’s sample 50 vectors from each component:\n\n\nCode\nz = np.asarray(sum(([k] * 50 for k in range(K)), []))\nprobs = theta[z]\nY = rng.binomial(1, probs)\n\nfig, ax = plt.subplots(figsize=(5, 2), dpi=250)\nsns.heatmap(Y.T, vmin=0, vmax=1, cmap=\"Greys_r\", ax=ax, square=False, xticklabels=False, yticklabels=False, cbar=False)\n\nax.set_xlabel(\"Samples\")\nax.set_ylabel(\"Genes\")\nplt.show()\n\n\n\n\n\nOn the other hand, if we sample from an admixture model, where \\(\\pi_n \\sim \\mathrm{Dirichlet}(\\alpha, \\alpha, \\alpha)\\) for different values of \\(\\alpha\\), we will get the following proportion vectors:\n\n\nCode\nN = 150\n\nalphas = [0.001, 0.1, 10.0]\nfig, axs = plt.subplots(3, 1, figsize=(7, 6), dpi=250, sharex=True, sharey=True)\n\npis = np.zeros((len(alphas), N, K))\n\nfor i, (alpha, ax) in enumerate(zip(alphas, axs)):\n  pi = rng.dirichlet(alpha * np.ones(K), size=N)\n  index = np.argsort( np.einsum(\"nk,k-&gt;n\", pi, np.arange(K)))\n  pi = pi[index, :]\n  pis[i, ...] = pi\n\n  x_axis = np.arange(0.5, N + 0.5)\n\n  prev = 0.0\n  for k in range(K):\n    ax.fill_between(x_axis, prev, prev + pi[:, k], color=f\"C{k+4}\")\n    prev = prev + pi[:, k]\n\n  ax.spines[[\"top\", \"right\"]].set_visible(False)\n  ax.set_xlabel(\"Samples\")\n  ax.set_ylabel(\"Proportions\")\n  ax.set_xticks([])\n  ax.set_title(f\"$\\\\alpha$: {alpha:.3f}\")\n\nfig.tight_layout()\nplt.show()\n\n\n\n\n\nNote that for \\(\\alpha \\approx 0\\) the proportion vectors are close to one-hot, so that the admixture reduces to the mixture above.\nWe can generate the following samples corresponding to proportion vectors above as following:\n\n\nCode\nfig, axs = plt.subplots(3, 1, figsize=(7, 6), dpi=250, sharex=True, sharey=True)\n\nfor alpha, pi, ax in zip(alphas, pis, axs):\n  Y = np.zeros((N, G), dtype=int)\n\n  for n in range(N):\n    T_ = rng.multinomial(1, pi[n], size=G)\n    T = T_ @ np.arange(K)\n    probs = theta[T, np.arange(G)]\n    Y[n, :] = rng.binomial(1, probs)\n\n  sns.heatmap(Y.T, vmin=0, vmax=1, cmap=\"Greys_r\", ax=ax, square=False, xticklabels=False, yticklabels=False, cbar=False)\n  ax.set_xlabel(\"Samples\")\n  ax.set_ylabel(\"Genes\")\n  ax.set_title(f\"Sparsity: {alpha:.3f}\")\n\nfig.tight_layout()\nplt.show()"
  },
  {
    "objectID": "posts/mixtures-and-admixtures.html#links",
    "href": "posts/mixtures-and-admixtures.html#links",
    "title": "On mixtures and admixtures",
    "section": "Links",
    "text": "Links\n\nWe discussed mixture models in the Dirichlet process.\nThere is an excellent introduction to mixture and admixture models in Jeff Miller’s Bayesian methodology in biostatistics course (see lectures 9–12).\nNicola Roberts used hierarchical Dirichlet processes (which are also an admixture model, although of a bit different kind) to study mutational patterns during her PhD.\nBarbara Engelhardt and Matthew Stephens wrote a nice paper interpreting admixture models as matrix factorizations. Of course, mixture models (being a special case of an admixture) also fit in this framework.\nQuantification is naturally related to admixture models. We discussed it here and there."
  }
]