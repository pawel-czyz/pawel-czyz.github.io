[
  {
    "objectID": "posts/histograms-vs-density-estimation.html",
    "href": "posts/histograms-vs-density-estimation.html",
    "title": "Histograms or kernel density estimators?",
    "section": "",
    "text": "I have recently seen Michael Betancourt’s talk in which he explains why kernel density estimators can be misleading when visualising samples and points to his wonderful case study which includes comparison between histograms and kernel density estimators, as well as many other things.\nI recommend reading this case study in depth; in this blog post we will only try to reproduce the example with kernel density estimators in Python."
  },
  {
    "objectID": "posts/histograms-vs-density-estimation.html#problem-setup",
    "href": "posts/histograms-vs-density-estimation.html#problem-setup",
    "title": "Histograms or kernel density estimators?",
    "section": "Problem setup",
    "text": "Problem setup\nWe will start with a Gaussian mixture with two components and draw the exact probability density function (PDF) as well as a histogram with a very large sample size.\n\n\nCode\nimport matplotlib.pyplot as plt\nimport numpy as np \nfrom scipy import stats\n\nplt.style.use(\"dark_background\")\n\nclass GaussianMixture:\n  def __init__(self, proportions, mus, sigmas) -&gt; None:\n    proportions = np.asarray(proportions)\n    self.proportions = proportions / proportions.sum()\n    assert np.min(self.proportions) &gt; 0\n\n    self.mus = np.asarray(mus)\n    self.sigmas = np.asarray(sigmas)\n\n    n = len(self.proportions)\n    self.n_classes = n\n    assert self.proportions.shape == (n,)\n    assert self.mus.shape == (n,)\n    assert self.sigmas.shape == (n,)\n\n  def sample(self, rng, n: int) -&gt; np.ndarray:\n    z = rng.choice(\n      self.n_classes,\n      p=self.proportions,\n      replace=True,\n      size=n,\n    )\n    return self.mus[z] + self.sigmas[z] * rng.normal(size=n)\n\n  def pdf(self, x):\n    ret = 0\n    for k in range(self.n_classes):\n      ret += self.proportions[k] * stats.norm.pdf(x, loc=self.mus[k], scale=self.sigmas[k])\n    return ret\n\nmixture = GaussianMixture(\n  proportions=[2, 1],\n  mus=[-2, 2],\n  sigmas=[1, 1],\n)\n\nrng = np.random.default_rng(32)\n\nlarge_data = mixture.sample(rng, 100_000)\n\nx_axis = np.linspace(np.min(large_data), np.max(large_data), 101)\npdf_values = mixture.pdf(x_axis)\n\nfig, ax = plt.subplots(figsize=(3, 2), dpi=100)\n\nax.hist(large_data, bins=150, density=True, histtype=\"stepfilled\", alpha=0.5, color=\"C0\")\nax.plot(x_axis, pdf_values, c=\"C2\", linestyle=\"--\")\n\nax.set_title(\"Probability density function\\nand histogram with large sample size\")\n\n\nText(0.5, 1.0, 'Probability density function\\nand histogram with large sample size')\n\n\n\n\n\nGreat, histogram with large sample size agreed well with the exact PDF!"
  },
  {
    "objectID": "posts/histograms-vs-density-estimation.html#plain-old-histograms",
    "href": "posts/histograms-vs-density-estimation.html#plain-old-histograms",
    "title": "Histograms or kernel density estimators?",
    "section": "Plain old histograms",
    "text": "Plain old histograms\nLet’s now move to a more challenging problem: we have only a moderate sample size available, say 100 points.\n\n\nCode\ndata = mixture.sample(rng, 100)\n\nfig, axs = plt.subplots(5, 1, figsize=(3.2, 3*5), dpi=100)\nbin_sizes = (3, 5, 10, 20, 50)\n\nfor bins, ax in zip(bin_sizes, axs):\n  ax.hist(data, bins=bins, density=True, histtype=\"stepfilled\", alpha=0.5, color=\"C0\")\n  ax.plot(x_axis, pdf_values, c=\"C2\", linestyle=\"--\")\n\n  ax.set_title(f\"{bins} bins\")\n\nfig.tight_layout()\n\n\n\n\n\nWe see that too few bins (three, but nobody will actually choose this number for 100 data points) we don’t see two modes and that for more than 20 and 50 bins the histogram looks quite noisy. Both 5 and 10 bins would make a sensible choice in this problem."
  },
  {
    "objectID": "posts/histograms-vs-density-estimation.html#kernel-density-estimators",
    "href": "posts/histograms-vs-density-estimation.html#kernel-density-estimators",
    "title": "Histograms or kernel density estimators?",
    "section": "Kernel density estimators",
    "text": "Kernel density estimators\nNow it’s the time for kernel density estimators. We will use several kernel families and several different bandwidths:\n\n\nCode\nfrom sklearn.neighbors import KernelDensity\n\n\nkernels = [\"gaussian\", \"tophat\", \"cosine\"]\nbandwidths = [0.1, 1.0, 3.0, \"scott\", \"silverman\"]\n\nfig, axs = plt.subplots(\n  len(kernels),\n  len(bandwidths),\n  figsize=(12, 8),\n  dpi=130,\n)\n\nfor i, kernel in enumerate(kernels):\n  axs[i, 0].set_ylabel(f\"Kernel: {kernel}\")\n  for j, bandwidth in enumerate(bandwidths):\n    ax = axs[i, j]\n\n    kde = KernelDensity(bandwidth=bandwidth, kernel=kernel)\n    kde.fit(data[:, None])\n\n    kde_pdf = np.exp(kde.score_samples(x_axis[:, None]))\n\n    ax.plot(x_axis, pdf_values, c=\"C2\", linestyle=\"--\")\n    ax.fill_between(x_axis, 0.0, kde_pdf, color=\"C0\", alpha=0.5)\n\n\nfor j, bandwidth in enumerate(bandwidths):\n  axs[0, j].set_title(f\"Bandwidth: {bandwidth}\")\n\nfig.tight_layout()\n\n\n\n\n\nI see the point now! Apart from the small bandwidth case (0.1 and sometimes Silverman) the issues with KDE plots are hard to diagnose. Moreover, conclusions from different plots are different: is the distribution multimodal? If so, how many modes are there? What are the “probability masses” of each modes? Observing only one of these plots can lead to wrong conclusions."
  },
  {
    "objectID": "posts/histograms-vs-density-estimation.html#links",
    "href": "posts/histograms-vs-density-estimation.html#links",
    "title": "Histograms or kernel density estimators?",
    "section": "Links",
    "text": "Links\n\nWhat’s wrong with a kernel density: a blog post by Andrew Gelman, explaining why he prefers histograms over kernel density plots.\nMichael Betancourt’s case study, which also discusses histograms with error bars."
  },
  {
    "objectID": "posts/em-gibbs-quantification.html",
    "href": "posts/em-gibbs-quantification.html",
    "title": "Expectation-maximization and Gibbs sampling in quantification",
    "section": "",
    "text": "Consider an unlabeled image data set \\(x_1, \\dotsc, x_N\\). We know that each image in this data set corresponds to a unique class (e.g., a cat or a dog) \\(y\\in \\{1, \\dotsc, L\\}\\) and we would like to estimate how many images \\(x_i\\) belong to each class. This problem is known as quantification and there exist numerous approaches to this problem, employing an auxiliary data set. Albert Ziegler and I were interested in additionally quantifying uncertainty1 around such estimates (see Ziegler and Czyż 2023) by building a generative model on summary statistic and performing Bayesian inference.\nWe got a very good question from the reviewer: if we compare our method to point estimates produced by an expectation-maximization algorithm (Saerens, Latinne, and Decaestecker 2001) and we are interested in uncertainty quantification, why don’t we upgrade this method to a Gibbs sampler?\nI like this question, because it’s very natural to ask, yet I overlooked the possibliby of doing it. As Richard McElreath explains here, Hamiltonian Markov chain Monte Carlo is usually the preferred way of sampling, but let’s see how exactly the expectation-maximization algorithm works in this case and how to adapt it to a Gibbs sampler."
  },
  {
    "objectID": "posts/em-gibbs-quantification.html#modelling-assumptions",
    "href": "posts/em-gibbs-quantification.html#modelling-assumptions",
    "title": "Expectation-maximization and Gibbs sampling in quantification",
    "section": "Modelling assumptions",
    "text": "Modelling assumptions\nGeneratively, the model is very similar to the one used in clustering problems: for each object we have an observed random variable \\(X_i\\) (with its realization being the image \\(x_i\\)) and a latent random variable \\(Y_i\\), which is valued in the set of labels \\(\\{1, \\dotsc, L\\}\\).\nAdditionally, there’s a latent vector \\(\\pi = (\\pi_1, \\dotsc, \\pi_L)\\) with non-negative entries, such that \\(\\pi_1 + \\cdots + \\pi_L = 1\\). In other words, vector \\(\\pi\\) is the proportion vector of interest.\nWe can visualise the assumed dependencies in the following graphical model:\n\n\nCode\nimport daft\nimport matplotlib.pyplot as plt\n\nplt.style.use('grayscale')\n\n# Instantiate a PGM object\npgm = daft.PGM()\n\n# Add nodes. The arguments are (name, label, x-position, y-position)\npgm.add_node(\"pi\", \"$\\\\pi$\", 0, 1)\npgm.add_node(\"Y_i\", r\"$Y_i$\", 2, 1)\npgm.add_node(\"X_i\", r\"$X_i$\", 4, 1, plot_params={\"facecolor\": \"cornflowerblue\"})\n\n# Add edges\npgm.add_edge(\"pi\", \"Y_i\")\npgm.add_edge(\"Y_i\", \"X_i\")\n\n# Add a plate\npgm.add_plate([1.5, 0.5, 3, 1.5], label=r\"$i = 1, \\ldots, N$\", shift=-0.1)\n\n# Render and show the PGM\npgm.render()\nplt.show()\n\n\n\n\n\nAs \\(\\pi\\) is simplex-valued, it’s convenient to model it with a Dirichlet prior. Then, \\(Y_i\\mid \\pi \\sim \\mathrm{Categorical}(\\pi)\\). Finally, we assume that each class \\(y\\) has a corresponding distribution \\(D_y\\) from which the image is sampled. In other words, \\(X_i\\mid Y_i=y \\sim D_y\\).\nIn case we know all distributions \\(D_y\\), this is quite a simple problem: we can marginalise the latent variables \\(Y_i\\) obtaining \\[\nP(\\{X_i=x_i\\} \\mid \\pi) = \\prod_{i=1}^N \\big( \\pi_1 D_1(x_i) + \\cdots + \\pi_L D_L(x_i) \\big)\n\\] which in turn can be used to infer \\(\\pi\\) using Hamiltonian Markov chain Monte Carlo algorithms. In fact, a variant of this approach, employing maximum likelihood estimate, rather than Bayesian inference, was proposed by Peters and Coberly (1976) as early as in 1976!"
  },
  {
    "objectID": "posts/em-gibbs-quantification.html#why-expectation-maximization",
    "href": "posts/em-gibbs-quantification.html#why-expectation-maximization",
    "title": "Expectation-maximization and Gibbs sampling in quantification",
    "section": "Why expectation-maximization?",
    "text": "Why expectation-maximization?\nHowever, learning well-calibrated generative models \\(D_y\\) may be very hard task. Saerens, Latinne, and Decaestecker (2001) instead propose to learn a well-calibrated probabilistic classifier \\(P(Y \\mid X, \\pi^{(0)})\\) on an auxiliary population.\nThe assumption on the auxiliary population is the following: the conditional probability distributions \\(D_y = P(X\\mid Y=y)\\) have to be the same. The only thing that can differ is the proportion vector \\(\\pi_0\\), assumed to be known. This assumption is called prior probability shift or label shift and is rather strong, but also quite hard to avoid: if arbitrary distribution shifts are avoided, it’s not possible to generalize from one distribution to another! Finding suitable ways how to weaken the prior probability shift is therefore an interesting research problem on its own.\nNote that if we have a well-calibrated classifier \\(P(Y\\mid X, \\pi^{(0)})\\), we also have an access to a distribution \\(P(Y\\mid X, \\pi)\\). Namely, note that \\[\\begin{align*}\nP(Y=y\\mid X=x, \\pi) &\\propto P(Y=y, X=x \\mid \\pi) \\\\\n&= P(X=x \\mid Y=y, \\pi) P(Y=y\\mid \\pi) \\\\\n&= P(X=x \\mid Y=y)\\, \\pi_y,\n\\end{align*}\n\\] where the proportionality constant does not depend on \\(y\\). Analogously, \\[\nP(Y=y\\mid X=x, \\pi^{(0)}) \\propto P(X=x\\mid Y=y)\\, \\pi^{(0)}_y,\n\\] where the key observation is that for both distributions we assume that the conditional distribution \\(P(X=x\\mid Y=y)\\) is the same. Now we can take the ratio of both expressions and obtain \\[\nP(Y=y\\mid X=x, \\pi) \\propto P(Y=y\\mid X=x, \\pi^{(0)}) \\frac{ \\pi_y }{\\pi^{(0)}_y},\n\\] where the proportionality does not depend on \\(y\\). Hence, we can calculate unnormalized probabilities in this manner and then normalize them, so that they sum up to \\(1\\).\nTo summarize, we have the access to:\n\nWell-calibrated probability \\(P(Y=y\\mid X=x, \\pi)\\);\nThe prior probability \\(P(\\pi)\\);\nThe probability \\(P(Y_i=y \\mid \\pi) = \\pi_y\\);\n\nand we want to do inference on the posterior \\(P(\\pi \\mid \\{X_i\\})\\)."
  },
  {
    "objectID": "posts/em-gibbs-quantification.html#expectation-maximization",
    "href": "posts/em-gibbs-quantification.html#expectation-maximization",
    "title": "Expectation-maximization and Gibbs sampling in quantification",
    "section": "Expectation-maximization",
    "text": "Expectation-maximization\nExpectation-maximization is an iterative algorithm trying to find a stationary point of the log-posterior \\[\\begin{align*}\n\\log P(\\pi \\mid \\{X_i=x_i\\}) &= P(\\pi) + \\log P(\\{X_i = x_i\\} \\mid \\pi) \\\\\n&= P(\\pi) + \\sum_{i=1}^N \\log P(X_i=x_i\\mid \\pi).\n\\end{align*}\n\\] In particular, by running the optimization procedure several times, we can hope to find the maximum a posteriori estimate (or the maximum likelihood estimate, when the uniform distribution over the simplex is used as \\(P(\\pi)\\)). Interestingly, this optimization procedure will not assume that we can compute \\(\\log P(X_i=x_i\\mid \\pi)\\), using instead quantities available to us.\nAssume that at the current iteration the proportion vector is \\(\\pi^{(t)}\\). Then, \\[\\begin{align*}\n\\log P(X_i = x_i\\mid \\pi) &= \\log \\sum_{y=1}^L P(X_i = x_i, Y_i = y\\mid \\pi) \\\\\n&= \\log \\sum_{y=1}^L P(Y_i=y \\mid \\pi^{(t)}, X_i = x_i ) \\frac{ P(X_i=x_i, Y_i=y \\mid \\pi) }{P(Y_i=y \\mid \\pi^{(t)}, X_i=x_i)} \\\\\n&\\ge \\sum_{y=1}^L P(Y_i=y\\mid \\pi^{(t)}, X_i=x_i) \\log \\frac{P(X_i=x_i, Y_i=y \\mid \\pi)}{P(Y_i=y \\mid \\pi^{(t)}, X_i=x_i)}\n\\end{align*}\n\\]\nwhere the inequality follows from Jensen’s inequality for concave functions2.\nWe can now bound the loglikelihood by \\[\\begin{align*}\n\\log P(\\{X_i = x_i \\}\\mid \\pi) &= \\sum_{i=1}^N \\log P(X_i=x_i\\mid \\pi) \\\\\n&\\ge \\sum_{i=1}^N \\sum_{y=1}^L P(Y_i=y\\mid \\pi^{(t)}, X_i=x_i) \\log \\frac{P(X_i=x_i, Y_i=y \\mid \\pi)}{P(Y_i=y \\mid \\pi^{(t)}, X_i=x_i)}.\n\\end{align*}\n\\]\nNow let \\[\nQ(\\pi, \\pi^{(t)}) = \\log P(\\pi) + \\sum_{i=1}^N \\sum_{y=1}^L P(Y_i=y\\mid \\pi^{(t)}, X_i=x_i) \\log \\frac{P(X_i=x_i, Y_i=y \\mid \\pi)}{P(Y_i=y \\mid \\pi^{(t)}, X_i=x_i)},\n\\] which is a lower bound on the log-posterior. We will define the value \\(\\pi^{(t+1)}\\) by optimizing this lower bound: \\[\n\\pi^{(t+1)} := \\mathrm{argmax}_\\pi Q(\\pi, \\pi^{(t)}).\n\\]\nLet’s define auxiliary quantities \\(\\xi_{iy} = P(Y_i=y \\mid \\pi^{(t)}, X_i=x_i)\\), which can be calculated using the probabilistic classifier, as outlined above. This is called the expectation step (although we are actually calculating just probabilities, rather than more general expectations). In the new notation we have \\[\nQ(\\pi, \\pi^{(t)}) = \\log P(\\pi) + \\sum_{i=1}^N\\sum_{y=1}^L \\left(\\xi_{iy} \\log P(X_i=x_i, Y_i=y\\mid \\pi) - \\xi_{iy} \\log \\xi_{iy}\\right)\n\\]\nThe term \\(\\xi_{iy}\\log \\xi_{iy}\\) does not depend on \\(\\pi\\), so we don’t have to include it in the optimization. Writing \\(\\log P(X_i = x_i, Y_i=y\\mid \\pi) = \\log D_y(x_i) + \\log \\pi_y\\) we see that it suffices to optimize for \\(\\pi\\) the expression \\[\n\\log P(\\pi) + \\sum_{i=1}^N\\sum_{y=1}^L \\xi_{iy}\\left( \\log \\pi_y + \\log D_y(x_i) \\right).\n\\] Even better: not only \\(\\xi_{iy}\\) does not depend on \\(\\pi\\), but also \\(\\log D_y(x_i)\\)! Hence, we can drop from the optimization the terms requiring the generative models and we are left only with the easy to calculate quantities: \\[\n\\log P(\\pi) + \\sum_{i=1}^N\\sum_{y=1}^L \\xi_{iy} \\log \\pi_y.\n\\]\nLet’s use the prior \\(P(\\pi) = \\mathrm{Dirichlet}(\\pi \\mid \\alpha_1, \\dotsc, \\alpha_L)\\), so that \\(\\log P(\\pi) = \\mathrm{const.} + \\sum_{y=1}^L (\\alpha_y-1)\\log \\pi_y\\). Hence, we are interested in optimising \\[\n\\sum_{y=1}^L \\left((\\alpha_y-1) + \\sum_{i=1}^N \\xi_{iy} \\right)\\log \\pi_y.\n\\]\nWrite \\(A_y = \\alpha_y - 1 + \\sum_{i=1}^N\\xi_{iy}\\). We have to optimize the expression \\[\n\\sum_{y=1}^L A_y\\log \\pi_y\n\\] under a constraint \\(\\pi_1 + \\cdots + \\pi_L = 1\\).\nSaerens, Latinne, and Decaestecker (2001) use Lagrange multipliers, but we will use the first \\(L-1\\) coordinates to parameterise the simplex and write \\(\\pi_L = 1 - (\\pi_1 + \\cdots + \\pi_{L-1})\\). In this case, if we differentiate with respect to \\(\\pi_l\\), we obtain \\[\n\\frac{A_l}{\\pi_l} + \\frac{A_L}{\\pi_L} \\cdot (-1) = 0,\n\\]\nwhich in turn gives that \\(\\pi_y = k A_y\\) for some constant \\(k &gt; 0\\). We have \\[\n\\sum_{y=1}^L A_y = \\sum_{y=1}^L \\alpha_y - L + \\sum_{i=1}^N\\sum_{y=1}^L \\xi_{iy} = \\sum_{y=1}^L \\alpha_y - L + N.\n\\] Hence, \\[\n\\pi_y = \\frac{1}{(\\alpha_1 + \\cdots + \\alpha_L) + N - L}\\left( \\alpha_y-1 + \\sum_{i=1}^N \\xi_{iy} \\right),\n\\] which is taken as the next \\(\\pi^{(t+1)}\\).\nAs a minor observation, note that for a uniform prior over the simplex (i.e., all \\(\\alpha_y = 1\\)) we have \\[\n\\pi^{(t+1)}_y = \\frac 1N\\sum_{i=1}^N P(Y_i=y_i \\mid X_i=x_i, \\pi^{(t)} ).\n\\] Once we have converged to a fixed point and we have \\(\\pi^{(t)} = \\pi^{(t+1)}\\), it very much looks like \\[\nP(Y) = \\frac 1N\\sum_{i=1}^N P(Y_i \\mid X_i, \\pi) \\approx \\mathbb E_{X \\sim \\pi_1 D_1 + \\dotsc + \\pi_L D_L}[ P(Y\\mid X) ]\n\\] when \\(N\\) is large."
  },
  {
    "objectID": "posts/em-gibbs-quantification.html#gibbs-sampler",
    "href": "posts/em-gibbs-quantification.html#gibbs-sampler",
    "title": "Expectation-maximization and Gibbs sampling in quantification",
    "section": "Gibbs sampler",
    "text": "Gibbs sampler\nFinally, let’s think how to implement a Gibbs sampler for this problem. Compared to the expectation-maximization this will be easy.\nTo solve the quantification problem we have to sample from the posterior distribution \\(P(\\pi \\mid \\{X_i\\})\\). Instead, let’s sample from a high-dimensional distribution \\(P(\\pi, \\{Y_i\\} \\mid \\{X_i\\})\\) — once we have samples of the form \\((\\pi, \\{Y_i\\})\\) we can simply forget about the \\(Y_i\\) values.\nThis is computationally a harder problem (we have many more variables to sample), however each sampling step will be very convenient. We will alternatively sample from \\[\n\\pi \\sim P(\\pi \\mid \\{X_i, Y_i\\})\n\\] and \\[\n\\{Y_i\\} \\sim P(\\{Y_i\\} \\mid \\{X_i\\}, \\pi).\n\\]\nThe first step is easy: \\(P(\\pi \\mid \\{X_i, Y_i\\}) = P(\\pi\\mid \\{Y_i\\})\\) which (assuming a Dirichlet prior) is a Dirichlet distribution. Namely, if \\(P(\\pi) = \\mathrm{Dirichlet}(\\alpha_1, \\dotsc, \\alpha_L)\\), then \\[\nP(\\pi\\mid \\{Y_i=y_i\\}) = \\mathrm{Dirichlet}\\left( \\alpha_1 + \\sum_{i=1}^N \\mathbf{1}[y_i = 1], \\dotsc, \\alpha_L + \\sum_{i=1}^N \\mathbf{1}[y_i=L] \\right).\n\\]\nLet’s think how to sample \\(\\{Y_i\\} \\sim P(\\{Y_i\\} \\mid \\{X_i\\}, \\pi)\\). This is a high-dimensional distribution, so let’s… use Gibbs sampling. Namely, we can iteratively sample \\[\nY_k \\sim P(Y_k \\mid \\{Y_1, \\dotsc, Y_{k-1}, Y_{k+1}, \\dotsc, Y_L\\}, \\{X_i\\}, \\pi).\n\\]\nThanks to the particular structure of this model, this is equivalent to sampling from \\[\nY_k \\sim P(Y_k \\mid X_k, \\pi) = \\mathrm{Categorical}(\\xi_{k1}, \\dotsc, \\xi_{kL}),\n\\] where \\(\\xi_{ky} = P(Y_k = y\\mid X_k = x_k, \\pi)\\) is obtained by recalibrating the given classifier."
  },
  {
    "objectID": "posts/em-gibbs-quantification.html#summary",
    "href": "posts/em-gibbs-quantification.html#summary",
    "title": "Expectation-maximization and Gibbs sampling in quantification",
    "section": "Summary",
    "text": "Summary\nTo sum up, the reviewer was right: it’s very simple to upgrade the inference scheme in this model from a point estimate to a sample from the posterior!\nI however haven’t run simulations to know how well this sampler works in practice: I expect that this approach could suffer from:\n\nProblems from not-so-well-calibrated probabilistic classifier.\nEach iteration of the algorithm (whether expectation-maximization or a Gibbs sampler) requires passing through all \\(N\\) examples.\nAs there are \\(N\\) latent variables sampled, the convergence may perhaps be slow.\n\nIt’d be interesting to see how problematic these points are in practice (perhaps not at all!)"
  },
  {
    "objectID": "posts/em-gibbs-quantification.html#appendix-numerical-implementation-in-jax",
    "href": "posts/em-gibbs-quantification.html#appendix-numerical-implementation-in-jax",
    "title": "Expectation-maximization and Gibbs sampling in quantification",
    "section": "Appendix: numerical implementation in JAX",
    "text": "Appendix: numerical implementation in JAX\nAs these algorithms are so simple, let’s quickly implement them in JAX. We will consider two Gaussian densities \\(D_1 = \\mathcal N(0, 1^2)\\) and \\(D_2 = \\mathcal N(\\mu, 1^2)\\). Let’s generate some data:\n\n\nCode\nimport jax\nimport jax.numpy as jnp\nfrom jax import random\nfrom jaxtyping import Array, Float, Int\nfrom jax.scipy.special import logsumexp\n\nn_cases: Int[Array, \" classes\"] = jnp.asarray([10, 40], dtype=int)\nmus: Float[Array, \" classes\"] = jnp.asarray([0.0, 1.0])\n\nkey = random.PRNGKey(42)\nkey, *subkeys = random.split(key, len(n_cases) + 1)\n\nxs: Float[Array, \" points\"] = jnp.concatenate(tuple(\n  mu + random.normal(subkey, shape=(n,))\n  for subkey, n, mu in zip(subkeys, n_cases, mus)\n))\n\nn_classes: int = len(n_cases)\nn_points: int = len(xs)\n\n\nAn NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n\n\nNow we need a probabilistic classifier. We will assume that it was calibrated on population with proportion \\(\\pi^{(0)} = (0.4, 0.6)\\).\n\n\nCode\n_normalizer: float = 0.5 * jnp.log(2 * jnp.pi)\n\ndef log_p(x, mu: float) -&gt; float:\n  \"\"\"Log-density N(x | mu, 1^2).\"\"\"\n  return -0.5 * jnp.square(x - mu) - _normalizer\n\n\n# Auxiliary matrix log P(X | Y)\n_log_p_x_y: Float[Array, \"points classes\"] = jnp.stack(tuple(log_p(xs, mu) for mu in mus)).T\nassert _log_p_x_y.shape == (n_points, n_classes), f\"Shape mismatch: {_log_p_x_y.shape}.\"\n\nlog_pi0: Float[Array, \" classes\"] = jnp.log(jnp.asarray([0.4, 0.6]))\n\n# Matrix representing log P(Y | X) for labeled population\nlog_p_y_x: Float[Array, \"points classes\"] = _log_p_x_y + log_pi0[None, :]\n# ... currently it's unnormalized, so we have to normalize it\n\ndef normalize_logprobs(log_ps: Float[Array, \"points classes\"]) -&gt; Float[Array, \"points classes\"]:\n  log_const = logsumexp(log_ps, keepdims=True, axis=-1)\n  return log_ps - log_const\n\nlog_p_y_x = normalize_logprobs(log_p_y_x)\n\n# Let's quickly check if it works\nsums = jnp.sum(jnp.exp(log_p_y_x), axis=1)\nassert sums.shape == (n_points,)\nassert jnp.min(sums) &gt; 0.999, f\"Minimum: {jnp.min(sums)}.\"\nassert jnp.max(sums) &lt; 1.001, f\"Maximum: {jnp.max(sums)}.\"\n\n\n\nExpectation-maximization algorithm\nIt’s time to implement expectation-maximization.\n\n\nCode\ndef expectation_maximization(\n  log_p_y_x: Float[Array, \"points classes\"],\n  log_pi0: Float[Array, \" classes\"],\n  log_start: None | Float[Array, \" classes\"] = None,\n  alpha: Float[Array, \" classes\"] | None = None,\n  n_iterations: int = 10_000,\n) -&gt; Float[Array, \" classes\"]:\n  \"\"\"Runs the expectation-maximization algorithm.\n\n  Args:\n    log_p_y_x: array log P(Y | X) for the calibrated population\n    log_pi0: array log P(Y) for the calibrated population\n    log_start: starting point. If not provided, `log_pi0` will be used\n    alpha: concentration parameters for the Dirichlet prior.\n      If not provided, the uniform prior will be used\n    n_iterations: number of iterations to run the algorithm for\n  \"\"\"\n  if log_start is None:\n    log_start = log_pi0\n  if alpha is None:\n    alpha = jnp.ones_like(log_pi0)\n\n  def iteration(_, log_pi: Float[Array, \" classes\"]) -&gt; Float[Array, \" classes\"]:\n    # Calculate log xi[n, y]\n    log_ps = normalize_logprobs(log_p_y_x + log_pi[None, :] - log_pi0[None, :])\n    # Sum xi[n, y] over n. We use the logsumexp, as we have log xi[n, y]\n    summed = jnp.exp(logsumexp(log_ps, axis=0, keepdims=False))\n    # The term inside the bracket (numerator)\n    numerator = summed + alpha - 1.0\n    # Denominator\n    denominator = jnp.sum(alpha) + log_p_y_x.shape[0] - log_p_y_x.shape[1]\n    return jnp.log(numerator / denominator)\n\n  return jax.lax.fori_loop(\n    0, n_iterations, iteration, log_start\n  )\n\nlog_estimated = expectation_maximization(\n  log_p_y_x=log_p_y_x,\n  log_pi0=log_pi0,\n  n_iterations=1000,\n  # Let's use slight shrinkage towards more uniform solutions\n  alpha=2.0 * jnp.ones_like(log_pi0),\n)\nestimated = jnp.exp(log_estimated)\nprint(f\"Estimated: {estimated}\")\nprint(f\"Actual:    {n_cases / n_cases.sum()}\")\n\n\nEstimated: [0.16425547 0.83574456]\nActual:    [0.2 0.8]\n\n\n\n\nGibbs sampler\nExpectation-maximization returns only a point estimate. We’ll explore the region around the posterior mode with a Gibbs sampler.\n\n\nCode\ndef gibbs_sampler(\n  key: random.PRNGKeyArray,\n  log_p_y_x: Float[Array, \"points classes\"],\n  log_pi0: Float[Array, \" classes\"],\n  log_start: None | Float[Array, \" classes\"] = None,\n  alpha: Float[Array, \" classes\"] | None = None,\n  n_warmup: int = 1_000,\n  n_samples: int = 1_000,\n) -&gt; Float[Array, \"n_samples classes\"]:\n  if log_start is None:\n    log_start = log_pi0\n  if alpha is None:\n    alpha = jnp.ones_like(log_pi0)\n\n  def iteration(\n    log_ps: Float[Array, \" classes\"],\n    key: random.PRNGKeyArray,\n  ) -&gt; tuple[Float[Array, \" classes\"], Float[Array, \" classes\"]]:\n    key, subkey1, subkey2 = random.split(key, 3)\n\n    ys = random.categorical(\n      subkey1,\n      log_ps[None, :] + log_p_y_x - log_pi0[None, :],\n      axis=-1,\n    )\n    counts = jnp.bincount(ys, length=log_pi0.shape[0])\n\n    new_log_pi = jnp.log(\n      random.dirichlet(subkey2, alpha + counts)\n    )\n\n    return new_log_pi, new_log_pi\n\n  _, samples = jax.lax.scan(\n    iteration,\n    log_start,\n    random.split(key, n_warmup + n_samples),\n  )\n  return samples[n_warmup:, :]\n\nkey, subkey = random.split(key)\nsamples = gibbs_sampler(\n  key=subkey,\n  log_p_y_x=log_p_y_x,\n  log_pi0=log_pi0,\n  # Let's use slight shrinkage towards more uniform solutions\n  alpha=2.0 * jnp.ones_like(log_pi0),\n  # Use EM point as a starting point\n  log_start=log_estimated,\n  n_samples=5_000,\n)\nsamples = jnp.exp(samples)\n\nprint(f\"Mean:   {jnp.mean(samples, axis=0)}\")\nprint(f\"Std:    {jnp.std(samples, axis=0)}\")\nprint(f\"Actual: {n_cases / n_cases.sum()}\")\n\n\n/tmp/ipykernel_80089/3942328187.py:2: DeprecationWarning: jax.random.PRNGKeyArray is deprecated. Use jax.Array for annotations, and jax.dtypes.issubdtype(arr.dtype, jax.dtypes.prng_key) for runtime detection of typed prng keys.\n  key: random.PRNGKeyArray,\n/tmp/ipykernel_80089/3942328187.py:17: DeprecationWarning: jax.random.PRNGKeyArray is deprecated. Use jax.Array for annotations, and jax.dtypes.issubdtype(arr.dtype, jax.dtypes.prng_key) for runtime detection of typed prng keys.\n  key: random.PRNGKeyArray,\n\n\nMean:   [0.20171012 0.79828984]\nStd:    [0.09912279 0.09912279]\nActual: [0.2 0.8]\n\n\nLet’s visualise the posterior samples, together with the expectation-maximization solution and the ground truth:\n\n\nCode\nimport matplotlib.pyplot as plt\nplt.style.use(\"dark_background\")\n\nfig, ax = plt.subplots(dpi=150)\n\nbins = jnp.linspace(0, 1, 40)\n\nfor y in range(n_classes):\n  color = f\"C{y+1}\"\n  ax.hist(samples[:, y], bins=bins, density=True, histtype=\"step\", color=color)\n  ax.axvline(n_cases[y] / n_cases.sum(), color=color, linewidth=3)\n  ax.axvline(estimated[y], color=color, linestyle=\"--\")\n\nax.set_title(\"Posterior distribution\")\nax.set_ylabel(\"Posterior density\")\nax.set_xlabel(\"Component value\")\nax.spines[[\"top\", \"right\"]].set_visible(False)\nfig.tight_layout()"
  },
  {
    "objectID": "posts/em-gibbs-quantification.html#footnotes",
    "href": "posts/em-gibbs-quantification.html#footnotes",
    "title": "Expectation-maximization and Gibbs sampling in quantification",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nLet’s call this problem “quantification of uncertainty in quantification problems”.↩︎\nIt’s good to remember: \\(\\log \\mathbb E[A] \\ge \\mathbb E[\\log A]\\).↩︎"
  },
  {
    "objectID": "posts/board-games-monte-carlo.html",
    "href": "posts/board-games-monte-carlo.html",
    "title": "Understanding board games with Monte Carlo simulations",
    "section": "",
    "text": "I like playing board games, but I never remember the probabilities of different interesting events. Let’s code a very simple Monte Carlo simulation to evaluate probabilities used in them, so I can revisit to this website and use it to (maybe eventually) win."
  },
  {
    "objectID": "posts/board-games-monte-carlo.html#fight-or-flight",
    "href": "posts/board-games-monte-carlo.html#fight-or-flight",
    "title": "Understanding board games with Monte Carlo simulations",
    "section": "Fight or flight?",
    "text": "Fight or flight?\nIn the rare days when I find time to play Runebound, I find myself in situations fighting monsters and trying to decide whether I should try to fight them or escape. I know a monster’s strength (high), I know my strength (low), but I don’t know how likely it is that the difference can be compensated by throwing two ten-sided dice.\nLet’s estimate the chances of getting at least \\(X\\) points due to the dice throw.\n\n\nCode\nimport numpy as np\n\nn_simulations: int = 100_000\ndice: int = 10\n\nrng = np.random.default_rng(42)\noccurrences = np.zeros(2 * dice + 1, dtype=float)\n\nthrows = rng.integers(1, dice, endpoint=True, size=(n_simulations, 2))\ntotal = throws.sum(axis=1)\n\nfor t in total:\n    occurrences[:t+1] += 1\n\noccurrences /= n_simulations\n\nfor i, p in enumerate(occurrences):\n    if i &lt; 1:\n        continue\n    print(f\"{i}: {100*p:.1f}%\")\n\n\n1: 100.0%\n2: 100.0%\n3: 99.0%\n4: 97.0%\n5: 94.0%\n6: 90.1%\n7: 85.2%\n8: 79.2%\n9: 72.2%\n10: 64.1%\n11: 55.1%\n12: 45.2%\n13: 36.0%\n14: 28.0%\n15: 21.1%\n16: 15.1%\n17: 10.0%\n18: 6.0%\n19: 3.0%\n20: 1.0%\n\n\nIn this case it’s also very easy to actually calculate the probabilities without Monte Carlo simulation:\n\n\nCode\nprobabilities = np.zeros(2*dice + 1, dtype=float)\n\nfor result1 in range(1, dice + 1):\n    for result2 in range(1, dice + 1):\n        total = result1 + result2\n        probabilities[:total + 1] += 1/dice**2\n\nfor i, p in enumerate(occurrences):\n    if i &lt; 1:\n        continue\n    print(f\"{i}: {100*p:.1f}%\")\n\n\n1: 100.0%\n2: 100.0%\n3: 99.0%\n4: 97.0%\n5: 94.0%\n6: 90.1%\n7: 85.2%\n8: 79.2%\n9: 72.2%\n10: 64.1%\n11: 55.1%\n12: 45.2%\n13: 36.0%\n14: 28.0%\n15: 21.1%\n16: 15.1%\n17: 10.0%\n18: 6.0%\n19: 3.0%\n20: 1.0%\n\n\nThe exact solution requires \\(O(K^2)\\) operations, where one uses two dice with \\(K\\) sides1. For a larger number of dice this solution may not be as tractable, so Monte Carlo approximations may shine."
  },
  {
    "objectID": "posts/board-games-monte-carlo.html#where-should-my-cheese-be",
    "href": "posts/board-games-monte-carlo.html#where-should-my-cheese-be",
    "title": "Understanding board games with Monte Carlo simulations",
    "section": "Where should my cheese be?",
    "text": "Where should my cheese be?\nIn Cashflow one way to win the end-game is to quickly get to the tile with a cheese-shaped token. As this token can be placed in advance, I was wondering what the optimal location of it should be.\nIf I put the token on the first tile, I need to throw exactly one in my first throw or I will need to travel across the whole board to close the loop and have another chance (or try to win the game in another way).\nLet’s use Monte Carlo simulation to estimate where I should put the token so I can win in at most five moves:\n\n\nCode\nimport numpy as np \n\nN_SIMULATIONS: int = 100_000\nN_THROWS: int = 5\nDICE: int = 6  # Number of sides on the dice\nrng = np.random.default_rng(101)\n\nvisitations = np.zeros(N_THROWS * DICE + 1)\n\nfor simulation in range(N_SIMULATIONS):\n    position = 0\n    for throw_index in range(N_THROWS):\n        result = rng.integers(1, DICE, endpoint=True)\n        position += result\n        visitations[position] += 1\n\nfor i in range(N_THROWS * DICE + 1):\n    percentage = 100 * visitations[i] / N_SIMULATIONS\n    print(f\"{i}: {percentage:.1f}\")\n\n\n0: 0.0\n1: 16.5\n2: 19.3\n3: 22.8\n4: 26.4\n5: 30.8\n6: 36.2\n7: 25.2\n8: 26.8\n9: 28.1\n10: 28.6\n11: 28.4\n12: 27.9\n13: 25.8\n14: 25.1\n15: 24.0\n16: 21.8\n17: 19.6\n18: 16.5\n19: 13.9\n20: 11.2\n21: 8.5\n22: 6.2\n23: 4.3\n24: 2.7\n25: 1.6\n26: 0.9\n27: 0.5\n28: 0.2\n29: 0.1\n30: 0.0\n\n\nAgain, we could do this in the exact fashion — for example, for 30 we know that the probability is exactly \\(6^{-5}\\approx 0.013\\%\\), but it’s quite clear that the sixth tile gives decent chances of winning in the first few moves."
  },
  {
    "objectID": "posts/board-games-monte-carlo.html#footnotes",
    "href": "posts/board-games-monte-carlo.html#footnotes",
    "title": "Understanding board games with Monte Carlo simulations",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe implemented solution works in \\(O(K^3)\\) due to the probabilities[:total + 1] operation. If the performance did really matter here, we could store the occurrences and then calculate cumulative sums only once in the end.↩︎"
  },
  {
    "objectID": "posts/expectations-student-mentorship.html",
    "href": "posts/expectations-student-mentorship.html",
    "title": "Student mentorship: expectations document",
    "section": "",
    "text": "Welcome! This document is supposed to explain my general mentoring style and act as a skeleton around we can build the collaboration and mentorship rules.\nPlease, note:"
  },
  {
    "objectID": "posts/expectations-student-mentorship.html#mission-statement",
    "href": "posts/expectations-student-mentorship.html#mission-statement",
    "title": "Student mentorship: expectations document",
    "section": "Mission statement",
    "text": "Mission statement\nWhen I advise on a project I try to keep the following in mind:\n\nI want you to learn and become a better researcher and engineer at the end of the project.\nIt’s more important that we understand each other and are happy with the mentorship, rather than we get an additional feature."
  },
  {
    "objectID": "posts/expectations-student-mentorship.html#expectations",
    "href": "posts/expectations-student-mentorship.html#expectations",
    "title": "Student mentorship: expectations document",
    "section": "Expectations",
    "text": "Expectations\n\nWhat you can expect from me\n\nI’ll find regular time to meet with you and advise on the steps which may be worth taking. While I will be more “hands-on” and have more precise ideas when you start, I want you to become an independent thinker with a good knowledge on the topic – it’s also likely that you’ll know more about the topic than me by the end of your project!\nI’ll advise you on your code and writing, to make sure that your skills improve.\n\nI’ll keep an open mindset to your comments and suggestions. If you encounter any issues, let me know and we’ll work together on resolving them.\n\n\n\nWhat I’d like to expect from you\n\nHonesty. If something doesn’t work for you (e.g., the expectations and the workload are too high), I said something ridiculously wrong, or the experiments fail, let’s discuss. I’m still learning both how to be a good mentor and a good scientist.\nConforming to use good research and coding practices. We will work on open-source projects and I expect you to write good code (with documentation and tests) and run reproducible experiments. Developing these skills takes time and we will work together to make sure that your research and programming skills are improving.\nTaking the ownership of conforming to the university rules. You should remind me when your thesis is due three months before submitting it, so we can discuss the outline, and send me the first draft three weeks before the deadline, so I can review it.\n\n\n\nConflict resolution\nIn case of a conflict with an academic or a student, please contact me and we will work together to resolve the conflict. If you feel that you do not want me to be involved (e.g., the conflict is between you and me), I encourage you to contact my mentor, Professor Niko Beerenwinkel or ETH’s Ombudspersons.\n\n\nMisc\nI’m not an established researcher in the field (and I don’t have a PhD!). Apart from the fact that I may be wrong in different aspects (happy to learn!), the reference letters written by me are unlikely to be accepted e.g., if you apply for a PhD. If you need a reference letter at the end of the project, I’d suggest to ask Professor Niko Beerenwinkel (and CC me) whether he could provide one."
  },
  {
    "objectID": "posts/expectations-student-mentorship.html#general-research-advice",
    "href": "posts/expectations-student-mentorship.html#general-research-advice",
    "title": "Student mentorship: expectations document",
    "section": "General research advice",
    "text": "General research advice\nAlthough I will supply you with an initial reading list tailored to your project, I’d like to share below some general advice on research, knowledge work, and learning. (Remember – if you see that some of these do not work you, feel free to replace them with better practices. I’d also be grateful if you could share them with me, so I can update this document).\n\nResearch notes and journal\nI’d strongly encourage you to book some time at the start and the end of every working day to work on your research notes and write your observations in a journal.\nThis serves multple purposes: - I believe it will help you to improve your understanding of the domain. - At some point you will need to write your thesis. You will see that it is much easier to edit a series of connected research notes into a first draft, rather than starting with an empty page. - By practicing this over the duration of the project, you will end up with a skill which is useful regardless whether you decide to move into industry or stay in academia.\nTo start writing research notes, read an Andy Matuschak’s note or watch Martin Adams’ video. Popular software includes Obsidian and Zettlr (and you can use them for the journal as well).\nFor your research journal, you may find this blog post useful. Journal can also be helpful to track your feelings and attitude towards the project, so we can adjust the workload or troubleshoot the process – see this post.\n\n\nReading scientific literature\n\nI’d suggest to read this Andy Matuschak’s note and this short P.N. Edwards’ article.\nThis is also a skill which takes time to master, so I’d suggest to practice it regularly and go back and refresh the principles of effective reading.\nYou will see that there is always too much literature to read than the time permits, so it’s critical to think what you want to learn from a paper.\n\nAre there specific questions I want to have the answer to by reading this paper? (It’s always good to read papers with several questions in mind.)\nIs this some maths or statistics which is crucial to deeply understand for the project? If so, several hours may be required and there is nothing to be done.\nIs this a paper which main conclusion can be quickly understood just by looking at one figure and the abstract or conclusions?\nIs this a paper which is potentially useful if problem X arises? If so, it’s probably good to say in a research problem on topic X that this paper may be useful to deeply understand it then.\n\nI like to use Connected Papers to find papers related to the paper of interest. Another strategy is to use Google Scholar to find papers which cited the paper of interest or see what the superstars are doing.\nSpeaking of superstars, Twitter has become a place where new research results are often announced and have short “tl;dr” threads. I would suggest to create a recurrent task (e.g., half an hour every two weeks) and check what the superstars have been doing. (Note that the temptation to procrastinate can be huge. This is why I recommend to set only a specific time to check it.)\n\n\n\nFinding a sustainable working style\n\nAs Bastian Rieck advises, it’s crucial that you find a sustainable workflow. Working on a project over a few months is a long time and “it’s rather a marathon than a sprint”.\nI’m interested in seeing that your expertise grows and that work you produce is of good quality, rather than in counting the hours you put into the work:\n\nIf you think that my expectations are unrealistic, just talk to me – we don’t need to rush and the scope of the thesis can always be adjusted to be more realistic.\nMake sure that you prioritize your mental health and well-being.\nPlease, please, please, no work on weekends and holidays.\n\nYou will see that different people have different working styles. This is fine – they are also working on different projects, have different backgrounds, and have different goals. Don’t compare yourself with them and embrace your way of working as well as theirs.\n\n\n\nProgramming\n\nWe will use Git version control and GitHub. Please, make sure that you have an account and send me your username, so I can add you to the project.\nIf you had not worked with Git before, I recommend (a) Creating a “sandbox” repository and playing with different commands. R. Dudler’s “The simple guide” is a nice way to get started.\nLearning good software practices is like learning a new language – working with a dictionary won’t make one proficient in one day, but using it regularly can help to avoid common errors. I recommend Google Style Guide and (to know what should be avoided) Python anti-patterns.\n\n\n\nMisc\n\nIf you want to become a researcher, R. Hamming’s “You and your research” talk is a classic.\nPatrick Kidger and Andrej Karpathy also wrote on this topic."
  },
  {
    "objectID": "posts/expectations-student-mentorship.html#references",
    "href": "posts/expectations-student-mentorship.html#references",
    "title": "Student mentorship: expectations document",
    "section": "References",
    "text": "References\nI used the following resources to draft the document above. However, all the mistakes (scientific, mentoring, grammar) are to blame on myself.\n\nK.S. Masters and P.K. Kreeger’s “Ten Simple Rules” article\nYinghzhen Li’s blogpost\nThe document issued by Niko Beerenwinkel to his PhD students."
  },
  {
    "objectID": "posts/kernel-regression-transformer.html",
    "href": "posts/kernel-regression-transformer.html",
    "title": "From kernel regression to the transformer",
    "section": "",
    "text": "I remember that when we read Attention is all you need at a journal club back in 2020, I did not really understand what attention was1.\nFortunately for me, Transformer dissection paper and Cosma Shalizi’s post on the topic appeared, which show the connection between attention and kernel regression. This point of view was exactly what I needed! I like this so much that when I explain attention to other people, I always start from kernel regression."
  },
  {
    "objectID": "posts/kernel-regression-transformer.html#kernel-regression",
    "href": "posts/kernel-regression-transformer.html#kernel-regression",
    "title": "From kernel regression to the transformer",
    "section": "Kernel regression",
    "text": "Kernel regression\nLet’s start with kernel regression as independently proposed by Nadaraya and Watson sixty years ago. We will generate some data with heteroskedastic noise, \\(y = f(x) + n(x)\\epsilon\\) where \\(\\epsilon \\sim \\mathcal N(0, 1)\\), \\(f(x)\\) is the expected value \\(\\mathbb E[y\\mid x]\\) and function \\(n(x)\\) makes the noise heteroskedastic.\nWe’ll plot the observed data points as well as \\(f(x) + 2 n(x)\\) and \\(f(x) - 2n(x)\\) as is often done.\n\n\nCode\nfrom functools import partial\n\nimport jax\nimport jax.numpy as jnp\nfrom jax import random\n\nimport equinox as eqx\nfrom jaxtyping import Float, Array\n\nimport matplotlib.pyplot as plt\nplt.style.use(\"dark_background\")\n\nVector = Float[Array, \" n_points\"]\n\ndef f(X: Vector) -&gt; Vector:\n    return 0.5 * jnp.sin(X) - 1 * jnp.sin(3 * X) + 0.2 * jnp.square(X)\n\ndef n(X: Vector) -&gt; Vector:\n    return 0.2 + 0.05 * jnp.abs(X)\n\nn_points: int = 150\n\nkey = random.PRNGKey(2024)\nkey, subkey = random.split(key)\nX = jnp.linspace(-3, 3, n_points)\nY = f(X) + n(X) * random.normal(subkey, shape=X.shape)\n\nfig, ax = plt.subplots(figsize=(4, 3), dpi=150)\nX_ax = jnp.linspace(-3, 3, 201)\nax.fill_between(\n    X_ax, f(X_ax)- 2 * n(X_ax), f(X_ax) + 2 * n(X_ax), alpha=0.4, color=\"maroon\"\n)\nax.plot(X_ax, f(X_ax), color=\"maroon\", alpha=0.8)\nax.scatter(X, Y, color=\"white\", s=5, alpha=1.0)\nax.set_xlabel(\"$X$\")\nax.set_ylabel(\"$Y$\")\n\n\nAn NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n\n\nText(0, 0.5, '$Y$')\n\n\n\n\n\nWe will be interested in finding \\(f(x)\\) via the weighted average: \\[\n\\hat f(x) = \\sum_{i=1}^n y_i\\, w_i(x)\n\\]\nwhere \\(w_i(x)\\) is the weight of the \\(i\\)-th data point used to estimate the value at \\(x\\). To make it a weighted average, we will ensure that \\(w_1(x) + \\cdots + w_n(x) = 1\\). In case where \\(w_i(x) = 1/n\\) we obtain just constant prediction, equal to the sample average over \\(y_i\\).\nMore generally, consider a positive function \\(K\\colon \\mathcal X \\times \\mathcal X \\to \\mathbb R^+\\) which measures similarity between two data points: we want \\(K(x, x)\\) to attain the largest possible value and for \\(x'\\) very far from \\(x\\) we want to have \\(K(x, x')\\) to be small. For such a function we can form a set of weights via \\[\nw_i(x) = \\frac{K(x, x_i)}{\\sum_{j=1}^n K(x, x_j)}.\n\\]\nLet’s restrict our attention for now to Gaussian kernels, \\(K(x, x'; \\ell) = \\exp \\left(\\left( \\frac{x-x'}{\\ell} \\right)^2 \\right)\\) with lengthscale \\(\\ell\\) and visualise the predictions for different lengthscales. As kernels are parameterised functions, we will use Equinox:\n\n\nCode\nclass GaussianKernel(eqx.Module):\n    _log_lengthscale: float\n\n    def __init__(self, lengthscale: float) -&gt; None:\n        assert lengthscale &gt; 0, \"Lengthscale should be positive.\"\n        self._log_lengthscale = jnp.log(lengthscale)\n\n    @property\n    def lengthscale(self) -&gt; float:\n        return jnp.exp(self._log_lengthscale)\n\n    def __call__(self, x: float, x_: float) -&gt; float:\n        return jnp.exp(-jnp.square((x-x_) / self.lengthscale))\n\n\n    def predict(self, X_test: Float[Array, \" n_test\"], X_obs: Vector, Y_obs: Vector) -&gt; Float[Array, \" n_test\"]:\n        kernel = self\n        def predict_one(x: float) -&gt; float:\n            ks = jax.vmap(partial(kernel, x))(X_obs)\n            ws = ks / (jnp.sum(ks) + 1e-16)\n            return jnp.sum(Y_obs * ws)\n        return jax.vmap(predict_one)(X_test)    \n\n\nkernels = {lengthscale: GaussianKernel(lengthscale) for lengthscale in [3.0, 0.5, 0.25, 0.05]} \n\n\nfig, axs = plt.subplots(2, 2, figsize=(2*4, 2*3), dpi=150, sharex=True, sharey=True)\n\nfor (lengthscale, k), ax in zip(kernels.items(), axs.ravel()):\n    pred = k.predict(X_ax, X_obs=X, Y_obs=Y)\n    ax.set_title(f\"$\\\\ell = {lengthscale}$\")\n    ax.plot(X_ax, f(X_ax), color=\"maroon\", alpha=0.8)\n    ax.plot(X_ax, pred, color=\"orangered\", alpha=0.8)\n    ax.scatter(X, Y, color=\"white\", s=5, alpha=0.8)\n    ax.set_xlabel(\"$X$\")\n    ax.set_ylabel(\"$Y$\")\n\nfig.tight_layout()\n\n\n\n\n\nIt seems that \\(\\ell=3.0\\) results in underfitted, almost constant, predictions, and \\(\\ell=0.05\\) arguably overfits, resulting in predictions changing a bit too quickly. Generally, it seems that \\(\\ell \\approx 0.25\\) is a reasonable choice.\n\nMasked training\nLet’s now think how we could find \\(\\ell\\) algorithmically (and when the true mean curve is not available for comparison!).\nFor example, we could use something like the leave-one-out cross-validation:\n\nHold out a data point \\((x_i, y_i)\\);\nFit the kernel regression with lengthscale \\(\\ell\\) to the data \\((x_1, y_1), \\dotsc, (x_{i-1}, y_{i-1}), (x_{i+1}, y_{i+1}), \\dotsc, (x_n, y_n)\\);\nPredict \\(y_i\\) from \\(x_i\\) given the kernel regression.\n\nLooking at different values \\(\\ell\\) and varying the index \\(i\\) of the hold-out data point may be a reasonable training procedure. Note however that if we use standard squared loss, this will have a drawback that points which are further from the mean (due to heteroskedasticity) will be treated similarly to the data points where the noise is small. We could try to reweight them, but we won’t do that and implement a vanilla variant.\nIn fact, we will try several variants of this approach, allowing to hold out more data points than \\(1\\). In terms of probabilistic interpretation this is even worse: apart from problems with interpreting square loss due to heteroskedasticity, now we are also predicting values at several locations at once, effectively assuming that they are independent, given the observed data. In a way, this is similar to the BERT training. XLNet considers different permutations, being closer to an orderless autoregressive model. Anyway, BERT had impressive performance, so let’s try different variants here:\n\n\nCode\nimport optax\n\n\ndef train(\n    key,\n    model: eqx.Module,\n    X: Vector,\n    Y: Vector,\n    learning_rate: float = 0.2,\n    hold_out_size: int = 1,\n    n_steps: int = 100,\n    print_every: int = 100\n) -&gt; eqx.Module:\n    assert n_steps &gt; 1\n    if print_every is None:\n        print_every = n_steps + 100\n    assert print_every &gt; 0\n    assert learning_rate &gt; 0\n\n    assert X.shape[0] == Y.shape[0]\n    n_total = X.shape[0]\n\n    def split_data(key):\n        \"\"\"Splits the data into training and test.\"\"\"\n        indices = random.permutation(key, jnp.arange(n_total))\n        ind_test = indices[:hold_out_size]\n        ind_obs = indices[hold_out_size:]\n\n        return X[ind_obs], Y[ind_obs], X[ind_test], Y[ind_test]\n\n    @jax.jit\n    def step(\n        model: eqx.Module,\n        opt_state,\n        X_obs: Vector,\n        Y_obs: Vector,\n        X_test: Float[Array, \" n_test\"],\n        Y_test: Float[Array, \" n_test\"],\n    ):\n        def loss_fn(model):\n            preds = model.predict(\n                X_test=X_test,\n                X_obs=X_obs,\n                Y_obs=Y_obs,\n            )\n            return jnp.mean(jnp.square(preds - Y_test))\n\n        loss, grads = jax.value_and_grad(loss_fn)(model)\n        updates, opt_state = optimizer.update(grads, opt_state, model)\n        model = optax.apply_updates(model, updates)\n        return model, opt_state, loss\n\n    optimizer = optax.adam(learning_rate=learning_rate)\n    opt_state = optimizer.init(model)\n\n    losses = []\n\n    for n_step in range(1, n_steps + 1):\n        key, subkey = random.split(key)\n        \n        X_obs, Y_obs, X_test, Y_test = split_data(subkey)\n\n        model, opt_state, loss = step(\n            model,\n            opt_state=opt_state,\n            X_obs=X_obs,\n            Y_obs=Y_obs,\n            X_test=X_test,\n            Y_test=Y_test\n        )\n\n        losses.append(loss)\n\n        if n_step % print_every == 0:\n            avg_loss = jnp.mean(jnp.asarray(losses[-20:]))\n            print(f\"Step: {n_step}\")\n            print(f\"Loss: {avg_loss:.2f}\")\n            print(\"-\" * 14)\n\n    return model\n\n\nfig, axs = plt.subplots(2, 2, figsize=(2*4, 2*3), dpi=150, sharex=True, sharey=True)\n\nfor holdout, ax in zip([1, 10, n_points // 2, int(0.8 * n_points)], axs.ravel()):\n    key, subkey = random.split(key)\n    \n    model = train(\n        key=subkey,\n        model=GaussianKernel(lengthscale=1.0),\n        X=X,\n        Y=Y,\n        print_every=None,\n        hold_out_size=holdout,\n        n_steps=100,\n    )\n    pred = model.predict(X_ax, X_obs=X, Y_obs=Y)\n    ax.set_title(f\"Hold-out={holdout}, $\\ell$={model.lengthscale:.2f}\")\n    ax.plot(X_ax, f(X_ax), color=\"maroon\", alpha=0.8)\n    ax.plot(X_ax, pred, color=\"orangered\", alpha=0.8)\n    ax.scatter(X, Y, color=\"white\", s=5, alpha=0.8)\n    ax.set_xlabel(\"$X$\")\n    ax.set_ylabel(\"$Y$\")\n\nfig.tight_layout()\n\n\n\n\n\nHey, this worked pretty well!\n\n\nMulti-headed kernel regression\nAt this point we’ll introduce yet another modification; later we’ll see that it’s analogous to multi-head attention. Consider a model with \\(H\\) “heads”. Each head will be a kernel with a potentially different lengthscale \\(\\ell_h\\). In this manner, we will allow different heads to capture information at a different lengthscale. Finally, we will combine the predictions using auxiliary parameters \\(u_1, \\dotsc, u_H\\): \\[\n\\hat f(x) = \\sum_{h=1}^H u_h\\, \\hat f_h(x) = \\sum_{h=1}^H u_h\\, \\sum_{i=1}^n y_i \\frac{ K(x, x_i; \\ell_h) }{ \\sum_{j=1}^n K(x, x_j; \\ell_h) }.\n\\]\nLet’s implement it quickly in Equinox:\n\n\nCode\nclass MultiheadGaussianKernel(eqx.Module):\n    kernels: list[GaussianKernel]\n    weights: jax.Array\n\n    def __init__(self, n_heads: int) -&gt; None:\n        assert n_heads &gt; 0\n\n        self.weights = jnp.full(shape=(n_heads,), fill_value=1 / n_heads)\n        self.kernels = [\n            GaussianKernel(lengthscale=l)\n            for l in jnp.linspace(0.1, 3, n_heads)\n        ]\n\n    @property\n    def lengthscale(self) -&gt; list[float]:\n        return [k.lengthscale for k in self.kernels]\n\n    def predict(self, X_test: Float[Array, \" n_test\"], X_obs: Vector, Y_obs: Vector) -&gt; Float[Array, \" n_test\"]:\n        # Shape (kernels, n_test)\n        preds = jnp.stack([k.predict(X_test=X_test, X_obs=X_obs, Y_obs=Y_obs) for k in self.kernels])\n        return jnp.einsum(\"kn,k-&gt;n\", preds, self.weights)\n\nfig, axs = plt.subplots(2, 2, figsize=(2*4, 2*3), dpi=150, sharex=True, sharey=True)\n\nfor n_heads, ax in zip([1, 2, 4, 8], axs.ravel()):\n    key, subkey = random.split(key)\n    \n    model = train(\n        key=subkey,\n        model=MultiheadGaussianKernel(n_heads=n_heads),\n        X=X,\n        Y=Y,\n        print_every=None,\n        hold_out_size=1,\n        n_steps=1_000,\n    )\n    pred = model.predict(X_ax, X_obs=X, Y_obs=Y)\n    ax.set_title(f\"Heads={n_heads}\") # $\\ell$={model.lengthscale:.2f}\")\n    ax.plot(X_ax, f(X_ax), color=\"maroon\", alpha=0.8)\n    ax.plot(X_ax, pred, color=\"orangered\", alpha=0.8)\n    ax.scatter(X, Y, color=\"white\", s=5, alpha=0.8)\n    ax.set_xlabel(\"$X$\")\n    ax.set_ylabel(\"$Y$\")\n\n    u_h_str = \", \".join([f\"{w:.2f}\" for w in model.weights])\n    l_h_str = \", \".join([f\"{k.lengthscale:.2f}\" for k in model.kernels])\n\n    print(f\"Number of heads: {n_heads}\")\n    print(f\"  Combination:  {u_h_str}\")\n    print(f\"  Lengthscales: {l_h_str}\")\n\nfig.tight_layout()\n\n\nNumber of heads: 1\n  Combination:  1.09\n  Lengthscales: 0.16\nNumber of heads: 2\n  Combination:  0.99, -0.10\n  Lengthscales: 0.08, 0.02\nNumber of heads: 4\n  Combination:  1.09, -0.36, -0.24, 0.10\n  Lengthscales: 0.16, 2.95, 2.18, 11.60\nNumber of heads: 8\n  Combination:  0.90, 0.02, 0.86, 0.95, -0.45, -0.53, -0.51, -0.22\n  Lengthscales: 0.08, 9.49, 20.43, 25.16, 210.44, 33.52, 34.75, 82.24\n\n\n\n\n\nWe see that coefficients \\(u_h\\) are not constrained to be positive and they do not have to sum up to 1: we allow an arbitrary linear combination of predictions, rather than a weighted sum. Note also that many heads allow for larger flexibility, although on such a small data set this can arguably result in some amount of overfitting."
  },
  {
    "objectID": "posts/kernel-regression-transformer.html#attention",
    "href": "posts/kernel-regression-transformer.html#attention",
    "title": "From kernel regression to the transformer",
    "section": "Attention",
    "text": "Attention\nRecall the equation \\[\n\\hat f(x) = \\sum_{i=1}^n y_i\\, \\frac{K(x, x_i; \\theta)}{ \\sum_{j=1}^n K(x, x_j; \\theta)},\n\\] where there kernel \\(K\\) is now parameterised by \\(\\theta\\). As we want the kernel to give positive values, let’s write \\[\nK(x, x'; \\theta) = \\exp s_\\theta(x, x')\n\\] for some function \\(s_\\theta\\). Hence, we can write \\[\n\\hat f(x) = \\sum_{i=1}^n y_i \\, \\mathrm{softmax}( s_\\theta(x, x_j)_{j = 1, \\dotsc, n} ).\n\\] The usual approach is to use \\(\\theta = (W^{(q)}, W^{(k)})\\) for matrices mapping from \\(\\mathcal X\\) to some space \\(\\mathbb R^{d_\\text{qk}}\\) and use a scalar product \\[\ns_\\theta(x, x') = \\frac{\\left\\langle W^{(q)}x, W^{(k)}x'\\right\\rangle}{\\sqrt{d_\\text{qk}}} = \\frac{ x^T \\left(W^{(q)}\\right)^T W^{(k)}x'}{\\sqrt{d_\\text{qk}}},\n\\] where the denominator takes various forms and is usually used to ensure that the values are properly normalized and the gradients can propagate through the softmax layer well.\nNow consider another modification. We will write \\(y_i = W^{(v)}x_i\\) for some matrix \\(W^{(v)}\\) mapping from \\(\\mathcal X\\) to some space \\(\\mathbb R^{d_\\text{v}}\\). (One can think that it’s a restriction when it comes to the regression (as we are not using values \\(y_i\\) as provided), but it’s not really a big issue: it just suffices to relabel as “point \\(x_i\\)” a tuple \\((x_i, y_i)\\) and redefine the introduced parameter matrices, so that they first project on the required component.)\nIn this case, we obtain a function \\[\nx\\mapsto \\sum_{i=1}^n W^{(v)}x_i \\, \\mathrm{softmax}\\left(  \\frac{ x^T \\left(W^{(q)}\\right)^T W^{(k)}x_i}{\\sqrt{d_\\text{qk}}}  \\right).\n\\]\nIf we apply this formula to each \\(x\\) from the sequence \\((x_1, \\dotsc, x_n) \\in \\mathcal X^n\\), we obtain a new sequence \\((x_1', \\dotsc, x'_n) \\in \\left(\\mathbb R^{d_\\text{v}}\\right)^n\\). This is exactly the self-attention layer used in transformers. How to obtain multi-head attention? Similarly as in multi-head kernel regression, we will introduce \\(H\\) different “heads” with individual parameters \\(W^{(k)}_h, W^{(q)}_h, W^{(v)}_h\\). Hence, for each data point \\(x\\) in the original sequence, we have \\(H\\) vectors in \\(\\mathbb R^{d_\\text{v}}\\) given by \\[\nx\\mapsto \\sum_{i=1}^n W^{(v)}_hx_i \\, \\mathrm{softmax}\\left(  \\frac{ x^T \\left(W^{(q)}_h\\right)^T W^{(k)}_h x_i}{\\sqrt{d_\\text{qk}}}  \\right) \\in \\mathbb R^{d_\\text{v}}.\n\\]\nIf we want to obtain a mapping into some vector space \\(\\mathcal Y\\), we can now introduce matrices \\(U_h\\colon \\mathbb R^{d_\\text{v}}\\to \\mathcal Y\\), so that in the end we have \\[\nx\\mapsto \\sum_{h=1}^H U_h \\sum_{i=1}^n W^{(v)}_hx_i \\, \\mathrm{softmax}\\left(  \\frac{ x^T \\left(W^{(q)}_h\\right)^T W^{(k)}_h x_i}{\\sqrt{d_\\text{qk}}}  \\right) \\in \\mathcal Y.\n\\]\nTo summarize, multi-head attention maps a sequence \\((x_1, \\dotsc, x_n)\\in \\mathcal X^n\\) to a sequence in \\(\\mathcal Y^n\\) and is parameterised by \\(H\\) tuples of matrices \\((W^{(q)}_h, W^{(k)}_h, W^{(v)}_h, U_h)\\), where index \\(h\\) corresponds to the attention head.\nConveniently, Equinox implements multi-head attention, from which I took dimension annotations."
  },
  {
    "objectID": "posts/kernel-regression-transformer.html#footnotes",
    "href": "posts/kernel-regression-transformer.html#footnotes",
    "title": "From kernel regression to the transformer",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe authors had to explain self-attention, its multi-head variant, the transformer architecture with encoder and decoder block, and positional encodings. All in a short conference paper, so it may indeed appear quite dense in ideas for people new to the domain.↩︎"
  },
  {
    "objectID": "posts/triangle-distributions.html",
    "href": "posts/triangle-distributions.html",
    "title": "Two distributions on a triangle",
    "section": "",
    "text": "Frederic, Alex and I have been discussing some experiments related to our work on mutual information estimators and Frederic suggested to look at one distribution. I misunderstood what he meant, but this mistake turned out to be quite an interesting object.\nSo let’s take a look at two distributions defined over a triangle \\[T = \\{ (x, y)\\in (0, 1)\\times (0, 1) \\mid y &lt; x \\}\\] and calculate their mutual information."
  },
  {
    "objectID": "posts/triangle-distributions.html#uniform-joint",
    "href": "posts/triangle-distributions.html#uniform-joint",
    "title": "Two distributions on a triangle",
    "section": "Uniform joint",
    "text": "Uniform joint\nConsider a probability distribution with constant probability density function (PDF) of the joint distribution: \\[p_{XY}(x, y) = 2 \\cdot \\mathbf{1}[y&lt;x].\\]\nWe have \\[p_X(x) = \\int\\limits_0^x p_{XY}(x, y)\\, \\mathrm{d}y = 2x\\] and \\[ p_Y(y) = \\int\\limits_0^1 p_{XY}(x, y) \\mathbf{1}[y &lt; x]  \\, \\mathrm{d}x = \\int\\limits_y^1 p_{XY}(x, y) \\, \\mathrm{d}x = 2(1-y).\\]\nHence, pointwise mutual information is given by \\[ i(x, y) = \\log \\frac{ p_{XY}(x, y) }{p_X(x) \\, p_Y(y) } = \\log \\frac{1}{2x(1-y)}\\] and mutual information is\n\\[I(X; Y) = \\int\\limits_0^1 \\mathrm{d}x \\int\\limits_x^1 i(x, y)\\, p_{XY}(x, y) \\mathrm{d}y = 1-\\log 2 \\approx 0.307.\\]\nFinally, let’s visualise this distribution to numerically validate the formulae above:\n\n\nCode\nfrom typing import Protocol\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nplt.style.use(\"dark_background\")\n\n\nclass Distribution(Protocol):\n  def sample(self, rng, n_samples: int) -&gt; np.ndarray:\n    pass\n\n  def p_xy(self, x: np.ndarray, y: np.ndarray) -&gt; np.ndarray:\n    pass\n\n  def p_x(self, x: np.ndarray) -&gt; np.ndarray:\n    pass\n\n  def p_y(self, y: np.ndarray) -&gt; np.ndarray:\n    pass\n\n  def pmi(self, x: np.ndarray, y: np.ndarray) -&gt; np.ndarray:\n    pass\n\n  @property\n  def mi(self) -&gt; float:\n    pass\n\n\nclass UniformJoint(Distribution):\n  def sample(self, rng, n_samples):\n    samples = rng.uniform(low=1e-9, size=(3 * n_samples, 2))\n    samples = np.asarray(list(filter(lambda point: point[1] &lt; point[0], samples)))\n    if len(samples) &lt; n_samples:\n      samples = self.sample(rng, n_samples)\n    \n    assert len(samples) &gt;= n_samples\n    return samples[:n_samples, ...]\n\n  def p_xy(self, x, y):\n    return np.where(y &lt; x, 2.0, 0.0)\n\n  def p_x(self, x):\n    return 2*x\n\n  def p_y(self, y):\n    return 2*(1-y)\n\n  def pmi(self, x, y):\n    return np.where(y &lt; x, -np.log(2*x*(1-y)), np.nan)\n\n  @property\n  def mi(self):\n    return 0.307\n\n\ndef visualise_dist(\n  rng,\n  dist: Distribution,\n  n_samples: int = 15_000,\n) -&gt; plt.Figure:\n  fig, axs = plt.subplots(2, 3, figsize=(3*2.2, 2*2.2))\n\n  samples = dist.sample(rng, n_samples=n_samples)\n\n  t_axis = np.linspace(1e-9, 1 - 1e-9, 51)\n\n  X, Y = np.meshgrid(t_axis, t_axis)\n\n  # Visualise joint probability\n  ax = axs[0, 0]\n  ax.scatter(samples[:, 0], samples[:, 1], rasterized=True, alpha=0.3, s=0.2, marker=\".\")\n  ax.set_xlim(0, 1)\n  ax.set_ylim(0, 1)\n  ax.set_title(\"Samples from $P_{XY}$\")\n  ax.set_xlabel(\"$x$\")\n  ax.set_ylabel(\"$y$\")\n\n  ax = axs[1, 0]\n  ax.imshow(dist.p_xy(X, Y), origin=\"lower\", extent=[0, 1, 0, 1], cmap=\"magma\")\n  ax.set_title(\"PDF $p_{XY}$\")\n  ax.set_xlabel(\"$x$\")\n  ax.set_ylabel(\"$y$\")\n\n  # Visualise marginal distributions\n  ax = axs[0, 1]\n  ax.set_xlim(0, 1)\n  ax.hist(samples[:, 0], bins=np.linspace(0, 1, 51), density=True, alpha=0.2, rasterized=True)\n  ax.plot(t_axis, dist.p_x(t_axis))\n  ax.set_xlabel(\"$x$\")\n  ax.set_title(\"PDF $p_X$\")\n\n  ax = axs[1, 1]\n  ax.set_xlim(0, 1)\n  ax.hist(samples[:, 1], bins=np.linspace(0, 1, 51), density=True, alpha=0.2, rasterized=True)\n  t_axis = np.linspace(0, 1, 51)\n  ax.plot(t_axis, dist.p_y(t_axis))\n  ax.set_xlabel(\"$y$\")\n  ax.set_title(\"PDF $p_Y$\")\n\n  # Visualise PMI\n  ax = axs[0, 2]\n  ax.set_xlim(0, 1)\n  ax.set_ylim(0, 1)\n  ax.imshow(dist.pmi(X, Y), origin=\"lower\", extent=[0, 1, 0, 1], cmap=\"magma\")\n  ax.set_title(\"PMI\")\n  ax.set_xlabel(\"$x$\")\n  ax.set_ylabel(\"$y$\")\n\n  ax = axs[1, 2]\n  pmi_profile = dist.pmi(samples[:, 0], samples[:, 1])\n  mi = np.mean(pmi_profile)\n  ax.set_title(f\"PMI histogram. MI={dist.mi:.2f}\")  \n  ax.axvline(mi, color=\"navy\", linewidth=1)\n  ax.axvline(dist.mi, color=\"salmon\", linewidth=1, linestyle=\"--\")\n  ax.hist(pmi_profile, bins=np.linspace(-2, 5, 21), density=True)\n  ax.set_xlabel(\"PMI value\")\n\n  return fig\n\nrng = np.random.default_rng(42)\ndist = UniformJoint()\n\nfig = visualise_dist(rng, dist)\nfig.tight_layout()"
  },
  {
    "objectID": "posts/triangle-distributions.html#uniform-margin",
    "href": "posts/triangle-distributions.html#uniform-margin",
    "title": "Two distributions on a triangle",
    "section": "Uniform margin",
    "text": "Uniform margin\nThe above distribution is interesting, but when I heard about the distribution over the triangle, I actually had the following generative model in mind: \\[\\begin{align*}\n  X &\\sim \\mathrm{Uniform}(0, 1),\\\\\n  Y \\mid X=x &\\sim \\mathrm{Uniform}(0, x).\n\\end{align*}\\]\nWe have \\(p_X(x) = 1\\) and therefore \\[p_{XY}(x, y) = p_{Y\\mid X}(y\\mid x) = \\frac{1}{x}\\,\\mathbf{1}[y &lt; x].\\]\nAgain, this distribution is defined on the triangle \\(T\\), although now the joint is not uniform.\nWe have \\[ p_Y(y) = \\int\\limits_y^1  \\frac{1}{x} \\, \\mathrm{d}x = -\\log y\\] and \\[i(x, y) = \\log \\frac{1}{-x \\log y} = -\\log \\big(x\\cdot (-\\log y)\\big )\n= - \\left(\\log(x) + \\log(-\\log y) \\right) = -\\log x - \\log(-\\log y).\\] This expression suggests that if \\(p_Y(y)\\) were uniform on \\((0, 1)\\) (but it is not), the pointwise mutual information \\(i(x, Y)\\) would be distributed according to Gumbel distribution.\nThe mutual information \\[\n  I(X; Y) = -\\int\\limits_0^1 \\mathrm{d}y \\int\\limits_y^1 \\frac{ \\log x + \\log(-\\log y)}{x} \\, \\mathrm{d}x = \\frac{1}{2} \\int\\limits_0^1 \\log y \\cdot \\log \\left(y \\log ^2 y\\right) \\, \\mathrm{d}y = \\gamma \\approx 0.577\n\\] is in this case the Euler–Mascheroni constant. I don’t know how to do this integral, but both Mathematica and Wolfram Alpha seem to be quite confident in it.\nPerhaps it shouldn’t be too surprising as \\(\\gamma\\) can appears in expressions involving mean of the Gumbel distribution. However, I’d like to understand this connection better.\nPerhaps another time; let’s finish this post with another visualisation:\n\n\nCode\nclass UniformMargin(Distribution):\n  def sample(self, rng, n_samples: int) -&gt; np.ndarray:\n    x = rng.uniform(size=(n_samples,))\n    y = rng.uniform(high=x)\n    return np.hstack([x.reshape((-1, 1)), y.reshape((-1, 1))])\n\n  def p_xy(self, x: np.ndarray, y: np.ndarray) -&gt; np.ndarray:\n    return np.where(y &lt; x, np.reciprocal(x), np.nan)\n\n  def p_x(self, x: np.ndarray) -&gt; np.ndarray:\n    return np.full_like(x, fill_value=1.0)\n\n  def p_y(self, y: np.ndarray) -&gt; np.ndarray:\n    return -np.log(y)\n\n  def pmi(self, x: np.ndarray, y: np.ndarray) -&gt; np.ndarray:\n    return np.where(y &lt; x, -np.log(-x * np.log(y)), np.nan)\n\n  @property\n  def mi(self):\n    return 0.577\n\n\nrng = np.random.default_rng(42)\ndist = UniformMargin()\n\nfig = visualise_dist(rng, dist)\nfig.tight_layout()\n\n\n/tmp/ipykernel_95941/2727834072.py:14: RuntimeWarning: divide by zero encountered in log\n  return -np.log(y)"
  },
  {
    "objectID": "posts/determinant-multilinear.html",
    "href": "posts/determinant-multilinear.html",
    "title": "Matrix determinant from the exterior algebra viewpoint",
    "section": "",
    "text": "In every linear algebra course matrix determinant is a must. Often it is introduced in the following form:\nThe definition above has a lot of advantages, but it also has an important drawback — the “why” of this construction is hidden and appears only later in a long list of its properties.\nWe’ll take an alternative viewpoint, which I have learned from Darling (1994, chap. 1), and is based around the exterior algebra."
  },
  {
    "objectID": "posts/determinant-multilinear.html#motivational-examples",
    "href": "posts/determinant-multilinear.html#motivational-examples",
    "title": "Matrix determinant from the exterior algebra viewpoint",
    "section": "Motivational examples",
    "text": "Motivational examples\nConsider \\(V=\\mathbb R^3\\). For vectors \\(v\\) and \\(w\\) we can define their vector product \\(v\\times w\\) with the following properties:\n\nBilinearity: \\((\\lambda v+v')\\times w = \\lambda (v\\times w) + v'\\times w\\) and \\(v\\times (\\lambda w+w') = \\lambda (v\\times w) + v\\times w'\\).\nAntisymmetry: \\(v\\times w = -w\\times v\\).\n\nGeometrically we can think of it as of a signed area of the parallelepiped spanned by \\(v\\) and \\(w\\).\nFor three vectors \\(v, w, u\\) we can form signed volume: \\[\\langle v, w, u\\rangle = v\\cdot (w\\times u),\\] which has similar properties:\n\nTrilinearity: \\(\\langle \\lambda v+v', w, u \\rangle = \\lambda \\langle v, w, u \\rangle + \\langle v', w, u\\rangle\\) (and similarly in \\(w\\) and \\(u\\) arguments).\nAntisymmetry: when we swap any two arguments the sign changes, e.g., \\(\\langle v, w, u\\rangle = -\\langle w, v, u\\rangle = \\langle w, u, v\\rangle = -\\langle u, w, v\\rangle\\).\n\nExterior algebra will be a generalisation of the above construction beyond the three-dimensional space \\(V=\\mathbb R^3\\)."
  },
  {
    "objectID": "posts/determinant-multilinear.html#exterior-algebra",
    "href": "posts/determinant-multilinear.html#exterior-algebra",
    "title": "Matrix determinant from the exterior algebra viewpoint",
    "section": "Exterior algebra",
    "text": "Exterior algebra\nLet’s start with the natural definition:\n\nDefinition 2 (Antisymmetric multilinear function) Let \\(V\\) and \\(U\\) be vector spaces and \\(f\\colon V\\times V \\times \\cdots \\times V \\to U\\) be a function. We will say that it is multilinear if for all \\(i = 1, 2, \\dotsc, n\\) it holds that \\[\nf(v_1, v_2, \\dotsc, \\lambda v_i + v_i', v_{i+1}, \\dotsc, v_n) = \\lambda f(v_1, \\dotsc, v_i, \\dotsc, v_n) + f(v_1, \\dotsc, v_i', \\dotsc, v_n).\n\\] We will say that it is antisymmetric if it changes the sign whenever we swap any two arguments: \\[\nf(v_1, \\dotsc, v_i, \\dotsc, v_j, \\dotsc, v_n) = -f(v_1, \\dotsc, v_j, \\dotsc, v_i, \\dotsc, v_n).\n\\]\n\nAs we have seen above both \\((v, w)\\mapsto v\\times w\\) and \\((v, w, u)\\mapsto v\\cdot (w\\times u)\\) are antisymmetric multilinear functions.\nNote that for every \\(\\sigma\\in S_n\\) it holds that \\[\nf(v_1, \\dotsc, v_n) = \\mathrm{sgn}\\,\\sigma \\, f(v_{\\sigma(1)}, \\dotsc, v_{\\sigma(n)})\n\\] as \\(\\mathrm{sgn}\\,\\sigma\\) counts transpositions modulo 2.\n\nExercise 1 Let \\(f\\colon V\\times V\\to U\\) be multilinear. Show that the following are equivalent:\n\n\\(f\\) is antisymmetric, i.e., \\(f(v, w) = -f(w, v)\\) for every \\(v, w \\in V\\).\n\\(f\\) is alternating, i.e., \\(f(v, v) = 0\\) for every \\(v\\in V\\).\n\nGeneralise to multilinear mappings \\(f\\colon V\\times V \\times \\cdots\\times V\\to U\\).\n\n\n\n\n\n\nHint\n\n\n\n\n\nExpand \\(f(v+w, v+w)\\) using multilinearity.\n\n\n\n\nNow we are ready to construct (a particular) exterior algebra.\n\nDefinition 3 (Second exterior power) Let \\(V\\) be a vector space. Its second exterior power \\(\\bigwedge^2 V\\) we be the vector space of expressions \\[\n\\lambda_1 v_1\\wedge w_1 + \\cdots + \\lambda_n v_n\\wedge w_n\n\\] with the following rules:\n\nThe wedge \\(\\wedge\\) operator is bilinear, i.e., \\((\\lambda v+v')\\wedge w = \\lambda v\\wedge w + v'\\wedge w\\) and \\(v\\wedge (\\lambda w+w') = \\lambda v\\wedge w + v\\wedge w'\\).\n\\(\\wedge\\) is antisymmetric, i.e., \\(v\\wedge w = -w\\wedge v\\) (or, equivalently, \\(v\\wedge v=0\\)).\nIf \\(e_1, \\dotsc, e_n\\) is a basis of \\(V\\), then \\[\\begin{align*}\n    &e_1\\wedge e_2, e_1\\wedge e_3, \\dotsc, e_1\\wedge e_n, \\\\\n    &e_2\\wedge e_3, \\dotsc, e_2\\wedge e_n\\\\\n    &\\qquad\\vdots\\\\\n    &e_{n-1}\\wedge e_n\n    \\end{align*}\n    \\] is a basis of \\(\\bigwedge^2 V\\).\n\n\nNote that \\(v\\wedge w\\) has the interpretation of a signed area of the parallelepiped spanned by \\(v\\) and \\(w\\). Such parallelepipeds can be formally added and there is a resemblance between the wedge product and the vector product in \\(\\mathbb R^3\\).\nWe just need to prove that such a space actually exists (this construction can be skipped at the first reading): similarly to the tensor space, build the free vector space on the set \\(V\\times V\\). Now quotient it by expressions like \\((v, v)\\), \\((\\lambda v, w) - (v, \\lambda w)\\), \\((v+v', w) - (v, w) - (v', w)\\) and \\((v, w+w') - (v, w) - (v, w')\\).\nThen define \\(v\\wedge w\\) to be the equivalence class \\([(v, w)]\\).\n\n\n\n\n\n\nNote\n\n\n\n\n\nIf we had introduced the determinant by other means, we could construct the exterior algebra \\(\\bigwedge^k V\\) also as the space of antisymmetric multilinear functions \\(V^*\\times V^*\\to \\mathbb R\\) (where \\(V^*\\) is the dual space) by\n\\[\n(v\\wedge w)(\\alpha, \\beta) := \\det \\begin{pmatrix}  \\alpha(v_1) & \\alpha(v_2) \\\\ \\beta(v_1) & \\beta(v_2) \\end{pmatrix}\n\\]\n\n\n\nAnalogously we can construct:\n\nDefinition 4 (Exterior power) Let \\(V\\) be a vector space. We define \\(\\bigwedge^0 V = \\mathbb R\\), \\(\\bigwedge^1 V = V\\) and for \\(k\\ge 2\\) its \\(k\\)th exterior power \\(\\bigwedge^k V\\) as the vector space of expressions \\[\n\\lambda_1 a_1\\wedge a_2\\wedge \\cdots\\wedge a_k + \\cdots + \\lambda_n v_1\\wedge v_2 \\wedge \\cdots\\wedge v_k\n\\] such that the wedge operator \\(\\wedge\\) is multilinear and antisymmetric (alternating) and that if \\(e_1, \\dotsc, e_n\\) is a basis of \\(V\\), then the set \\[\n\\{ e_{i_1}\\wedge e_{i_2}\\wedge \\cdots \\wedge e_{i_k}\\mid i_1 &lt; i_2 &lt; \\cdots &lt; i_k \\}\n\\]\nis a basis of \\(\\bigwedge^k V\\).\n\n\nExercise 2 Show that if \\(\\dim V = n\\), then \\(\\dim \\bigwedge^k V = \\binom{n}{k}\\). (And that in particular for \\(k &gt; n\\) we have \\(\\bigwedge^k V = 0\\), the trivial vector space).\n\nThe introduced space can be used to convert between antisymmetric multilinear and linear functions by the means of the universal property:\n\nTheorem 1 (Universal property) Let \\(f\\colon V\\times V \\cdots\\times V\\to U\\) be an antisymmetric multilinear function. Then, there exists a unique linear mapping \\(\\tilde f\\colon \\bigwedge^k V\\to U\\) such that for every set of vectors \\(v_1, \\dotsc, v_k\\) \\[\nf(v_1, \\dotsc, v_k) = \\tilde f(v_1\\wedge \\dotsc \\wedge v_k).\n\\]\n\n\nProof. (Can be skipped at the first reading.)\nAs \\(f\\) is multlilinear, its values are determined by the values on the tuples \\((e_{i_1}, \\dotsc, e_{i_k})\\), where \\(\\{e_1, \\dotsc, e_n\\}\\) is a basis of \\(V\\).\nWe can use antisymmetry to show that by “sorting out” the elements such that \\(i_1 \\le i_2\\cdots \\le i_k\\) and defining \\(\\tilde f(e_{i_1} \\wedge \\dotsc, \\wedge e_{i_k}) = f(e_{i_1}, \\dotsc, e_{i_k})\\) we obtain a well-defined mapping. Linearity is easy to proof.\nNow the uniqueness is proven by observing that antisymmetry and multilinearity uniquely prescribe the values at the basis elements of \\(\\bigwedge^k V\\).\n\nIts importance is the following: to show that a linear map \\(\\bigwedge^k V\\to U\\) is well-defined, one can construct a multilinear antisymmetric map \\(V\\times V\\times \\cdots \\times V\\to U\\)."
  },
  {
    "objectID": "posts/determinant-multilinear.html#determinants",
    "href": "posts/determinant-multilinear.html#determinants",
    "title": "Matrix determinant from the exterior algebra viewpoint",
    "section": "Determinants",
    "text": "Determinants\nFinally, we can define the determinant. Note that if \\(\\dim V = n\\), then \\(\\dim \\bigwedge^n V = 1\\).\n\nDefinition 5 (Determinant) Let \\(n=\\dim V\\) and \\(A\\colon V\\to V\\) be a linear mapping. We consider the mapping \\[\n(v_1, \\dotsc, v_n) \\mapsto (Av_1) \\wedge \\cdots \\wedge (Av_n).\n\\]\nAs it is antisymmetric and multilinear, we know that it induces a unique linear mapping \\(\\bigwedge^n V\\to \\bigwedge^n V\\).\nBecause \\(\\bigwedge^n V\\) is one-dimensional, this mapping must be multiplication by a number. Namely, we define the determinant \\(\\det A\\) to be the number such that for every set of vectors \\(v_1, \\dotsc, v_n\\) \\[\nAv_1 \\wedge \\cdots \\wedge Av_n = \\det A\\, (v_1\\wedge \\cdots \\wedge v_n).\n\\]\n\nIn other words, determinant measures the volume stretch of the parallelepiped spanned by the vectors after they are transformed by the mapping.\nI like this geometric intuition, especially that it is clear that determinant depends only on the linear map, rather than a particular matrix representation — it is independent on the chosen basis.\nWe can now show a number of lemmata.\n\nProposition 1 If \\(\\mathrm{id}_V\\colon V\\to V\\) is the identity mapping, then \\(\\det \\mathrm{id}_V = 1\\).\n\n\nProof. Obvious from the definition! Similarly, it’s clear that \\(\\det \\left(\\lambda\\cdot \\mathrm{id}_V\\right) = \\lambda^{\\dim V}\\).\n\n\nProposition 2 For every two mappings \\(A, B\\colon V\\to V\\) it holds that \\(\\det (B\\circ A) = \\det B\\cdot \\det A\\).\n\n\nProof. For every set of vectors we have \\[\n\\begin{align*}\n\\det (B\\circ A) \\, v_1\\wedge \\cdots \\wedge v_n &= (BAv_1) \\wedge \\cdots \\wedge (BAv_n) \\\\\n&= B(Av_1) \\wedge \\cdots \\wedge B(Av_n) \\\\\n&= \\det B \\, (Av_1) \\wedge \\cdots \\wedge (Av_n) \\\\\n&= \\det B\\cdot \\det A\\, v_1\\wedge \\cdots\\wedge v_n.\n\\end{align*}\n\\]\n\n\nProposition 3 (Only invertible matrices have non-zero determinants) A mapping is an isomorphism if and only if it has non-zero determinant.\n\n\nProof. If the mapping is invertible, then \\(A\\circ A^{-1} = \\mathrm{id}\\) and we have \\(\\det A \\cdot \\det A^{-1} = 1\\), so its determinant must be non-zero.\nNow assume that the mapping is non-invertible. This means that there exists a non-zero vector \\(k\\in \\ker A\\) such that \\(Ak=0\\). Let’s complete \\(k\\) to a basis \\(k, e_1, \\dotsc, e_{n-1}\\). Then \\[\n\\det A\\, k\\wedge e_1\\wedge \\cdots\\wedge e_{n-1} = (Ak) \\wedge \\cdots \\wedge (Ae_{n-1}) = 0,\n\\] which means that \\(\\det A=0\\) as \\(\\{k\\wedge e_1\\wedge \\dotsc \\wedge e_{n-1}\\}\\) is a basis of \\(\\bigwedge^n V\\).\n\nLet’s now connect the usual definition of the determinant to the one coming from exterior algebra:\n\nProposition 4 (Recovering the standard expression) Let \\(e_1, \\dotsc, e_n\\) be a basis of \\(V\\) and \\((A^{i}_j)\\) be the matrix of coordinates, i.e., \\[\nAe_k = \\sum_i A^{i}_k e_i.\n\\] Then the determinant \\(\\det A\\) can be calculated as \\[\n\\det A = \\sum_{\\sigma\\in S_n} \\mathrm{sgn}\\,\\sigma \\, A^{\\sigma(1)}_1 A^{\\sigma(2)}_2 \\dotsc A^{\\sigma(n)}_n.\n\\]\n\n\nProof. Observe that \\[\\begin{align*}\n\\det A e_1 \\wedge \\cdots \\wedge e_n &= Ae_1 \\wedge \\cdots \\wedge Ae_n\\\\\n&= \\left( \\sum_{i_1} A^{i_1}_1 e_{i_1} \\right) \\wedge \\cdots \\wedge \\left( \\sum_{i_n} A^{i_n}_n e_{i_n} \\right)\\\\\n&= \\sum_{i_1, \\dotsc, i_n} A^{i_1}A^{i_2}\\cdots A^{i_n} \\, e_{i_1} \\wedge \\cdots \\wedge e_{i_n}.\n\\end{align*}\\]\nNow we see that repeated indices give zero contribution to this sum, so we can only consider the indices which are permutations of \\(1, 2, \\dotsc, n\\). We also see that \\(e_{i_1} \\wedge \\cdots \\wedge e_{i_n}\\) can be then written as \\(\\pm 1\\, e_1\\wedge \\dotsc \\wedge e_n\\), where the sign is the number of required transpositions, that is the sign of the permutation. This ends the proof.\n\nGoing just a bit further into exterior algebra we can also show that matrix transposition does not change the determinant.\nTo represent matrix transposition, we will use the dual mapping: if \\(A\\colon V\\to V\\) there is the dual mapping \\(A^*\\colon V^*\\to V^*\\), given as \\[\n  (A^*\\omega)(v) := \\omega(Av).\n\\]\nWe can therefore build the \\(n\\)th exterior power of \\(V^*\\): \\(\\bigwedge^n (V^*)\\) and consider the determinant \\(\\det A^*\\).\nWe will formally show that\n\nProposition 5 (Determinant of the transpose) Let \\(A\\colon V\\to V\\) be a linear map and \\(A^*\\colon V^*\\to V^*\\) be its dual. Then \\[\n\\det A^* = \\det A.\n\\]\n\n\nProof. To do this we will need an isomorphism \\[\n\\iota \\colon {\\bigwedge}^n (V^*) \\to \\left({\\bigwedge}^n V\\right)^*\n\\] given on basis elements by \\[\n\\iota( \\omega^1 \\wedge \\cdots \\wedge \\omega^n ) (v_1\\wedge \\cdots \\wedge v_n) = \\det \\big(\\omega^i(v_j) \\big)_{i, j = 1, \\cdots, n},\n\\] where on the right side we use any already known formula for the determinant. It is easy to show that this mapping is well-defined and linear, as it descends from a multilinear alternating mapping.\nHaving this, the proof becomes straightforward calculation: \\[\n\\begin{align*}\n  \\det A^* \\iota\\left(  \\omega^1\\wedge \\cdots\\wedge \\omega^n  \\right)(v_1\\wedge \\cdots\\wedge v_n ) &=\n  \\iota\\bigg( \\det A^* \\, \\omega^1\\wedge \\cdots\\wedge \\omega^n  \\bigg)(v_1\\wedge \\cdots\\wedge v_n ) \\\\\n  &=\\iota\\bigg( A^*\\omega^1 \\wedge \\cdots\\wedge A^*\\omega^n  \\bigg )(v_1\\wedge \\cdots\\wedge v_n) \\\\\n  &= \\det \\bigg((A^*\\omega^i)(v_j)\\bigg) = \\det \\bigg( \\omega^i(Av_j ) \\bigg) \\\\\n  &= \\iota\\left(\\omega^1\\wedge \\cdots\\wedge \\omega^n \\right)(Av_1\\wedge\\cdots\\wedge Av_n) \\\\\n  &= \\iota\\left(\\omega^1\\wedge \\cdots\\wedge \\omega^n \\right)(\\det A\\, v_1\\wedge\\cdots\\wedge v_n) \\\\\n  &= \\det A~ \\iota\\left(\\omega^1\\wedge \\cdots\\wedge \\omega^n\\right)(v_1\\wedge\\cdots\\wedge v_n)\n\\end{align*}\n\\]\n\nEstablishing such isomorphisms is quite a nice technique, which also can be used to prove\n\nProposition 6 (Determinant of a block-diagonal matrix) Let \\(A\\colon V\\to V\\) and \\(B\\colon W\\to W\\) be two linear mappings and \\(A\\oplus B\\colon V\\oplus W\\to V\\oplus W\\) be the mapping given by \\[\n(A\\oplus B)(v, w) = (Av, Bw).\n\\]\nThen \\(\\det (A\\oplus B) = \\det A\\cdot \\det B\\).\n\n\nProof. We will use this approach: there exists an isomorphism \\[\n{\\bigwedge}^p (V\\oplus W) \\simeq \\bigoplus_k {\\bigwedge}^k V \\otimes {\\bigwedge}^{p-k} W,\n\\] so if we take \\(n=\\dim V\\) and \\(m=\\dim W\\) and note that \\(\\bigwedge^{p} V = 0\\) for \\(p &gt; n\\) (and similarly for \\(W\\)) we have \\[\n\\iota\\colon {\\bigwedge}^{n+m} (V\\oplus W) \\simeq {\\bigwedge}^n V\\otimes {\\bigwedge}^m W.\n\\] If \\(i\\colon V\\to V\\oplus W\\) and \\(j\\colon W\\to V\\oplus W\\) are the two “canonical” inclusions, this isomorphism is given as \\[\n\\iota\\big( iv_1 \\wedge \\cdots \\wedge iv_n\\wedge jw_1 \\wedge \\cdots \\wedge jw_m \\big) = (v_1\\wedge \\cdots\\wedge v_n) \\otimes (w_1\\wedge\\cdots\\wedge w_m).\n\\] Now we calculate: \\[\\begin{align*}\n(A\\oplus B)( iv_1 \\wedge \\cdots \\wedge iv_n\\wedge jw_1 \\wedge \\cdots \\wedge jw_m ) &=\niAv_1 \\wedge \\cdots \\wedge iAv_n \\wedge jBw_1\\wedge\\cdots\\wedge jBw_m \\\\\n&= \\iota^{-1}\\big( Av_1\\wedge \\cdots\\wedge Av_n \\otimes Bw_1\\wedge\\cdots\\wedge Bw_m   \\big) \\\\\n&= \\iota^{-1}\\big(\\det A\\cdot \\det B\\, v_1\\wedge \\cdots \\wedge v_n \\otimes w_1\\wedge\\cdots\\wedge w_m) \\\\\n&= \\det A\\cdot \\det B \\, \\iota^{-1}\\big( v_1\\wedge \\cdots \\wedge v_n \\otimes w_1\\wedge\\cdots\\wedge w_m \\big)\\\\\n&= \\det A\\cdot \\det B \\, iv_1\\wedge \\cdots \\wedge iv_n \\wedge jw_1\\wedge\\cdots\\wedge jw_m.\n\\end{align*}\n\\]\n\n\nProposition 7 (Determinant of an upper-triangular matrix) Let \\(A\\colon V\\to V\\) be a linear mapping and \\(e_1, \\dotsc, e_n\\) be a basis of \\(V\\) such that matrix \\((A^i_j)\\) is upper-triangular, that is \\[\n\\begin{align*}\n  Ae_1 &= A^1_1 e_1\\\\\n  Ae_2 &= A^1_2 e_1 + A^2_2 e_2\\\\\n  &\\vdots\\\\\n  Ae_n &= A^1_n e_1 + A^2_ne_2 + \\dotsc + A^n_n e_n\n\\end{align*}\n\\] Then \\[\n\\det A = \\prod_{i=1}^n A^i_i.\n\\]\n\nOnce proven, this result can also be used for lower-triangular matrices due to Proposition 5.\n\nProof. Recall that whenever there is \\(i_j=i_k\\), then \\(e_{i_1}\\wedge \\cdots\\wedge e_{i_n} = 0\\). Hence, there is only one term that may be non-zero: \\[\nAe_1\\wedge Ae_2 \\wedge \\cdots \\wedge Ae_n = A^1_1 e_1 \\wedge \\cdots \\wedge A^n_n e_n = \\prod_{i=1}^n A^i_i\\, e_1\\wedge \\cdots\\wedge e_n.\n\\]"
  },
  {
    "objectID": "posts/determinant-multilinear.html#acknowledgements",
    "href": "posts/determinant-multilinear.html#acknowledgements",
    "title": "Matrix determinant from the exterior algebra viewpoint",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nI would like to thank Adam Klukowski for helpful editing suggestions."
  },
  {
    "objectID": "posts/dirichlet-process.html",
    "href": "posts/dirichlet-process.html",
    "title": "The Dirichlet process",
    "section": "",
    "text": "In this post we will quickly review different constructions of the Dirichlet process, following Teh et al. (2006) and Gelman et al. (2013, chap. 23)."
  },
  {
    "objectID": "posts/dirichlet-process.html#finite-dimensional-dirichlet-prior",
    "href": "posts/dirichlet-process.html#finite-dimensional-dirichlet-prior",
    "title": "The Dirichlet process",
    "section": "Finite-dimensional Dirichlet prior",
    "text": "Finite-dimensional Dirichlet prior\nConsider the simplest Gaussian mixture model: there are several normal distributions with unit variance \\(\\mathcal N(\\mu_k, 1)\\) for \\(k\\in \\{1, \\dotsc, K\\}\\) and mixture proportions vector \\(\\pi = (\\pi_1, \\dotsc, \\pi_K)\\) with \\(\\pi_k\\ge 0\\) and \\(\\sum_k \\pi_k=1\\).\nA convenient prior for \\(\\pi\\) is the Dirichlet distribution. We put some \\(F_0\\) prior on the parameters \\(\\{\\mu_k\\}\\) of the model, so the generative process looks like: \\[\\begin{align*}\n  \\pi \\mid \\alpha &\\sim \\mathrm{Dirichlet}(\\alpha_1, \\dotsc, \\alpha_K)\\\\\n  \\mu_k \\mid F_0 &\\sim F_0, & k=1, \\dotsc, K\\\\\n  Z_n \\mid \\pi &\\sim \\mathrm{Categorical}(\\pi_1, \\dotsc, \\pi_K), & n=1, \\dotsc, N\\\\\n  X_n\\mid Z_n=z_n, \\{\\mu_k\\} &\\sim \\mathcal N(\\mu_{z_n}, 1),\\quad & n=1, \\dotsc, N.\n\\end{align*}\\]\n\nAnother point of view\nRather than using individual random variables \\(Z_n\\) and a shared set of parameters \\(\\{\\mu_k\\}\\) we could reparametrize the model to use individual means \\(\\tilde \\mu_n = \\mu_{Z_n}\\). In other words, we could consider a probability measure with atoms ${_k}$ given by \\[F = \\sum_{k=1}^K \\pi_k \\delta_{\\mu_k}.\\]\nIf we only know the Dirichlet weights vector \\((\\alpha_1, \\dotsc, \\alpha_K)\\) and the base measure \\(F_0\\) we can think of \\(F\\) as of a random probability measure generated according to \\[\\begin{align*}\n  \\pi \\mid \\alpha &\\sim \\mathrm{Dirichlet}(\\alpha_1, \\dotsc, \\alpha_K)\\\\\n  \\mu_k &\\sim F_0, \\quad k = 1, \\dotsc, K\\\\\n  F &:= \\sum_{k=1}^K \\pi_k \\delta_{\\mu_k}.\n\\end{align*}\\]\nThen sampling individual data points amounts to the following model with \\(n=1, \\dotsc, N\\): \\[\\begin{align*}\n  F\\mid \\alpha, F_0 &\\sim \\text{the procedure above}\\\\\n  \\theta_n \\mid F &\\sim F, \\\\\n  X_n\\mid \\theta_n &\\sim \\mathcal N(\\theta_n, 1).\n\\end{align*}\\]\nNote that the values of \\(\\theta_n\\) come from the set \\(\\{\\mu_1, \\dotsc, \\mu_K\\}\\) as \\(F\\) is atomic."
  },
  {
    "objectID": "posts/dirichlet-process.html#dirichlet-process-prior",
    "href": "posts/dirichlet-process.html#dirichlet-process-prior",
    "title": "The Dirichlet process",
    "section": "Dirichlet process prior",
    "text": "Dirichlet process prior\n\nStick-breaking construction\nWith the following example in mind we will pass now to a general distribution \\(F_0\\) defined over some infinite space \\(\\mathcal M\\) (which can be \\(\\mathbb R\\) as above) and a single positive parameter \\(\\alpha &gt; 0\\).\nWe will generate a random measure \\(F\\) from \\(F_0\\) using the construction known as the Dirichlet process.\nSample for \\(k=1, 2, \\dotsc\\) \\[\\begin{align*}\nv_k \\mid \\alpha &\\sim \\mathrm{Beta}(1, \\alpha)\\\\\n\\mu_k \\mid F_0 &\\sim F_0\n\\end{align*}\\] and define \\[\\begin{align*}\n  p_1 &= v_1\\\\\n  p_k &= v_k \\prod_{i=1}^{k-1} (1-v_k) \\quad \\text{for } k\\ge 2,\\\\\n  F &= \\sum_{k=1}^\\infty p_k \\delta_{\\mu_k}\n\\end{align*}\\]\nWith probability 1 it holds that \\[\\sum_{k=1}^\\infty  p_k = 1,\\] i.e., \\((p_k)\\) is a valid proportions vector.\nWe say that the distribution \\(F\\) was drawn from the Dirichlet process: \\[F \\sim \\mathrm{DP}(\\alpha, F_0).\\]\n\n\nInfinite limit\nThe atomic distributions generated with finite-dimensional proportions \\((\\pi_k)_{k=1, 2, \\dotsc, K}\\) and infinite sequence of weights \\((p_k)_{k=1, 2, \\dotsc, \\infty}\\) look optically similar. There is a close relation between these two generative processes.\nConsider a random measure \\(F^K\\) defined using a symmetric Dirichlet distribution: \\[\\begin{align*}\n  \\pi^K \\mid \\alpha &\\sim \\mathrm{Dirichlet}(\\alpha/K, \\cdots, \\alpha/K)\\\\\n  \\mu^K_k \\mid F_0 &\\sim F_0\\\\\n  F^K &= \\sum_{k=1}^K \\pi^K_k\\delta_{\\mu^K_k}\n\\end{align*}\\]\nNow if \\(F^{\\infty} \\sim \\mathrm{DP}(\\alpha, F_0)\\) and \\(u\\) is any measurable function integrable with respect to \\(F_0\\), then the sequence of random variables \\[ \\int_{\\mathcal M} u\\, \\mathrm{d} F^{K} \\] converges in distribution (that is, weakly) to \\[ \\int_{\\mathcal M} u\\, \\mathrm{d} F^{\\infty}.\\]\n\nWhere the difference really is\nWe see that \\((p_k)\\) looks deceptively similar as \\((\\pi_k^K)\\) for large \\(K\\). There are some differences, though. First of all, \\((p_k)\\) is infinite and the number of atoms appearing in the analysis of a particular data set is implicitly controlled by the number of data points. If \\(F_0\\) is non-atomic,one can expect \\(O(\\alpha\\log N)\\) atoms in a data set with \\(N\\) points. In the finite-dimensional case more than \\(K\\) clusters are impossible.\nHowever, for \\(K\\gg N\\) it’s natural to expect that several entries from \\((\\pi^K_k)\\) should be matching several entries of \\((p_k)\\). However, the intuition that \\(p_1 = \\pi_1^K\\), \\(p_2 = \\pi_2^K\\), … is wrong. In the stick-breaking construction of the Dirichlet process we expect the first few entries to have the most of the mass, while in the finite-dimensional case the Dirichlet prior is symmetric — we don’t know which weights \\(\\pi_k^K\\) will have vanishing mass.\nAlthough it seems obvious I spent quite some time trying to understand why the stick-breaking sampling procedure from the Dirichlet distribution gives different results!\nThe stick-breaking sampling procedure for the \\(\\mathrm{Dirichlet}(\\alpha/K, \\dotsc, \\alpha/K)\\) distribution works as follows: \\[\\begin{align*}\n  u_k &\\sim \\mathrm{Beta}( \\alpha/K, \\alpha\\cdot (1-k/K) )\\\\\n  \\pi_1 &= u_1\\\\\n  \\pi_k &= u_k \\prod_{j &lt; k} (1-u_k), \\quad k = 2, \\dotsc, K-1\\\\\n  \\pi_K &= 1 - (\\pi_1 + \\dotsc + \\pi_{K-1})\n\\end{align*}\\]\nwhich for \\(k \\ll K\\) corresponds to sampling from (approximately) \\(\\mathrm{Beta}(\\alpha/K, \\alpha)\\), rather than \\(\\mathrm{Beta}(1, \\alpha)\\).\nPitman (1996) describes size-biased permutations, which perhaps can be used to establish link between \\((\\pi_k)\\) for large \\(K\\) and \\((p_k)\\), but I haven’t understood it yet.\n\n\n\nDefining property\nWe have seen in what sense the Dirichlet process prior can be thought as of an infinite-dimensional generalization of the Dirichlet prior. However, there is another link.\nRecall that the defining property of a Gaussian process is that it is a continuous-time stochastic process \\(\\{X_t\\}_{t\\in [0, 1]}\\) such that for every finite set of indices \\(t_1, t_2, \\dotsc, t_m\\) random vector \\((X_{t_1}, \\dotsc, X_{t_m})\\) is distributed according to multivariate normal distribution. (In particular every \\(X_t\\) is a normal random variable). While this defining property is not sufficient without a proof of existence (e.g., an explicit construction), it is useful in many calculations involving them.\nWe will now give the defining property of the Dirichlet process. Take a probability measure \\(F_0\\) over \\(\\mathcal M\\) and the concentration parameter \\(\\alpha &gt; 0\\). We say that \\(\\mathrm{DP}(\\alpha, F_0)\\) is a Dirichlet process if every sample \\(F\\sim \\mathrm{DP}(\\alpha, F_0)\\) is a probability measure over \\(\\mathcal M\\) such that for every partition \\(A_1, \\cdots, A_m\\) of \\(\\mathcal M\\) the following holds: \\[ \\left( F(A_1), \\dotsc, F(A_K) \\right) \\sim \\mathrm{Dirichlet}\\big(\\alpha F_0(A_1), \\dotsc, \\alpha F_0(A_K) \\big) \\]\nIn particular if \\(A\\subseteq \\mathcal X\\) is any measurable subset, then we can use the partition \\(\\{A, \\mathcal M\\setminus A\\}\\) to get \\[ F(A) \\sim \\mathrm{Beta}\\big( \\alpha F_0(A), \\alpha(1-F_0(A)) \\big),\\] so that \\[\\mathbb E[ F(A) ] = F_0(A)\\] and \\[\\mathrm{Var}[F(A)] = \\frac{ F_0(A)\\cdot (1-F_0(A)) }{1+\\alpha}\\]\nHence, each draw \\(F\\) is centered around \\(F_0\\) and the variance is small for large parameter values \\(\\alpha\\).\n\n\nPólya urn scheme\nFinally, we give an interpretation in terms of Pólya urn scheme.\nAbove we considered the sampling process from the finite-dimensional Dirichlet distribution: \\[\\begin{align*}\n  F\\mid \\alpha, F_0 &\\sim \\text{construct atomic measure},\\\\\n  \\theta_n \\mid F &\\sim F,\n\\end{align*}\\] where each of the \\(\\theta_n\\) was actually some atom of the distribution \\(\\mu_k\\).\nThis interpretation is also easy to understand when the atomic measure \\(F\\) is drawn from the Dirichlet process using the stick-breaking construction.\nConsider now a sampling procedure of \\(\\theta_n\\) where we do not have direct access to \\(F\\), but only to the distribution \\(F_0\\), concentration parameter \\(\\alpha &gt; 0\\) and previous draws \\(\\theta_1, \\dotsc, \\theta_{n-1}\\). It holds that \\[\\theta_n \\mid \\alpha, F_0, \\theta_1, \\dotsc, \\theta_{n-1} \\sim \\frac{\\alpha}{ (n-1) + \\alpha }F_0 + \\sum_{u=1}^{n-1} \\frac{1}{(n-1)+\\alpha}\\delta_{ \\theta_n }.\\]\nIf \\(\\alpha\\) is a positive integer we can interpret this sampling procedure as follows: we want to draw the \\(n\\)th ball and we have an urn with \\(\\alpha\\) transparent balls and \\(n-1\\) balls of different colors. We draw a random ball. If it is transparent, we use \\(G_0\\) to sample a colored ball from \\(F_0\\), note it down, and put it to the urn.\nThis also suggests a clustering property: if there is a color \\(\\mu_k\\) such that there are already \\(m_k\\) balls inside the urn (i.e., \\(m_k\\) is the number of indices \\(1\\le i\\le n-1\\) such that \\(\\theta_i = \\mu_k\\)), then we have a larger chance to draw a ball of this color: \\[\\theta_n \\mid \\alpha, F_0, \\theta_1, \\dotsc, \\theta_{n-1} \\sim \\frac{\\alpha}{(n-1) + \\alpha}F_0 + \\sum_{k} \\frac{m_k}{ (n-1) + \\alpha } \\delta_{ \\mu_k }.\\]\nWe also see that for the concentration parameter \\(\\alpha \\gg n\\) this sampling procedure approximates independent sampling from \\(F_0\\).\n\nAsymptotic number of clusters\nThe above formulation can be used to argue why the number of clusters grows as \\(O(\\alpha\\log n)\\) if \\(F_0\\) is non-atomic. Define \\(D_1 = 1\\) and for \\(n\\ge 1\\) \\[D_n = \\begin{cases} 1 &\\text{ if } \\theta_n \\notin \\{\\theta_1, \\dotsc, \\theta_{n-1}\\}\\\\\n0 &\\text{otherwise}\\end{cases}\\] From the above construction we know the probability of drawing a new atom, so \\[\\mathbb E[D_n] = \\alpha / (\\alpha + n-1)\\] The number of distinct atoms in \\(F\\) is then \\[\\mathbb E[C_n] = \\mathbb E[D_1 + \\dotsc + D_n] = \\alpha \\sum_{i=1}^n \\frac{1}{\\alpha + n - 1}.\\] We recognise that this sum is similar to the harmonic series and (this can be proven formally) also grows as \\(O(\\log n)\\), so that \\(\\mathbb E[C_n] = O(\\alpha\\log n)\\). To provide a more precise result: \\[\\lim_{n\\to\\infty}\\frac{ \\mathbb E[C_n] }{\\alpha \\log n} = 1.\\] For this and related results consult these notes.\n\n\n\nChinese restaurant process\nThe procedure above is also closely related to the Chinese restaurant process, where the metaphor is that there are \\(K\\) occupied tables (where a dish \\(\\mu_k\\) is served) and there are \\(m_k\\) people sitting around the \\(k\\)th table. When a new customer enters the restaurant, they can either join an existing table (with probability proportional to \\(m_k\\)) or start a new (\\((K+1)\\)th) table with probability proportional to \\(\\alpha\\), where a new dish \\(\\mu_{K+1}\\sim F_0\\) will be served."
  },
  {
    "objectID": "posts/dirichlet-process.html#afterword",
    "href": "posts/dirichlet-process.html#afterword",
    "title": "The Dirichlet process",
    "section": "Afterword",
    "text": "Afterword\nThe Dirichlet process is a useful construction, which can be used as a nonparametric prior in clustering problems — instead of specifying a fixed number of clusters one can specify the growth rate via \\(\\alpha\\).\nIn practice the results (including the number of inferred clusters in a particular data set) need to be treated with caution: the clusters found in the data set do not need to have the “real world” meaning (or perhaps “clusters” are a wrong abstration at all, with heterogeneity attributable e.g., to some continuous covariates which could be measured). Careful validation is often needed, epsecially that these models may be non-robust to misspecification (see this paper on coarsening) or the inferences may be hard to do (see this overview of their intrinsic non-identifiability).\nAnyway, although difficult, clustering can provide useful information about a given problem, so we often need do it. For example, take a look at this application of Chinese restaurant process to the clustering of single-cell DNA profiles."
  },
  {
    "objectID": "posts/almost-binomial-markov-chain.html",
    "href": "posts/almost-binomial-markov-chain.html",
    "title": "An almost binomial Markov chain",
    "section": "",
    "text": "Recall that \\(Y\\sim \\mathrm{Bernoulli}(p)\\) if \\(Y\\) can attain values from the set \\(\\{0, 1\\}\\) with probability \\(P(Y=1) = p\\) and \\(P(Y=0) = 1-p\\). It’s easy to see that:\n\nFor every \\(k\\ge 1\\) the random variables \\(Y^k\\) and \\(Y\\) are equal.\nThe expected value is \\(\\mathbb E[Y] = p\\).\nThe variance is \\(\\mathbb{V}[Y]=\\mathbb E[Y^2]-\\mathbb E[Y]^2 = p-p^2 = p(1-p).\\) From AM-GM we see that \\(\\mathbb{V}[Y] \\le (1/2)^2=1/4\\).\n\nNow, if we consider independent and identically distributed variables \\(Y_1\\sim \\mathrm{Bernoulli}(p)\\), \\(Y_2\\sim \\mathrm{Bernoulli}(p)\\), …, \\(Y_n\\sim \\mathrm{Bernoulli}(p)\\), we can define a new variable \\(N_n = Y_1 + \\cdots + Y_n\\) and an average \\[ \\bar Y^{(n)} = \\frac{N_n}{n}.\\]\nThe random variable \\(N_n\\) is distributed according to the binomial distribution and it’s easy to calculate the mean \\(\\mathbb E[N_n] = np\\) and variance \\(\\mathbb V[N_n] = np(1-p)\\). Consequently, \\(\\mathbb E[ \\bar Y^{(n)} ] = p\\) and \\(\\mathbb V[\\bar Y^{(n)}] = np(1-p)/n^2 = p(1-p)/n \\le 1/4n\\).\nHence, we see that if we want to estimate \\(p\\), then \\(\\bar Y^{(n)}\\) is a reasonable estimator to use, and we can control its variance by choosing appropriate \\(n\\).\nThinking about very large \\(n\\), recall that the strong law of large numbers guarantees that \\[\nP\\left( \\lim\\limits_{n\\to \\infty} \\bar Y^{(n)} = p \\right) = 1.\n\\]"
  },
  {
    "objectID": "posts/almost-binomial-markov-chain.html#a-bernoulli-random-variable",
    "href": "posts/almost-binomial-markov-chain.html#a-bernoulli-random-variable",
    "title": "An almost binomial Markov chain",
    "section": "",
    "text": "Recall that \\(Y\\sim \\mathrm{Bernoulli}(p)\\) if \\(Y\\) can attain values from the set \\(\\{0, 1\\}\\) with probability \\(P(Y=1) = p\\) and \\(P(Y=0) = 1-p\\). It’s easy to see that:\n\nFor every \\(k\\ge 1\\) the random variables \\(Y^k\\) and \\(Y\\) are equal.\nThe expected value is \\(\\mathbb E[Y] = p\\).\nThe variance is \\(\\mathbb{V}[Y]=\\mathbb E[Y^2]-\\mathbb E[Y]^2 = p-p^2 = p(1-p).\\) From AM-GM we see that \\(\\mathbb{V}[Y] \\le (1/2)^2=1/4\\).\n\nNow, if we consider independent and identically distributed variables \\(Y_1\\sim \\mathrm{Bernoulli}(p)\\), \\(Y_2\\sim \\mathrm{Bernoulli}(p)\\), …, \\(Y_n\\sim \\mathrm{Bernoulli}(p)\\), we can define a new variable \\(N_n = Y_1 + \\cdots + Y_n\\) and an average \\[ \\bar Y^{(n)} = \\frac{N_n}{n}.\\]\nThe random variable \\(N_n\\) is distributed according to the binomial distribution and it’s easy to calculate the mean \\(\\mathbb E[N_n] = np\\) and variance \\(\\mathbb V[N_n] = np(1-p)\\). Consequently, \\(\\mathbb E[ \\bar Y^{(n)} ] = p\\) and \\(\\mathbb V[\\bar Y^{(n)}] = np(1-p)/n^2 = p(1-p)/n \\le 1/4n\\).\nHence, we see that if we want to estimate \\(p\\), then \\(\\bar Y^{(n)}\\) is a reasonable estimator to use, and we can control its variance by choosing appropriate \\(n\\).\nThinking about very large \\(n\\), recall that the strong law of large numbers guarantees that \\[\nP\\left( \\lim\\limits_{n\\to \\infty} \\bar Y^{(n)} = p \\right) = 1.\n\\]"
  },
  {
    "objectID": "posts/almost-binomial-markov-chain.html#a-bit-lazy-coin-tossing",
    "href": "posts/almost-binomial-markov-chain.html#a-bit-lazy-coin-tossing",
    "title": "An almost binomial Markov chain",
    "section": "A bit lazy coin tossing",
    "text": "A bit lazy coin tossing\nAbove we defined a sequence of independent Bernoulli variables. Let’s introduce some dependency between them: define \\[\nY_1\\sim \\mathrm{Bernoulli}(p)\n\\] and, for \\(n\\ge 1\\), \\[\nY_{n+1}\\mid Y_n \\sim w\\,\\delta_{Y_n} +(1-w)\\, \\mathrm{Bernoulli}(p).\n\\]\nHence, to draw \\(Y_1\\) we simply toss a coin, but to draw \\(Y_2\\) we can be lazy with probability \\(w\\) and use the sampled value \\(Y_1\\) or, with probability \\(1-w\\), actually do the hard work of tossing a coin again.\nLet’s think about the marginal distributions, i.e., we observe only the \\(n\\)th coin toss. As \\(Y_n\\) takes values in \\(\\{0, 1\\}\\), it has to be distributed according to some Bernoulli distribution.\nOf course, we have \\(\\mathbb E[Y_1] = p\\), but what is \\(\\mathbb E[Y_2]\\)? Using the law of total expectation we have \\[\n\\mathbb E[Y_2] = \\mathbb E[ \\mathbb E[Y_2\\mid Y_1] ] = \\mathbb E[ w Y_1 + (1-w)p ] = p.\n\\] Interesting! Even if we have large \\(w\\), e.g., \\(w=0.9\\), we will still see \\(Y_2=1\\) with original probability \\(p\\). More generally, we can prove by induction that that \\(\\mathbb E[Y_n] = p\\) for all \\(n\\ge 1\\).\nTo calculate the variance, we could try the law of total variance, but there is a simpler way: from the above observations we see that all the variables are distributed as \\(Y_n\\sim \\mathrm{Bernoulli}(p)\\) (so they are identically distributed, but not independent for \\(w&gt;0\\)) and the variance has to be \\(\\mathbb V[Y_n] = p(1-p)\\).\nLet’s now introduce variables \\(N_n = Y_1 + \\cdots + Y_n\\) and \\(\\bar Y^{(n)}=N_n/n\\). As expectation is a linear operator, we know that \\(\\mathbb E[N_n] = np\\) and \\(\\mathbb E[\\bar Y^{(n)}]=p\\), but how exactly are these variables distributed? Or, at least, can we say anything about their variance?\nIt’s instructive to see what happens for \\(w=1\\): intuitively, we only tossed the coin once, and then just “copied” the result \\(n\\) times, so that the sample size used to estimate \\(\\bar Y^{(n)}\\) is still one.\nMore formally, with probability 1 we have \\(Y_1 = Y_2 = \\cdots = Y_n\\), so that \\(N_n = nY_1\\) and \\(\\mathbb V[N_n] = n^2p(1-p)\\). Then, also with probability 1, we also have \\(\\bar Y^{(n)}=Y_1\\) and \\(\\mathbb V[\\bar Y^{(n)}]=p(1-p)\\).\nMore generally, we have \\[\n\\mathbb V[N_n] = \\sum_{i=1}^n \\mathbb V[Y_i] + \\sum_{i\\neq j} \\mathrm{cov}[Y_i, Y_j]\n\\] and we can suspect that the covariance terms will be non-negative, usually incurring larger variance than a corresponding binomial distribution (obtained from independent draws). Let’s prove that.\n\nMarkov chain\nWe will be interested in covariance terms \\[\\begin{align*}\n\\mathrm{cov}(Y_i, Y_{i+k}) &= \\mathbb E[Y_i\\cdot Y_{i+k}] - \\mathbb E[Y_i]\\cdot \\mathbb E[Y_{i+k}] \\\\\n&= P(Y_i=1, Y_{i+k}=1)-p^2 \\\\\n&= P(Y_i=1)P( Y_{i+k}=1\\mid Y_i=1) -p^2 \\\\\n&= p\\cdot P(Y_{i+k}=1 \\mid Y_i=1) - p^2.\n\\end{align*}\n\\]\nTo calculate the probability \\(P(Y_{i+k}=1\\mid Y_i=1)\\) we need an observation: the sampling procedure defines a Markov chain with the transition matrix \\[\nT = \\begin{pmatrix}\n    P(0\\to 0) & P(0 \\to 1)\\\\\n    P(1\\to 0) & P(1\\to 1)\n\\end{pmatrix}\n= \\begin{pmatrix}\n    w+(1-w)(1-p) & p(1-w)\\\\\n    (1-w)(1-p) & w + p(1-w)\n\\end{pmatrix}.\n\\]\nBy induction and a handy identity \\((1-x)(1+x+\\cdots + x^{k-1}) = 1-x^{k}\\) one can prove that \\[\nT^k = \\begin{pmatrix}\n    1-p(1-w^k) & p(1-w^k)\\\\\n    (1-p)(1-w^k) & p+w^k(1-p),\n\\end{pmatrix}\n\\] from which we can conveniently read \\[\nP(Y_{i+k}=1\\mid Y_i=1) = p+w^k(1-p)\n\\] and \\[\\mathrm{cov}(Y_i, Y_{i+k}) = w^k\\cdot p(1-p).\\]\nGreat, these terms are always non-negative! Let’s do a quick check: for \\(w=0\\) the covariance terms vanish, resulting in \\(\\mathbb V[N_n]=np(1-p) + 0\\) and for \\(w=1\\) we have \\(\\mathbb V[N_n] = np(1-p) + n(n-1)p(1-p)=n^2p(1-p)\\).\nFor \\(w\\neq 1\\) we can use the same identity as before to get \\[\\begin{align*}\n    \\mathbb V[N_n] &= p(1-p)\\cdot \\left(n+\\sum_{i=1}^n \\sum_{k=1}^{n-i} w^k \\right) \\\\\n    &= p(1-p)\\left( n+ \\frac{2 w \\left(w^n-n w+n-1\\right)}{(w-1)^2} \\right)\n\\end{align*}\n\\]\nLet’s numerically check whether this formula seems right:\n\n\nCode\nfrom functools import partial\n\nimport jax\nimport jax.numpy as jnp\nfrom jax import random, lax\n\n@partial(jax.jit, static_argnames=[\"n\"])\ndef simulate_markov_chain(key, n: int, p: float, w: float) -&gt; jnp.ndarray:\n    keys = random.split(key, n)\n\n    def step(i, y):\n        key_w, key_p = random.split(keys[i])\n        y_prev = y[i-1]\n        mixture_sample = random.bernoulli(key_w, w)\n        y_next = jnp.where(mixture_sample, y_prev, random.bernoulli(key_p, p))\n        y = y.at[i].set(y_next)\n        return y\n    \n    y_init = jnp.zeros(n, dtype=jnp.int32)\n    y_init = y_init.at[0].set(random.bernoulli(keys[0], p))\n\n    y_final = lax.fori_loop(1, n, step, y_init)\n    return y_final\n\ndef simulate_correlated_binomial(key, n: int, p: float, w: float) -&gt; int: \n    return simulate_markov_chain(key=key, n=n, p=p, w=w).sum()\n\n@partial(jax.jit, static_argnames=[\"n\", \"n_samples\"])\ndef sample_correlated_binomial(key, n: int, p: float, w: float, n_samples: int = 1_000_000) -&gt; jnp.ndarray:\n    keys = random.split(key, n_samples)\n    return jax.vmap(partial(simulate_correlated_binomial, n=n, p=p, w=w))(keys)\n\ndef variance_correlated_binomial(n: int, p: float, w: float) -&gt; float:\n    factor = n**2\n    if w &lt; 1.0:\n        factor = n + ( 2 * w * (-1 + n - n * w + w**n)) / (-1 + w)**2\n    return p*(1-p) * factor\n\nkey = random.PRNGKey(2024-1-19)\n\ntest_cases = [\n    (10, 0.5, 0.5),\n    (10, 0.3, 0.8),\n    (10, 0.2, 0.1),\n    (5, 0.4, 0.3),\n    (20, 0.8, 0.7),\n]\n\nfor n, p, w in test_cases:\n    key, subkey = random.split(key)\n    approx = jnp.var(sample_correlated_binomial(subkey, n, p, w))\n    exact = variance_correlated_binomial(n, p, w)\n\n    print(f\"Variance (appr.): {approx:.2f}\")\n    print(f\"Variance (exact): {exact:.2f}\")\n    print(\"-\"*23)\n\n\nAn NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n\n\nVariance (appr.): 6.50\nVariance (exact): 6.50\n-----------------------\nVariance (appr.): 11.40\nVariance (exact): 11.40\n-----------------------\nVariance (appr.): 1.92\nVariance (exact): 1.92\n-----------------------\nVariance (appr.): 1.93\nVariance (exact): 1.94\n-----------------------\nVariance (appr.): 15.66\nVariance (exact): 15.65\n-----------------------"
  },
  {
    "objectID": "posts/almost-binomial-markov-chain.html#markov-chain-monte-carlo",
    "href": "posts/almost-binomial-markov-chain.html#markov-chain-monte-carlo",
    "title": "An almost binomial Markov chain",
    "section": "Markov chain Monte Carlo",
    "text": "Markov chain Monte Carlo\nRecall that when the samples are independent, we can estimate \\(p\\) via \\(\\bar Y^{(n)}\\) which is an unbiased estimator, i.e., \\(\\mathbb E[\\bar Y^{(n)}] = p\\) and its variance is \\(\\mathbb V[\\bar Y^{(n)}]=p(1-p)/n\\le 1/4n\\).\nWhen we passed to a Markov chain introducing parameter \\(w\\), we also found out that \\(\\mathbb E[\\bar Y^{(n)}]=p\\). Moreover, for \\(w&lt;1\\) (i.e., there’s some genuine sampling, rather than copying the first result) the variance of \\(N_n\\) also grows as \\(\\mathcal O(n + w^n)=\\mathcal O(n)\\), so that \\(\\mathbb V[\\bar Y^{(n)}] =\\mathcal O(1/n)\\), so that for a large \\(n\\) the estimator \\(\\bar Y^{(n)}\\) can be a reliable estimator for \\(p\\). However, note that in the variance there’s a term \\(1/(1-w)^2\\), so that for \\(w\\) close to \\(1\\) one may have to use very, very, very large \\(n\\) to make sure that the variance is small enough.\nThis Markov chain is in fact connected to Markov chain Monte Carlo samplers, used to sample from a given distribution.\nThe Markov chain \\(Y_2, Y_3, \\dotsc\\) has transition matrix \\(T\\) and initial distribution of \\(Y_1\\) (namely \\(\\mathrm{Bernoulli}(p)\\)). For \\(0 &lt; p &lt; 1\\) and \\(w &lt; 1\\) the transition matrix has positive entries, which implies that this Markov chain is ergodic (both irreducibility and aperiodicity are trivially satisfied in this case; more generally quasi-positivity, i.e., \\(T^k\\) has positive entries for some \\(k\\ge 1\\), is equivalent to ergodicity).\nWe can deduce two things. First, there’s a unique stationary distribution of this Markov chain. It can be found by solving the equations for the eigenvector \\(T^{t}\\pi=\\pi\\); in this case \\(\\pi=(1-p, p)\\) (what a surprise!), meaning that the stationary distribution is \\(\\mathrm{Bernoulli}(p)\\).\nSecondly, we can use the ergodic theorem. The ergodic theorem states that in this case for every function1 \\(f\\colon \\{0, 1\\}\\to \\mathbb R\\) it holds that \\[\nP\\left(\\lim\\limits_{n\\to\\infty} \\frac{1}{n}\\sum_{i=1}^n f(Y_i) = \\mathbb E[f] \\right) = 1\n\\] where the expectation \\(\\mathbb E[f]\\) is taken with respect to \\(\\pi\\).\nNote that for \\(f(x) = x\\) we find out that with probability \\(1\\) it holds that \\(\\lim\\limits_{n\\to \\infty} \\bar Y^{(n)} = p\\).\nPerhaps it’s worth commenting on why the stationary distribution is \\(\\mathrm{Bernoulli}(p)\\). Consider any distribution \\(\\mathcal D\\) and a Markov chain \\[\nY_{n+1} \\mid Y_n \\sim w\\, \\delta_{Y_{n}} + (1-w)\\, \\mathcal D\n\\] for \\(w &lt; 1\\). Intuitively, this Markov chain will either jump to a new location with the right probability, or stay at a current point by some additional time. This additional time depends only on \\(w\\), so that on average, at each point we spend the same time. Hence, it should not affect time averages over very, very, very long sequences. (However, as we have seen, large \\(w\\) may imply large autocorrelation in the Markov chain and the chain would have to be extremely long to yield acceptable variance).\nI think it should not be hard to formalize and prove the above observation, but it’s not for today. This review could be useful for investigating this further."
  },
  {
    "objectID": "posts/almost-binomial-markov-chain.html#how-does-it-differ-from-beta-binomial",
    "href": "posts/almost-binomial-markov-chain.html#how-does-it-differ-from-beta-binomial",
    "title": "An almost binomial Markov chain",
    "section": "How does it differ from beta-binomial?",
    "text": "How does it differ from beta-binomial?\nRecall that a beta-binomial distribution generates samples as follows:\n\nDraw \\(b\\sim \\mathrm{Beta}(\\alpha, \\beta)\\);\nThen, draw \\(M \\mid b \\sim \\mathrm{Binomial}(n, b)\\).\n\nHence, first a random coin is selected from a set of coins with different biases, and then it’s tossed \\(n\\) times. This distribution has two degrees of freedom: \\(\\alpha\\) and \\(\\beta\\), and allows one a more flexible control over both the mean and the variance. The mean is given by \\[\n\\mathbb E[M] = n\\frac{\\alpha}{\\alpha + \\beta},\n\\] so if we write \\(p = \\alpha/(\\alpha + \\beta)\\), we match the mean of a “corresponding” binomial distribution. The variance is given by \\[\n\\mathbb V[M] = np(1-p)\\left(1 + \\frac{n-1}{\\alpha + \\beta + 1} \\right),\n\\] so that for \\(n \\ge 2\\) we will see a larger variance than for a binomial distribution with corresponding mean.\nWe see that this variance is quadratic in \\(n\\), which is different from the formula for the variance of the almost binomial Markov chain. Nevertheless, we can ask ourselves a question whether beta-binomial can be a good approximation to the distribution studied before.\nThis intuition may be formalized in many ways, e.g., as minimization of statistical discrepancy measures, including total variation, various Wasserstain distances or \\(f\\)-divergences. Instead, we will just match mean and variance.\nSo, of course, we will take \\(p=\\alpha/(\\alpha + \\beta)\\) and additionally solve for \\(\\mathbb V[M] = V\\). The solution is then given by \\[\\begin{align*}\n\\alpha &= pR,\\\\\n\\beta &= (1-p)R,\\\\\nR &= \\frac{n^2 p(1-p)-V}{V - n p(1-p)}.\n\\end{align*}\n\\]\nNow it’s coding time! We could use TensorFlow Probability on JAX to sample from beta-binomial distribution, but we will resort to core JAX.\n\n\nCode\nimport matplotlib.pyplot as plt \nplt.style.use(\"dark_background\")\n\ndef find_alpha_beta(n: int, p: float, variance: float) -&gt; tuple[float, float]:\n    num = n**2 * p * (1-p) - variance\n    den = variance - n * p * (1-p)\n    r = num / den\n\n    if r &lt;= 0 or p &lt;= 0 or p &gt;= 1:\n        raise ValueError(\"Input results in non-positive alpha or beta\")\n\n    return p*r, (1-p) * r\n\n@partial(jax.jit, static_argnames=[\"n\"])\ndef _sample_beta_binomial(key, n: int, alpha: float, beta: float) -&gt; int:\n    key_p, key_b = random.split(key)\n    p = random.beta(key_p, a=alpha, b=beta)\n    ber = random.bernoulli(key_b, p=p, shape=(n,))\n    return jnp.sum(ber)\n\n@partial(jax.jit, static_argnames=[\"n\", \"n_samples\"])\ndef sample_beta_binomial(key, n: int, alpha: float, beta: float, n_samples: int = 1_000_000) -&gt; jnp.ndarray:\n    keys = random.split(key, n_samples)\n    return jax.vmap(partial(_sample_beta_binomial, n=n, alpha=alpha, beta=beta))(keys)\n\n\ndef plot_compare(key, ax: plt.Axes, n: int, p: float, w: float, n_samples: int = 1_000_000, n_bins: int | None = None) -&gt; None:\n    variance = variance_correlated_binomial(n=n, p=p, w=w)\n    alpha, beta = find_alpha_beta(n=n, p=p, variance=variance)\n\n    key1, key2 = random.split(key)\n    sample_corr = sample_correlated_binomial(key1, n=n, p=p, w=w, n_samples=n_samples)\n    sample_betabin = sample_beta_binomial(key2, n=n, alpha=alpha, beta=beta, n_samples=n_samples)\n\n    if n_bins is None:\n        bins = jnp.arange(-0.5, n + 1.5, 1)\n    else:\n        bins = jnp.linspace(-0.1, n + 0.1, n_bins)\n\n    ax.hist(\n        sample_corr, bins=bins, density=True, rasterized=True,\n        color=\"yellow\",\n        label=\"Markov chain\",\n        histtype=\"step\",\n    )\n    ax.hist(\n        sample_betabin, bins=bins, density=True, rasterized=True,\n        color=\"orange\",\n        label=\"Beta-binomial\",\n        histtype=\"step\",\n        linestyle=\"--\"\n    )\n    ax.spines[[\"top\", \"right\"]].set_visible(False)\n    ax.set_xlabel(\"Number of heads\")\n    ax.set_ylabel(\"Probability\")\n\n\nfig, _axs = plt.subplots(2, 2, dpi=250)\naxs = _axs.ravel()\n\nkey, *keys = random.split(key, 1 + len(axs))\n\nparams = [\n    # (n, p, w, n_bins)\n    (10, 0.5, 0.9, None),\n    (10, 0.3, 0.2, None),\n    (100, 0.7, 0.98, 41),\n    (100, 0.3, 0.6, 41),\n]\nassert len(params) == len(axs)\n\nfor key, ax, (n, p, w, n_bins) in zip(keys, axs, params):\n    plot_compare(\n        key=key,\n        ax=ax,\n        n=n,\n        p=p,\n        w=w,\n        n_samples=1_000_000,\n        n_bins=n_bins,\n)\naxs[0].legend(frameon=False)\nfig.tight_layout()"
  },
  {
    "objectID": "posts/almost-binomial-markov-chain.html#footnotes",
    "href": "posts/almost-binomial-markov-chain.html#footnotes",
    "title": "An almost binomial Markov chain",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nUsually one has to add that the function is bounded. But we are working with a finite domain \\(\\{0, 1\\}\\), so literally every function is bounded.↩︎"
  },
  {
    "objectID": "posts/getting-started-with-Bayesian-statistics.html",
    "href": "posts/getting-started-with-Bayesian-statistics.html",
    "title": "Getting started with Bayesian statistics",
    "section": "",
    "text": "Getting started\n\nTake any two examples from PyMC Gallery. For example, GLM and hierarchical partial pooling. Implement them in a Jupyter notebook. Rather than copying and running the code, type it on your own and think what it is supposed to do.\nRead Michael Betancourt’s Inferring gravity from data. Reproduce it in PyMC — the data are available on GitHub.\nIf you would like a lecture series to watch, there’s Richard McElreath’s Statistical rethinking.\n\n\n\nConsult regularly\n\nLearning Bayesian Statistics is a truly excellent podcast hosted by Alexandre Andorra. Many leading statisticians, including Frank Harrell, Jessica Hullman, Kevin Murphy and Aki Vehtari. I find it the best place to learn about various perspectives on modelling techniques and important problems people are working on. I also enjoy listening to Data and Science with Glen Wright Colopy, which covers a wide range of topics and featured many prominent statisticians, such as Deborah Mayo, Chris Holmes, Andrew Gelman and David Dunson.\nAndrew Gelman’s blog and Frank Harrell’s blog.\nAnd, of course, whenever I open Bayesian data analysis, Probabilistic machine learning and Michael Betancourt’s notes, I learn something new.\n\n\n\nHandbooks\n\nGeneral references, covering many topics\n\nBayesian Data Analysis from Andrew Gelman et al.\nBiostatistics for Biomedical Research written by Frank Harrell.\nMichael Betancourt’s notes.\nProbabilistic Machine Learning written by Kevin Murphy.\n\n\n\nPrincipled modelling workflow\n\nThe Bayesian workflow manuscript.\nMichael Betancourt’s workflow description.\nKris Sankaran and Susan Holmes wrote a great paper Generative models: an interdisciplinary perspective.\nThere’s David Blei’s Build, compute, critique, repeat paper.\n\n\n\nInference methods\n\nVariational inference: a review for statisticians.\nHandbook of Markov chain Monte Carlo (unfortunately, this book is not freely available).\nAn introduction to sequential Monte Carlo (unfortunately, this book is not freely available).\n\n\n\n\nGreat to watch\nIf you are in mood for something as as engaging as a TV series episode, but more informative, take a look at these talks:\n\nDavid Dunson’s Debunking the hype.\nKristin Lennox’s Everything wrong with statistics.\nAndrew Gelman’s Solve all your statistics problems using p-values."
  },
  {
    "objectID": "posts/no-free-lunch-in-research.html",
    "href": "posts/no-free-lunch-in-research.html",
    "title": "The no free lunch theorem for scientific research",
    "section": "",
    "text": "Yesterday David and I went for pizza after work. As typical for our conversations, we spent quite some time discussing applied statistics and machine learning, and reached our usual conclusion that logistic regression is a wonderful model in so many problems.\nHowever, finding logistic regression or other “simple” methods in research papers can be quite hard, as we tend to look for methodological novelty. As Kristin Lennox nicely summarized, “you don’t get a lot of points for doing really good statistics on really important problems, if these statistics were invented in 1950s”. (In particular, Cynthia Rudin investigated how much applied research goes into the most presitigious machine learning conferences).\nThis is one of the reasons for the phenomenon which everybody in the field knows too well: from time to time you take a paper claiming state-of-the-art performance (“They are 0.02% better on CIFAR-10 than others! Let’s apply it to my problem”), and then find out that the method requires heavy hyperparameter tuning and hours of playing with the brittleness that makes the method impossible to use in practice. And, what’s even worse, the performance isn’t that different from a simple baseline.\nSimilarly, there are voices from many statisticians raising the issue that several of the grandiose results, which often involve solving important problems with the state-of-the-art methodology, may be simply invalid.\nTo summarize, a perfect paper should use novel methodology, aim at solving an important problem, and be correct (which should go without saying). The no free lunch of scientific research says that you can pick two out of three, at most.\nThis “theorem” is not very serious and is, of course, not universal – there exist great papers, but achieving all three goals in one manuscript is very hard to execute and they are exceptions, not the standard. Additionally, I don’t want to dichotomise here: methodological novelty, problem importance and correctness have many facets and subtleties (and may also be hard to assess upfront!), so it’s better to think about the level of each of these traits desired in a study.\nAs a first-order approximation, I found myself usually doing research on either novel methodology (illustrated on toy problems, without direct applications) or working on problems, which I find practical and important, but which require standard and well-trusted tools (at least as a starting point and a baseline).\nOn a more positive note, some novel (as of today) methods will have become standard and well-trusted tools in the coming years, with a lot practical impact to come. And practical problems often lead to improvement on existing methods or asking fundamental questions (see Pasteur’s quadrant). And they usually are much harder to solve, than it seems at the beginning! Let’s finish with a quote from Andrew Gelman: “Whenever I have an applied project, I’m always trying to do the stupidest possible thing, that will allow me to get out of the project alive. Unfortunately, the stupidest possible thing, that could possibly work, always seems to be a little more complicated, than the most complicated thing I already know how to do.”"
  },
  {
    "objectID": "posts/on-beyond-normal.html",
    "href": "posts/on-beyond-normal.html",
    "title": "The mutual information saga",
    "section": "",
    "text": "Where and when should we start this story? Probably a good origin will be in the Laboratory of Modeling in Biology and Medicine in Warsaw, where Tomasz Lipniacki and Marek Kochańczyk decided to mentor two students who just completed high-school education, namely the younger versions of Frederic and myself.\nInformally speaking, we tried to model the MAPK pathway as a communication channel. Imagine that the cell is given some input \\(x\\in \\mathcal X\\) and we measure the response \\(y\\in \\mathcal Y\\). Once we vary \\(x\\) and we record different values \\(y\\) we may start observing some patterns – perhaps changes with \\(y\\) is somehow associated with changes in \\(x\\). To make this more formal, consider a random variable \\(X\\) representing the given inputs and another random variable \\(Y\\) representing the outputs. The mutual information \\(I(X; Y)\\) measures how dependent these variables are: \\(I(X; Y) = 0\\) if and only if \\(X\\) and \\(Y\\) are independent. Contrary to correlation, mutual information works for any kind of variables (continuous, discrete, of arbitrary dimension…) and can capture nonlinear dependencies.\nI enjoyed my time in the project and the provided mentorship very much! We wrote a paper, and – perhaps more importantly – I learned that information theory and biology are great fields to study!\nMany years have passed and I thought that perhaps it’s the time to become a mentor on my own. Fortunately, my very first Master’s student I supervised, Anej, was so bright and motivated that he wrote a great Master’s thesis even with such an experienced supervisor as myself! Anyway, Anej was working on representation learning and extensively used mutual information estimators. But I had not done my homework: when I wrote the project proposal, I happily assumed that mutual information estimators have to work if the space \\(\\mathcal X\\times \\mathcal Y\\) is of moderate dimension and the number of points is large. Apparently I was wrong. Different mutual information estimators seemed to give very different estimates even in low-dimensions problems. That was concerning."
  },
  {
    "objectID": "posts/on-beyond-normal.html#beyond-normal",
    "href": "posts/on-beyond-normal.html#beyond-normal",
    "title": "The mutual information saga",
    "section": "Beyond normal",
    "text": "Beyond normal\nI did the only rational thing in this situation: I ran screaming to two mutual information experts, Frederic and Alex. We started thinking:\n\nHow do we really know when mutual information estimators work? As mutual information is analytically known only for the simplest distributions, the estimators are evaluated usually on “simple” low-dimensional distributions (or moderate-dimensional multivariate normal distributions).\nIs it possible to construct more expressive distributions with known ground-truth mutual information?\nHow invariant are the estimators to diffeomorphisms? Namely, if \\(f\\) and \\(g\\) are diffeomorphisms, then \\(I(X; Y) = I(f(X); g(Y))\\). Do the numerical estimates have the same property?\n\nThe 1st and 2nd question are related. But so are 2nd and 3rd! Suppose that we can easily sample points \\((x_1, y_1), \\dotsc, (x_n, y_n)\\) from the joint distribution \\(P_{XY}\\). If \\(f\\colon \\mathcal X\\to \\mathcal X\\) and \\(g\\colon \\mathcal Y\\to \\mathcal Y\\) are diffeomorphisms, we can apply them to obtain a sample \\((f(x_1), g(y_1)), \\dotsc, (f(x_n), g(y_n))\\) from \\(P_{f(X)g(Y)}\\), which is a joint distribution between variables \\(f(X)\\) and \\(g(Y)\\). As we apply a diffeomorphism1, the mutual information does not change: \\(I(X; Y) = I(f(X); g(Y))\\).\nFrederic and I started programming2 different distributions and transformations, in the meantime learning , and after five months we had a ready manuscript titled Are mutual information estimators homeomorphism-invariant?, which shows that the 3rd question was the most important one for a lapsed differential geometer who is currently trying to be an imposter in the machine learning world me.\nWell, I was wrong: after the manuscript got rejected from ICML, we realized that the most important aspect of our work was actually using the transformed distributions to study the strenghts and limitations of existing estimators3. We improved the experiments in the paper and changed the story to Beyond normal: on the evaluation of the mutual information estimators4, which was accepted to NeurIPS.\nOf course, we were very happy. But there were some important aspects that deserved to be studied a bit more…"
  },
  {
    "objectID": "posts/on-beyond-normal.html#here-comes-the-trouble",
    "href": "posts/on-beyond-normal.html#here-comes-the-trouble",
    "title": "The mutual information saga",
    "section": "Here comes the trouble",
    "text": "Here comes the trouble\n\nReally that expressive?\nOur distributions were only “beyond normal”, rather than “just amazing”: we suspected that one cannot construct all the interesting distributions.\nConsider \\(\\mathcal X = \\mathbb R^m\\), \\(\\mathcal Y = \\mathbb R^n\\) and a random vector \\(Z \\sim \\mathcal N(0, I_{m+n})\\). Normalizing flows guarantee that there exists a diffeomorphism \\(u: \\mathcal X\\times \\mathcal Y \\to \\mathcal X\\times \\mathcal Y\\) such that \\(P_{XY}\\) is well-approximated5 by the distribution of \\(u(Z)\\).\nHowever, the diffeomorphism \\(u\\) does not have to be of the form \\(f\\times g\\) (recall that \\((f\\times g)(x, y) = (f(x), g(y))\\)), which leaves the mutual information invariant. Thinking geometrically, the product group \\(\\mathrm{Diff}(\\mathcal X)\\times \\mathrm{Diff}(\\mathcal Y)\\) is usually a very small subgroup of \\(\\mathrm{Diff}(\\mathcal X\\times \\mathcal Y)\\), the group of all diffeomorphisms of \\(\\mathcal X\\times \\mathcal Y\\).\nWe had this intuition quite early, but we did not have a convincing counterexample that our distributions were not sufficient. However, Frederic started plotting histograms of pointwise mutual information, which let us formalize this intuition.\nConsider the pointwise mutual information: \\[ i_{XY}(x, y) = \\log \\frac{ p_{XY}(x, y) }{ p_X(x)\\, p_Y(y) },\\] where \\(p_{XY}\\) is the PDF of the joint distribution and \\(p_X\\) and \\(p_Y\\) are the PDFs of the marginal distributions. It is easy to prove that if \\(f\\) and \\(g\\) are diffeomorphisms and that \\(x'=f(x)\\) and \\(y'=g(y)\\), then6 \\[i_{XY}(x, y) = i_{f(X)g(Y)}(x', y').\\] From this it is easy to observe that the distribution of the random variable \\(i_{XY}(X; Y)\\) is the same as of \\(i_{f(X)g(Y)}(f(X); g(Y))\\). We termed it the pointwise mutual information profile, although it’s more than likely that people had already studied this before us.\nHence, diffeomorphisms leave invariant not only the mutual information: they leave invariant also the whole pointwise mutual information profile, what limits how expressive the distributions can be. We did not have yet a counterexample, but a strong feeling that it should exist: we just needed to find distributions with different profiles, but the same mutual information.\n\n\nModel-based mutual information estimation\nWe started the project with the idea that if \\((A, B)\\sim \\mathcal N(0, \\Sigma)\\), then \\(I(A; B)\\) is analytically known in terms of the covariance matrix \\(\\Sigma\\) and we can obtain more complicated dependencies between \\(X=f(A)\\) and \\(Y=f(B)\\) without changing the mutual information: \\(I(X; Y) = I(A; B)\\).\nAt the same time we asked the inverse question: if we have \\(X\\) and \\(Y\\), can we perhaps find a covariance matrix \\(\\Sigma\\) and a normalizing flow \\(f\\times g\\) such that \\((X, Y) = (f(A), g(B))\\) and \\((A, B)\\) are distributed according to the multivariate normal distribution with covariance matrix \\(\\Sigma\\)? If \\(f\\) and \\(g\\) are identity functions, this construction corresponds to the assumption that \\((X, Y)\\) are multivariate normal and calculating the mutual information via the estimation of the joint covariance matrix. A particular example of this approach is canonical correlation analysis, which worked remarkably well for multivariate normal distributions, providing more accurate estimates and requiring a lower number of samples available.\nHowever, as discussed above, generally we cannot expect that a normalizing flow of the form \\(f\\times g\\) will transform a distribution to a multivariate normal. So there is some potential for more explicit modelling of the joint distribution \\(P_{XY}\\), but we needed to make sure that it is expressive enough to cover some interesting cases.\n\n\nDo outliers break everything?\nThere’s no real data without real noise and we wanted to have distributions which can be used in practice. One source of noise are outliers, which sometimes can be attributed to errors in data collection or recording (e.g., the equipment was apparently switched off or some piece of experimental setup broke), and are well-known suspects when an estimator behaves badly. In Beyond normal we investigated heavy-tailed distributions (either by applying some transformations to multivariate normal distributions to make the tails havier, or by using multivariate Student distributions), but we felt that it was not enough."
  },
  {
    "objectID": "posts/on-beyond-normal.html#the-mixtures-and-the-critics",
    "href": "posts/on-beyond-normal.html#the-mixtures-and-the-critics",
    "title": "The mutual information saga",
    "section": "The mixtures and the critics",
    "text": "The mixtures and the critics\nThe outliers here were the most concerning and I had the feeling that if \\(P_{XY}\\) is the distribution from which we want to sample and \\(P_{\\tilde X \\tilde Y}\\) is the “noise distribution” with \\(I(\\tilde X; \\tilde Y) = 0\\), then we could perhaps calculate the information contained in the mixture distribution: \\[P_{X'Y'} = (1-\\alpha)\\, P_{XY} + \\alpha \\, P_{\\tilde X \\tilde Y},\\] where \\(\\alpha \\ll 1\\) is the fraction of outliers.\nI spent quite some time with a pen and paper trying to calculate \\(I(X'; Y')\\) in terms of \\(I(X; Y)\\), but I could not really derive anything. Even proving the conjectured bound that \\(I(X'; Y')\\) should not exceed \\(I(X; Y)\\) was hard…\nAnd it’s actually good that I didn’t manage to prove it: this conjecture is false. When I asked Frederic about it, he immediately responded with: 1. An example of two distributions such that each of them encodes 0 bits, but their mixture encodes 1 bit. 2. An example of two distributions such that each of them encodes 1 bit, but their mixture encodes 0 bits.\nThis is disturbing. As Larry Wasserman said: I have decided that mixtures, like tequila, are inherently evil and should be avoided at all costs. Fortunately, when I was still struggling with trying to prove it, I recalled Frederic’s histograms, approximating the pointwise mutual information profile. For multivariate normal and Student distributions he sampled a lot of data points \\((x_1, y_1), \\dotsc, (x_n, y_n)\\) and then evaluated the pointwise mutual information \\(i_{XY}(x_i, y_i)\\) – which is easy to evaluate using \\(\\log p_{XY}\\), \\(\\log p_X\\) and \\(\\log p_Y\\) densities – to construct a histogram. The mean of this sample is the estimate of the mutual information \\(I(X; Y) = \\mathbb E_{(x, y)\\sim P_{XY} }[i_{XY}(x; y)]\\).\nThis works for multivariate normal distributions and Student distributions, so why wouldn’t it work for mixture distributions? In the end this is simply a Monte Carlo estimator: we only need to sample a lot of data points (and sampling from a mixture distribution is trivial if one can sample from the component distributions) and evaluate the pointwise mutual information (which can be calculated from the PDFs of the involved variables. The PDF of the mixture distribution can be evaluated using the PDFs of the components).\nHence, although we do not have an exact formula for \\(I(X; Y)\\) where \\(P_{XY}\\) is a mixture of multivariate normal or Student distributions, we have its Monte Carlo approximation. Now we can apply diffeomorphisms to this distribution obtaining more expressive distribution, having e.g., a normalizing flow applied to a Gaussian mixture or a mixture of normalizing flow, or mix all of these again…\nImplementation of a prototype in TensorFlow Probability took one day7 and after a month we had The mixtures and the neural critics8 ready.\nI very much like this concept because:\n\nThis construction can model outliers as a mixture of different distributions.\nWe can construct a mixture distribution with a different pointwise mutual information profile than the multivariate normal distribution. Due to the fact that Gaussian mixtures are very expressive, we can build a whole new family of distributions!\nWe also do not have to model \\(P_{XY}\\) as a multivariate normal distribution transformed by diffeomorphism \\(f\\times g\\) – we can use Gaussian (or Student) mixtures! We quickly built a prototype of model-based mutual information estimation with mixtures, which can provide uncertainty quantification on mutual information in the usual Bayesian manner.\n\nLet’s finish this post at the place where we started: in Beyond normal we could apply diffeomorphisms to transform continuous \\(X\\) and \\(Y\\) variables. The Monte Carlo estimator from The mixtures and the critics applies in exactly the same manner also to the case when \\(X\\) is discrete variable and \\(Y\\) is continuous, which is exactly the case we investigated many years ago!"
  },
  {
    "objectID": "posts/on-beyond-normal.html#footnotes",
    "href": "posts/on-beyond-normal.html#footnotes",
    "title": "The mutual information saga",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nMore precisely, if all the spaces involved are standard Borel, then \\(f\\colon \\mathcal X\\to \\mathcal X'\\) and \\(g\\colon \\mathcal Y\\to \\mathcal Y'\\) can be continuous injective mappings and e.g., increase the dimensionality of the space. See Theorem 2.1 here, which is a well-known fact, but it still took us quite some time to prove it. M.S. Pinsker’s Information and information stability of random variables and processes proved to be an invaluable resource.↩︎\nAs a byproduct we learned Snakemake, which transformed my approach to data science entirely. But this is a different story.↩︎\nAlso, Frederic figured out that the 3rd question (whether the estimators which are invariant to diffeomorphisms) has a trivial answer. If \\(\\mathcal M\\) is a connected smooth manifold of dimension at least 2, then for any two sets of distinct points \\(\\{a_1, \\dotsc, a_n\\}\\) and \\(\\{b_1, \\dotsc, b_n\\}\\) there exists a diffeomorphism \\(u\\colon \\mathcal M\\to \\mathcal M\\) such that \\(b_i = u(a_i)\\) (the proof of this fact can be e.g., found in P.W. Michor’s and Cornelia Vizman’s \\(n\\)-transitivity of certain diffeomorphisms groups). Hence, if \\(\\mathcal X\\) and \\(\\mathcal Y\\) fulfil the assumptions above, we can move a finite data set as we wish and the only invariant estimator has to return the same answer for any set of input data points.↩︎\nLet me state two obvious facts. First: “beyond” refers to the fact that we can transform multivariate normal distributions to obtain more expressive distributions. Second: although I intended to name the paper after a wonderful musical, Next to Normal, we worried that the title was copyrighted.↩︎\nImportant detail: “well-approximated” using one statistical distance may me “badly approximated” with respect to another one.↩︎\nIt’s tempting to say that pointwise mutual information transforms as a scalar under the transformations from the group \\(\\mathrm{Diff}(\\mathcal X)\\times \\mathrm{Diff}(\\mathcal Y)\\).↩︎\nActually, it took three weeks of me complaining that we couldn’t use multivariate Student distributions in TensorFlow Probability on JAX. Since I had learned that I was wrong, a few hours passed before we had the prototype and a couple of more days before it was refactored into a stable solution.↩︎\nYes, this is named after J.R.R. Tolkien’s The monsters and the critics. I’m very stubborn, so it’s great that Frederic and Alex are so patient!↩︎"
  },
  {
    "objectID": "posts/strict-linear-independence-measures.html",
    "href": "posts/strict-linear-independence-measures.html",
    "title": "Strict linear independence of measures",
    "section": "",
    "text": "Here we discussed some possible approaches to tackle the quantification problem. Today let’s take a more theoretical look on it, as proposed in A Unified View of Label Shift Estimation."
  },
  {
    "objectID": "posts/strict-linear-independence-measures.html#quantification",
    "href": "posts/strict-linear-independence-measures.html#quantification",
    "title": "Strict linear independence of measures",
    "section": "Quantification",
    "text": "Quantification\nWe have many objects of distinct types \\(y\\in \\mathcal Y = \\{1, \\dotsc, K\\}\\), for example chairs, tables, cups, plates… However, in our setting the label is not available: we only have a list of features. For example, the features may include weight (which helps to distinguish chairs from beds), number of legs (which helps to distinguish cups from chairs), the main construction material.\nWe will assume that each object is given a point in the feature space \\(\\mathcal X\\). Of course, each type of object \\(y\\) may result in a lot of different features observed: there are heavier and lighter tables. Some chairs have three legs. Cups can be made out of wood, glass or metal. In other words, for each category \\(y\\) we have a probability measure \\(Q_y\\) on \\(\\mathcal X\\), representing the conditional probability distribution \\(P(X\\mid Y=y)\\).\nLet’s assume that the probability distributions \\(Q_y\\) are known, but we are trying to find the proportions of different object, \\(P(Y=y)\\), in the data set.\nIn other words, we have access to the finite sample from the mixture distribution\n\\[\nP(X) = \\sum_{y\\in \\mathcal Y} P(X\\mid Y=y) P(Y=y) = \\sum_{y\\in \\mathcal Y} \\pi_y Q_y,\n\\]\nwhere \\(\\pi = (\\pi_1, \\dotsc, \\pi_K)\\) is the list of proportions we are trying to find. Note that all entries have to be non-negative and that \\(\\pi_1 + \\dotsc + \\pi_K=1\\), what results in \\(K-1\\) degrees of freedom.\nHaving access to the finite samples is one of the actual difficulties of solving quantification problems. Another one is working with misspecified models, i.e., actually there may be more than \\(K\\) classes (but some of them we are not aware of), or out distributions \\(Q_y\\) may take a different form than assumed.\nHowever, let’s forget about these difficulties for now, and see if we can solve the quantification problem under the ideal circumstances."
  },
  {
    "objectID": "posts/strict-linear-independence-measures.html#identifiability",
    "href": "posts/strict-linear-independence-measures.html#identifiability",
    "title": "Strict linear independence of measures",
    "section": "Identifiability",
    "text": "Identifiability\n\n\n\n\n\n\nIdeal quantification problem: Let \\(P\\) and \\(Q_1, \\dotsc, Q_K\\) be probability measures on \\(\\mathcal X\\), such that there exist a decomposition into a mixture \\[\nP = \\sum_{y\\in \\mathcal Y} \\pi_y Q_y.\n\\]\nWhat are the necessary conditions on \\(Q_y\\) to ensure that the mixture components vector \\(\\pi\\) can be uniquely recovered given \\(Q_y\\) and \\(P\\)?\n\n\n\nFirst, let’s consider the case in which \\(\\mathcal X\\) is finite, with \\(|\\mathcal X| = D\\). If we order the elements of \\(\\mathcal X\\), we can represent each distribution \\(Q_y\\) by a vector of probabilities \\(Q_y(\\{x\\})\\) for \\(x\\in \\mathcal X\\). Equivalently, we are constructing conditional probability vectors \\(P(X=x\\mid Y=y)\\) and have to solve a set of equations\n\\[\nP(X=x) = \\sum_{y\\in \\mathcal Y} P(X=x\\mid Y=y) \\pi_y.\n\\]\nClearly, if the probability vectors representing \\(Q_y\\) are linearly independent, the solution for \\(\\pi\\) has to be unique (assuming that it exists, which is related to the assumption that we have no misspecification). The technique based on solving such a solution of linear equations has been proposed in various forms over the years."
  },
  {
    "objectID": "posts/strict-linear-independence-measures.html#strict-linear-independence-of-measures",
    "href": "posts/strict-linear-independence-measures.html#strict-linear-independence-of-measures",
    "title": "Strict linear independence of measures",
    "section": "Strict linear independence of measures",
    "text": "Strict linear independence of measures\nWhat if \\(\\mathcal X\\) is not finite? In particular, what if \\(\\mathcal X\\) is a continuous space in which singletons \\(\\{x\\}\\) have probability zero? In this case, the measures \\(Q_y\\) do not have such a convenient finite-dimensional vector representation and the concept of linear independence seems to be less useful.\nThe authors of A Unified View of Label Shift Estimation propose the following notion of strict linear independence of probability measures: for every vector \\(\\lambda \\in \\mathbb R^K\\) such that \\(\\lambda\\neq 0\\) it holds that \\[\n\\int_{\\mathcal X} \\left| \\sum_{y\\in \\mathcal Y} p(z\\mid y) \\right|  \\, \\mathrm{d}x \\neq 0.\n\\]\nI personally prefer a bit different formulation (although perhaps a bit more complicated). Assume that we have a \\(\\sigma\\)-finite measure \\(\\mu\\) on \\(\\mathcal X\\), such that all \\(Q_k \\ll \\mu\\). Often there is a natural reference measure in many problems (e.g., the Lebesgue measure on \\(\\mathbb R^n\\), with the assumption that all \\(Q_k\\) have PDFs), but generally at least one exists, for example \\(\\mu = Q_1 + \\dotsc + Q_K\\) (or it can be normalised by \\(K\\) to yield a probability measure!)\nThe equation above is a requirement that \\[\n\\int_{\\mathcal X} \\left| \\sum_{y\\in \\mathcal Y} \\lambda_y \\frac{\\mathrm d Q_y}{\\mathrm d \\mu} \\right|  \\, \\mathrm{d}\\mu \\neq 0\n\\] which in turn can be written as \\[\n\\left| \\sum_{y\\in \\mathcal Y} \\lambda_y Q_y \\right|(X) \\neq 0.\n\\]\nIt’s not hard to prove that the above condition is equivalent to an existence of a measurable set \\(A_\\lambda\\) such that \\[\n\\lambda_1 Q_1(A_\\lambda) + \\cdots + \\lambda_K Q_K(A_\\lambda) \\neq 0.\n\\]\nHence, we will prefer to use the equivalent definition:\n\n\n\n\n\n\nDefinition: We say that probability measures \\(Q_1, \\dotsc, Q_K\\) are strictly linearly independent if for every vector \\(\\lambda \\neq 0\\) there exists a measurable subset \\(A_\\lambda\\subseteq \\mathcal X\\) such that \\[\n\\lambda_1 Q_1(A_\\lambda) + \\cdots + \\lambda_K Q_K(A_\\lambda) \\neq 0.\n\\]\n\n\n\nLet’s think why this is a sufficient condition for the uniqueness of \\(\\pi\\). Assume that the true composition vector is \\(\\pi\\) and suppose that we have a candidate composition vector \\(\\gamma\\) such that \\(\\gamma\\neq \\pi\\). Take now \\(\\lambda = \\pi - \\gamma \\in \\mathbb R^K\\). From strict linear independence, we know that there exists \\(A_\\lambda\\) such that \\[\nP(A_\\lambda) = \\sum_{y} \\pi_y Q_y(A_\\lambda) \\neq \\sum_{y} \\gamma_y Q_y(A_\\lambda).\n\\]\nHence, the observed measure \\(P\\) is different from the mixture parameterised by \\(\\gamma\\)."
  },
  {
    "objectID": "posts/strict-linear-independence-measures.html#examples",
    "href": "posts/strict-linear-independence-measures.html#examples",
    "title": "Strict linear independence of measures",
    "section": "Examples",
    "text": "Examples\nFinally, let’s think about examples of strictly linearly independent measures.\n\nDiscrete spaces\nProbably the simplest example is for discrete measures on finite spaces: if \\(\\mathcal X\\) is finite, strict linear independence and linear independence are equivalent.\nThe proof is easy: consider the probability vectors \\(q^y_x = P(X=x\\mid Y=y) = Q_y(\\{x\\})\\). If the vectors are linearly independent, for every \\(\\lambda \\neq 0\\) we have \\(\\lambda_1 q^1 + \\cdots + \\lambda_K q^K\\neq 0\\), meaning that there exists a component \\(x\\in \\mathcal X\\) such that \\(\\lambda_1 q^1_x + \\cdots + \\lambda_K q^K_x \\neq 0\\). So, we define \\(A_\\lambda = \\{x\\}\\).\nConversely, if we have \\(\\lambda\\neq 0\\) and we use strict linear independence to ensure the existence of a set \\(A_\\lambda\\) such that \\[\n    0 \\neq \\lambda_1 Q_1(A_\\lambda) + \\cdots + \\lambda_K Q_K(A_\\lambda) = \\sum_{x\\in A_\\lambda} (\\lambda_1 q^1_x + \\cdots + \\lambda_K q^K_x),\n\\] then we see that for at least one component \\(x\\) we have \\(\\lambda_1 q^1_x + \\cdots + \\lambda_K q^K_x\\neq 0\\), which suffices for linear independence.\n\n\nA lemma\nFor continuous spaces the situation is a bit more complex. However, let’s prove a useful lemma, which is in fact a generalisation of the previous result.\n\n\n\n\n\n\nLemma: Assume that \\(\\mathcal X\\) is a standard Borel space and \\(Q_1, \\dotsc, Q_K\\) have continuous PDFs \\(q_1, \\dotsc, q_K\\), with respect to a \\(\\sigma\\)-finite and strictly positive measure \\(\\mu\\). Then, if \\(q_1, \\dotsc, q_K\\) are linearly independent as vectors in the space of continuous real-valued functions \\(C(\\mathcal X, \\mathbb R)\\), then the measures \\(Q_1, \\dotsc, Q_K\\) are strictly linearly independent.\n\n\n\nProof: Take any \\(\\lambda\\neq 0\\) and write \\(u = |\\lambda_1 q_1 + \\cdots + \\lambda_K q_K|\\). From the linear independence it follows that there exists \\(x_0\\in \\mathcal X\\) such that \\(u(x_0) &gt; 0\\). Now use continuity of \\(u\\) to find an open neighborhood \\(A\\) of \\(x_0\\) such that for all \\(x\\in A\\) we have \\(u(x) &gt; u(x_0) / 2\\). As \\(u\\) is non-negative and \\(\\mu\\) is strictly positive, we have \\(\\mu(A) &gt; 0\\), so that \\[\n\\int_X u\\, \\mathrm{d}\\mu \\ge \\int_{A} u\\, \\mathrm{d}\\mu \\ge \\frac{u(x_0)}{2} \\cdot \\mu(A) &gt; 0.\n\\]\nI personally find this lemma useful: verifying linear independence of functions is a well-studied problem in mathematics. For example, if \\(\\mathcal X\\subseteq \\mathbb R\\) is an interval and the densities \\(q_y\\) are sufficiently smooth, one can use Wronskian (introduced by Józef Wroński in 1812, so it’s a classic tool) to study linear independence.\n\n\nExponential variables\nConsider a space \\(\\mathcal X = \\mathbb R^+\\) and the family of exponential random variables, which have densities \\(q_y(x)=\\mu_k\\exp(-\\mu_y x)\\). Now assume that all the parameters are different. We will prove that these densities are linearly independent functions.\n\nWronskian approach\nNote that the \\(m\\)-th derivative is \\(q^{(m)}(x) = (-\\mu_y)^{m} q_y(x)\\). The Wronskian is in this case given by \\[\\begin{align*}\nW(x) &= \\det \\begin{pmatrix}\n    q_1(x) & \\cdots & q_K(x)\\\\\n    -\\mu_1 q_1(x) & \\cdots & -\\mu_K q_K(x) \\\\\n    \\vdots & \\ddots & \\vdots \\\\\n    (-\\mu_1)^{K-1} q_1(x) & \\cdots & (-\\mu_K)^{K-1} q_K(x)\n\\end{pmatrix} \\\\\n  &= \\left(\\prod_{y} q_y(x)\\right) \\cdot \\det \\begin{pmatrix}\n    1 & \\cdots & 1\\\\\n    -\\mu_1 & \\cdots & -\\mu_K \\\\\n    \\vdots & \\ddots & \\vdots \\\\\n    (-\\mu_1)^{K-1}  & \\cdots & (-\\mu_K)^{K-1}\n    \\end{pmatrix}\n\\end{align*}\n\\]\nNote that all \\(q_y(x) &gt; 0\\) and that the determinant of the last matrix has to be positive, as it’s a Vandermonde polynomial: \\[\n\\prod -(\\mu_i - \\mu_j) \\neq 0,\n\\] from the assumption that the means are different.\n\n\nAsymptotics\nLet’s do another proof, this time going to the limit, similarly to this solution.\nWithout loss of generality, assume \\(0 &lt; \\mu_1 &lt; \\mu_2 &lt; \\dotsc &lt; \\mu_K\\).\nIf there’s \\(\\lambda \\in \\mathbb R^K\\) such that \\[\n\\sum_k \\lambda_k \\mu_k \\exp(-\\mu_k x) = 0,\n\\] identically, then we can multiply both sides by \\(\\exp(\\mu_1 x)\\) to obtain \\[\n\\sum_k \\lambda_k \\mu_k \\exp\\big(-(\\mu_k-\\mu_1) x\\big) = 0.\n\\] For \\(x\\to \\infty\\) the first term becomes \\(\\lambda_1\\mu_1\\) and the rest of the terms goes to \\(0\\). Hence, we have \\(\\lambda_1 = 0\\). Repeating this procedure for \\(\\lambda_2\\), \\(\\lambda_3\\) and other coefficients, we end up with \\(\\lambda = 0\\), proving linear independence.\n\n\nEigenvectors and eigenvalues\nSheldon Axler provides a wonderful proof: each \\(q_y\\) is an eigenvector of the differentiation operator: \\[\n\\frac{\\mathrm{d}}{\\mathrm{d}x} q_y(x) = -\\mu_y q_y(x).\n\\]\nAs all these eigenvalues are distinct, the eigenvectors have to be independent (a useful lemma, one proof follows via induction on \\(K\\)).\n\n\n\nHow about the normal distributions?\nHere is a proof strategy for the normal distributions on \\(\\mathbb R\\), which employs asymptotics. However, I expect the result should generally hold for multivariate normal distributions, provided that the mean vectors are different. But how to prove that? Possibly the strategy employing asymptotics would work, but I am not sure about the details. Similarly, I expect that multivariate Student distributions with different location vectors should be strictly linearly independent.\nI somewhat feel that topic should have been already studied in measure theory and, perhaps, information geometry, although probably under a different name than strict linear independence. It would be interesting to see a reference on this topic!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome!",
    "section": "",
    "text": "My name is Paweł Czyż and I am a data scientist interested in modelling complex biological data using techniques originating in probabilistic machine learning and applied Bayesian statistics.\nCurrently I am a Doctoral Fellow of the ETH AI Center working in Computational Biology Group and Computational Cancer Genomics Laboratory.\nPrior to starting my PhD I was an AI Resident at Microsoft Research Cambridge and studied mathematical physics at University of Oxford."
  },
  {
    "objectID": "Almost-Nonidentifiable.html",
    "href": "Almost-Nonidentifiable.html",
    "title": "Paweł Czyż",
    "section": "",
    "text": "import jax\nimport jax.numpy as jnp\nfrom jax import random\n\n\ndef sample_covariates(key, correlation: float, n_points: int) -&gt; jnp.ndarray:\n    cov = jnp.asarray([\n        [1.0, correlation],\n        [correlation, 1.0],\n    ])\n    return random.multivariate_normal(key, mean=jnp.zeros(2), cov=cov, shape=(n_points,))\n\n\ndef sample_response(\n    key, \n    covariates: jnp.ndarray,\n    coefs: jnp.ndarray,\n    intercept: float,\n    noise: float\n) -&gt; jnp.ndarray:\n    noise_var = noise * random.normal(key, shape=(covariates.shape[0],))\n    return intercept + jnp.einsum(\"j,nj -&gt; n\", coefs, covariates) + noise_var\n\n\ndef solve(\n    covariates,\n    response,\n) -&gt; tuple[float, jnp.ndarray]:\n    n = covariates.shape[0]\n    ones = jnp.ones((n, 1))\n    # Append 1 so that the matrix has columns:\n    # 1, cov1, cov2\n    xs = jnp.hstack((ones, covariates))\n\n    coefs = jnp.linalg.inv(xs.T @ xs) @ xs.T @ response\n    return coefs[0], coefs[1:]\n\n\ndef simulate_and_solve(\n    key,\n    correlation: float,\n    n_points: int,\n    intercept: float,\n    coefs: jnp.ndarray,\n    noise: float,\n) -&gt; tuple[float, jnp.ndarray]:\n    key1, key2 = random.split(key)\n    \n    X = sample_covariates(key1, correlation=correlation, n_points=n_points)\n    y = sample_response(key2, covariates=X, coefs=coefs, intercept=intercept, noise=noise)\n\n    return solve(X, y)\n\n\ncoefs = jnp.asarray([0.5, 0.8])\ncorrelation = 0.75\nn_points = 30\nintercept = 0.0\nnoise = 0.3\nn_simulations: int = 1000\n\nkey = random.PRNGKey(123)\nkey, *subkeys = random.split(key, n_simulations + 1)\n\nb_, coefs_ = jax.vmap(simulate_and_solve, in_axes=(0, None, None, None, None, None))(\n    jnp.asarray(subkeys), correlation, n_points, intercept, coefs, noise    \n)\n\nfig, axs = plt.subplots(2, 2, figsize=(6, 6))\n\nax = axs[0, 0]\nax.hist(b_, bins=20, alpha=0.8, color=\"navy\", density=True)\nax.axvline(intercept, c=\"crimson\", linestyle=\"--\")\nax.set_xlabel(\"Intercept\")\n\nax = axs[0, 1]\nax.hist(coefs_[:, 0], bins=20, alpha=0.8, color=\"navy\", density=True)\nax.axvline(coefs[0], c=\"crimson\", linestyle=\"--\")\nax.set_xlabel(\"Coefficient $\\\\alpha_1$\")\n\nax = axs[1, 0]\nax.hist(coefs_[:, 1], bins=20, alpha=0.8, color=\"navy\", density=True)\nax.axvline(coefs[1], c=\"crimson\", linestyle=\"--\")\nax.set_xlabel(\"Coefficient $\\\\alpha_2$\")\n\nax = axs[1, 1]\nax.scatter(coefs_[:, 0], coefs_[:, 1], c=\"navy\", alpha=0.3, s=3)\nax.axvline(coefs[0], c=\"crimson\", linestyle=\"--\")\nax.axhline(coefs[1], c=\"crimson\", linestyle=\"--\")\n\nax.set_xlabel(\"Coefficient $\\\\alpha_1$\")\nax.set_ylabel(\"Coefficient $\\\\alpha_2$\")\n\n\nfig.tight_layout()\n\n\n\n\n\ncoef_\n\nArray([1.266081 , 1.2590657], dtype=float32)\n\n\n\nimport matplotlib.pyplot as plt\n\n\nfig, axs = plt.subplots(1, 3, figsize=(6, 2), dpi=250)\n\nax = axs[0]\nax.scatter(X[:, 0], X[:, 1], s=0.1, c=\"navy\", alpha=0.8, marker=\".\")\nax.set_xlabel(\"$X_1$\")\nax.set_ylabel(\"$X_2$\")\nax.plot(X[:, 0], 0.5 * X[:, 0], linewidth=1, c=\"k\", linestyle=\"-\")\n\nax = axs[1]\nax.scatter(X[:, 0], y, s=0.1, c=X[:, 1], cmap=\"magma\")\nax.set_xlabel(\"$X_1$\")\nax.set_ylabel(\"$Y$\")\n\nax.plot(X[:, 0], coefs[0] * X[:, 0], c=\"k\", linestyle=\"-\", linewidth=1)\n\nax = axs[2]\nax.scatter(X[:, 1], y, s=0.1, c=X[:, 0], cmap=\"magma\")\nax.set_xlabel(\"$X_2$\")\nax.set_ylabel(\"$Y$\")\nax.plot(X[:, 1], coefs[1] * X[:, 1], c=\"k\", linestyle=\"-\", linewidth=1)\n\nfig.tight_layout()\n\n\n\n\n\ncoef_\n\nArray([1.518926  , 0.69501114], dtype=float32)"
  },
  {
    "objectID": "private/prezenty.html",
    "href": "private/prezenty.html",
    "title": "Lista prezentów",
    "section": "",
    "text": "Bardzo mi miło, że rozważasz podarowanie mi prezentu! Jestem jednak minimalistą (walczącym z książkoholizmem) i staram się nie posiadać zbyt wielu rzeczy (lub duplikatów książek). Postanowiłem przygotować listę rzeczy, z których jednak będę bardzo zadowolony."
  },
  {
    "objectID": "private/prezenty.html#zawsze-w-modzie",
    "href": "private/prezenty.html#zawsze-w-modzie",
    "title": "Lista prezentów",
    "section": "Zawsze w modzie",
    "text": "Zawsze w modzie\n\nKartka lub list.\nPotwierdzenie datku na wybraną fundację charytatywną. Na przykład UNICEF, Against Malaria czy Krajowy Fundusz na rzecz Dzieci."
  },
  {
    "objectID": "private/prezenty.html#artykuły-biurowe",
    "href": "private/prezenty.html#artykuły-biurowe",
    "title": "Lista prezentów",
    "section": "Artykuły biurowe",
    "text": "Artykuły biurowe\n\nCienkopisy.\nOłówki"
  },
  {
    "objectID": "private/prezenty.html#książki-i-komiksy",
    "href": "private/prezenty.html#książki-i-komiksy",
    "title": "Lista prezentów",
    "section": "Książki i komiksy",
    "text": "Książki i komiksy\n\nNeil Gaiman, Sandman, tom 1, 2, 3 lub 4"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "I’m still maintaining my digital garden in a non-public manner1, but from time to time I encounter an idea which I’d just love to share with others or I am asked frequently about to make it a topic of a separate post."
  },
  {
    "objectID": "blog.html#footnotes",
    "href": "blog.html#footnotes",
    "title": "Blog",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSometimes my ideas are just silly and need more time before I decide whether they are useful at all or they are just noise. Sometimes it’s joint work with other people and I can’t share their intellectual property without approval. And I still haven’t figured out how to integrate Obsidian with Quarto, which are wonderful tools.↩︎"
  },
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "Publications",
    "section": "",
    "text": "Beyond normal: on the evaluation of mutual information estimators   \n    P. Czyż, F. Grabowski, J.E. Vogt, N. Beerenwinkel and A. Marx \n    Code\n    Manuscript\n    Jun, 2023\n  \n  \n    Bayesian quantification with black-box estimators   \n    A. Ziegler and P. Czyż \n    Code\n    Manuscript\n    Feb, 2023\n  \n  \n    CanSig: discovery of shared transcriptional states across cancer patients from single-cell RNA sequencing data   \n    J. Yates, F. Barkmann, P. Czyż, ..., N. Beerenwinkel and V. Boeva \n    Code\n    Manuscript\n    Apr, 2022\n  \n  \n    Limits to the rate of information transmission through the MAPK pathway   \n    F. Grabowski, P. Czyż, M. Kochańczyk and T. Lipniacki \n    Code\n    Manuscript\n    Mar, 2019\n  \n\n\nNo matching items"
  }
]