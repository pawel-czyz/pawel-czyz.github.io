<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Paweł Czyż">
<meta name="dcterms.date" content="2024-07-11">
<meta name="description" content="High-dimensional discrete models suffer from the issue with intractable normalising constants. A recent manuscript introducing discrete Fisher divergence helps to resolve this issue.">

<title>Paweł Czyż - Learning models with discrete Fisher divergence</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-PZ6W6SFE14"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
 
  gtag('consent', 'default', {
    'ad_storage': 'denied',
    'analytics_storage': 'denied'
  });
gtag('config', 'G-PZ6W6SFE14', { 'anonymize_ip': true});
</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">Paweł Czyż</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../index.html" rel="" target="">
 <span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../blog.html" rel="" target="">
 <span class="menu-text">Blog</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../publications.html" rel="" target="">
 <span class="menu-text">Publications</span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools ms-auto">
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#kullbackleibler-divergence" id="toc-kullbackleibler-divergence" class="nav-link active" data-scroll-target="#kullbackleibler-divergence">Kullback–Leibler divergence</a></li>
  <li><a href="#discrete-fisher-divergence" id="toc-discrete-fisher-divergence" class="nav-link" data-scroll-target="#discrete-fisher-divergence">Discrete Fisher divergence</a></li>
  <li><a href="#experiments-on-binary-data" id="toc-experiments-on-binary-data" class="nav-link" data-scroll-target="#experiments-on-binary-data">Experiments on binary data</a></li>
  <li><a href="#summary" id="toc-summary" class="nav-link" data-scroll-target="#summary">Summary</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Learning models with discrete Fisher divergence</h1>
</div>

<div>
  <div class="description">
    High-dimensional discrete models suffer from the issue with intractable normalising constants. A recent manuscript introducing discrete Fisher divergence helps to resolve this issue.
  </div>
</div>


<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Paweł Czyż </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">July 11, 2024</p>
    </div>
  </div>
  
    
  </div>
  

</header>

<p>ISBA 2024 was a phenomenal experience, with a lot of great people, conversations, and presented projects. Summarising it would take a separate post. Or two. Or three.</p>
<p>In this one, however, we’ll take a look only at one topic, presented by <a href="https://sites.google.com/view/takuomatsubara/home">Takuo Matsubara</a>. I remember that during the talk I was sitting amazed for the whole time: not only was the talk great and engaging, but also the method was particularly elegant. In January, I have been thinking about using the <a href="https://en.wikipedia.org/wiki/Fisher%27s_noncentral_hypergeometric_distribution">Fisher’s noncentral hypergeometric distribution</a>, but its normalising constant is computationally too expensive. I wish I had known Takuo’s paper back then!</p>
<p>The <a href="https://doi.org/10.1080/01621459.2023.2257891">paper</a> (or the <a href="https://arxiv.org/abs/2206.08420">preprint</a>) focuses on the following problem: we have a distribution defined on a discrete space <span class="math inline">\(\mathcal Y = \{0, 1, \dotsc, K-1\}^G\)</span> and its PMF is <em>everywhere positive</em> and <em>known up to the normalizing constant</em>. I.e., we have</p>
<p><span class="math display">\[
  p_\theta(y) = \frac{1}{\mathcal Z(\theta)} q_\theta(y),
\]</span></p>
<p>where <span class="math inline">\(q_\theta(y) &gt; 0\)</span> everywhere and this function is easy to evaluate. However, evaluating <span class="math inline">\(\mathcal Z(\theta)\)</span> is prohibitively expensive, as it usually requires <span class="math inline">\(O(K^G)\)</span> evaluations of <span class="math inline">\(q_\theta\)</span>.</p>
<p>Takuo’s framework helps to do inference in this model without the need to calculate <span class="math inline">\(\mathcal Z(\theta)\)</span> at all! In fact, it applies to more general spaces, although in this blog post we restrict our attention to the special case above.</p>
<p>Before we discuss this method, let’s quickly summarise how likelihood-based inference works.</p>
<section id="kullbackleibler-divergence" class="level2">
<h2 class="anchored" data-anchor-id="kullbackleibler-divergence">Kullback–Leibler divergence</h2>
<p>As Takuo explained in his presentation, once we observe data points <span class="math inline">\(y_1, \dotsc, y_N\)</span>, we can form the empirical distribution <span class="math inline">\(p_\text{emp} = \frac{1}{N} \sum_{n=1}^N \delta_{y_n}\)</span> and consider the Kullback–Leibler divergence <span class="math display">\[
\mathrm{KL}(p_\text{emp} \parallel p_\theta ) = -\frac{1}{N} \sum_{n=1}^N \log p_\theta(y_n) + \frac{1}{N} \sum_{n=1}^N \log p_\text{emp}(y_n) = -\frac{1}{N} \sum_{n=1}^N \log p_\theta(y_n) - H(p_\text{emp}).
\]</span></p>
<p>Hence, <span class="math inline">\(N \cdot \mathrm{KL}(p_\text{emp} \parallel p_\theta)\)</span> is the negative loglikelihood (up to an additive constant, which depends on the entropy of the empirical data distribution), which can be then optimised in maximum likelihood approaches. A nice property is that <span class="math inline">\(\mathrm{KL}(p_1\parallel p_2) \ge 0\)</span> and becomes <span class="math inline">\(0\)</span> if and only if <span class="math inline">\(p_1 = p_2\)</span>. When we have <span class="math inline">\(N\)</span> large enough, so that <span class="math inline">\(p_\text{emp}\)</span> is close to the data distribution, using maximum likelihood should result in a distribution being “the closest” to the data distribution amongh the family <span class="math inline">\(p_\theta\)</span>. In particular, under no misspecification we should rediscover the data distribution.</p>
<p>As Takuo explained, also Bayesian inference also proceeds in this manner: <span class="math display">\[
  p( \theta \mid y_1, \dotsc, y_N ) \propto p(\theta) \cdot \exp(-N\cdot \mathrm{KL}(p_\text{emp} \parallel p_\theta) ),
\]</span></p>
<p>where the entropy of <span class="math inline">\(p_\text{emp}\)</span> is effectively hidden in the proportionality constant.</p>
<p>Before Takuo’s talk, I haven’t thought about Bayesian inference in terms of the Kullback–Leibler divergence from the <em>empirical</em> data distribution <span class="math inline">\(p_\text{emp}\)</span> to the model <span class="math inline">\(p_\theta\)</span>: on continuous spaces <span class="math inline">\(p_\text{emp}\)</span> is atomic, while <span class="math inline">\(p_\theta\)</span> is (usually) not and <span class="math inline">\(\mathrm{KL}(p_\text{emp} \parallel p_\theta) = +\infty\)</span> for all parameters <span class="math inline">\(\theta\)</span>. However, for a discrete space <span class="math inline">\(\mathcal Y\)</span> both measures are necessarily atomic and this works nicely.</p>
<p>However, maximum likelihood, minimisation of the Kullback–Leibler divergence, and Bayesian inference all rely on having the access to <span class="math inline">\(\log p_\theta(y) = \log q_\theta(y) - \log \mathcal Z(\theta)\)</span>, which is not tractable in our case.</p>
</section>
<section id="discrete-fisher-divergence" class="level2">
<h2 class="anchored" data-anchor-id="discrete-fisher-divergence">Discrete Fisher divergence</h2>
<p>Define operators on <span class="math inline">\(\mathcal Y\)</span> incrementing and decrementing a specific position: <span class="math display">\[
  (\mathcal I_g y)_h = \begin{cases}
    (y_g + 1) \mod K &amp;\text{ if } g = h\\
    y_h &amp;\text{ otherwise}
  \end{cases}
\]</span> and <span class="math display">\[
  (\mathcal D_g y)_h = \begin{cases}
    (y_g - 1) \mod K &amp;\text{ if } g = h\\
    y_h &amp;\text{ otherwise}
  \end{cases}
\]</span></p>
<p>where the “mod <span class="math inline">\(K\)</span>” means that we “increment” <span class="math inline">\(K-1\)</span> to <span class="math inline">\(0\)</span> (and, conversely, “decrement” <span class="math inline">\(0\)</span> to <span class="math inline">\(K-1\)</span>). When the data are binary, <span class="math inline">\(K=2\)</span>, we have <span class="math inline">\(\mathcal I_g = \mathcal D_g\)</span> and they reduce to flipping the <span class="math inline">\(g\)</span>-th bit.</p>
<p>The <em>discrete Fisher divergence</em> is then given by <span class="math display">\[
  \mathrm{DFD}(p_1\parallel p_2) = \mathbb E_{y\sim p_2}\left[ \sum_{g=1}^G \left( \frac{ p_1( \mathcal D_gy ) }{ p_1(y) } \right)^2 -2 \frac{  p_1(y) }{ p_1(\mathcal I_g y) }  \right] + C(p_2),
\]</span></p>
<p>where <span class="math inline">\(C(p_2)\)</span> is a particular expression which does not depend on <span class="math inline">\(p_1\)</span>. In the paper you can find the formula for it, as well as the proof that <span class="math inline">\(\mathrm{DFD}(p_1\parallel p_2)\ge 0\)</span> and is zero if and only if <span class="math inline">\(p_1 = p_2\)</span>. Hence, a generalised posterior is proposed: <span class="math display">\[
  p^{\mathrm{DFD}} \propto p(\theta) \cdot \exp(-\tau N\cdot \mathrm{DFD}(p_\theta \parallel p_\text{emp} )),
\]</span></p>
<p>where <span class="math inline">\(\tau\)</span> is the temperature parameter, used in <a href="https://arxiv.org/abs/1306.6430">generalised Bayesian inference</a> and as a <a href="https://arxiv.org/abs/1412.3730">solution</a> to model <a href="https://arxiv.org/abs/1506.06101">misspecification</a>.</p>
<p>Note that <span class="math display">\[\begin{align*}
\mathrm{DFD}(p_\theta \parallel p_\text{emp}) &amp;= \frac{1}{N} \sum_{n=1}^N\sum_{g=1}^G \left(\frac{ p_\theta(\mathcal D_g y_n) }{ p_\theta(y_n) }\right)^2 - 2 \frac{p_\theta(y_n)}{p_\theta(\mathcal{I}_{g}y_n)}  + C(p_\text{emp}) \\
    &amp;= \frac{1}{N} \sum_{n=1}^N\sum_{g=1}^G \left(\frac{ q_\theta(\mathcal D_g y_n) }{ q_\theta(y_n) }\right)^2 - 2 \frac{q_\theta(y_n)}{q_\theta(\mathcal{I}_{g}y_n)}  + C(p_\text{emp})
\end{align*}
\]</span></p>
<p>meaning that it does not need the (intractable) normalising constant <span class="math inline">\(\mathcal Z(\theta)\)</span>! This comes at the price of <span class="math inline">\(O(NG)\)</span> evaluations of <span class="math inline">\(q_\theta\)</span> (rather than <span class="math inline">\(O(N)\)</span> calls to <span class="math inline">\(p_\theta\)</span> as in the likelihood-based methods) and the fact that we are using now a generalised Bayesian approach, rather than the typical Bayesian one (the usual question is “what is the value of the temperature <span class="math inline">\(\tau\)</span> one should use?”).</p>
<p>Overall, I very much like this idea, with potentially large impact on applications: in computational biology we use many discrete models and likelihood based methods could not be used because of intractable normalising constants.</p>
</section>
<section id="experiments-on-binary-data" class="level2">
<h2 class="anchored" data-anchor-id="experiments-on-binary-data">Experiments on binary data</h2>
<p>Let’s consider <span class="math inline">\(K=2\)</span>, so that <span class="math inline">\(\mathcal X = \{0, 1\}^G\)</span>. If <span class="math inline">\(\mathcal F_g := \mathcal I_g = \mathcal D_g\)</span> is the bitflip operator, the discrete Fisher divergence takes the form</p>
<p><span class="math display">\[
  \mathrm{DFD}(p_\theta \parallel p_\text{emp})
    = \frac{1}{N} \sum_{n=1}^N\sum_{g=1}^G \left(\frac{ q_\theta(\mathcal F_g y_n) }{ q_\theta(y_n) }\right)^2 - 2 \frac{q_\theta(y_n)}{q_\theta(\mathcal{F}_{g}y_n)}  + C(p_\text{emp}).
\]</span></p>
<p>I will simulate the data by tossing <span class="math inline">\(G\)</span> independent coins, each with its own bias <span class="math inline">\(\pi_g\)</span>. In this case the likelihood is tractable, as it is just <span class="math display">\[
  p_\pi( y ) = \prod_{g=1}^G \pi_g^{y_g} (1-\pi_g)^{1-y_g}.
\]</span></p>
<p>It is convenient to write this model in the exponential family form, by reparameterising it into log-odds, <span class="math inline">\(\alpha_g = \log\frac{\pi_g}{1-\pi_g}\)</span>. Then, we have <span class="math display">\[
  q_\alpha(y) = \exp\left( \sum_{g=1}^G \alpha_g y_g \right)
\]</span></p>
<div class="cell" data-execution_count="1">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> functools <span class="im">import</span> partial</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> typing <span class="im">import</span> Callable</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> jaxtyping <span class="im">import</span> Float, Int, Array</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> jax</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> jax.numpy <span class="im">as</span> jnp</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> jax.scipy <span class="im">import</span> optimize</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>rng <span class="op">=</span> np.random.default_rng(<span class="dv">42</span>)</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>n_samples: <span class="bu">int</span> <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>n_genes: <span class="bu">int</span> <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> logit(p):</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> np.log(p) <span class="op">-</span> np.log1p(<span class="op">-</span>p)</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> expit(x):</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> <span class="fl">1.0</span> <span class="op">/</span> (<span class="dv">1</span> <span class="op">+</span> np.exp(<span class="op">-</span>x))</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>true_bias <span class="op">=</span> np.linspace(<span class="fl">0.2</span>, <span class="fl">0.7</span>, n_genes)</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>true_alpha <span class="op">=</span> logit(true_bias)</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>Y <span class="op">=</span> rng.binomial(<span class="dv">1</span>, p<span class="op">=</span>true_bias, size<span class="op">=</span>(n_samples, n_genes))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>The simplest option is to calculate the maximum likelihood solution. Let’s do that:</p>
<div class="cell" data-execution_count="2">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> print_estimate_summary(alphas, name: <span class="bu">str</span> <span class="op">|</span> <span class="va">None</span> <span class="op">=</span> <span class="va">None</span>) <span class="op">-&gt;</span> <span class="va">None</span>:</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> name <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"----- </span><span class="sc">{</span>name<span class="sc">}</span><span class="ss"> -----"</span>)</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>  <span class="bu">print</span>(<span class="st">"Absolute error on the bias:"</span>)</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>  found_bias <span class="op">=</span> expit(alphas)</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>  <span class="bu">print</span>(true_bias <span class="op">-</span> found_bias)</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>  <span class="bu">print</span>(<span class="st">"True bias:"</span>)</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>  <span class="bu">print</span>(true_bias)</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>mle_bias <span class="op">=</span> np.mean(Y, axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>mle_alpha <span class="op">=</span> logit(mle_bias)</span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>print_estimate_summary(mle_alpha, name<span class="op">=</span><span class="st">"maximum likelihood"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>----- maximum likelihood -----
Absolute error on the bias:
[ 0.04       -0.07444444 -0.03888889  0.04666667 -0.02777778  0.00777778
  0.01333333 -0.02111111 -0.06555556 -0.03      ]
True bias:
[0.2        0.25555556 0.31111111 0.36666667 0.42222222 0.47777778
 0.53333333 0.58888889 0.64444444 0.7       ]</code></pre>
</div>
</div>
<p>As we have the luxury of doing full Bayesian inference, let’s try it. We will use a hierarchical model, in which the prior on <span class="math inline">\(\alpha\)</span> is a flexible normal distribution:</p>
<div class="cell" data-execution_count="3">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpyro</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpyro.distributions <span class="im">as</span> dist</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> independent_model(mutations):</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>    n_samples, n_genes <span class="op">=</span> mutations.shape</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>    mu <span class="op">=</span> numpyro.sample(<span class="st">"_mu"</span>, dist.Normal(<span class="fl">0.0</span>, <span class="fl">5.0</span>))</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>    sigma <span class="op">=</span> numpyro.sample(<span class="st">"_sigma"</span>, dist.HalfCauchy(scale<span class="op">=</span><span class="fl">3.0</span>))</span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>    z <span class="op">=</span>  numpyro.sample(<span class="st">'_z'</span>, dist.Normal(jnp.zeros(n_genes), <span class="dv">1</span>))</span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a>    alpha <span class="op">=</span> numpyro.deterministic(<span class="st">"alpha"</span>, mu <span class="op">+</span> sigma <span class="op">*</span> z)    </span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> numpyro.plate(<span class="st">"samples"</span>, n_samples, dim<span class="op">=-</span><span class="dv">2</span>):</span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a>        <span class="cf">with</span> numpyro.plate(<span class="st">"genes"</span>, n_genes, dim<span class="op">=-</span><span class="dv">1</span>):</span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a>            numpyro.sample(<span class="st">"obs"</span>, dist.BernoulliLogits(alpha[<span class="va">None</span>, :]), obs<span class="op">=</span>mutations)</span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a>key <span class="op">=</span> jax.random.PRNGKey(<span class="dv">0</span>)</span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a>key, subkey <span class="op">=</span> jax.random.split(key)</span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a>posterior <span class="op">=</span> numpyro.infer.MCMC(</span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a>  numpyro.infer.NUTS(independent_model),</span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a>  num_chains<span class="op">=</span><span class="dv">4</span>,</span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true" tabindex="-1"></a>  num_samples<span class="op">=</span><span class="dv">500</span>,</span>
<span id="cb4-27"><a href="#cb4-27" aria-hidden="true" tabindex="-1"></a>  num_warmup<span class="op">=</span><span class="dv">500</span>,</span>
<span id="cb4-28"><a href="#cb4-28" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb4-29"><a href="#cb4-29" aria-hidden="true" tabindex="-1"></a>posterior.run(subkey, mutations<span class="op">=</span>Y)</span>
<span id="cb4-30"><a href="#cb4-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-31"><a href="#cb4-31" aria-hidden="true" tabindex="-1"></a>posterior.print_summary()</span>
<span id="cb4-32"><a href="#cb4-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-33"><a href="#cb4-33" aria-hidden="true" tabindex="-1"></a>alpha_samples <span class="op">=</span> posterior.get_samples()[<span class="st">"alpha"</span>]</span>
<span id="cb4-34"><a href="#cb4-34" aria-hidden="true" tabindex="-1"></a>bias_samples <span class="op">=</span> expit(alpha_samples)</span>
<span id="cb4-35"><a href="#cb4-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-36"><a href="#cb4-36" aria-hidden="true" tabindex="-1"></a>posterior_mean_alpha <span class="op">=</span> alpha_samples.mean(axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb4-37"><a href="#cb4-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-38"><a href="#cb4-38" aria-hidden="true" tabindex="-1"></a>print_estimate_summary(posterior_mean_alpha, name<span class="op">=</span><span class="st">"posterior mean"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stderr">
<pre><code>/tmp/ipykernel_60846/1763543017.py:23: UserWarning: There are not enough devices to run parallel chains: expected 4 but got 1. Chains will be drawn sequentially. If you are running MCMC in CPU, consider using `numpyro.set_host_device_count(4)` at the beginning of your program. You can double-check how many devices are available in your system using `jax.local_device_count()`.
  posterior = numpyro.infer.MCMC(
  0%|          | 0/1000 [00:00&lt;?, ?it/s]warmup:   0%|          | 1/1000 [00:01&lt;22:10,  1.33s/it, 1 steps of size 2.34e+00. acc. prob=0.00]warmup:  45%|████▍     | 449/1000 [00:01&lt;00:01, 434.47it/s, 15 steps of size 3.03e-01. acc. prob=0.79]sample:  93%|█████████▎| 933/1000 [00:01&lt;00:00, 969.62it/s, 7 steps of size 1.94e-01. acc. prob=0.89] sample: 100%|██████████| 1000/1000 [00:01&lt;00:00, 646.29it/s, 15 steps of size 1.94e-01. acc. prob=0.89]
  0%|          | 0/1000 [00:00&lt;?, ?it/s]warmup:  49%|████▉     | 494/1000 [00:00&lt;00:00, 4937.19it/s, 15 steps of size 3.51e-01. acc. prob=0.79]sample: 100%|█████████▉| 998/1000 [00:00&lt;00:00, 4996.62it/s, 7 steps of size 2.14e-01. acc. prob=0.91] sample: 100%|██████████| 1000/1000 [00:00&lt;00:00, 4971.57it/s, 15 steps of size 2.14e-01. acc. prob=0.91]
  0%|          | 0/1000 [00:00&lt;?, ?it/s]warmup:  49%|████▉     | 489/1000 [00:00&lt;00:00, 4886.30it/s, 31 steps of size 1.42e-01. acc. prob=0.79]sample:  98%|█████████▊| 981/1000 [00:00&lt;00:00, 4903.42it/s, 15 steps of size 1.80e-01. acc. prob=0.94]sample: 100%|██████████| 1000/1000 [00:00&lt;00:00, 4884.00it/s, 15 steps of size 1.80e-01. acc. prob=0.94]
  0%|          | 0/1000 [00:00&lt;?, ?it/s]warmup:  48%|████▊     | 482/1000 [00:00&lt;00:00, 4814.70it/s, 47 steps of size 1.64e-01. acc. prob=0.79]sample:  98%|█████████▊| 979/1000 [00:00&lt;00:00, 4901.74it/s, 15 steps of size 1.66e-01. acc. prob=0.93]sample: 100%|██████████| 1000/1000 [00:00&lt;00:00, 4869.76it/s, 15 steps of size 1.66e-01. acc. prob=0.93]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
                mean       std    median      5.0%     95.0%     n_eff     r_hat
       _mu     -0.18      0.31     -0.18     -0.67      0.30    319.12      1.01
    _sigma      0.92      0.29      0.86      0.50      1.34    290.48      1.01
     _z[0]     -1.59      0.56     -1.55     -2.44     -0.68    428.13      1.01
     _z[1]     -0.58      0.41     -0.57     -1.21      0.11    439.12      1.01
     _z[2]     -0.48      0.40     -0.48     -1.12      0.16    452.72      1.01
     _z[3]     -0.64      0.42     -0.63     -1.37      0.01    472.24      1.00
     _z[4]     -0.03      0.39     -0.02     -0.65      0.60    450.87      1.01
     _z[5]      0.07      0.39      0.08     -0.51      0.70    432.61      1.01
     _z[6]      0.28      0.39      0.29     -0.37      0.90    408.89      1.01
     _z[7]      0.70      0.42      0.70      0.02      1.39    363.06      1.01
     _z[8]      1.19      0.49      1.19      0.39      2.00    343.35      1.00
     _z[9]      1.29      0.49      1.29      0.42      2.07    350.32      1.00

Number of divergences: 0
----- posterior mean -----
Absolute error on the bias:
[ 0.02347436 -0.08082746 -0.04482673  0.04107978 -0.02729267  0.00835171
  0.01834139 -0.01336413 -0.05330887 -0.01561599]
True bias:
[0.2        0.25555556 0.31111111 0.36666667 0.42222222 0.47777778
 0.53333333 0.58888889 0.64444444 0.7       ]</code></pre>
</div>
</div>
<p>This looks like both maximum likelihood and Bayesian inference do reasonable job in this problem. Let’s implement now the discrete Fisher divergence:</p>
<div class="cell" data-execution_count="4">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>DataPoint <span class="op">=</span> Int[Array, <span class="st">" G"</span>]</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> bitflip(g: <span class="bu">int</span>, y: DataPoint) <span class="op">-&gt;</span> DataPoint:</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> y.at[g].<span class="bu">set</span>(<span class="dv">1</span> <span class="op">-</span> y[g])</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> dfd_onepoint(</span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>  log_q: Callable[[DataPoint], <span class="bu">float</span>],</span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>  y: DataPoint,</span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>) <span class="op">-&gt;</span> <span class="bu">float</span>:</span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a>  log_qy: <span class="bu">float</span> <span class="op">=</span> log_q(y)</span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a>  <span class="kw">def</span> log_q_flip_fn(g: <span class="bu">int</span>):</span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> log_q(bitflip(g, y))</span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a>  log_qflipped <span class="op">=</span> jax.vmap(log_q_flip_fn)(jnp.arange(y.shape[<span class="dv">0</span>]))</span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-19"><a href="#cb7-19" aria-hidden="true" tabindex="-1"></a>  log_ratio <span class="op">=</span> log_qflipped <span class="op">-</span> log_qy</span>
<span id="cb7-20"><a href="#cb7-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-21"><a href="#cb7-21" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> jnp.<span class="bu">sum</span>( jnp.exp(<span class="dv">2</span> <span class="op">*</span> log_ratio) <span class="op">-</span> <span class="dv">2</span> <span class="op">*</span> jnp.exp(<span class="op">-</span>log_ratio))</span>
<span id="cb7-22"><a href="#cb7-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-23"><a href="#cb7-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-24"><a href="#cb7-24" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> dfd(log_q, ys) <span class="op">-&gt;</span> <span class="bu">float</span>:</span>
<span id="cb7-25"><a href="#cb7-25" aria-hidden="true" tabindex="-1"></a>  f <span class="op">=</span> partial(dfd_onepoint, log_q)</span>
<span id="cb7-26"><a href="#cb7-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-27"><a href="#cb7-27" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> jnp.mean(jax.vmap(f)(ys))</span>
<span id="cb7-28"><a href="#cb7-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-29"><a href="#cb7-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-30"><a href="#cb7-30" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> linear(alpha: Float[Array, <span class="st">" G"</span>], y: DataPoint) <span class="op">-&gt;</span> <span class="bu">float</span>:</span>
<span id="cb7-31"><a href="#cb7-31" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> jnp.<span class="bu">sum</span>(alpha <span class="op">*</span> y)</span>
<span id="cb7-32"><a href="#cb7-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-33"><a href="#cb7-33" aria-hidden="true" tabindex="-1"></a><span class="at">@jax.jit</span></span>
<span id="cb7-34"><a href="#cb7-34" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> loss(alpha):</span>
<span id="cb7-35"><a href="#cb7-35" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> dfd(partial(linear, alpha), Y)</span>
<span id="cb7-36"><a href="#cb7-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-37"><a href="#cb7-37" aria-hidden="true" tabindex="-1"></a>result <span class="op">=</span> optimize.minimize(loss, jnp.zeros(n_genes), method<span class="op">=</span><span class="st">"BFGS"</span>)</span>
<span id="cb7-38"><a href="#cb7-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-39"><a href="#cb7-39" aria-hidden="true" tabindex="-1"></a>dfd_alpha <span class="op">=</span> result.x</span>
<span id="cb7-40"><a href="#cb7-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-41"><a href="#cb7-41" aria-hidden="true" tabindex="-1"></a>print_estimate_summary(dfd_alpha, name<span class="op">=</span><span class="st">"DFD"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>----- DFD -----
Absolute error on the bias:
[ 0.03998992 -0.07451569 -0.0388778   0.04673009 -0.02779001  0.00777489
  0.01333484 -0.02109128 -0.06553831 -0.02999656]
True bias:
[0.2        0.25555556 0.31111111 0.36666667 0.42222222 0.47777778
 0.53333333 0.58888889 0.64444444 0.7       ]</code></pre>
</div>
</div>
<p>Wow, this looks pretty good to me! Let’s now visualise the performance of all methods:</p>
<div class="cell" data-execution_count="5">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>plt.style.use(<span class="st">"dark_background"</span>)</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>fig, axs <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">2</span>, sharex<span class="op">=</span><span class="va">True</span>, sharey<span class="op">=</span><span class="va">False</span>, figsize<span class="op">=</span>(<span class="dv">4</span>, <span class="dv">2</span>), dpi<span class="op">=</span><span class="dv">300</span>)</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> ax <span class="kw">in</span> axs:</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>  ax.spines[[<span class="st">"top"</span>, <span class="st">"right"</span>]].set_visible(<span class="va">False</span>)</span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>colors <span class="op">=</span> {</span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a>  <span class="st">"true"</span>: <span class="st">"white"</span>,</span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a>  <span class="st">"mle"</span>: <span class="st">"maroon"</span>,</span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a>  <span class="st">"posterior_sample"</span>: <span class="st">"lightgrey"</span>,</span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a>  <span class="st">"dfd"</span>: <span class="st">"gold"</span>,</span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> axs[<span class="dv">0</span>]</span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a>x_axis <span class="op">=</span> np.arange(<span class="dv">1</span>, n_genes <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"></a>ax.set_xticks(x_axis)</span>
<span id="cb9-18"><a href="#cb9-18" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="st">"$</span><span class="ch">\\</span><span class="st">alpha$"</span>)</span>
<span id="cb9-19"><a href="#cb9-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-20"><a href="#cb9-20" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> axs[<span class="dv">1</span>]</span>
<span id="cb9-21"><a href="#cb9-21" aria-hidden="true" tabindex="-1"></a>ax.set_xticks(x_axis)</span>
<span id="cb9-22"><a href="#cb9-22" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="st">"$</span><span class="ch">\\</span><span class="st">pi$"</span>)</span>
<span id="cb9-23"><a href="#cb9-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-24"><a href="#cb9-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-25"><a href="#cb9-25" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot(alpha_values, color, alpha<span class="op">=</span><span class="fl">1.0</span>, scatter: <span class="bu">bool</span> <span class="op">=</span> <span class="va">True</span>):</span>
<span id="cb9-26"><a href="#cb9-26" aria-hidden="true" tabindex="-1"></a>  ax <span class="op">=</span> axs[<span class="dv">0</span>]</span>
<span id="cb9-27"><a href="#cb9-27" aria-hidden="true" tabindex="-1"></a>  ax.plot(x_axis, alpha_values, c<span class="op">=</span>color, alpha<span class="op">=</span>alpha)</span>
<span id="cb9-28"><a href="#cb9-28" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> scatter:</span>
<span id="cb9-29"><a href="#cb9-29" aria-hidden="true" tabindex="-1"></a>    ax.scatter(x_axis, alpha_values, c<span class="op">=</span>color, alpha<span class="op">=</span>alpha)</span>
<span id="cb9-30"><a href="#cb9-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-31"><a href="#cb9-31" aria-hidden="true" tabindex="-1"></a>  ax <span class="op">=</span> axs[<span class="dv">1</span>]</span>
<span id="cb9-32"><a href="#cb9-32" aria-hidden="true" tabindex="-1"></a>  bias_values <span class="op">=</span> expit(alpha_values)</span>
<span id="cb9-33"><a href="#cb9-33" aria-hidden="true" tabindex="-1"></a>  ax.plot(x_axis, bias_values, c<span class="op">=</span>color, alpha<span class="op">=</span>alpha)</span>
<span id="cb9-34"><a href="#cb9-34" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span> scatter:</span>
<span id="cb9-35"><a href="#cb9-35" aria-hidden="true" tabindex="-1"></a>    ax.scatter(x_axis, bias_values, c<span class="op">=</span>color, alpha<span class="op">=</span>alpha)</span>
<span id="cb9-36"><a href="#cb9-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-37"><a href="#cb9-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-38"><a href="#cb9-38" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> sample <span class="kw">in</span> alpha_samples[::<span class="dv">40</span>]:</span>
<span id="cb9-39"><a href="#cb9-39" aria-hidden="true" tabindex="-1"></a>  plot(sample, colors[<span class="st">"posterior_sample"</span>], alpha<span class="op">=</span><span class="fl">0.1</span>, scatter<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb9-40"><a href="#cb9-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-41"><a href="#cb9-41" aria-hidden="true" tabindex="-1"></a>plot(true_alpha, colors[<span class="st">"true"</span>])</span>
<span id="cb9-42"><a href="#cb9-42" aria-hidden="true" tabindex="-1"></a>plot(mle_alpha, colors[<span class="st">"mle"</span>])</span>
<span id="cb9-43"><a href="#cb9-43" aria-hidden="true" tabindex="-1"></a>plot(dfd_alpha, colors[<span class="st">"dfd"</span>])</span>
<span id="cb9-44"><a href="#cb9-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-45"><a href="#cb9-45" aria-hidden="true" tabindex="-1"></a>fig.tight_layout()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="discrete-intractable-likelihood_files/figure-html/cell-6-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>This looks pretty good to me! Let’s do one more thing: sample from the DFD posterior:</p>
<div class="cell" data-execution_count="6">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> dfd_model(mutations, temperature):</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>    n_samples, n_genes <span class="op">=</span> mutations.shape</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>    mu <span class="op">=</span> numpyro.sample(<span class="st">"_mu"</span>, dist.Normal(<span class="fl">0.0</span>, <span class="fl">5.0</span>))</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>    sigma <span class="op">=</span> numpyro.sample(<span class="st">"_sigma"</span>, dist.HalfCauchy(scale<span class="op">=</span><span class="fl">3.0</span>))</span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a>    z <span class="op">=</span> numpyro.sample(<span class="st">'_z'</span>, dist.Normal(jnp.zeros(n_genes), <span class="dv">1</span>))</span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a>    alpha <span class="op">=</span> numpyro.deterministic(<span class="st">"alpha"</span>, mu <span class="op">+</span> sigma <span class="op">*</span> z)    </span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a>    numpyro.factor(<span class="st">"dfd"</span>, <span class="op">-</span>temperature <span class="op">*</span> n_samples <span class="op">*</span> loss(alpha))</span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-14"><a href="#cb10-14" aria-hidden="true" tabindex="-1"></a>dfd_samples <span class="op">=</span> {}</span>
<span id="cb10-15"><a href="#cb10-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-16"><a href="#cb10-16" aria-hidden="true" tabindex="-1"></a>temperature_range <span class="op">=</span> [<span class="fl">0.01</span>, <span class="fl">0.1</span>, <span class="fl">0.5</span>, <span class="fl">1.0</span>, <span class="fl">2.0</span>]</span>
<span id="cb10-17"><a href="#cb10-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-18"><a href="#cb10-18" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> temperature <span class="kw">in</span> temperature_range:</span>
<span id="cb10-19"><a href="#cb10-19" aria-hidden="true" tabindex="-1"></a>  key, subkey <span class="op">=</span> jax.random.split(key)</span>
<span id="cb10-20"><a href="#cb10-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-21"><a href="#cb10-21" aria-hidden="true" tabindex="-1"></a>  posterior <span class="op">=</span> numpyro.infer.MCMC(</span>
<span id="cb10-22"><a href="#cb10-22" aria-hidden="true" tabindex="-1"></a>    numpyro.infer.NUTS(dfd_model),</span>
<span id="cb10-23"><a href="#cb10-23" aria-hidden="true" tabindex="-1"></a>    num_chains<span class="op">=</span><span class="dv">4</span>,</span>
<span id="cb10-24"><a href="#cb10-24" aria-hidden="true" tabindex="-1"></a>    num_samples<span class="op">=</span><span class="dv">500</span>,</span>
<span id="cb10-25"><a href="#cb10-25" aria-hidden="true" tabindex="-1"></a>    num_warmup<span class="op">=</span><span class="dv">500</span>,</span>
<span id="cb10-26"><a href="#cb10-26" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb10-27"><a href="#cb10-27" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb10-28"><a href="#cb10-28" aria-hidden="true" tabindex="-1"></a>  posterior.run(</span>
<span id="cb10-29"><a href="#cb10-29" aria-hidden="true" tabindex="-1"></a>    subkey,</span>
<span id="cb10-30"><a href="#cb10-30" aria-hidden="true" tabindex="-1"></a>    mutations<span class="op">=</span>Y,</span>
<span id="cb10-31"><a href="#cb10-31" aria-hidden="true" tabindex="-1"></a>    temperature<span class="op">=</span>temperature,</span>
<span id="cb10-32"><a href="#cb10-32" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb10-33"><a href="#cb10-33" aria-hidden="true" tabindex="-1"></a>  <span class="bu">print</span>(<span class="ss">f"----- Temperature: </span><span class="sc">{</span>temperature<span class="sc">:.2f}</span><span class="ss"> -----"</span>)</span>
<span id="cb10-34"><a href="#cb10-34" aria-hidden="true" tabindex="-1"></a>  posterior.print_summary()</span>
<span id="cb10-35"><a href="#cb10-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-36"><a href="#cb10-36" aria-hidden="true" tabindex="-1"></a>  dfd_samples[temperature] <span class="op">=</span> posterior.get_samples()[<span class="st">"alpha"</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stderr">
<pre><code>/tmp/ipykernel_60846/908130843.py:21: UserWarning: There are not enough devices to run parallel chains: expected 4 but got 1. Chains will be drawn sequentially. If you are running MCMC in CPU, consider using `numpyro.set_host_device_count(4)` at the beginning of your program. You can double-check how many devices are available in your system using `jax.local_device_count()`.
  posterior = numpyro.infer.MCMC(
  0%|          | 0/1000 [00:00&lt;?, ?it/s]warmup:   0%|          | 1/1000 [00:01&lt;22:38,  1.36s/it, 1 steps of size 2.34e+00. acc. prob=0.00]warmup:  12%|█▏        | 121/1000 [00:01&lt;00:07, 114.70it/s, 31 steps of size 1.92e-01. acc. prob=0.77]warmup:  23%|██▎       | 229/1000 [00:01&lt;00:03, 230.07it/s, 31 steps of size 3.50e-02. acc. prob=0.78]warmup:  34%|███▍      | 345/1000 [00:01&lt;00:01, 365.50it/s, 31 steps of size 2.54e-01. acc. prob=0.78]warmup:  49%|████▊     | 486/1000 [00:01&lt;00:00, 544.55it/s, 31 steps of size 8.26e-02. acc. prob=0.78]sample:  63%|██████▎   | 631/1000 [00:01&lt;00:00, 723.12it/s, 15 steps of size 2.17e-01. acc. prob=0.77]sample:  78%|███████▊  | 783/1000 [00:01&lt;00:00, 897.42it/s, 15 steps of size 2.17e-01. acc. prob=0.76]sample:  94%|█████████▍| 944/1000 [00:02&lt;00:00, 1067.34it/s, 15 steps of size 2.17e-01. acc. prob=0.74]sample: 100%|██████████| 1000/1000 [00:02&lt;00:00, 475.86it/s, 15 steps of size 2.17e-01. acc. prob=0.74]
  0%|          | 0/1000 [00:00&lt;?, ?it/s]warmup:  14%|█▍        | 138/1000 [00:00&lt;00:00, 1376.21it/s, 31 steps of size 1.25e-01. acc. prob=0.78]warmup:  28%|██▊       | 276/1000 [00:00&lt;00:00, 1326.50it/s, 7 steps of size 1.89e-01. acc. prob=0.78] warmup:  44%|████▍     | 439/1000 [00:00&lt;00:00, 1459.82it/s, 7 steps of size 1.84e-01. acc. prob=0.79]sample:  61%|██████    | 606/1000 [00:00&lt;00:00, 1539.66it/s, 15 steps of size 3.01e-01. acc. prob=0.89]sample:  78%|███████▊  | 776/1000 [00:00&lt;00:00, 1596.10it/s, 15 steps of size 3.01e-01. acc. prob=0.83]sample:  94%|█████████▍| 938/1000 [00:00&lt;00:00, 1601.95it/s, 15 steps of size 3.01e-01. acc. prob=0.84]sample: 100%|██████████| 1000/1000 [00:00&lt;00:00, 1551.24it/s, 15 steps of size 3.01e-01. acc. prob=0.84]
  0%|          | 0/1000 [00:00&lt;?, ?it/s]warmup:  11%|█         | 109/1000 [00:00&lt;00:00, 1078.77it/s, 31 steps of size 4.75e-01. acc. prob=0.78]warmup:  26%|██▌       | 257/1000 [00:00&lt;00:00, 1312.59it/s, 15 steps of size 6.13e-02. acc. prob=0.78]warmup:  41%|████      | 407/1000 [00:00&lt;00:00, 1395.57it/s, 7 steps of size 5.14e-01. acc. prob=0.79] sample:  56%|█████▋    | 563/1000 [00:00&lt;00:00, 1458.26it/s, 15 steps of size 2.28e-01. acc. prob=0.78]sample:  73%|███████▎  | 727/1000 [00:00&lt;00:00, 1521.32it/s, 15 steps of size 2.28e-01. acc. prob=0.78]sample:  89%|████████▉ | 891/1000 [00:00&lt;00:00, 1559.12it/s, 15 steps of size 2.28e-01. acc. prob=0.76]sample: 100%|██████████| 1000/1000 [00:00&lt;00:00, 1489.34it/s, 15 steps of size 2.28e-01. acc. prob=0.77]
  0%|          | 0/1000 [00:00&lt;?, ?it/s]warmup:  11%|█         | 107/1000 [00:00&lt;00:00, 1066.82it/s, 15 steps of size 4.23e-01. acc. prob=0.77]warmup:  21%|██▏       | 214/1000 [00:00&lt;00:00, 1067.49it/s, 15 steps of size 6.69e-02. acc. prob=0.78]warmup:  32%|███▎      | 325/1000 [00:00&lt;00:00, 1077.00it/s, 63 steps of size 1.11e-01. acc. prob=0.78]warmup:  46%|████▌     | 457/1000 [00:00&lt;00:00, 1171.20it/s, 15 steps of size 4.02e-01. acc. prob=0.79]sample:  57%|█████▊    | 575/1000 [00:00&lt;00:00, 1060.12it/s, 31 steps of size 1.64e-01. acc. prob=0.92]sample:  68%|██████▊   | 683/1000 [00:00&lt;00:00, 1043.34it/s, 31 steps of size 1.64e-01. acc. prob=0.93]sample:  79%|███████▉  | 791/1000 [00:00&lt;00:00, 1054.47it/s, 15 steps of size 1.64e-01. acc. prob=0.92]sample:  90%|█████████ | 903/1000 [00:00&lt;00:00, 1073.16it/s, 31 steps of size 1.64e-01. acc. prob=0.91]sample: 100%|██████████| 1000/1000 [00:00&lt;00:00, 1078.38it/s, 31 steps of size 1.64e-01. acc. prob=0.91]
/tmp/ipykernel_60846/908130843.py:21: UserWarning: There are not enough devices to run parallel chains: expected 4 but got 1. Chains will be drawn sequentially. If you are running MCMC in CPU, consider using `numpyro.set_host_device_count(4)` at the beginning of your program. You can double-check how many devices are available in your system using `jax.local_device_count()`.
  posterior = numpyro.infer.MCMC(
  0%|          | 0/1000 [00:00&lt;?, ?it/s]warmup:   0%|          | 1/1000 [00:01&lt;24:04,  1.45s/it, 1 steps of size 2.34e+00. acc. prob=0.00]warmup:  10%|█         | 105/1000 [00:01&lt;00:09, 94.11it/s, 15 steps of size 3.41e-01. acc. prob=0.77]warmup:  18%|█▊        | 185/1000 [00:01&lt;00:04, 174.29it/s, 31 steps of size 1.46e-01. acc. prob=0.78]warmup:  28%|██▊       | 275/1000 [00:01&lt;00:02, 275.43it/s, 63 steps of size 6.17e-02. acc. prob=0.78]warmup:  36%|███▌      | 357/1000 [00:01&lt;00:01, 365.77it/s, 31 steps of size 2.04e-01. acc. prob=0.78]warmup:  46%|████▋     | 465/1000 [00:01&lt;00:01, 502.31it/s, 15 steps of size 4.07e-02. acc. prob=0.78]sample:  57%|█████▋    | 570/1000 [00:02&lt;00:00, 619.46it/s, 15 steps of size 1.29e-01. acc. prob=0.91]sample:  69%|██████▉   | 691/1000 [00:02&lt;00:00, 757.85it/s, 31 steps of size 1.29e-01. acc. prob=0.90]sample:  81%|████████  | 810/1000 [00:02&lt;00:00, 866.06it/s, 15 steps of size 1.29e-01. acc. prob=0.89]sample:  93%|█████████▎| 934/1000 [00:02&lt;00:00, 963.63it/s, 15 steps of size 1.29e-01. acc. prob=0.89]sample: 100%|██████████| 1000/1000 [00:02&lt;00:00, 415.10it/s, 15 steps of size 1.29e-01. acc. prob=0.89]
  0%|          | 0/1000 [00:00&lt;?, ?it/s]warmup:   8%|▊         | 81/1000 [00:00&lt;00:01, 805.87it/s, 15 steps of size 1.37e-02. acc. prob=0.76]warmup:  17%|█▋        | 168/1000 [00:00&lt;00:00, 838.95it/s, 31 steps of size 7.85e-02. acc. prob=0.77]warmup:  25%|██▌       | 252/1000 [00:00&lt;00:00, 818.74it/s, 2 steps of size 2.54e-01. acc. prob=0.78] warmup:  35%|███▌      | 351/1000 [00:00&lt;00:00, 882.08it/s, 31 steps of size 1.21e-01. acc. prob=0.78]warmup:  46%|████▌     | 460/1000 [00:00&lt;00:00, 954.33it/s, 15 steps of size 3.02e-01. acc. prob=0.79]sample:  56%|█████▌    | 561/1000 [00:00&lt;00:00, 969.55it/s, 63 steps of size 1.16e-01. acc. prob=0.94]sample:  66%|██████▋   | 663/1000 [00:00&lt;00:00, 985.18it/s, 47 steps of size 1.16e-01. acc. prob=0.95]sample:  76%|███████▌  | 762/1000 [00:00&lt;00:00, 986.43it/s, 31 steps of size 1.16e-01. acc. prob=0.95]sample:  87%|████████▋ | 868/1000 [00:00&lt;00:00, 1008.45it/s, 15 steps of size 1.16e-01. acc. prob=0.94]sample:  98%|█████████▊| 979/1000 [00:01&lt;00:00, 1036.30it/s, 63 steps of size 1.16e-01. acc. prob=0.94]sample: 100%|██████████| 1000/1000 [00:01&lt;00:00, 968.19it/s, 15 steps of size 1.16e-01. acc. prob=0.94]
  0%|          | 0/1000 [00:00&lt;?, ?it/s]warmup:   9%|▉         | 88/1000 [00:00&lt;00:01, 876.77it/s, 31 steps of size 2.28e-02. acc. prob=0.76]warmup:  18%|█▊        | 176/1000 [00:00&lt;00:00, 873.79it/s, 15 steps of size 1.80e-01. acc. prob=0.78]warmup:  26%|██▋       | 264/1000 [00:00&lt;00:00, 857.51it/s, 63 steps of size 4.83e-02. acc. prob=0.78]warmup:  36%|███▌      | 357/1000 [00:00&lt;00:00, 883.21it/s, 31 steps of size 1.13e-01. acc. prob=0.78]warmup:  45%|████▍     | 446/1000 [00:00&lt;00:00, 861.09it/s, 31 steps of size 1.80e-01. acc. prob=0.79]sample:  54%|█████▎    | 535/1000 [00:00&lt;00:00, 869.41it/s, 15 steps of size 1.32e-01. acc. prob=0.95]sample:  63%|██████▎   | 628/1000 [00:00&lt;00:00, 888.53it/s, 15 steps of size 1.32e-01. acc. prob=0.94]sample:  74%|███████▎  | 737/1000 [00:00&lt;00:00, 947.07it/s, 63 steps of size 1.32e-01. acc. prob=0.92]sample:  85%|████████▌ | 853/1000 [00:00&lt;00:00, 1011.16it/s, 63 steps of size 1.32e-01. acc. prob=0.93]sample:  96%|█████████▌| 962/1000 [00:01&lt;00:00, 1033.78it/s, 15 steps of size 1.32e-01. acc. prob=0.93]sample: 100%|██████████| 1000/1000 [00:01&lt;00:00, 952.63it/s, 31 steps of size 1.32e-01. acc. prob=0.93]
  0%|          | 0/1000 [00:00&lt;?, ?it/s]warmup:   9%|▊         | 86/1000 [00:00&lt;00:01, 852.33it/s, 127 steps of size 3.91e-02. acc. prob=0.77]warmup:  17%|█▋        | 172/1000 [00:00&lt;00:00, 843.41it/s, 63 steps of size 1.43e-01. acc. prob=0.78]warmup:  28%|██▊       | 278/1000 [00:00&lt;00:00, 940.86it/s, 127 steps of size 8.49e-02. acc. prob=0.78]warmup:  42%|████▎     | 425/1000 [00:00&lt;00:00, 1145.04it/s, 23 steps of size 1.69e-01. acc. prob=0.79]sample:  54%|█████▍    | 540/1000 [00:00&lt;00:00, 1096.10it/s, 15 steps of size 1.14e-01. acc. prob=0.96]sample:  65%|██████▌   | 651/1000 [00:00&lt;00:00, 983.26it/s, 15 steps of size 1.14e-01. acc. prob=0.96] sample:  75%|███████▌  | 752/1000 [00:00&lt;00:00, 957.32it/s, 15 steps of size 1.14e-01. acc. prob=0.96]sample:  85%|████████▍ | 849/1000 [00:00&lt;00:00, 909.34it/s, 31 steps of size 1.14e-01. acc. prob=0.96]sample:  94%|█████████▍| 941/1000 [00:00&lt;00:00, 888.96it/s, 31 steps of size 1.14e-01. acc. prob=0.96]sample: 100%|██████████| 1000/1000 [00:01&lt;00:00, 943.11it/s, 15 steps of size 1.14e-01. acc. prob=0.96]
/tmp/ipykernel_60846/908130843.py:21: UserWarning: There are not enough devices to run parallel chains: expected 4 but got 1. Chains will be drawn sequentially. If you are running MCMC in CPU, consider using `numpyro.set_host_device_count(4)` at the beginning of your program. You can double-check how many devices are available in your system using `jax.local_device_count()`.
  posterior = numpyro.infer.MCMC(
  0%|          | 0/1000 [00:00&lt;?, ?it/s]warmup:   0%|          | 1/1000 [00:01&lt;22:51,  1.37s/it, 1 steps of size 2.34e+00. acc. prob=0.00]warmup:   7%|▋         | 66/1000 [00:01&lt;00:15, 61.80it/s, 79 steps of size 2.44e-02. acc. prob=0.76]warmup:  13%|█▎        | 132/1000 [00:01&lt;00:06, 132.57it/s, 79 steps of size 1.10e-01. acc. prob=0.77]warmup:  19%|█▊        | 187/1000 [00:01&lt;00:04, 192.42it/s, 191 steps of size 6.23e-02. acc. prob=0.77]warmup:  26%|██▌       | 262/1000 [00:01&lt;00:02, 286.73it/s, 255 steps of size 1.26e-02. acc. prob=0.78]warmup:  32%|███▏      | 322/1000 [00:01&lt;00:01, 346.03it/s, 31 steps of size 9.07e-02. acc. prob=0.78] warmup:  39%|███▉      | 393/1000 [00:01&lt;00:01, 424.76it/s, 15 steps of size 1.07e-01. acc. prob=0.78]warmup:  46%|████▌     | 456/1000 [00:02&lt;00:01, 466.23it/s, 95 steps of size 3.43e-02. acc. prob=0.78]sample:  52%|█████▏    | 518/1000 [00:02&lt;00:00, 499.05it/s, 15 steps of size 6.86e-02. acc. prob=0.93]sample:  60%|█████▉    | 595/1000 [00:02&lt;00:00, 569.38it/s, 15 steps of size 6.86e-02. acc. prob=0.93]sample:  67%|██████▋   | 671/1000 [00:02&lt;00:00, 620.11it/s, 79 steps of size 6.86e-02. acc. prob=0.92]sample:  75%|███████▌  | 753/1000 [00:02&lt;00:00, 674.96it/s, 15 steps of size 6.86e-02. acc. prob=0.92]sample:  83%|████████▎ | 834/1000 [00:02&lt;00:00, 709.85it/s, 63 steps of size 6.86e-02. acc. prob=0.92]sample:  91%|█████████ | 910/1000 [00:02&lt;00:00, 723.79it/s, 95 steps of size 6.86e-02. acc. prob=0.92]sample:  99%|█████████▊| 986/1000 [00:02&lt;00:00, 713.96it/s, 63 steps of size 6.86e-02. acc. prob=0.92]sample: 100%|██████████| 1000/1000 [00:02&lt;00:00, 354.69it/s, 15 steps of size 6.86e-02. acc. prob=0.92]
  0%|          | 0/1000 [00:00&lt;?, ?it/s]warmup:   7%|▋         | 70/1000 [00:00&lt;00:01, 697.31it/s, 15 steps of size 2.99e-02. acc. prob=0.76]warmup:  14%|█▍        | 140/1000 [00:00&lt;00:01, 655.18it/s, 95 steps of size 6.36e-02. acc. prob=0.77]warmup:  21%|██        | 206/1000 [00:00&lt;00:01, 633.97it/s, 31 steps of size 9.03e-02. acc. prob=0.78]warmup:  28%|██▊       | 275/1000 [00:00&lt;00:01, 652.89it/s, 31 steps of size 7.94e-02. acc. prob=0.78]warmup:  34%|███▍      | 343/1000 [00:00&lt;00:00, 659.87it/s, 63 steps of size 4.77e-02. acc. prob=0.78]warmup:  41%|████      | 410/1000 [00:00&lt;00:00, 652.81it/s, 31 steps of size 4.90e-02. acc. prob=0.78]warmup:  48%|████▊     | 480/1000 [00:00&lt;00:00, 661.88it/s, 127 steps of size 1.30e-01. acc. prob=0.78]sample:  55%|█████▍    | 547/1000 [00:00&lt;00:00, 648.31it/s, 23 steps of size 6.96e-02. acc. prob=0.95] sample:  61%|██████    | 612/1000 [00:00&lt;00:00, 630.96it/s, 31 steps of size 6.96e-02. acc. prob=0.94]sample:  68%|██████▊   | 676/1000 [00:01&lt;00:00, 621.71it/s, 63 steps of size 6.96e-02. acc. prob=0.94]sample:  74%|███████▍  | 745/1000 [00:01&lt;00:00, 638.28it/s, 63 steps of size 6.96e-02. acc. prob=0.94]sample:  81%|████████  | 809/1000 [00:01&lt;00:00, 626.69it/s, 111 steps of size 6.96e-02. acc. prob=0.93]sample:  87%|████████▋ | 872/1000 [00:01&lt;00:00, 600.58it/s, 15 steps of size 6.96e-02. acc. prob=0.93] sample:  94%|█████████▍| 938/1000 [00:01&lt;00:00, 615.82it/s, 47 steps of size 6.96e-02. acc. prob=0.93]sample: 100%|██████████| 1000/1000 [00:01&lt;00:00, 635.73it/s, 15 steps of size 6.96e-02. acc. prob=0.93]
  0%|          | 0/1000 [00:00&lt;?, ?it/s]warmup:   8%|▊         | 78/1000 [00:00&lt;00:01, 778.00it/s, 15 steps of size 1.88e-02. acc. prob=0.76]warmup:  16%|█▌        | 156/1000 [00:00&lt;00:01, 671.59it/s, 159 steps of size 4.04e-02. acc. prob=0.77]warmup:  22%|██▎       | 225/1000 [00:00&lt;00:01, 666.63it/s, 15 steps of size 1.91e-01. acc. prob=0.78] warmup:  29%|██▉       | 293/1000 [00:00&lt;00:01, 633.64it/s, 15 steps of size 1.07e-01. acc. prob=0.78]warmup:  36%|███▋      | 364/1000 [00:00&lt;00:00, 658.81it/s, 15 steps of size 5.66e-02. acc. prob=0.78]warmup:  45%|████▌     | 454/1000 [00:00&lt;00:00, 733.90it/s, 191 steps of size 2.03e-02. acc. prob=0.78]sample:  53%|█████▎    | 529/1000 [00:00&lt;00:00, 671.25it/s, 47 steps of size 6.27e-02. acc. prob=0.93] sample:  61%|██████    | 608/1000 [00:00&lt;00:00, 703.53it/s, 31 steps of size 6.27e-02. acc. prob=0.91]sample:  68%|██████▊   | 685/1000 [00:00&lt;00:00, 722.10it/s, 15 steps of size 6.27e-02. acc. prob=0.91]sample:  77%|███████▋  | 769/1000 [00:01&lt;00:00, 754.85it/s, 47 steps of size 6.27e-02. acc. prob=0.91]sample:  86%|████████▌ | 856/1000 [00:01&lt;00:00, 788.83it/s, 15 steps of size 6.27e-02. acc. prob=0.92]sample:  94%|█████████▎| 936/1000 [00:01&lt;00:00, 784.19it/s, 15 steps of size 6.27e-02. acc. prob=0.92]sample: 100%|██████████| 1000/1000 [00:01&lt;00:00, 725.87it/s, 47 steps of size 6.27e-02. acc. prob=0.91]
  0%|          | 0/1000 [00:00&lt;?, ?it/s]warmup:   6%|▋         | 64/1000 [00:00&lt;00:01, 638.15it/s, 7 steps of size 7.81e-03. acc. prob=0.75]warmup:  13%|█▎        | 131/1000 [00:00&lt;00:01, 654.81it/s, 63 steps of size 8.02e-02. acc. prob=0.77]warmup:  20%|██        | 201/1000 [00:00&lt;00:01, 652.50it/s, 383 steps of size 5.09e-02. acc. prob=0.78]warmup:  30%|██▉       | 295/1000 [00:00&lt;00:00, 755.78it/s, 255 steps of size 3.45e-02. acc. prob=0.78]warmup:  37%|███▋      | 371/1000 [00:00&lt;00:00, 754.53it/s, 63 steps of size 5.71e-02. acc. prob=0.78] warmup:  46%|████▌     | 455/1000 [00:00&lt;00:00, 771.29it/s, 255 steps of size 2.88e-02. acc. prob=0.78]sample:  53%|█████▎    | 533/1000 [00:00&lt;00:00, 744.85it/s, 47 steps of size 5.87e-02. acc. prob=0.92] sample:  62%|██████▏   | 619/1000 [00:00&lt;00:00, 777.03it/s, 111 steps of size 5.87e-02. acc. prob=0.92]sample:  70%|██████▉   | 697/1000 [00:00&lt;00:00, 757.16it/s, 15 steps of size 5.87e-02. acc. prob=0.93] sample:  78%|███████▊  | 785/1000 [00:01&lt;00:00, 791.90it/s, 63 steps of size 5.87e-02. acc. prob=0.91]sample:  86%|████████▋ | 865/1000 [00:01&lt;00:00, 788.40it/s, 31 steps of size 5.87e-02. acc. prob=0.92]sample:  95%|█████████▌| 953/1000 [00:01&lt;00:00, 811.67it/s, 63 steps of size 5.87e-02. acc. prob=0.92]sample: 100%|██████████| 1000/1000 [00:01&lt;00:00, 766.68it/s, 95 steps of size 5.87e-02. acc. prob=0.92]
/tmp/ipykernel_60846/908130843.py:21: UserWarning: There are not enough devices to run parallel chains: expected 4 but got 1. Chains will be drawn sequentially. If you are running MCMC in CPU, consider using `numpyro.set_host_device_count(4)` at the beginning of your program. You can double-check how many devices are available in your system using `jax.local_device_count()`.
  posterior = numpyro.infer.MCMC(
  0%|          | 0/1000 [00:00&lt;?, ?it/s]warmup:   0%|          | 1/1000 [00:01&lt;24:31,  1.47s/it, 1 steps of size 2.34e+00. acc. prob=0.00]warmup:   5%|▍         | 49/1000 [00:01&lt;00:22, 43.00it/s, 63 steps of size 1.83e-02. acc. prob=0.75]warmup:  11%|█         | 108/1000 [00:01&lt;00:08, 104.09it/s, 111 steps of size 1.33e-01. acc. prob=0.77]warmup:  16%|█▌        | 158/1000 [00:01&lt;00:05, 158.20it/s, 63 steps of size 2.03e-02. acc. prob=0.77] warmup:  21%|██▏       | 214/1000 [00:01&lt;00:03, 224.04it/s, 7 steps of size 5.26e-02. acc. prob=0.78] warmup:  28%|██▊       | 276/1000 [00:01&lt;00:02, 296.36it/s, 255 steps of size 6.84e-02. acc. prob=0.78]warmup:  34%|███▍      | 338/1000 [00:02&lt;00:01, 364.10it/s, 135 steps of size 8.97e-02. acc. prob=0.78]warmup:  40%|████      | 400/1000 [00:02&lt;00:01, 421.03it/s, 95 steps of size 6.67e-02. acc. prob=0.78] warmup:  46%|████▋     | 463/1000 [00:02&lt;00:01, 466.42it/s, 511 steps of size 3.62e-02. acc. prob=0.78]sample:  52%|█████▏    | 521/1000 [00:02&lt;00:00, 493.23it/s, 15 steps of size 4.22e-02. acc. prob=0.90] sample:  58%|█████▊    | 584/1000 [00:02&lt;00:00, 527.39it/s, 63 steps of size 4.22e-02. acc. prob=0.93]sample:  64%|██████▍   | 643/1000 [00:02&lt;00:00, 506.63it/s, 63 steps of size 4.22e-02. acc. prob=0.94]sample:  70%|███████   | 705/1000 [00:02&lt;00:00, 536.83it/s, 15 steps of size 4.22e-02. acc. prob=0.95]sample:  76%|███████▋  | 765/1000 [00:02&lt;00:00, 553.06it/s, 175 steps of size 4.22e-02. acc. prob=0.94]sample:  83%|████████▎ | 832/1000 [00:02&lt;00:00, 582.79it/s, 87 steps of size 4.22e-02. acc. prob=0.95] sample:  89%|████████▉ | 893/1000 [00:03&lt;00:00, 564.51it/s, 15 steps of size 4.22e-02. acc. prob=0.95]sample:  95%|█████████▌| 951/1000 [00:03&lt;00:00, 566.00it/s, 79 steps of size 4.22e-02. acc. prob=0.95]sample: 100%|██████████| 1000/1000 [00:03&lt;00:00, 310.19it/s, 31 steps of size 4.22e-02. acc. prob=0.95]
  0%|          | 0/1000 [00:00&lt;?, ?it/s]warmup:  10%|▉         | 99/1000 [00:00&lt;00:00, 980.29it/s, 63 steps of size 2.15e-02. acc. prob=0.77]warmup:  20%|█▉        | 198/1000 [00:00&lt;00:01, 681.27it/s, 79 steps of size 8.71e-02. acc. prob=0.78]warmup:  27%|██▋       | 272/1000 [00:00&lt;00:01, 570.17it/s, 7 steps of size 2.75e-02. acc. prob=0.78] warmup:  34%|███▍      | 339/1000 [00:00&lt;00:01, 599.04it/s, 39 steps of size 6.02e-02. acc. prob=0.78]warmup:  42%|████▏     | 424/1000 [00:00&lt;00:00, 672.02it/s, 95 steps of size 1.16e-01. acc. prob=0.78]warmup:  50%|████▉     | 495/1000 [00:00&lt;00:00, 606.26it/s, 15 steps of size 2.15e-02. acc. prob=0.78]sample:  56%|█████▌    | 559/1000 [00:00&lt;00:00, 603.29it/s, 79 steps of size 3.82e-02. acc. prob=0.93]sample:  62%|██████▏   | 622/1000 [00:00&lt;00:00, 608.28it/s, 15 steps of size 3.82e-02. acc. prob=0.94]sample:  68%|██████▊   | 685/1000 [00:01&lt;00:00, 608.70it/s, 31 steps of size 3.82e-02. acc. prob=0.94]sample:  75%|███████▍  | 747/1000 [00:01&lt;00:00, 605.91it/s, 15 steps of size 3.82e-02. acc. prob=0.93]sample:  81%|████████  | 809/1000 [00:01&lt;00:00, 608.14it/s, 15 steps of size 3.82e-02. acc. prob=0.94]sample:  87%|████████▋ | 872/1000 [00:01&lt;00:00, 614.24it/s, 95 steps of size 3.82e-02. acc. prob=0.94]sample:  93%|█████████▎| 934/1000 [00:01&lt;00:00, 606.82it/s, 63 steps of size 3.82e-02. acc. prob=0.94]sample: 100%|█████████▉| 996/1000 [00:01&lt;00:00, 609.79it/s, 31 steps of size 3.82e-02. acc. prob=0.94]sample: 100%|██████████| 1000/1000 [00:01&lt;00:00, 619.40it/s, 31 steps of size 3.82e-02. acc. prob=0.94]
  0%|          | 0/1000 [00:00&lt;?, ?it/s]warmup:   9%|▉         | 92/1000 [00:00&lt;00:00, 914.80it/s, 31 steps of size 1.27e-02. acc. prob=0.76]warmup:  18%|█▊        | 184/1000 [00:00&lt;00:01, 538.61it/s, 63 steps of size 1.11e-01. acc. prob=0.78]warmup:  25%|██▍       | 247/1000 [00:00&lt;00:01, 402.53it/s, 63 steps of size 4.24e-02. acc. prob=0.78]warmup:  30%|███       | 305/1000 [00:00&lt;00:01, 446.32it/s, 15 steps of size 1.24e-01. acc. prob=0.78]warmup:  37%|███▋      | 368/1000 [00:00&lt;00:01, 492.28it/s, 103 steps of size 9.20e-02. acc. prob=0.78]warmup:  43%|████▎     | 433/1000 [00:00&lt;00:01, 533.02it/s, 47 steps of size 7.91e-02. acc. prob=0.79] warmup:  50%|████▉     | 499/1000 [00:00&lt;00:00, 566.63it/s, 191 steps of size 2.37e-02. acc. prob=0.78]sample:  56%|█████▌    | 562/1000 [00:01&lt;00:00, 578.15it/s, 167 steps of size 4.39e-02. acc. prob=0.93]sample:  62%|██████▏   | 624/1000 [00:01&lt;00:00, 587.64it/s, 63 steps of size 4.39e-02. acc. prob=0.92] sample:  70%|██████▉   | 698/1000 [00:01&lt;00:00, 631.73it/s, 31 steps of size 4.39e-02. acc. prob=0.91]sample:  76%|███████▋  | 763/1000 [00:01&lt;00:00, 598.29it/s, 95 steps of size 4.39e-02. acc. prob=0.92]sample:  83%|████████▎ | 831/1000 [00:01&lt;00:00, 616.18it/s, 127 steps of size 4.39e-02. acc. prob=0.92]sample:  91%|█████████ | 911/1000 [00:01&lt;00:00, 667.86it/s, 15 steps of size 4.39e-02. acc. prob=0.92] sample:  98%|█████████▊| 979/1000 [00:01&lt;00:00, 643.35it/s, 95 steps of size 4.39e-02. acc. prob=0.92]sample: 100%|██████████| 1000/1000 [00:01&lt;00:00, 579.95it/s, 15 steps of size 4.39e-02. acc. prob=0.92]
  0%|          | 0/1000 [00:00&lt;?, ?it/s]warmup:   6%|▌         | 58/1000 [00:00&lt;00:01, 577.58it/s, 15 steps of size 2.84e-02. acc. prob=0.75]warmup:  13%|█▎        | 133/1000 [00:00&lt;00:01, 678.18it/s, 31 steps of size 1.48e-01. acc. prob=0.77]warmup:  20%|██        | 201/1000 [00:00&lt;00:01, 524.95it/s, 31 steps of size 1.07e-01. acc. prob=0.78]warmup:  26%|██▋       | 265/1000 [00:00&lt;00:01, 560.97it/s, 191 steps of size 3.58e-02. acc. prob=0.78]warmup:  34%|███▎      | 335/1000 [00:00&lt;00:01, 603.47it/s, 159 steps of size 8.17e-02. acc. prob=0.78]warmup:  40%|███▉      | 398/1000 [00:00&lt;00:00, 607.00it/s, 135 steps of size 1.05e-01. acc. prob=0.78]warmup:  46%|████▌     | 461/1000 [00:00&lt;00:00, 586.03it/s, 127 steps of size 1.43e-02. acc. prob=0.78]sample:  52%|█████▏    | 523/1000 [00:00&lt;00:00, 591.42it/s, 103 steps of size 3.85e-02. acc. prob=0.96]sample:  60%|█████▉    | 595/1000 [00:00&lt;00:00, 627.54it/s, 31 steps of size 3.85e-02. acc. prob=0.95] sample:  66%|██████▌   | 659/1000 [00:01&lt;00:00, 601.07it/s, 15 steps of size 3.85e-02. acc. prob=0.95]sample:  72%|███████▏  | 720/1000 [00:01&lt;00:00, 586.66it/s, 47 steps of size 3.85e-02. acc. prob=0.94]sample:  78%|███████▊  | 780/1000 [00:01&lt;00:00, 578.14it/s, 47 steps of size 3.85e-02. acc. prob=0.94]sample:  84%|████████▍ | 843/1000 [00:01&lt;00:00, 591.36it/s, 95 steps of size 3.85e-02. acc. prob=0.94]sample:  90%|█████████ | 903/1000 [00:01&lt;00:00, 581.78it/s, 31 steps of size 3.85e-02. acc. prob=0.94]sample:  97%|█████████▋| 971/1000 [00:01&lt;00:00, 608.61it/s, 31 steps of size 3.85e-02. acc. prob=0.94]sample: 100%|██████████| 1000/1000 [00:01&lt;00:00, 595.93it/s, 31 steps of size 3.85e-02. acc. prob=0.94]
/tmp/ipykernel_60846/908130843.py:21: UserWarning: There are not enough devices to run parallel chains: expected 4 but got 1. Chains will be drawn sequentially. If you are running MCMC in CPU, consider using `numpyro.set_host_device_count(4)` at the beginning of your program. You can double-check how many devices are available in your system using `jax.local_device_count()`.
  posterior = numpyro.infer.MCMC(
  0%|          | 0/1000 [00:00&lt;?, ?it/s]warmup:   0%|          | 1/1000 [00:01&lt;22:11,  1.33s/it, 1 steps of size 2.34e+00. acc. prob=0.00]warmup:   6%|▋         | 64/1000 [00:01&lt;00:15, 61.23it/s, 319 steps of size 9.04e-03. acc. prob=0.75]warmup:  11%|█         | 111/1000 [00:01&lt;00:08, 110.52it/s, 63 steps of size 3.10e-02. acc. prob=0.76]warmup:  16%|█▋        | 164/1000 [00:01&lt;00:04, 169.33it/s, 447 steps of size 1.95e-02. acc. prob=0.77]warmup:  22%|██▏       | 217/1000 [00:01&lt;00:03, 230.57it/s, 47 steps of size 4.15e-02. acc. prob=0.78] warmup:  28%|██▊       | 276/1000 [00:01&lt;00:02, 299.04it/s, 383 steps of size 2.14e-02. acc. prob=0.78]warmup:  33%|███▎      | 329/1000 [00:01&lt;00:01, 346.68it/s, 447 steps of size 2.53e-02. acc. prob=0.78]warmup:  39%|███▉      | 391/1000 [00:02&lt;00:01, 408.89it/s, 111 steps of size 6.06e-02. acc. prob=0.78]warmup:  46%|████▌     | 455/1000 [00:02&lt;00:01, 465.08it/s, 159 steps of size 2.42e-02. acc. prob=0.78]sample:  51%|█████     | 512/1000 [00:02&lt;00:01, 430.42it/s, 31 steps of size 3.50e-02. acc. prob=0.95] sample:  56%|█████▋    | 563/1000 [00:02&lt;00:01, 413.99it/s, 319 steps of size 3.50e-02. acc. prob=0.95]sample:  61%|██████    | 610/1000 [00:02&lt;00:00, 423.45it/s, 127 steps of size 3.50e-02. acc. prob=0.95]sample:  66%|██████▌   | 662/1000 [00:02&lt;00:00, 442.35it/s, 191 steps of size 3.50e-02. acc. prob=0.94]sample:  71%|███████   | 711/1000 [00:02&lt;00:00, 453.16it/s, 63 steps of size 3.50e-02. acc. prob=0.94] sample:  76%|███████▋  | 763/1000 [00:02&lt;00:00, 465.17it/s, 191 steps of size 3.50e-02. acc. prob=0.93]sample:  81%|████████  | 812/1000 [00:02&lt;00:00, 465.18it/s, 15 steps of size 3.50e-02. acc. prob=0.93] sample:  87%|████████▋ | 870/1000 [00:03&lt;00:00, 494.00it/s, 159 steps of size 3.50e-02. acc. prob=0.93]sample:  92%|█████████▏| 921/1000 [00:03&lt;00:00, 477.23it/s, 15 steps of size 3.50e-02. acc. prob=0.93] sample:  97%|█████████▋| 970/1000 [00:03&lt;00:00, 454.22it/s, 15 steps of size 3.50e-02. acc. prob=0.93]sample: 100%|██████████| 1000/1000 [00:03&lt;00:00, 297.69it/s, 15 steps of size 3.50e-02. acc. prob=0.93]
  0%|          | 0/1000 [00:00&lt;?, ?it/s]warmup:   6%|▌         | 61/1000 [00:00&lt;00:01, 605.09it/s, 63 steps of size 6.04e-03. acc. prob=0.74]warmup:  12%|█▏        | 122/1000 [00:00&lt;00:01, 584.52it/s, 54 steps of size 1.01e-02. acc. prob=0.76]warmup:  18%|█▊        | 181/1000 [00:00&lt;00:01, 576.41it/s, 63 steps of size 5.78e-02. acc. prob=0.77]warmup:  24%|██▍       | 239/1000 [00:00&lt;00:01, 535.90it/s, 31 steps of size 1.15e-01. acc. prob=0.78]warmup:  29%|██▉       | 293/1000 [00:00&lt;00:01, 442.11it/s, 15 steps of size 1.02e-01. acc. prob=0.78]warmup:  34%|███▍      | 340/1000 [00:00&lt;00:01, 449.55it/s, 127 steps of size 7.71e-02. acc. prob=0.78]warmup:  40%|████      | 401/1000 [00:00&lt;00:01, 487.35it/s, 255 steps of size 4.94e-02. acc. prob=0.78]warmup:  45%|████▌     | 454/1000 [00:00&lt;00:01, 474.94it/s, 767 steps of size 8.95e-03. acc. prob=0.78]sample:  50%|█████     | 503/1000 [00:01&lt;00:01, 439.51it/s, 175 steps of size 2.65e-02. acc. prob=0.96]sample:  55%|█████▍    | 548/1000 [00:01&lt;00:01, 432.44it/s, 175 steps of size 2.65e-02. acc. prob=0.95]sample:  59%|█████▉    | 592/1000 [00:01&lt;00:01, 403.54it/s, 31 steps of size 2.65e-02. acc. prob=0.96] sample:  63%|██████▎   | 634/1000 [00:01&lt;00:00, 402.29it/s, 255 steps of size 2.65e-02. acc. prob=0.95]sample:  68%|██████▊   | 678/1000 [00:01&lt;00:00, 412.09it/s, 255 steps of size 2.65e-02. acc. prob=0.95]sample:  72%|███████▏  | 723/1000 [00:01&lt;00:00, 415.83it/s, 287 steps of size 2.65e-02. acc. prob=0.95]sample:  76%|███████▋  | 765/1000 [00:01&lt;00:00, 383.89it/s, 15 steps of size 2.65e-02. acc. prob=0.95] sample:  81%|████████  | 811/1000 [00:01&lt;00:00, 402.33it/s, 95 steps of size 2.65e-02. acc. prob=0.95]sample:  85%|████████▌ | 852/1000 [00:01&lt;00:00, 388.70it/s, 95 steps of size 2.65e-02. acc. prob=0.95]sample:  91%|█████████ | 910/1000 [00:02&lt;00:00, 438.71it/s, 127 steps of size 2.65e-02. acc. prob=0.96]sample:  96%|█████████▌| 955/1000 [00:02&lt;00:00, 409.88it/s, 255 steps of size 2.65e-02. acc. prob=0.96]sample: 100%|█████████▉| 997/1000 [00:02&lt;00:00, 398.76it/s, 15 steps of size 2.65e-02. acc. prob=0.95] sample: 100%|██████████| 1000/1000 [00:02&lt;00:00, 433.97it/s, 127 steps of size 2.65e-02. acc. prob=0.95]
  0%|          | 0/1000 [00:00&lt;?, ?it/s]warmup:   5%|▍         | 49/1000 [00:00&lt;00:01, 488.03it/s, 39 steps of size 1.41e-02. acc. prob=0.74]warmup:  11%|█         | 110/1000 [00:00&lt;00:01, 554.03it/s, 111 steps of size 8.65e-02. acc. prob=0.77]warmup:  17%|█▋        | 166/1000 [00:00&lt;00:01, 531.65it/s, 31 steps of size 4.23e-02. acc. prob=0.77] warmup:  22%|██▏       | 220/1000 [00:00&lt;00:01, 469.94it/s, 31 steps of size 3.82e-02. acc. prob=0.78]warmup:  27%|██▋       | 270/1000 [00:00&lt;00:01, 470.52it/s, 191 steps of size 1.21e-02. acc. prob=0.78]warmup:  32%|███▏      | 318/1000 [00:00&lt;00:01, 424.56it/s, 239 steps of size 2.51e-02. acc. prob=0.78]warmup:  38%|███▊      | 378/1000 [00:00&lt;00:01, 470.23it/s, 95 steps of size 3.77e-02. acc. prob=0.78] warmup:  43%|████▎     | 427/1000 [00:00&lt;00:01, 465.21it/s, 47 steps of size 2.71e-02. acc. prob=0.78]warmup:  48%|████▊     | 475/1000 [00:01&lt;00:01, 414.12it/s, 191 steps of size 2.49e-02. acc. prob=0.78]sample:  52%|█████▏    | 520/1000 [00:01&lt;00:01, 421.12it/s, 199 steps of size 3.39e-02. acc. prob=0.91]sample:  56%|█████▋    | 564/1000 [00:01&lt;00:01, 421.10it/s, 15 steps of size 3.39e-02. acc. prob=0.93] sample:  61%|██████    | 610/1000 [00:01&lt;00:00, 430.16it/s, 95 steps of size 3.39e-02. acc. prob=0.93]sample:  65%|██████▌   | 654/1000 [00:01&lt;00:00, 402.25it/s, 79 steps of size 3.39e-02. acc. prob=0.93]sample:  72%|███████▏  | 716/1000 [00:01&lt;00:00, 461.58it/s, 31 steps of size 3.39e-02. acc. prob=0.93]sample:  76%|███████▋  | 764/1000 [00:01&lt;00:00, 465.96it/s, 95 steps of size 3.39e-02. acc. prob=0.92]sample:  81%|████████  | 812/1000 [00:01&lt;00:00, 452.43it/s, 175 steps of size 3.39e-02. acc. prob=0.93]sample:  86%|████████▌ | 861/1000 [00:01&lt;00:00, 462.43it/s, 15 steps of size 3.39e-02. acc. prob=0.92] sample:  91%|█████████ | 908/1000 [00:02&lt;00:00, 439.42it/s, 15 steps of size 3.39e-02. acc. prob=0.92]sample:  95%|█████████▌| 953/1000 [00:02&lt;00:00, 438.54it/s, 31 steps of size 3.39e-02. acc. prob=0.92]sample: 100%|██████████| 1000/1000 [00:02&lt;00:00, 453.73it/s, 15 steps of size 3.39e-02. acc. prob=0.92]
  0%|          | 0/1000 [00:00&lt;?, ?it/s]warmup:   7%|▋         | 72/1000 [00:00&lt;00:01, 678.61it/s, 255 steps of size 1.16e-02. acc. prob=0.75]warmup:  14%|█▍        | 140/1000 [00:00&lt;00:01, 454.79it/s, 39 steps of size 7.82e-03. acc. prob=0.77]warmup:  19%|█▉        | 190/1000 [00:00&lt;00:02, 352.63it/s, 159 steps of size 3.29e-02. acc. prob=0.77]warmup:  24%|██▎       | 236/1000 [00:00&lt;00:02, 381.03it/s, 79 steps of size 6.74e-02. acc. prob=0.78] warmup:  28%|██▊       | 278/1000 [00:00&lt;00:02, 344.27it/s, 15 steps of size 6.07e-02. acc. prob=0.78]warmup:  32%|███▎      | 325/1000 [00:00&lt;00:01, 375.88it/s, 45 steps of size 2.14e-02. acc. prob=0.78]warmup:  37%|███▋      | 374/1000 [00:00&lt;00:01, 405.79it/s, 159 steps of size 4.15e-02. acc. prob=0.78]warmup:  43%|████▎     | 427/1000 [00:01&lt;00:01, 440.22it/s, 7 steps of size 3.09e-02. acc. prob=0.78]  warmup:  47%|████▋     | 473/1000 [00:01&lt;00:01, 396.00it/s, 15 steps of size 5.15e-02. acc. prob=0.78]sample:  52%|█████▏    | 520/1000 [00:01&lt;00:01, 415.52it/s, 63 steps of size 3.29e-02. acc. prob=0.91]sample:  56%|█████▋    | 564/1000 [00:01&lt;00:01, 418.76it/s, 15 steps of size 3.29e-02. acc. prob=0.92]sample:  62%|██████▏   | 623/1000 [00:01&lt;00:00, 462.00it/s, 159 steps of size 3.29e-02. acc. prob=0.91]sample:  67%|██████▋   | 671/1000 [00:01&lt;00:00, 425.96it/s, 15 steps of size 3.29e-02. acc. prob=0.91] sample:  72%|███████▏  | 715/1000 [00:01&lt;00:00, 410.81it/s, 7 steps of size 3.29e-02. acc. prob=0.91] sample:  76%|███████▌  | 757/1000 [00:01&lt;00:00, 392.43it/s, 15 steps of size 3.29e-02. acc. prob=0.91]sample:  81%|████████  | 808/1000 [00:01&lt;00:00, 423.23it/s, 31 steps of size 3.29e-02. acc. prob=0.92]sample:  85%|████████▌ | 852/1000 [00:02&lt;00:00, 409.48it/s, 95 steps of size 3.29e-02. acc. prob=0.92]sample:  90%|████████▉ | 897/1000 [00:02&lt;00:00, 416.68it/s, 143 steps of size 3.29e-02. acc. prob=0.92]sample:  94%|█████████▍| 944/1000 [00:02&lt;00:00, 430.65it/s, 255 steps of size 3.29e-02. acc. prob=0.91]sample:  99%|█████████▉| 988/1000 [00:02&lt;00:00, 406.61it/s, 111 steps of size 3.29e-02. acc. prob=0.91]sample: 100%|██████████| 1000/1000 [00:02&lt;00:00, 412.09it/s, 31 steps of size 3.29e-02. acc. prob=0.91]</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>----- Temperature: 0.01 -----

                mean       std    median      5.0%     95.0%     n_eff     r_hat
       _mu     -0.13      0.26     -0.14     -0.52      0.31    727.16      1.00
    _sigma      0.43      0.29      0.39      0.00      0.82    547.15      1.00
     _z[0]     -0.85      0.99     -0.92     -2.65      0.62   1237.86      1.00
     _z[1]     -0.30      0.82     -0.32     -1.66      1.03   1398.45      1.00
     _z[2]     -0.26      0.84     -0.28     -1.67      1.07   1504.57      1.00
     _z[3]     -0.33      0.87     -0.34     -1.72      1.10   1825.18      1.00
     _z[4]     -0.02      0.85     -0.02     -1.50      1.29   1658.80      1.00
     _z[5]      0.01      0.84      0.01     -1.28      1.36   1918.23      1.00
     _z[6]      0.14      0.81      0.13     -1.14      1.43   1666.27      1.00
     _z[7]      0.30      0.79      0.32     -1.08      1.52   1669.95      1.00
     _z[8]      0.56      0.85      0.57     -0.82      1.99   1633.24      1.00
     _z[9]      0.57      0.86      0.59     -0.86      1.94   1463.64      1.00

Number of divergences: 21
----- Temperature: 0.10 -----

                mean       std    median      5.0%     95.0%     n_eff     r_hat
       _mu     -0.14      0.33     -0.13     -0.61      0.40    228.95      1.00
    _sigma      0.91      0.28      0.86      0.52      1.29    226.41      1.03
     _z[0]     -1.75      0.58     -1.72     -2.65     -0.76    228.53      1.02
     _z[1]     -0.60      0.43     -0.59     -1.32      0.05    280.44      1.01
     _z[2]     -0.50      0.42     -0.50     -1.17      0.23    356.88      1.00
     _z[3]     -0.65      0.42     -0.65     -1.29      0.07    299.21      1.01
     _z[4]     -0.05      0.41     -0.06     -0.75      0.59    342.85      1.00
     _z[5]      0.03      0.40      0.04     -0.60      0.68    315.56      1.00
     _z[6]      0.23      0.41      0.22     -0.42      0.88    315.50      1.00
     _z[7]      0.62      0.43      0.62     -0.13      1.26    330.74      1.01
     _z[8]      1.13      0.47      1.14      0.34      1.85    310.08      1.01
     _z[9]      1.25      0.49      1.24      0.45      2.03    286.92      1.02

Number of divergences: 0
----- Temperature: 0.50 -----

                mean       std    median      5.0%     95.0%     n_eff     r_hat
       _mu     -0.13      0.27     -0.14     -0.55      0.33    193.62      1.03
    _sigma      0.92      0.25      0.87      0.54      1.26    216.76      1.00
     _z[0]     -1.76      0.51     -1.73     -2.68     -1.00    258.98      1.01
     _z[1]     -0.64      0.33     -0.64     -1.21     -0.10    278.92      1.02
     _z[2]     -0.54      0.32     -0.54     -1.06     -0.01    272.43      1.02
     _z[3]     -0.70      0.33     -0.70     -1.21     -0.09    275.79      1.02
     _z[4]     -0.06      0.30     -0.07     -0.53      0.43    239.25      1.02
     _z[5]      0.03      0.31      0.03     -0.46      0.53    240.01      1.02
     _z[6]      0.25      0.32      0.24     -0.28      0.79    220.40      1.02
     _z[7]      0.67      0.36      0.67      0.06      1.23    193.99      1.02
     _z[8]      1.19      0.43      1.19      0.48      1.89    181.31      1.01
     _z[9]      1.31      0.46      1.31      0.51      2.01    181.15      1.01

Number of divergences: 1
----- Temperature: 1.00 -----

                mean       std    median      5.0%     95.0%     n_eff     r_hat
       _mu     -0.19      0.27     -0.17     -0.56      0.29    176.79      1.01
    _sigma      0.92      0.24      0.87      0.57      1.26    215.09      1.01
     _z[0]     -1.70      0.51     -1.70     -2.63     -0.93    211.78      1.00
     _z[1]     -0.60      0.33     -0.60     -1.20     -0.12    197.15      1.00
     _z[2]     -0.50      0.32     -0.51     -1.01      0.02    200.85      1.00
     _z[3]     -0.65      0.34     -0.66     -1.17     -0.05    203.18      1.00
     _z[4]     -0.02      0.28     -0.03     -0.45      0.50    188.09      1.01
     _z[5]      0.07      0.28      0.06     -0.36      0.59    189.28      1.01
     _z[6]      0.30      0.28      0.28     -0.17      0.78    195.43      1.01
     _z[7]      0.72      0.31      0.70      0.17      1.21    177.20      1.02
     _z[8]      1.24      0.37      1.22      0.61      1.84    186.58      1.02
     _z[9]      1.35      0.39      1.33      0.67      1.97    187.23      1.02

Number of divergences: 0
----- Temperature: 2.00 -----

                mean       std    median      5.0%     95.0%     n_eff     r_hat
       _mu     -0.21      0.33     -0.20     -0.72      0.28    173.57      1.01
    _sigma      0.93      0.25      0.89      0.57      1.30    182.60      1.02
     _z[0]     -1.66      0.54     -1.62     -2.54     -0.85    153.43      1.04
     _z[1]     -0.57      0.37     -0.57     -1.12      0.03    158.81      1.03
     _z[2]     -0.47      0.35     -0.47     -1.01      0.10    161.59      1.03
     _z[3]     -0.62      0.37     -0.63     -1.20     -0.03    158.92      1.03
     _z[4]      0.01      0.33      0.00     -0.55      0.58    176.08      1.02
     _z[5]      0.10      0.33      0.09     -0.42      0.70    186.36      1.02
     _z[6]      0.33      0.34      0.32     -0.26      0.87    191.94      1.01
     _z[7]      0.75      0.38      0.74      0.08      1.32    200.20      1.01
     _z[8]      1.26      0.45      1.26      0.49      1.96    200.88      1.00
     _z[9]      1.37      0.47      1.37      0.57      2.10    199.99      1.00

Number of divergences: 1</code></pre>
</div>
</div>
<p>We can visualise the samples and compare them with the usual Bayesian approach:</p>
<div class="cell" data-execution_count="7">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>fig, axs <span class="op">=</span> plt.subplots(n_genes, <span class="bu">len</span>(temperature_range) <span class="op">+</span> <span class="dv">1</span>)</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n_genes):</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(temperature_range) <span class="op">+</span> <span class="dv">1</span>):</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>    samples <span class="op">=</span> alpha_samples <span class="cf">if</span> j <span class="op">==</span> <span class="bu">len</span>(temperature_range) <span class="cf">else</span>  dfd_samples[temperature_range[j]]</span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>    color <span class="op">=</span> colors[<span class="st">"posterior_sample"</span>] <span class="cf">if</span> j <span class="op">==</span> <span class="bu">len</span>(temperature_range) <span class="cf">else</span> colors[<span class="st">"dfd"</span>]</span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> j <span class="op">==</span> <span class="bu">len</span>(temperature_range):</span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a>      c_alpha <span class="op">=</span> <span class="fl">1.0</span></span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a>      c_alpha <span class="op">=</span> <span class="fl">0.3</span> <span class="op">+</span> <span class="fl">0.7</span> <span class="op">*</span> j <span class="op">/</span> <span class="bu">len</span>(temperature_range)</span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a>    bias_samples <span class="op">=</span> expit(samples)</span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-15"><a href="#cb13-15" aria-hidden="true" tabindex="-1"></a>    ax <span class="op">=</span> axs[i, j]</span>
<span id="cb13-16"><a href="#cb13-16" aria-hidden="true" tabindex="-1"></a>    ax.hist(bias_samples[:, i], bins<span class="op">=</span>np.linspace(<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">20</span>), density<span class="op">=</span><span class="va">True</span>, alpha<span class="op">=</span>c_alpha, color<span class="op">=</span>color)</span>
<span id="cb13-17"><a href="#cb13-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-18"><a href="#cb13-18" aria-hidden="true" tabindex="-1"></a>    ax.set_yticks([]) </span>
<span id="cb13-19"><a href="#cb13-19" aria-hidden="true" tabindex="-1"></a>    ax.set_xticks([])</span>
<span id="cb13-20"><a href="#cb13-20" aria-hidden="true" tabindex="-1"></a>    ax.spines[[<span class="st">"top"</span>, <span class="st">"right"</span>]].set_visible(<span class="va">False</span>)</span>
<span id="cb13-21"><a href="#cb13-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-22"><a href="#cb13-22" aria-hidden="true" tabindex="-1"></a>    ax.axvline(true_bias[i], color<span class="op">=</span>colors[<span class="st">"true"</span>], linestyle<span class="op">=</span><span class="st">"--"</span>  )</span>
<span id="cb13-23"><a href="#cb13-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-24"><a href="#cb13-24" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, ax <span class="kw">in</span> <span class="bu">enumerate</span>(axs[:, <span class="dv">0</span>]):</span>
<span id="cb13-25"><a href="#cb13-25" aria-hidden="true" tabindex="-1"></a>  ax.set_ylabel(<span class="ss">f"</span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb13-26"><a href="#cb13-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-27"><a href="#cb13-27" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> j, ax <span class="kw">in</span> <span class="bu">enumerate</span>(axs[<span class="dv">0</span>, :]):</span>
<span id="cb13-28"><a href="#cb13-28" aria-hidden="true" tabindex="-1"></a>  name <span class="op">=</span> <span class="st">"Bayes"</span> <span class="cf">if</span> j <span class="op">==</span> <span class="bu">len</span>(temperature_range) <span class="cf">else</span>  <span class="ss">f"$T$=</span><span class="sc">{</span>temperature_range[j]<span class="sc">:.2f}</span><span class="ss">"</span></span>
<span id="cb13-29"><a href="#cb13-29" aria-hidden="true" tabindex="-1"></a>  ax.set_title(name)</span>
<span id="cb13-30"><a href="#cb13-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-31"><a href="#cb13-31" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> ax <span class="kw">in</span> axs[<span class="op">-</span><span class="dv">1</span>, :]:</span>
<span id="cb13-32"><a href="#cb13-32" aria-hidden="true" tabindex="-1"></a>  ax.set_xticks([<span class="dv">0</span>, <span class="fl">0.5</span>, <span class="dv">1</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="discrete-intractable-likelihood_files/figure-html/cell-8-output-1.png" width="566" height="431"></p>
</div>
</div>
<p>If the temperature is too low, the prior seems to have too large influence. In this case <span class="math inline">\(T = 0.1\)</span> seems to be the most reasonable, yielding reasonable uncertainty quantification.</p>
</section>
<section id="summary" class="level2">
<h2 class="anchored" data-anchor-id="summary">Summary</h2>
<p>Overall, I have to say that I very much like the discrete Fisher divergence idea and I congratulate the authors on this article!</p>
<p>In the manuscript there are some experiments with the <a href="https://en.wikipedia.org/wiki/Ising_model">Ising model</a> – I will need to try this method on the related high-dimensional problems, as (on the toy problem above) it showed some very promising performance. This method is also easy to implement and fast to run.</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>