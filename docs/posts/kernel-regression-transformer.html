<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Paweł Czyż">
<meta name="dcterms.date" content="2024-01-26">
<meta name="description" content="We’ll go from kernel regression to attention.">

<title>Paweł Czyż - From kernel regression to the transformer</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">Paweł Czyż</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../index.html" rel="" target="">
 <span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../blog.html" rel="" target="">
 <span class="menu-text">Blog</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../publications.html" rel="" target="">
 <span class="menu-text">Publications</span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools ms-auto">
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#kernel-regression" id="toc-kernel-regression" class="nav-link active" data-scroll-target="#kernel-regression">Kernel regression</a>
  <ul class="collapse">
  <li><a href="#masked-training" id="toc-masked-training" class="nav-link" data-scroll-target="#masked-training">Masked training</a></li>
  <li><a href="#multi-headed-kernel-regression" id="toc-multi-headed-kernel-regression" class="nav-link" data-scroll-target="#multi-headed-kernel-regression">Multi-headed kernel regression</a></li>
  </ul></li>
  <li><a href="#attention" id="toc-attention" class="nav-link" data-scroll-target="#attention">Attention</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">From kernel regression to the transformer</h1>
  <div class="quarto-categories">
    <div class="quarto-category">Deep learning</div>
  </div>
  </div>

<div>
  <div class="description">
    We’ll go from kernel regression to attention.
  </div>
</div>


<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Paweł Czyż </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">January 26, 2024</p>
    </div>
  </div>
  
    
  </div>
  

</header>

<p>I remember that when we read <a href="https://arxiv.org/abs/1706.03762"><em>Attention is all you need</em></a> at a journal club back in 2020, I did not really understand what attention was<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>.</p>
<p>Fortunately for me, <a href="https://arxiv.org/abs/1908.11775"><em>Transformer dissection</em></a> paper and Cosma Shalizi’s <a href="http://bactra.org/notebooks/nn-attention-and-transformers.html">post on the topic</a> appeared, which show the connection between attention and <a href="https://en.wikipedia.org/wiki/Kernel_regression">kernel regression</a>. This point of view was exactly what I needed! I like this so much that when I explain attention to other people, I always start from kernel regression.</p>
<section id="kernel-regression" class="level2">
<h2 class="anchored" data-anchor-id="kernel-regression">Kernel regression</h2>
<p>Let’s start with <a href="https://en.wikipedia.org/wiki/Kernel_regression">kernel regression</a> as independently proposed by Nadaraya and Watson sixty years ago. We will generate some data with <a href="https://en.wikipedia.org/wiki/Homoscedasticity_and_heteroscedasticity">heteroskedastic noise</a>, <span class="math inline">\(y = f(x) + n(x)\epsilon\)</span> where <span class="math inline">\(\epsilon \sim \mathcal N(0, 1)\)</span>, <span class="math inline">\(f(x)\)</span> is the expected value <span class="math inline">\(\mathbb E[y\mid x]\)</span> and function <span class="math inline">\(n(x)\)</span> makes the noise heteroskedastic.</p>
<p>We’ll plot the observed data points as well as <span class="math inline">\(f(x) + 2 n(x)\)</span> and <span class="math inline">\(f(x) - 2n(x)\)</span> as is <a href="https://en.wikipedia.org/wiki/68%E2%80%9395%E2%80%9399.7_rule">often done</a>.</p>
<div class="cell" data-execution_count="1">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> functools <span class="im">import</span> partial</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> jax</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> jax.numpy <span class="im">as</span> jnp</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> jax <span class="im">import</span> random</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> equinox <span class="im">as</span> eqx</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> jaxtyping <span class="im">import</span> Float, Array</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>plt.style.use(<span class="st">"dark_background"</span>)</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>Vector <span class="op">=</span> Float[Array, <span class="st">" n_points"</span>]</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> f(X: Vector) <span class="op">-&gt;</span> Vector:</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="fl">0.5</span> <span class="op">*</span> jnp.sin(X) <span class="op">-</span> <span class="dv">1</span> <span class="op">*</span> jnp.sin(<span class="dv">3</span> <span class="op">*</span> X) <span class="op">+</span> <span class="fl">0.2</span> <span class="op">*</span> jnp.square(X)</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> n(X: Vector) <span class="op">-&gt;</span> Vector:</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="fl">0.2</span> <span class="op">+</span> <span class="fl">0.05</span> <span class="op">*</span> jnp.<span class="bu">abs</span>(X)</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>n_points: <span class="bu">int</span> <span class="op">=</span> <span class="dv">150</span></span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>key <span class="op">=</span> random.PRNGKey(<span class="dv">2024</span>)</span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>key, subkey <span class="op">=</span> random.split(key)</span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> jnp.linspace(<span class="op">-</span><span class="dv">3</span>, <span class="dv">3</span>, n_points)</span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>Y <span class="op">=</span> f(X) <span class="op">+</span> n(X) <span class="op">*</span> random.normal(subkey, shape<span class="op">=</span>X.shape)</span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>(<span class="dv">4</span>, <span class="dv">3</span>), dpi<span class="op">=</span><span class="dv">150</span>)</span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>X_ax <span class="op">=</span> jnp.linspace(<span class="op">-</span><span class="dv">3</span>, <span class="dv">3</span>, <span class="dv">201</span>)</span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a>ax.fill_between(</span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a>    X_ax, f(X_ax)<span class="op">-</span> <span class="dv">2</span> <span class="op">*</span> n(X_ax), f(X_ax) <span class="op">+</span> <span class="dv">2</span> <span class="op">*</span> n(X_ax), alpha<span class="op">=</span><span class="fl">0.4</span>, color<span class="op">=</span><span class="st">"maroon"</span></span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a>ax.plot(X_ax, f(X_ax), color<span class="op">=</span><span class="st">"maroon"</span>, alpha<span class="op">=</span><span class="fl">0.8</span>)</span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a>ax.scatter(X, Y, color<span class="op">=</span><span class="st">"white"</span>, s<span class="op">=</span><span class="dv">5</span>, alpha<span class="op">=</span><span class="fl">1.0</span>)</span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">"$X$"</span>)</span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="st">"$Y$"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stderr">
<pre><code>An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="1">
<pre><code>Text(0, 0.5, '$Y$')</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="kernel-regression-transformer_files/figure-html/cell-2-output-3.png" class="img-fluid"></p>
</div>
</div>
<p>We will be interested in finding <span class="math inline">\(f(x)\)</span> via the weighted average: <span class="math display">\[
\hat f(x) = \sum_{i=1}^n y_i\, w_i(x)
\]</span></p>
<p>where <span class="math inline">\(w_i(x)\)</span> is the weight of the <span class="math inline">\(i\)</span>-th data point used to estimate the value at <span class="math inline">\(x\)</span>. To make it a weighted average, we will ensure that <span class="math inline">\(w_1(x) + \cdots + w_n(x) = 1\)</span>. In case where <span class="math inline">\(w_i(x) = 1/n\)</span> we obtain just constant prediction, equal to the sample average over <span class="math inline">\(y_i\)</span>.</p>
<p>More generally, consider a positive function <span class="math inline">\(K\colon \mathcal X \times \mathcal X \to \mathbb R^+\)</span> which measures similarity between two data points: we want <span class="math inline">\(K(x, x')\)</span> to attain the largest possible value and for <span class="math inline">\(x'\)</span> very far from <span class="math inline">\(x\)</span> we want to have <span class="math inline">\(K(x, x')\)</span> to be small. For such a function we can form a set of weights via <span class="math display">\[
w_i(x) = \frac{K(x, x_i)}{\sum_{j=1}^n K(x, x_j)}.
\]</span></p>
<p>Let’s restrict our attention for now to Gaussian kernels, <span class="math inline">\(K(x, x'; \ell) = \exp \left(\left( \frac{x-x'}{\ell} \right)^2 \right)\)</span> with lengthscale <span class="math inline">\(\ell\)</span> and visualise the predictions for different lengthscales. As kernels are parameterised functions, we will use <a href="https://github.com/patrick-kidger/equinox/">Equinox</a>:</p>
<div class="cell" data-execution_count="2">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> GaussianKernel(eqx.Module):</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>    _log_lengthscale: <span class="bu">float</span></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, lengthscale: <span class="bu">float</span>) <span class="op">-&gt;</span> <span class="va">None</span>:</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>        <span class="cf">assert</span> lengthscale <span class="op">&gt;</span> <span class="dv">0</span>, <span class="st">"Lengthscale should be positive."</span></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>._log_lengthscale <span class="op">=</span> jnp.log(lengthscale)</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>    <span class="at">@property</span></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> lengthscale(<span class="va">self</span>) <span class="op">-&gt;</span> <span class="bu">float</span>:</span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> jnp.exp(<span class="va">self</span>._log_lengthscale)</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__call__</span>(<span class="va">self</span>, x: <span class="bu">float</span>, x_: <span class="bu">float</span>) <span class="op">-&gt;</span> <span class="bu">float</span>:</span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> jnp.exp(<span class="op">-</span>jnp.square((x<span class="op">-</span>x_) <span class="op">/</span> <span class="va">self</span>.lengthscale))</span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> predict(<span class="va">self</span>, X_test: Float[Array, <span class="st">" n_test"</span>], X_obs: Vector, Y_obs: Vector) <span class="op">-&gt;</span> Float[Array, <span class="st">" n_test"</span>]:</span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a>        kernel <span class="op">=</span> <span class="va">self</span></span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a>        <span class="kw">def</span> predict_one(x: <span class="bu">float</span>) <span class="op">-&gt;</span> <span class="bu">float</span>:</span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a>            ks <span class="op">=</span> jax.vmap(partial(kernel, x))(X_obs)</span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a>            ws <span class="op">=</span> ks <span class="op">/</span> (jnp.<span class="bu">sum</span>(ks) <span class="op">+</span> <span class="fl">1e-16</span>)</span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> jnp.<span class="bu">sum</span>(Y_obs <span class="op">*</span> ws)</span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> jax.vmap(predict_one)(X_test)    </span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a>kernels <span class="op">=</span> {lengthscale: GaussianKernel(lengthscale) <span class="cf">for</span> lengthscale <span class="kw">in</span> [<span class="fl">3.0</span>, <span class="fl">0.5</span>, <span class="fl">0.25</span>, <span class="fl">0.05</span>]} </span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-27"><a href="#cb4-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-28"><a href="#cb4-28" aria-hidden="true" tabindex="-1"></a>fig, axs <span class="op">=</span> plt.subplots(<span class="dv">2</span>, <span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">2</span><span class="op">*</span><span class="dv">4</span>, <span class="dv">2</span><span class="op">*</span><span class="dv">3</span>), dpi<span class="op">=</span><span class="dv">150</span>, sharex<span class="op">=</span><span class="va">True</span>, sharey<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb4-29"><a href="#cb4-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-30"><a href="#cb4-30" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (lengthscale, k), ax <span class="kw">in</span> <span class="bu">zip</span>(kernels.items(), axs.ravel()):</span>
<span id="cb4-31"><a href="#cb4-31" aria-hidden="true" tabindex="-1"></a>    pred <span class="op">=</span> k.predict(X_ax, X_obs<span class="op">=</span>X, Y_obs<span class="op">=</span>Y)</span>
<span id="cb4-32"><a href="#cb4-32" aria-hidden="true" tabindex="-1"></a>    ax.set_title(<span class="ss">f"$</span><span class="ch">\\</span><span class="ss">ell = </span><span class="sc">{</span>lengthscale<span class="sc">}</span><span class="ss">$"</span>)</span>
<span id="cb4-33"><a href="#cb4-33" aria-hidden="true" tabindex="-1"></a>    ax.plot(X_ax, f(X_ax), color<span class="op">=</span><span class="st">"maroon"</span>, alpha<span class="op">=</span><span class="fl">0.8</span>)</span>
<span id="cb4-34"><a href="#cb4-34" aria-hidden="true" tabindex="-1"></a>    ax.plot(X_ax, pred, color<span class="op">=</span><span class="st">"orangered"</span>, alpha<span class="op">=</span><span class="fl">0.8</span>)</span>
<span id="cb4-35"><a href="#cb4-35" aria-hidden="true" tabindex="-1"></a>    ax.scatter(X, Y, color<span class="op">=</span><span class="st">"white"</span>, s<span class="op">=</span><span class="dv">5</span>, alpha<span class="op">=</span><span class="fl">0.8</span>)</span>
<span id="cb4-36"><a href="#cb4-36" aria-hidden="true" tabindex="-1"></a>    ax.set_xlabel(<span class="st">"$X$"</span>)</span>
<span id="cb4-37"><a href="#cb4-37" aria-hidden="true" tabindex="-1"></a>    ax.set_ylabel(<span class="st">"$Y$"</span>)</span>
<span id="cb4-38"><a href="#cb4-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-39"><a href="#cb4-39" aria-hidden="true" tabindex="-1"></a>fig.tight_layout()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="kernel-regression-transformer_files/figure-html/cell-3-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>It seems that <span class="math inline">\(\ell=3.0\)</span> results in underfitted, almost constant, predictions, and <span class="math inline">\(\ell=0.05\)</span> arguably overfits, resulting in predictions changing a bit too quickly. Generally, it seems that <span class="math inline">\(\ell \approx 0.25\)</span> is a reasonable choice.</p>
<section id="masked-training" class="level3">
<h3 class="anchored" data-anchor-id="masked-training">Masked training</h3>
<p>Let’s now think how we could find <span class="math inline">\(\ell\)</span> algorithmically (and when the true mean curve is not available for comparison!).</p>
<p>For example, we could use something like the leave-one-out cross-validation:</p>
<ol type="1">
<li>Hold out a data point <span class="math inline">\((x_i, y_i)\)</span>;</li>
<li>Fit the kernel regression with lengthscale <span class="math inline">\(\ell\)</span> to the data <span class="math inline">\((x_1, y_1), \dotsc, (x_{i-1}, y_{i-1}), (x_{i+1}, y_{i+1}), \dotsc, (x_n, y_n)\)</span>;</li>
<li>Predict <span class="math inline">\(y_i\)</span> from <span class="math inline">\(x_i\)</span> given the kernel regression.</li>
</ol>
<p>Looking at different values <span class="math inline">\(\ell\)</span> and varying the index <span class="math inline">\(i\)</span> of the hold-out data point may be a reasonable training procedure. Note however that if we use standard squared loss, this will have a drawback that points which are further from the mean (due to heteroskedasticity) will be treated similarly to the data points where the noise is small. We could try to reweight them, but we won’t do that and implement a vanilla variant.</p>
<p>In fact, we will try several variants of this approach, allowing to hold out more data points than <span class="math inline">\(1\)</span>. In terms of probabilistic interpretation this is even worse: apart from problems with interpreting square loss due to heteroskedasticity, now we are also predicting values at several locations at once, effectively assuming that they are independent, given the observed data. In a way, this is similar to the <a href="https://en.wikipedia.org/wiki/BERT_(language_model)">BERT training</a>. <a href="https://arxiv.org/abs/1906.08237">XLNet</a> considers different permutations, being closer to an orderless autoregressive model. Anyway, BERT had impressive performance, so let’s try different variants here:</p>
<div class="cell" data-execution_count="3">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> optax</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> train(</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>    key,</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>    model: eqx.Module,</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>    X: Vector,</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>    Y: Vector,</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>    learning_rate: <span class="bu">float</span> <span class="op">=</span> <span class="fl">0.2</span>,</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>    hold_out_size: <span class="bu">int</span> <span class="op">=</span> <span class="dv">1</span>,</span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>    n_steps: <span class="bu">int</span> <span class="op">=</span> <span class="dv">100</span>,</span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>    print_every: <span class="bu">int</span> <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>) <span class="op">-&gt;</span> eqx.Module:</span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> n_steps <span class="op">&gt;</span> <span class="dv">1</span></span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> print_every <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a>        print_every <span class="op">=</span> n_steps <span class="op">+</span> <span class="dv">100</span></span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> print_every <span class="op">&gt;</span> <span class="dv">0</span></span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> learning_rate <span class="op">&gt;</span> <span class="dv">0</span></span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> X.shape[<span class="dv">0</span>] <span class="op">==</span> Y.shape[<span class="dv">0</span>]</span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a>    n_total <span class="op">=</span> X.shape[<span class="dv">0</span>]</span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-23"><a href="#cb5-23" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> split_data(key):</span>
<span id="cb5-24"><a href="#cb5-24" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Splits the data into training and test."""</span></span>
<span id="cb5-25"><a href="#cb5-25" aria-hidden="true" tabindex="-1"></a>        indices <span class="op">=</span> random.permutation(key, jnp.arange(n_total))</span>
<span id="cb5-26"><a href="#cb5-26" aria-hidden="true" tabindex="-1"></a>        ind_test <span class="op">=</span> indices[:hold_out_size]</span>
<span id="cb5-27"><a href="#cb5-27" aria-hidden="true" tabindex="-1"></a>        ind_obs <span class="op">=</span> indices[hold_out_size:]</span>
<span id="cb5-28"><a href="#cb5-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-29"><a href="#cb5-29" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> X[ind_obs], Y[ind_obs], X[ind_test], Y[ind_test]</span>
<span id="cb5-30"><a href="#cb5-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-31"><a href="#cb5-31" aria-hidden="true" tabindex="-1"></a>    <span class="at">@jax.jit</span></span>
<span id="cb5-32"><a href="#cb5-32" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> step(</span>
<span id="cb5-33"><a href="#cb5-33" aria-hidden="true" tabindex="-1"></a>        model: eqx.Module,</span>
<span id="cb5-34"><a href="#cb5-34" aria-hidden="true" tabindex="-1"></a>        opt_state,</span>
<span id="cb5-35"><a href="#cb5-35" aria-hidden="true" tabindex="-1"></a>        X_obs: Vector,</span>
<span id="cb5-36"><a href="#cb5-36" aria-hidden="true" tabindex="-1"></a>        Y_obs: Vector,</span>
<span id="cb5-37"><a href="#cb5-37" aria-hidden="true" tabindex="-1"></a>        X_test: Float[Array, <span class="st">" n_test"</span>],</span>
<span id="cb5-38"><a href="#cb5-38" aria-hidden="true" tabindex="-1"></a>        Y_test: Float[Array, <span class="st">" n_test"</span>],</span>
<span id="cb5-39"><a href="#cb5-39" aria-hidden="true" tabindex="-1"></a>    ):</span>
<span id="cb5-40"><a href="#cb5-40" aria-hidden="true" tabindex="-1"></a>        <span class="kw">def</span> loss_fn(model):</span>
<span id="cb5-41"><a href="#cb5-41" aria-hidden="true" tabindex="-1"></a>            preds <span class="op">=</span> model.predict(</span>
<span id="cb5-42"><a href="#cb5-42" aria-hidden="true" tabindex="-1"></a>                X_test<span class="op">=</span>X_test,</span>
<span id="cb5-43"><a href="#cb5-43" aria-hidden="true" tabindex="-1"></a>                X_obs<span class="op">=</span>X_obs,</span>
<span id="cb5-44"><a href="#cb5-44" aria-hidden="true" tabindex="-1"></a>                Y_obs<span class="op">=</span>Y_obs,</span>
<span id="cb5-45"><a href="#cb5-45" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb5-46"><a href="#cb5-46" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> jnp.mean(jnp.square(preds <span class="op">-</span> Y_test))</span>
<span id="cb5-47"><a href="#cb5-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-48"><a href="#cb5-48" aria-hidden="true" tabindex="-1"></a>        loss, grads <span class="op">=</span> jax.value_and_grad(loss_fn)(model)</span>
<span id="cb5-49"><a href="#cb5-49" aria-hidden="true" tabindex="-1"></a>        updates, opt_state <span class="op">=</span> optimizer.update(grads, opt_state, model)</span>
<span id="cb5-50"><a href="#cb5-50" aria-hidden="true" tabindex="-1"></a>        model <span class="op">=</span> optax.apply_updates(model, updates)</span>
<span id="cb5-51"><a href="#cb5-51" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> model, opt_state, loss</span>
<span id="cb5-52"><a href="#cb5-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-53"><a href="#cb5-53" aria-hidden="true" tabindex="-1"></a>    optimizer <span class="op">=</span> optax.adam(learning_rate<span class="op">=</span>learning_rate)</span>
<span id="cb5-54"><a href="#cb5-54" aria-hidden="true" tabindex="-1"></a>    opt_state <span class="op">=</span> optimizer.init(model)</span>
<span id="cb5-55"><a href="#cb5-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-56"><a href="#cb5-56" aria-hidden="true" tabindex="-1"></a>    losses <span class="op">=</span> []</span>
<span id="cb5-57"><a href="#cb5-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-58"><a href="#cb5-58" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> n_step <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, n_steps <span class="op">+</span> <span class="dv">1</span>):</span>
<span id="cb5-59"><a href="#cb5-59" aria-hidden="true" tabindex="-1"></a>        key, subkey <span class="op">=</span> random.split(key)</span>
<span id="cb5-60"><a href="#cb5-60" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb5-61"><a href="#cb5-61" aria-hidden="true" tabindex="-1"></a>        X_obs, Y_obs, X_test, Y_test <span class="op">=</span> split_data(subkey)</span>
<span id="cb5-62"><a href="#cb5-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-63"><a href="#cb5-63" aria-hidden="true" tabindex="-1"></a>        model, opt_state, loss <span class="op">=</span> step(</span>
<span id="cb5-64"><a href="#cb5-64" aria-hidden="true" tabindex="-1"></a>            model,</span>
<span id="cb5-65"><a href="#cb5-65" aria-hidden="true" tabindex="-1"></a>            opt_state<span class="op">=</span>opt_state,</span>
<span id="cb5-66"><a href="#cb5-66" aria-hidden="true" tabindex="-1"></a>            X_obs<span class="op">=</span>X_obs,</span>
<span id="cb5-67"><a href="#cb5-67" aria-hidden="true" tabindex="-1"></a>            Y_obs<span class="op">=</span>Y_obs,</span>
<span id="cb5-68"><a href="#cb5-68" aria-hidden="true" tabindex="-1"></a>            X_test<span class="op">=</span>X_test,</span>
<span id="cb5-69"><a href="#cb5-69" aria-hidden="true" tabindex="-1"></a>            Y_test<span class="op">=</span>Y_test</span>
<span id="cb5-70"><a href="#cb5-70" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb5-71"><a href="#cb5-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-72"><a href="#cb5-72" aria-hidden="true" tabindex="-1"></a>        losses.append(loss)</span>
<span id="cb5-73"><a href="#cb5-73" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-74"><a href="#cb5-74" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> n_step <span class="op">%</span> print_every <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb5-75"><a href="#cb5-75" aria-hidden="true" tabindex="-1"></a>            avg_loss <span class="op">=</span> jnp.mean(jnp.asarray(losses[<span class="op">-</span><span class="dv">20</span>:]))</span>
<span id="cb5-76"><a href="#cb5-76" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f"Step: </span><span class="sc">{</span>n_step<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb5-77"><a href="#cb5-77" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f"Loss: </span><span class="sc">{</span>avg_loss<span class="sc">:.2f}</span><span class="ss">"</span>)</span>
<span id="cb5-78"><a href="#cb5-78" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="st">"-"</span> <span class="op">*</span> <span class="dv">14</span>)</span>
<span id="cb5-79"><a href="#cb5-79" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-80"><a href="#cb5-80" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> model</span>
<span id="cb5-81"><a href="#cb5-81" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-82"><a href="#cb5-82" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-83"><a href="#cb5-83" aria-hidden="true" tabindex="-1"></a>fig, axs <span class="op">=</span> plt.subplots(<span class="dv">2</span>, <span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">2</span><span class="op">*</span><span class="dv">4</span>, <span class="dv">2</span><span class="op">*</span><span class="dv">3</span>), dpi<span class="op">=</span><span class="dv">150</span>, sharex<span class="op">=</span><span class="va">True</span>, sharey<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb5-84"><a href="#cb5-84" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-85"><a href="#cb5-85" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> holdout, ax <span class="kw">in</span> <span class="bu">zip</span>([<span class="dv">1</span>, <span class="dv">10</span>, n_points <span class="op">//</span> <span class="dv">2</span>, <span class="bu">int</span>(<span class="fl">0.8</span> <span class="op">*</span> n_points)], axs.ravel()):</span>
<span id="cb5-86"><a href="#cb5-86" aria-hidden="true" tabindex="-1"></a>    key, subkey <span class="op">=</span> random.split(key)</span>
<span id="cb5-87"><a href="#cb5-87" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb5-88"><a href="#cb5-88" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> train(</span>
<span id="cb5-89"><a href="#cb5-89" aria-hidden="true" tabindex="-1"></a>        key<span class="op">=</span>subkey,</span>
<span id="cb5-90"><a href="#cb5-90" aria-hidden="true" tabindex="-1"></a>        model<span class="op">=</span>GaussianKernel(lengthscale<span class="op">=</span><span class="fl">1.0</span>),</span>
<span id="cb5-91"><a href="#cb5-91" aria-hidden="true" tabindex="-1"></a>        X<span class="op">=</span>X,</span>
<span id="cb5-92"><a href="#cb5-92" aria-hidden="true" tabindex="-1"></a>        Y<span class="op">=</span>Y,</span>
<span id="cb5-93"><a href="#cb5-93" aria-hidden="true" tabindex="-1"></a>        print_every<span class="op">=</span><span class="va">None</span>,</span>
<span id="cb5-94"><a href="#cb5-94" aria-hidden="true" tabindex="-1"></a>        hold_out_size<span class="op">=</span>holdout,</span>
<span id="cb5-95"><a href="#cb5-95" aria-hidden="true" tabindex="-1"></a>        n_steps<span class="op">=</span><span class="dv">100</span>,</span>
<span id="cb5-96"><a href="#cb5-96" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb5-97"><a href="#cb5-97" aria-hidden="true" tabindex="-1"></a>    pred <span class="op">=</span> model.predict(X_ax, X_obs<span class="op">=</span>X, Y_obs<span class="op">=</span>Y)</span>
<span id="cb5-98"><a href="#cb5-98" aria-hidden="true" tabindex="-1"></a>    ax.set_title(<span class="ss">f"Hold-out=</span><span class="sc">{</span>holdout<span class="sc">}</span><span class="ss">, $\ell$=</span><span class="sc">{</span>model<span class="sc">.</span>lengthscale<span class="sc">:.2f}</span><span class="ss">"</span>)</span>
<span id="cb5-99"><a href="#cb5-99" aria-hidden="true" tabindex="-1"></a>    ax.plot(X_ax, f(X_ax), color<span class="op">=</span><span class="st">"maroon"</span>, alpha<span class="op">=</span><span class="fl">0.8</span>)</span>
<span id="cb5-100"><a href="#cb5-100" aria-hidden="true" tabindex="-1"></a>    ax.plot(X_ax, pred, color<span class="op">=</span><span class="st">"orangered"</span>, alpha<span class="op">=</span><span class="fl">0.8</span>)</span>
<span id="cb5-101"><a href="#cb5-101" aria-hidden="true" tabindex="-1"></a>    ax.scatter(X, Y, color<span class="op">=</span><span class="st">"white"</span>, s<span class="op">=</span><span class="dv">5</span>, alpha<span class="op">=</span><span class="fl">0.8</span>)</span>
<span id="cb5-102"><a href="#cb5-102" aria-hidden="true" tabindex="-1"></a>    ax.set_xlabel(<span class="st">"$X$"</span>)</span>
<span id="cb5-103"><a href="#cb5-103" aria-hidden="true" tabindex="-1"></a>    ax.set_ylabel(<span class="st">"$Y$"</span>)</span>
<span id="cb5-104"><a href="#cb5-104" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-105"><a href="#cb5-105" aria-hidden="true" tabindex="-1"></a>fig.tight_layout()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="kernel-regression-transformer_files/figure-html/cell-4-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>Hey, this worked pretty well!</p>
</section>
<section id="multi-headed-kernel-regression" class="level3">
<h3 class="anchored" data-anchor-id="multi-headed-kernel-regression">Multi-headed kernel regression</h3>
<p>At this point we’ll introduce yet another modification; later we’ll see that it’s analogous to multi-head attention. Consider a model with <span class="math inline">\(H\)</span> “heads”. Each head will be a kernel with a potentially different lengthscale <span class="math inline">\(\ell_h\)</span>. In this manner, we will allow different heads to capture information at a different lengthscale. Finally, we will combine the predictions using auxiliary parameters <span class="math inline">\(u_1, \dotsc, u_H\)</span>: <span class="math display">\[
\hat f(x) = \sum_{h=1}^H u_h\, \hat f_h(x) = \sum_{h=1}^H u_h\, \sum_{i=1}^n y_i \frac{ K(x, x_i; \ell_h) }{ \sum_{j=1}^n K(x, x_j; \ell_h) }.
\]</span></p>
<p>Let’s implement it quickly in Equinox:</p>
<div class="cell" data-execution_count="4">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> MultiheadGaussianKernel(eqx.Module):</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>    kernels: <span class="bu">list</span>[GaussianKernel]</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>    weights: jax.Array</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, n_heads: <span class="bu">int</span>) <span class="op">-&gt;</span> <span class="va">None</span>:</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>        <span class="cf">assert</span> n_heads <span class="op">&gt;</span> <span class="dv">0</span></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.weights <span class="op">=</span> jnp.full(shape<span class="op">=</span>(n_heads,), fill_value<span class="op">=</span><span class="dv">1</span> <span class="op">/</span> n_heads)</span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.kernels <span class="op">=</span> [</span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>            GaussianKernel(lengthscale<span class="op">=</span>l)</span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> l <span class="kw">in</span> jnp.linspace(<span class="fl">0.1</span>, <span class="dv">3</span>, n_heads)</span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a>        ]</span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a>    <span class="at">@property</span></span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> lengthscale(<span class="va">self</span>) <span class="op">-&gt;</span> <span class="bu">list</span>[<span class="bu">float</span>]:</span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> [k.lengthscale <span class="cf">for</span> k <span class="kw">in</span> <span class="va">self</span>.kernels]</span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> predict(<span class="va">self</span>, X_test: Float[Array, <span class="st">" n_test"</span>], X_obs: Vector, Y_obs: Vector) <span class="op">-&gt;</span> Float[Array, <span class="st">" n_test"</span>]:</span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Shape (kernels, n_test)</span></span>
<span id="cb6-20"><a href="#cb6-20" aria-hidden="true" tabindex="-1"></a>        preds <span class="op">=</span> jnp.stack([k.predict(X_test<span class="op">=</span>X_test, X_obs<span class="op">=</span>X_obs, Y_obs<span class="op">=</span>Y_obs) <span class="cf">for</span> k <span class="kw">in</span> <span class="va">self</span>.kernels])</span>
<span id="cb6-21"><a href="#cb6-21" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> jnp.einsum(<span class="st">"kn,k-&gt;n"</span>, preds, <span class="va">self</span>.weights)</span>
<span id="cb6-22"><a href="#cb6-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-23"><a href="#cb6-23" aria-hidden="true" tabindex="-1"></a>fig, axs <span class="op">=</span> plt.subplots(<span class="dv">2</span>, <span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">2</span><span class="op">*</span><span class="dv">4</span>, <span class="dv">2</span><span class="op">*</span><span class="dv">3</span>), dpi<span class="op">=</span><span class="dv">150</span>, sharex<span class="op">=</span><span class="va">True</span>, sharey<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb6-24"><a href="#cb6-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-25"><a href="#cb6-25" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> n_heads, ax <span class="kw">in</span> <span class="bu">zip</span>([<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">4</span>, <span class="dv">8</span>], axs.ravel()):</span>
<span id="cb6-26"><a href="#cb6-26" aria-hidden="true" tabindex="-1"></a>    key, subkey <span class="op">=</span> random.split(key)</span>
<span id="cb6-27"><a href="#cb6-27" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb6-28"><a href="#cb6-28" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> train(</span>
<span id="cb6-29"><a href="#cb6-29" aria-hidden="true" tabindex="-1"></a>        key<span class="op">=</span>subkey,</span>
<span id="cb6-30"><a href="#cb6-30" aria-hidden="true" tabindex="-1"></a>        model<span class="op">=</span>MultiheadGaussianKernel(n_heads<span class="op">=</span>n_heads),</span>
<span id="cb6-31"><a href="#cb6-31" aria-hidden="true" tabindex="-1"></a>        X<span class="op">=</span>X,</span>
<span id="cb6-32"><a href="#cb6-32" aria-hidden="true" tabindex="-1"></a>        Y<span class="op">=</span>Y,</span>
<span id="cb6-33"><a href="#cb6-33" aria-hidden="true" tabindex="-1"></a>        print_every<span class="op">=</span><span class="va">None</span>,</span>
<span id="cb6-34"><a href="#cb6-34" aria-hidden="true" tabindex="-1"></a>        hold_out_size<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb6-35"><a href="#cb6-35" aria-hidden="true" tabindex="-1"></a>        n_steps<span class="op">=</span><span class="dv">1_000</span>,</span>
<span id="cb6-36"><a href="#cb6-36" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb6-37"><a href="#cb6-37" aria-hidden="true" tabindex="-1"></a>    pred <span class="op">=</span> model.predict(X_ax, X_obs<span class="op">=</span>X, Y_obs<span class="op">=</span>Y)</span>
<span id="cb6-38"><a href="#cb6-38" aria-hidden="true" tabindex="-1"></a>    ax.set_title(<span class="ss">f"Heads=</span><span class="sc">{</span>n_heads<span class="sc">}</span><span class="ss">"</span>) <span class="co"># $\ell$={model.lengthscale:.2f}")</span></span>
<span id="cb6-39"><a href="#cb6-39" aria-hidden="true" tabindex="-1"></a>    ax.plot(X_ax, f(X_ax), color<span class="op">=</span><span class="st">"maroon"</span>, alpha<span class="op">=</span><span class="fl">0.8</span>)</span>
<span id="cb6-40"><a href="#cb6-40" aria-hidden="true" tabindex="-1"></a>    ax.plot(X_ax, pred, color<span class="op">=</span><span class="st">"orangered"</span>, alpha<span class="op">=</span><span class="fl">0.8</span>)</span>
<span id="cb6-41"><a href="#cb6-41" aria-hidden="true" tabindex="-1"></a>    ax.scatter(X, Y, color<span class="op">=</span><span class="st">"white"</span>, s<span class="op">=</span><span class="dv">5</span>, alpha<span class="op">=</span><span class="fl">0.8</span>)</span>
<span id="cb6-42"><a href="#cb6-42" aria-hidden="true" tabindex="-1"></a>    ax.set_xlabel(<span class="st">"$X$"</span>)</span>
<span id="cb6-43"><a href="#cb6-43" aria-hidden="true" tabindex="-1"></a>    ax.set_ylabel(<span class="st">"$Y$"</span>)</span>
<span id="cb6-44"><a href="#cb6-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-45"><a href="#cb6-45" aria-hidden="true" tabindex="-1"></a>    u_h_str <span class="op">=</span> <span class="st">", "</span>.join([<span class="ss">f"</span><span class="sc">{</span>w<span class="sc">:.2f}</span><span class="ss">"</span> <span class="cf">for</span> w <span class="kw">in</span> model.weights])</span>
<span id="cb6-46"><a href="#cb6-46" aria-hidden="true" tabindex="-1"></a>    l_h_str <span class="op">=</span> <span class="st">", "</span>.join([<span class="ss">f"</span><span class="sc">{</span>k<span class="sc">.</span>lengthscale<span class="sc">:.2f}</span><span class="ss">"</span> <span class="cf">for</span> k <span class="kw">in</span> model.kernels])</span>
<span id="cb6-47"><a href="#cb6-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-48"><a href="#cb6-48" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Number of heads: </span><span class="sc">{</span>n_heads<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb6-49"><a href="#cb6-49" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"  Combination:  </span><span class="sc">{</span>u_h_str<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb6-50"><a href="#cb6-50" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"  Lengthscales: </span><span class="sc">{</span>l_h_str<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb6-51"><a href="#cb6-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-52"><a href="#cb6-52" aria-hidden="true" tabindex="-1"></a>fig.tight_layout()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Number of heads: 1
  Combination:  1.09
  Lengthscales: 0.16
Number of heads: 2
  Combination:  0.99, -0.10
  Lengthscales: 0.08, 0.02
Number of heads: 4
  Combination:  1.09, -0.36, -0.24, 0.10
  Lengthscales: 0.16, 2.95, 2.18, 11.60
Number of heads: 8
  Combination:  0.90, 0.02, 0.86, 0.95, -0.45, -0.53, -0.51, -0.22
  Lengthscales: 0.08, 9.49, 20.43, 25.16, 210.44, 33.52, 34.75, 82.24</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="kernel-regression-transformer_files/figure-html/cell-5-output-2.png" class="img-fluid"></p>
</div>
</div>
<p>We see that coefficients <span class="math inline">\(u_h\)</span> are not constrained to be positive and they do not have to sum up to 1: we allow an arbitrary linear combination of predictions, rather than a weighted sum. Note also that many heads allow for larger flexibility, although on such a small data set this can arguably result in some amount of overfitting.</p>
</section>
</section>
<section id="attention" class="level2">
<h2 class="anchored" data-anchor-id="attention">Attention</h2>
<p>Recall the equation <span class="math display">\[
\hat f(x) = \sum_{i=1}^n y_i\, \frac{K(x, x_i; \theta)}{ \sum_{j=1}^n K(x, x_j; \theta)},
\]</span> where there kernel <span class="math inline">\(K\)</span> is now parameterised by <span class="math inline">\(\theta\)</span>. As we want the kernel to give positive values, let’s write <span class="math display">\[
K(x, x'; \theta) = \exp s_\theta(x, x')
\]</span> for some function <span class="math inline">\(s_\theta\)</span>. Hence, we can write <span class="math display">\[
\hat f(x) = \sum_{i=1}^n y_i \, \mathrm{softmax}( s_\theta(x, x_j)_{j = 1, \dotsc, n} ).
\]</span> The usual approach is to use <span class="math inline">\(\theta = (W^{(q)}, W^{(k)})\)</span> for matrices mapping from <span class="math inline">\(\mathcal X\)</span> to some space <span class="math inline">\(\mathbb R^{d_\text{qk}}\)</span> and use a scalar product <span class="math display">\[
s_\theta(x, x') = \frac{\left\langle W^{(q)}x, W^{(k)}x'\right\rangle}{\sqrt{d_\text{qk}}} = \frac{ x^T \left(W^{(q)}\right)^T W^{(k)}x'}{\sqrt{d_\text{qk}}},
\]</span> where the denominator takes various forms and is usually used to ensure that the values are properly normalized and the gradients can propagate through the softmax layer well.</p>
<p>Now consider another modification. We will write <span class="math inline">\(y_i = W^{(v)}x_i\)</span> for some matrix <span class="math inline">\(W^{(v)}\)</span> mapping from <span class="math inline">\(\mathcal X\)</span> to some space <span class="math inline">\(\mathbb R^{d_\text{v}}\)</span>. (One can think that it’s a restriction when it comes to the regression (as we are not using values <span class="math inline">\(y_i\)</span> as provided), but it’s not really a big issue: it just suffices to relabel as “point <span class="math inline">\(x_i\)</span>” a tuple <span class="math inline">\((x_i, y_i)\)</span> and redefine the introduced parameter matrices, so that they first project on the required component.)</p>
<p>In this case, we obtain a function <span class="math display">\[
x\mapsto \sum_{i=1}^n W^{(v)}x_i \, \mathrm{softmax}\left(  \frac{ x^T \left(W^{(q)}\right)^T W^{(k)}x_i}{\sqrt{d_\text{qk}}}  \right).
\]</span></p>
<p>If we apply this formula to each <span class="math inline">\(x\)</span> from the sequence <span class="math inline">\((x_1, \dotsc, x_n) \in \mathcal X^n\)</span>, we obtain a new sequence <span class="math inline">\((x_1', \dotsc, x'_n) \in \left(\mathbb R^{d_\text{v}}\right)^n\)</span>. This is exactly the self-attention layer used in transformers. How to obtain multi-head attention? Similarly as in <a href="#multi-headed-kernel-regression">multi-head kernel regression</a>, we will introduce <span class="math inline">\(H\)</span> different “heads” with individual parameters <span class="math inline">\(W^{(k)}_h, W^{(q)}_h, W^{(v)}_h\)</span>. Hence, for each data point <span class="math inline">\(x\)</span> in the original sequence, we have <span class="math inline">\(H\)</span> vectors in <span class="math inline">\(\mathbb R^{d_\text{v}}\)</span> given by <span class="math display">\[
x\mapsto \sum_{i=1}^n W^{(v)}_hx_i \, \mathrm{softmax}\left(  \frac{ x^T \left(W^{(q)}_h\right)^T W^{(k)}_h x_i}{\sqrt{d_\text{qk}}}  \right) \in \mathbb R^{d_\text{v}}.
\]</span></p>
<p>If we want to obtain a mapping into some vector space <span class="math inline">\(\mathcal Y\)</span>, we can now introduce matrices <span class="math inline">\(U_h\colon \mathbb R^{d_\text{v}}\to \mathcal Y\)</span>, so that in the end we have <span class="math display">\[
x\mapsto \sum_{h=1}^H U_h \sum_{i=1}^n W^{(v)}_hx_i \, \mathrm{softmax}\left(  \frac{ x^T \left(W^{(q)}_h\right)^T W^{(k)}_h x_i}{\sqrt{d_\text{qk}}}  \right) \in \mathcal Y.
\]</span></p>
<p>To summarize, multi-head attention maps a sequence <span class="math inline">\((x_1, \dotsc, x_n)\in \mathcal X^n\)</span> to a sequence in <span class="math inline">\(\mathcal Y^n\)</span> and is parameterised by <span class="math inline">\(H\)</span> tuples of matrices <span class="math inline">\((W^{(q)}_h, W^{(k)}_h, W^{(v)}_h, U_h)\)</span>, where index <span class="math inline">\(h\)</span> corresponds to the attention head.</p>
<p>Conveniently, Equinox <a href="https://docs.kidger.site/equinox/api/nn/attention/">implements multi-head attention</a>, from which I took dimension annotations.</p>


</section>


<div id="quarto-appendix" class="default"><section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>The authors had to explain self-attention, its multi-head variant, the transformer architecture with encoder and decoder block, and positional encoding. All in a short conference paper, so it may indeed appear quite dense in ideas.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>