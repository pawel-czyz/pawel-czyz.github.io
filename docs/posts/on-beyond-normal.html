<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Paweł Czyż">
<meta name="dcterms.date" content="2023-11-20">
<meta name="description" content="Behind the scenes of our mutual information research.">

<title>Paweł Czyż - The mutual information saga</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">Paweł Czyż</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../index.html" rel="" target="">
 <span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../blog.html" rel="" target="">
 <span class="menu-text">Blog</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../publications.html" rel="" target="">
 <span class="menu-text">Publications</span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools ms-auto">
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#beyond-normal" id="toc-beyond-normal" class="nav-link active" data-scroll-target="#beyond-normal">Beyond normal</a></li>
  <li><a href="#here-comes-the-trouble" id="toc-here-comes-the-trouble" class="nav-link" data-scroll-target="#here-comes-the-trouble">Here comes the trouble</a></li>
  <li><a href="#the-mixtures-and-the-critics" id="toc-the-mixtures-and-the-critics" class="nav-link" data-scroll-target="#the-mixtures-and-the-critics">The mixtures (and the critics)</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">The mutual information saga</h1>
</div>

<div>
  <div class="description">
    Behind the scenes of our mutual information research.
  </div>
</div>


<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Paweł Czyż </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">November 20, 2023</p>
    </div>
  </div>
  
    
  </div>
  

</header>

<p>Where and when should we start this story? Probably a good origin will be in the <a href="https://pmbm.ippt.pan.pl/web/Main_Page">Laboratory of Modeling in Biology and Medicine</a> in Warsaw, where <a href="http://www.ippt.pan.pl/en/staff/tlipnia">Tomasz Lipniacki</a> and <a href="https://pmbm.ippt.pan.pl/web/Marek_Kocha%C5%84czyk">Marek Kochańczyk</a> decided to mentor two students who just completed high-school education, namely the younger versions of <a href="https://pmbm.ippt.pan.pl/web/Frederic_Grabowski">Frederic</a> and myself.</p>
<p>Informally speaking, we tried to model the <a href="https://en.wikipedia.org/wiki/MAPK/ERK_pathway">MAPK pathway</a> as a communication channel. Imagine that the cell is given some input <span class="math inline">\(x\in \mathcal X\)</span> and we measure the response <span class="math inline">\(y\in \mathcal Y\)</span>. Once we vary <span class="math inline">\(x\)</span> and we record different values <span class="math inline">\(y\)</span> we may start observing some patterns – perhaps changes with <span class="math inline">\(y\)</span> is somehow associated with changes in <span class="math inline">\(x\)</span>. To make this more formal, consider a random variable <span class="math inline">\(X\)</span> representing the given inputs and another random variable <span class="math inline">\(Y\)</span> representing the outputs. The <a href="https://en.wikipedia.org/wiki/Mutual_information">mutual information</a> <span class="math inline">\(I(X; Y)\)</span> measures how dependent these variables are: <span class="math inline">\(I(X; Y) = 0\)</span> if and only if <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are independent. Contrary to correlation, mutual information works for any kind of variables (continuous, discrete, of arbitrary dimension…) and can capture nonlinear dependencies.</p>
<p>I enjoyed my time in the project and the provided mentorship very much! We wrote <a href="../publications/limits-mapk.html">a paper</a>, and – perhaps more importantly – I learned that information theory and biology are great fields to study!</p>
<p>Many years have passed and I thought that perhaps it’s the time to become a mentor on my own. Fortunately, the very first Master’s student I supervised, <a href="https://anejsvete.github.io/">Anej</a>, was so bright and motivated that he wrote a great Master’s thesis despite having such an unexperienced supervisor as me! Anej was working on representation learning and extensively used mutual information estimators. But I had not done my homework: when I wrote the project proposal, I happily assumed that mutual information estimators have to work, if the space <span class="math inline">\(\mathcal X\times \mathcal Y\)</span> is of moderate dimension and the number of points is quite large. I was wrong. Different mutual information estimators seemed to give very different estimates and that was concerning.</p>
<section id="beyond-normal" class="level2">
<h2 class="anchored" data-anchor-id="beyond-normal">Beyond normal</h2>
<p>I did the only rational thing in this situation: I ran screaming for help to two mutual information experts, Frederic and <a href="https://www.a-marx.com/">Alex</a>. We started thinking:</p>
<ol type="1">
<li>How do we really know when mutual information estimators work? As mutual information is analytically known only for the simplest distributions, the estimators are evaluated usually on “simple” low-dimensional distributions (or moderate-dimensional multivariate normal distributions).</li>
<li>Is it possible to construct more expressive distributions with known ground-truth mutual information?</li>
<li>How invariant are the estimators to diffeomorphisms? Namely, if <span class="math inline">\(f\)</span> and <span class="math inline">\(g\)</span> are diffeomorphisms, then <span class="math inline">\(I(X; Y) = I(f(X); g(Y))\)</span>. Do the numerical estimates have the same property?</li>
</ol>
<p>The 1st and 2nd question are related. But so are 2nd and 3rd! Suppose that we can easily sample points <span class="math inline">\((x_1, y_1), \dotsc, (x_n, y_n)\)</span> from the joint distribution <span class="math inline">\(P_{XY}\)</span>. If <span class="math inline">\(f\colon \mathcal X\to \mathcal X\)</span> and <span class="math inline">\(g\colon \mathcal Y\to \mathcal Y\)</span> are diffeomorphisms, we can apply them to obtain a sample <span class="math inline">\((f(x_1), g(y_1)), \dotsc, (f(x_n), g(y_n))\)</span> from <span class="math inline">\(P_{f(X)g(Y)}\)</span>, which is a joint distribution between variables <span class="math inline">\(f(X)\)</span> and <span class="math inline">\(g(Y)\)</span>. As we apply a diffeomorphism<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>, the mutual information does not change: <span class="math inline">\(I(X; Y) = I(f(X); g(Y))\)</span>.</p>
<p>Frederic and I started programming<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a> different distributions and transformations, in the meantime learning , and after five months we had a ready manuscript titled <em>Are mutual information estimators homeomorphism-invariant?</em>, which shows that the 3rd question was the most important one for <del>a lapsed differential geometer who is currently trying to be an imposter in the machine learning world</del> me.</p>
<p>Well, I was wrong: after the manuscript got rejected from ICML, we realized that the most important aspect of our work was actually using the transformed distributions to study the strengths and limitations of existing estimators<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a>. We improved the experiments in the paper and changed the story to <a href="../publications/beyond-normal.html"><em>Beyond normal: on the evaluation of the mutual information estimators</em></a><a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a>, which was accepted to NeurIPS.</p>
<p>Of course, we were very happy. But there were some important aspects that deserved to be studied a bit more…</p>
</section>
<section id="here-comes-the-trouble" class="level2">
<h2 class="anchored" data-anchor-id="here-comes-the-trouble">Here comes the trouble</h2>
<section id="really-that-expressive" class="level4">
<h4 class="anchored" data-anchor-id="really-that-expressive">Really that expressive?</h4>
<p>Our distributions were only “beyond normal”, rather than “just amazing”: we suspected that one cannot construct all the interesting distributions.</p>
<p>Consider <span class="math inline">\(\mathcal X = \mathbb R^m\)</span>, <span class="math inline">\(\mathcal Y = \mathbb R^n\)</span> and a random vector <span class="math inline">\(Z \sim \mathcal N(0, I_{m+n})\)</span>. Normalizing flows guarantee that there exists a diffeomorphism <span class="math inline">\(u: \mathcal X\times \mathcal Y \to \mathcal X\times \mathcal Y\)</span> such that <span class="math inline">\(P_{XY}\)</span> is well-approximated<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a> by the distribution of <span class="math inline">\(u(Z)\)</span>.</p>
<p>However, the diffeomorphism <span class="math inline">\(u\)</span> does not have to be of the form <span class="math inline">\(f\times g\)</span> (recall that <span class="math inline">\((f\times g)(x, y) = (f(x), g(y))\)</span>), which leaves the mutual information invariant. Thinking geometrically, the product group <span class="math inline">\(\mathrm{Diff}(\mathcal X)\times \mathrm{Diff}(\mathcal Y)\)</span> is usually a very small subgroup of <span class="math inline">\(\mathrm{Diff}(\mathcal X\times \mathcal Y)\)</span>, the group of all diffeomorphisms of <span class="math inline">\(\mathcal X\times \mathcal Y\)</span>.</p>
<p>We had this intuition quite early, but we did not have a convincing counterexample that our distributions were not sufficient. However, Frederic started plotting histograms of pointwise mutual information, which let us formalize this intuition.</p>
<p>Consider the pointwise mutual information: <span class="math display">\[ i_{XY}(x, y) = \log \frac{ p_{XY}(x, y) }{ p_X(x)\, p_Y(y) },\]</span> where <span class="math inline">\(p_{XY}\)</span> is the PDF of the joint distribution and <span class="math inline">\(p_X\)</span> and <span class="math inline">\(p_Y\)</span> are the PDFs of the marginal distributions. It is easy to prove that if <span class="math inline">\(f\)</span> and <span class="math inline">\(g\)</span> are diffeomorphisms and that <span class="math inline">\(x'=f(x)\)</span> and <span class="math inline">\(y'=g(y)\)</span>, then<a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a> <span class="math display">\[i_{XY}(x, y) = i_{f(X)g(Y)}(x', y').\]</span> From this it is easy to observe that the distribution of the random variable <span class="math inline">\(i_{XY}(X; Y)\)</span> is the same as of <span class="math inline">\(i_{f(X)g(Y)}(f(X); g(Y))\)</span>. We termed it the <em>pointwise mutual information profile</em>, although it’s more than likely that people had already studied this before us.</p>
<p>Hence, diffeomorphisms leave invariant not only the mutual information: they leave invariant also the whole pointwise mutual information profile, what limits how expressive the distributions can be; see also <a href="../posts/invariance-mi-pmi-profile.html">this post</a> for more general invariance results. We did not have yet a counterexample, but a strong feeling that it should exist: we just needed to find distributions with different profiles, but the same mutual information.</p>
</section>
<section id="model-based-mutual-information-estimation" class="level4">
<h4 class="anchored" data-anchor-id="model-based-mutual-information-estimation">Model-based mutual information estimation</h4>
<p>We started the project with the idea that if <span class="math inline">\((A, B)\sim \mathcal N(0, \Sigma)\)</span>, then <span class="math inline">\(I(A; B)\)</span> is analytically known in terms of the covariance matrix <span class="math inline">\(\Sigma\)</span> and we can obtain more complicated dependencies between <span class="math inline">\(X=f(A)\)</span> and <span class="math inline">\(Y=f(B)\)</span> without changing the mutual information: <span class="math inline">\(I(X; Y) = I(A; B)\)</span>.</p>
<p>At the same time we asked the inverse question: if we have <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, can we perhaps find a covariance matrix <span class="math inline">\(\Sigma\)</span> and a normalizing flow <span class="math inline">\(f\times g\)</span> such that <span class="math inline">\((X, Y) = (f(A), g(B))\)</span> and <span class="math inline">\((A, B)\)</span> are distributed according to the multivariate normal distribution with covariance matrix <span class="math inline">\(\Sigma\)</span>? If <span class="math inline">\(f\)</span> and <span class="math inline">\(g\)</span> are identity functions, this construction corresponds to the assumption that <span class="math inline">\((X, Y)\)</span> are multivariate normal and calculating the mutual information via the estimation of the joint covariance matrix. A particular example of this approach is <a href="https://en.wikipedia.org/wiki/Canonical_correlation">canonical correlation analysis</a>, which worked remarkably well for multivariate normal distributions, providing more accurate estimates and requiring a lower number of samples available.</p>
<p>However, as discussed above, generally we cannot expect that a normalizing flow of the form <span class="math inline">\(f\times g\)</span> will transform a distribution to a multivariate normal. So there is some potential for more explicit modelling of the joint distribution <span class="math inline">\(P_{XY}\)</span>, but we needed to make sure that it is expressive enough to cover some interesting cases.</p>
</section>
<section id="do-outliers-break-everything" class="level4">
<h4 class="anchored" data-anchor-id="do-outliers-break-everything">Do outliers break everything?</h4>
<p>There’s no real data without real noise and we wanted to have distributions which can be used in practice. One source of noise are <em>outliers</em>, which sometimes can be attributed to errors in data collection or recording (e.g., the equipment was apparently switched off or some piece of experimental setup broke), and are well-known suspects when an estimator behaves badly. In <em>Beyond normal</em> we investigated heavy-tailed distributions (either by applying some transformations to multivariate normal distributions to make the tails heavier, or by using multivariate Student distributions), but we felt that it was not enough.</p>
</section>
</section>
<section id="the-mixtures-and-the-critics" class="level2">
<h2 class="anchored" data-anchor-id="the-mixtures-and-the-critics">The mixtures (and the critics)</h2>
<p>The outliers here were the most concerning and I had the feeling that if <span class="math inline">\(P_{XY}\)</span> is the distribution from which we want to sample and <span class="math inline">\(P_{\tilde X \tilde Y}\)</span> is the “noise distribution” with <span class="math inline">\(I(\tilde X; \tilde Y) = 0\)</span>, then we could perhaps calculate the information contained in the mixture distribution: <span class="math display">\[P_{X'Y'} = (1-\alpha)\, P_{XY} + \alpha \, P_{\tilde X \tilde Y},\]</span> where <span class="math inline">\(\alpha \ll 1\)</span> is the fraction of outliers.</p>
<p>I spent quite some time with a pen and paper trying to calculate <span class="math inline">\(I(X'; Y')\)</span> in terms of <span class="math inline">\(I(X; Y)\)</span>, but I could not really derive anything. Even proving the conjectured bound that <span class="math inline">\(I(X'; Y')\)</span> should not exceed <span class="math inline">\(I(X; Y)\)</span> was hard…</p>
<p>And it’s actually good that I didn’t manage to prove it: this conjecture is false. When I asked Frederic about it, he immediately responded with: 1. An example of two distributions such that each of them encodes 0 bits, but their mixture encodes 1 bit. 2. An example of two distributions such that each of them encodes 1 bit, but their mixture encodes 0 bits.</p>
<p>This is disturbing. As <a href="https://normaldeviate.wordpress.com/2012/08/04/mixture-models-the-twilight-zone-of-statistics/">Larry Wasserman said</a>: <em>I have decided that mixtures, like tequila, are inherently evil and should be avoided at all costs.</em> Fortunately, when I was still struggling with trying to prove it, I recalled Frederic’s histograms, approximating the pointwise mutual information profile. For multivariate normal and Student distributions he sampled a lot of data points <span class="math inline">\((x_1, y_1), \dotsc, (x_n, y_n)\)</span> and then evaluated the pointwise mutual information <span class="math inline">\(i_{XY}(x_i, y_i)\)</span> – which is easy to evaluate using <span class="math inline">\(\log p_{XY}\)</span>, <span class="math inline">\(\log p_X\)</span> and <span class="math inline">\(\log p_Y\)</span> densities – to construct a histogram. The mean of this sample is the estimate of the mutual information <span class="math inline">\(I(X; Y) = \mathbb E_{(x, y)\sim P_{XY} }[i_{XY}(x; y)]\)</span>.</p>
<p>This works for multivariate normal distributions and Student distributions, so why wouldn’t it work for mixture distributions? In the end this is simply a Monte Carlo estimator: we only need to sample a lot of data points (and sampling from a mixture distribution is trivial if one can sample from the component distributions) and evaluate the pointwise mutual information (which can be calculated from the PDFs of the involved variables. The PDF of the mixture distribution can be evaluated using the PDFs of the components).</p>
<p>Hence, although we do not have an <em>exact</em> formula for <span class="math inline">\(I(X; Y)\)</span> where <span class="math inline">\(P_{XY}\)</span> is a mixture of multivariate normal or Student distributions, we have its Monte Carlo approximation. Now we can apply diffeomorphisms to this distribution obtaining more expressive distribution, having e.g., a normalizing flow applied to a Gaussian mixture or a mixture of normalizing flow, or mix all of these again…</p>
<p>Implementation of a prototype in TensorFlow Probability took one day<a href="#fn7" class="footnote-ref" id="fnref7" role="doc-noteref"><sup>7</sup></a> and after a month we had a <a href="../publications/pmi-profiles.html">manuscript</a> ready.</p>
<p>I very much like this concept because:</p>
<ul>
<li>This construction can model outliers as a mixture of different distributions.</li>
<li>We can construct a mixture distribution with a different pointwise mutual information profile than the multivariate normal distribution. Due to the fact that Gaussian mixtures are <em>very</em> expressive, we can build a whole new family of distributions!</li>
<li>We also do not have to model <span class="math inline">\(P_{XY}\)</span> as a multivariate normal distribution transformed by diffeomorphism <span class="math inline">\(f\times g\)</span> – we can use Gaussian (or Student) mixtures! We quickly built a prototype of model-based mutual information estimation with mixtures, which can provide uncertainty quantification on mutual information in the usual Bayesian manner.</li>
</ul>
<p>Let’s finish this post at the place where we started: in <em>Beyond normal</em> we could apply diffeomorphisms to transform continuous <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> variables. The Monte Carlo estimator from the <a href="../publications/pmi-profiles.html">follow-up manuscript</a> applies in exactly the same manner also to the case when <span class="math inline">\(X\)</span> is discrete variable and <span class="math inline">\(Y\)</span> is continuous, which is exactly the case we investigated many years ago!</p>


</section>


<div id="quarto-appendix" class="default"><section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>More precisely, if all the spaces involved are standard Borel, then <span class="math inline">\(f\colon \mathcal X\to \mathcal X'\)</span> and <span class="math inline">\(g\colon \mathcal Y\to \mathcal Y'\)</span> can be continuous injective mappings and e.g., increase the dimensionality of the space. See <a href="https://openreview.net/forum?id=25vRtG56YH">Theorem 2.1 here</a>, which is a well-known fact, but it still took us quite some time to prove it. M.S. Pinsker’s <a href="https://books.google.pl/books/about/Information_and_Information_Stability_of.html?id=seEyAAAAMAAJ"><em>Information and information stability of random variables and processes</em></a> proved to be an invaluable resource.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>As a byproduct we learned <a href="https://snakemake.readthedocs.io/">Snakemake</a>, which transformed my approach to data science entirely. But this is a different story.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>Also, Frederic figured out that the 3rd question (whether the estimators which are invariant to diffeomorphisms) has a trivial answer. If <span class="math inline">\(\mathcal M\)</span> is a connected smooth manifold of dimension at least 2, then for any two sets of distinct points <span class="math inline">\(\{a_1, \dotsc, a_n\}\)</span> and <span class="math inline">\(\{b_1, \dotsc, b_n\}\)</span> there exists a diffeomorphism <span class="math inline">\(u\colon \mathcal M\to \mathcal M\)</span> such that <span class="math inline">\(b_i = u(a_i)\)</span> (the proof of this fact can be e.g., found in P.W. Michor’s and Cornelia Vizman’s <em><span class="math inline">\(n\)</span>-transitivity of certain diffeomorphisms groups</em>). Hence, if <span class="math inline">\(\mathcal X\)</span> and <span class="math inline">\(\mathcal Y\)</span> fulfil the assumptions above, we can move a finite data set as we wish and the only invariant estimator has to return the same answer for <em>any</em> set of input data points.<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p>Let me state two obvious facts. First: “beyond” refers to the fact that we can transform multivariate normal distributions to obtain more expressive distributions. Second: although I intended to name the paper after a wonderful musical, <a href="https://en.wikipedia.org/wiki/Next_to_Normal">Next to Normal</a>, we worried that the title was copyrighted.<a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5"><p>Important detail: “well-approximated” using one statistical distance may me “badly approximated” with respect to another one.<a href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn6"><p>It’s tempting to say that pointwise mutual information <a href="https://en.wikipedia.org/wiki/Tensor#Tensor_densities">transforms as a scalar</a> under the transformations from the group <span class="math inline">\(\mathrm{Diff}(\mathcal X)\times \mathrm{Diff}(\mathcal Y)\)</span>.<a href="#fnref6" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn7"><p>Actually, it took three weeks of me complaining that we couldn’t use multivariate Student distributions in TensorFlow Probability on JAX. Since <a href="https://github.com/tensorflow/probability/issues/1733">I had learned that I was wrong</a>, a few hours passed before <a href="https://github.com/cbg-ethz/bmi/pull/110">we had the prototype</a> and a couple of more days before it was refactored into a stable solution.<a href="#fnref7" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>