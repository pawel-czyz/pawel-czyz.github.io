<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Paweł Czyż">
<meta name="dcterms.date" content="2024-02-19">
<meta name="description" content="Bernoulli, binomial, beta-binomial… Why don’t we talk about the beta-Bernoulli distribution?">

<title>Paweł Czyż - Beta-Bernoulli distribution</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">Paweł Czyż</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../index.html" rel="" target="">
 <span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../blog.html" rel="" target="">
 <span class="menu-text">Blog</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../publications.html" rel="" target="">
 <span class="menu-text">Publications</span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools ms-auto">
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#famous-bernoulli-and-the-company" id="toc-famous-bernoulli-and-the-company" class="nav-link active" data-scroll-target="#famous-bernoulli-and-the-company">Famous Bernoulli and the company</a>
  <ul class="collapse">
  <li><a href="#counting-the-distributions" id="toc-counting-the-distributions" class="nav-link" data-scroll-target="#counting-the-distributions">Counting the distributions</a></li>
  <li><a href="#a-few-coins" id="toc-a-few-coins" class="nav-link" data-scroll-target="#a-few-coins">A few coins</a></li>
  <li><a href="#even-more-coins" id="toc-even-more-coins" class="nav-link" data-scroll-target="#even-more-coins">Even more coins</a></li>
  <li><a href="#noisy-communication-channel" id="toc-noisy-communication-channel" class="nav-link" data-scroll-target="#noisy-communication-channel">Noisy communication channel</a></li>
  <li><a href="#tossing-multiple-coins" id="toc-tossing-multiple-coins" class="nav-link" data-scroll-target="#tossing-multiple-coins">Tossing multiple coins</a></li>
  </ul></li>
  <li><a href="#two-deceptively-similar-distributions" id="toc-two-deceptively-similar-distributions" class="nav-link" data-scroll-target="#two-deceptively-similar-distributions">Two deceptively similar distributions</a>
  <ul class="collapse">
  <li><a href="#denoising-problem" id="toc-denoising-problem" class="nav-link" data-scroll-target="#denoising-problem">Denoising problem</a></li>
  <li><a href="#a-bit-different-model" id="toc-a-bit-different-model" class="nav-link" data-scroll-target="#a-bit-different-model">A bit different model</a></li>
  </ul></li>
  <li><a href="#which-model-is-better-for-denoising" id="toc-which-model-is-better-for-denoising" class="nav-link" data-scroll-target="#which-model-is-better-for-denoising">Which model is better for denoising?</a></li>
  <li><a href="#appendix" id="toc-appendix" class="nav-link" data-scroll-target="#appendix">Appendix</a>
  <ul class="collapse">
  <li><a href="#list-of-distributions" id="toc-list-of-distributions" class="nav-link" data-scroll-target="#list-of-distributions">List of distributions</a></li>
  <li><a href="#beta-bernoulli-sparsity-magic" id="toc-beta-bernoulli-sparsity-magic" class="nav-link" data-scroll-target="#beta-bernoulli-sparsity-magic">Beta-Bernoulli sparsity magic</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Beta-Bernoulli distribution</h1>
</div>

<div>
  <div class="description">
    Bernoulli, binomial, beta-binomial… Why don’t we talk about the beta-Bernoulli distribution?
  </div>
</div>


<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Paweł Czyż </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">February 19, 2024</p>
    </div>
  </div>
  
    
  </div>
  

</header>

<p>I have <a href="../posts/almost-binomial-markov-chain.html">already demonstrated</a> that I don’t know how to properly toss a coin. Let’s do it again.</p>
<section id="famous-bernoulli-and-the-company" class="level2">
<h2 class="anchored" data-anchor-id="famous-bernoulli-and-the-company">Famous Bernoulli and the company</h2>
<section id="counting-the-distributions" class="level3">
<h3 class="anchored" data-anchor-id="counting-the-distributions">Counting the distributions</h3>
<p>Before we go any further: how many distributions can produce outcomes from the set <span class="math inline">\(\{0, 1\}\)</span>?</p>
<p>In other words, we have a measurable space <span class="math inline">\(\{0, 1\}\)</span> (where all four subsets are measurable) and we would like to know how many probability measures exist on this space. We have <span class="math inline">\(P(\varnothing) = 0\)</span> and <span class="math inline">\(P(\{0, 1\}) = 1\)</span> straight from the definition of a probability measure. As we need to have <span class="math inline">\(P(\{0\}) + P(\{1\}) = 1\)</span> we see that there is a bijection between these probability measures and numbers from the set <span class="math inline">\([0, 1]\)</span>, given by <span class="math inline">\(P_b(\{1\}) = b\)</span> for any <em>bias</em> <span class="math inline">\(b\in [0, 1]\)</span>. This distribution is called <span class="math inline">\(\mathrm{Bernoulli}(b)\)</span> and it’s easy to prove that if <span class="math inline">\(X\sim \mathrm{Bernoulli}(b)\)</span>, then <span class="math display">\[
\mathbb E[X] = P(X=1) = b.
\]</span></p>
<p>Hence, the first moment fully determines any distribution on <span class="math inline">\(\{0, 1\}\)</span>.</p>
<p>The derivation of variance is quite elegant, once one notices that if <span class="math inline">\(X\sim \mathrm{Bernoulli}(b)\)</span>, then also <span class="math inline">\(X^2\sim \mathrm{Bernoulli}(b)\)</span>, because it has to be <em>some</em> Bernoulli distribution and <span class="math inline">\(P(X^2=1) = P(X=1)\)</span>. Then, we have: <span class="math display">\[
\mathbb V[X] = \mathbb E[X^2] - \mathbb E[X]^2 = b - b^2 = b(1-b).
\]</span></p>
<p>By considering both cases, we see that for every outcome <span class="math inline">\(x \in \{0, 1\}\)</span>, the likelihood is given by <span class="math display">\[
\mathrm{Bernoulli}(x\mid b) = b^x(1-b)^{1-x}.
\]</span></p>
</section>
<section id="a-few-coins" class="level3">
<h3 class="anchored" data-anchor-id="a-few-coins">A few coins</h3>
<p>This characterization of distributions over <span class="math inline">\(\{0, 1\}\)</span> is very powerful.</p>
<p>Consider the following problem: we have <span class="math inline">\(K\)</span> coins with biases <span class="math inline">\(b_1, \dotsc, b_K\)</span> and we throw a loaded die which can give <span class="math inline">\(K\)</span> different outcomes, each with probability <span class="math inline">\(d_1, \dotsc, d_K\)</span> (which, of course, sum up to <span class="math inline">\(1\)</span>) to decide which coin we will toss. What is the outcome of this distribution? It is, of course, a coin toss outcome, which is a number from the set <span class="math inline">\(\{0, 1\}\)</span>. Hence, this has to be some Bernoulli distribution. Which one? Bernoulli distributions are fully determined by their expectations, and the expectation in this case is given by a weighted average <span class="math inline">\(\bar b = b_1 d_1 + \cdots + b_K d_K\)</span>. In other words, we can replace a loaded die and <span class="math inline">\(K\)</span> biased coins with just a single biased coin.</p>
<p>To have more equations, the first procedure corresponds to <span class="math display">\[\begin{align*}
D &amp;\sim \mathrm{Categorical}(d_1, \dotsc, d_K)\\
X \mid D &amp;\sim \mathrm{Bernoulli}(b_D)
\end{align*}
\]</span></p>
<p>and the second one to <span class="math display">\[
Y \sim \mathrm{Bernoulli}(\bar b),
\]</span></p>
<p>with <span class="math inline">\(\bar b = b_1d_1 + \cdots + b_Kd_K\)</span>.</p>
<p>Both of these distributions are the same, i.e., <span class="math inline">\(\mathrm{law}\, X = \mathrm{law}\, Y\)</span>.</p>
<p>In particular, the likelihood has to be the same, proving an equality <span class="math display">\[
\sum_{k=1}^K d_k\, b_k^x(1-b_k)^{1-x} = \bar b^x(1-\bar b)^{1-x}
\]</span></p>
<p>for every <span class="math inline">\(x\in \{0, 1\}\)</span>.</p>
</section>
<section id="even-more-coins" class="level3">
<h3 class="anchored" data-anchor-id="even-more-coins">Even more coins</h3>
<p>Even more interesting, consider infinitely many coins with different biases, which are chosen according to a beta distribution. Once the coin is chosen, we toss it: <span class="math display">\[\begin{align*}
B &amp;\sim \mathrm{Beta}(\alpha, \beta)\\
X\mid B &amp;\sim \mathrm{Bernoulli}(B)
\end{align*}
\]</span></p>
<p>This is a continuous mixture, which we might call <span class="math inline">\(\mathrm{BetaBernoulli}(\alpha, \beta)\)</span>… if it weren’t just a plain Bernoulli distribution with bias <span class="math display">\[
\mathbb E[X] = \mathbb E[\mathbb E[X\mid B]] = \mathbb E[B] = \frac{\alpha}{\alpha + \beta}.
\]</span></p>
</section>
<section id="noisy-communication-channel" class="level3">
<h3 class="anchored" data-anchor-id="noisy-communication-channel">Noisy communication channel</h3>
<p>Let’s consider an example involving plain Bernoulli distributions and a noisy communication channel.</p>
<p>Let <span class="math inline">\(X\sim \mathrm{Bernoulli}(b)\)</span> be an input variable. The binary output, <span class="math inline">\(Y\)</span>, is a noisy version of <span class="math inline">\(X\)</span>, with <span class="math inline">\(\alpha\)</span> controlling the false positive rate and <span class="math inline">\(\beta\)</span> the false negative rate: <span class="math display">\[
P(Y = 1 \mid X=1) = 1-\beta, \quad P(Y=1\mid X=0) = \alpha.
\]</span></p>
<p>We can write this model as: <span class="math display">\[\begin{align*}
X &amp;\sim \mathrm{Bernoulli}(b)\\
Y \mid X &amp;\sim \mathrm{Bernoulli}( X(1-\beta) + (1-X) \alpha)
\end{align*}
\]</span></p>
<p>In fact, we have <a href="#finite-mixture">already seen</a> this example: we can treat <span class="math inline">\(X\)</span> as a special case of a loaded dice, indexing a finite mixture with just two components. Hence, the marginal distribution of <span class="math inline">\(Y\)</span> is <span class="math display">\[
Y \sim \mathrm{Bernoulli}(b(1-\beta) + (1-b) \alpha).
\]</span></p>
</section>
<section id="tossing-multiple-coins" class="level3">
<h3 class="anchored" data-anchor-id="tossing-multiple-coins">Tossing multiple coins</h3>
<p>We know that a single coin toss characterises all probability distributions on the set <span class="math inline">\(\{0, 1\}\)</span>. However, once we consider <span class="math inline">\(N\ge 2\)</span> coin tosses, yielding outcomes in the set <span class="math inline">\(\{0, 1, 2, \dotsc, N\}\)</span>, many <em>different distributions</em> will appear.</p>
<p>We mentioned some of these distributions in <a href="../posts/almost-binomial-markov-chain.html">this post</a>, but just for completeness, <a href="#list-of-distributions">at the end</a> there is a list of standard distributions.</p>
<p>Similarly, if one is interested in modeling binary vectors, which are from the set <span class="math inline">\(\{0, 1\}\times \{0, 1\} \cdots \{0, 1\}\)</span>, many different distributions will appear. Let’s analyse an example below.</p>
</section>
</section>
<section id="two-deceptively-similar-distributions" class="level2">
<h2 class="anchored" data-anchor-id="two-deceptively-similar-distributions">Two deceptively similar distributions</h2>
<section id="denoising-problem" class="level3">
<h3 class="anchored" data-anchor-id="denoising-problem">Denoising problem</h3>
<p>We have a fixed bit <span class="math inline">\(T \in \{0, 1\}\)</span> and we observe its noisy measurements, with false negative rate <span class="math inline">\(\beta\)</span> and false positive rate <span class="math inline">\(\alpha\)</span> (recall <a href="#noisy-communication-channel">this section</a>). We will write <span class="math inline">\(c_0 = \alpha\)</span> and <span class="math inline">\(c_1 = 1-\beta\)</span> and put some prior on <span class="math inline">\(T\)</span>: <span class="math display">\[\begin{align*}
  \theta &amp;\sim \mathrm{Beta}(\alpha, \beta)\\
  T\mid \theta &amp;\sim \mathrm{Bernoulli}(\theta)\\
  X_n\mid T &amp;\sim \mathrm{Bernoulli}( c_0(1-T) + c_1T) \text{ for } n = 1, \cdots, N
\end{align*}
\]</span></p>
<p>Let’s use shorthand notation <span class="math inline">\(\mathbf{X} = (X_1, \dotsc, X_N)\)</span> and note that the likelihood is: <span class="math display">\[\begin{align*}
  P(\mathbf{X} \mid T) &amp;= \prod_{n=1}^N P(X_n \mid b(T) ) \\
  &amp;= \prod_{n=1}^N b(T) ^{X_n} \left(1-b(T)\right)^{1-X_n} \\
  &amp;= b(T)^S \left(1-b(T)\right)^{N-S}
\end{align*}
\]</span></p>
<p>where <span class="math inline">\(b(T) = c_0(1-T) + c_1T\)</span> and <span class="math inline">\(S = X_1 + \cdots + X_N\)</span>.</p>
<p>After reading the <a href="#famous-bernoulli-and-the-company">discussion above</a>, there is a natural question: why is <span class="math inline">\(\theta\)</span> introduced at all? For a simple denoising question, i.e., finding <span class="math inline">\(P(T\mid \mathbf{X}) \propto P(\mathbf{X} \mid T) P(T)\)</span>, this parameter is not needed at all: <span class="math inline">\(P(T)\)</span> is just a Bernoulli variable, with bias parameter <span class="math inline">\(\bar \theta = \alpha/(\alpha + \beta)\)</span>. Then, <span class="math display">\[
  P(T=1\mid \mathbf{X}) = \frac{ c_1^S (1-c_1)^{N-S} \cdot \bar \theta }{ c_1^S(1-c_1)^{N-S} \cdot \bar \theta + c_0^S(1-c_0)^{N-S}\cdot (1-\bar \theta) }.
\]</span></p>
<p>Let’s quickly implement this formula and see how the posterior looks if we start with an unbiased coin (i.e., <span class="math inline">\(\bar \theta=0.5\)</span>), observe <span class="math inline">\(S = N - 1\)</span> successes, and we vary the noise level <span class="math inline">\(\alpha = \beta = c_0 = 1 - c_1\)</span>:</p>
<div class="cell" data-execution_count="1">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> typing <span class="im">import</span> Callable, NamedTuple</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> jax</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> jax.numpy <span class="im">as</span> jnp</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpyro</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpyro.distributions <span class="im">as</span> dist</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>plt.style.use(<span class="st">"dark_background"</span>)</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> _binomial_loglikelihood(n: <span class="bu">int</span>, s: <span class="bu">int</span>, bias: <span class="bu">float</span>) <span class="op">-&gt;</span> <span class="bu">float</span>:</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> s <span class="op">*</span> jnp.log(bias) <span class="op">+</span> (n <span class="op">-</span> s) <span class="op">*</span> jnp.log1p(<span class="op">-</span>bias)</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> LikelihoodArgs(NamedTuple):</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>  n: <span class="bu">int</span>  <span class="co"># Number of throws</span></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>  s: <span class="bu">int</span>  <span class="co"># Number of successes, 0 &lt;= s &lt;= n</span></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>  c0: <span class="bu">float</span>  <span class="co"># Probability of observing success if T = 0 (false positive rate) </span></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>  c1: <span class="bu">float</span>  <span class="co"># Probability of observing success if T = 1 (true positive rate)</span></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> BetaPriorArgs(NamedTuple):</span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>  alpha: <span class="bu">float</span></span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>  beta: <span class="bu">float</span></span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>  <span class="at">@property</span></span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>  <span class="kw">def</span> mean(<span class="va">self</span>) <span class="op">-&gt;</span> <span class="bu">float</span>:</span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="va">self</span>.alpha <span class="op">/</span> (<span class="va">self</span>.alpha <span class="op">+</span> <span class="va">self</span>.beta)</span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> posterior_t(</span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a>  bias: <span class="bu">float</span>,</span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a>  like: LikelihoodArgs,</span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a>) <span class="op">-&gt;</span> <span class="bu">float</span>:</span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a>  <span class="co">"""Calculates P(T | data),</span></span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a><span class="co">  where the prior is given by T ~ Bernoulli(bias)."""</span></span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a>  n, s, c0, c1 <span class="op">=</span> like.n, like.s, like.c0, like.c1</span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a>  log_0 <span class="op">=</span> _binomial_loglikelihood(n<span class="op">=</span>n, s<span class="op">=</span>s, bias<span class="op">=</span>c0) <span class="op">+</span> jnp.log1p(<span class="op">-</span>bias)</span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a>  log_1 <span class="op">=</span> _binomial_loglikelihood(n<span class="op">=</span>n, s<span class="op">=</span>s, bias<span class="op">=</span>c1) <span class="op">+</span> jnp.log(bias)</span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> jax.nn.softmax(jnp.array([log_0, log_1]))</span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true" tabindex="-1"></a>fig, axs <span class="op">=</span> plt.subplots(</span>
<span id="cb1-45"><a href="#cb1-45" aria-hidden="true" tabindex="-1"></a>  <span class="dv">1</span>, <span class="dv">3</span>, dpi<span class="op">=</span><span class="dv">120</span>, figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="fl">2.4</span>), sharex<span class="op">=</span><span class="va">True</span>, sharey<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb1-46"><a href="#cb1-46" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb1-47"><a href="#cb1-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-48"><a href="#cb1-48" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> ax <span class="kw">in</span> axs:</span>
<span id="cb1-49"><a href="#cb1-49" aria-hidden="true" tabindex="-1"></a>  ax.spines[[<span class="st">'top'</span>, <span class="st">'right'</span>]].set_visible(<span class="va">False</span>)</span>
<span id="cb1-50"><a href="#cb1-50" aria-hidden="true" tabindex="-1"></a>  ax.set_xticks([<span class="dv">0</span>, <span class="dv">1</span>])</span>
<span id="cb1-51"><a href="#cb1-51" aria-hidden="true" tabindex="-1"></a>  ax.set_xlim([<span class="op">-</span><span class="fl">0.5</span>, <span class="fl">1.5</span>])</span>
<span id="cb1-52"><a href="#cb1-52" aria-hidden="true" tabindex="-1"></a>  ax.set_ylim([<span class="op">-</span><span class="fl">0.1</span>, <span class="fl">1.1</span>])</span>
<span id="cb1-53"><a href="#cb1-53" aria-hidden="true" tabindex="-1"></a>  ax.set_yticks([<span class="dv">0</span>, <span class="fl">0.25</span>, <span class="fl">0.5</span>, <span class="fl">0.75</span>, <span class="dv">1</span>])</span>
<span id="cb1-54"><a href="#cb1-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-55"><a href="#cb1-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-56"><a href="#cb1-56" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> ax, n <span class="kw">in</span> <span class="bu">zip</span>(axs, [<span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">5</span>]):</span>
<span id="cb1-57"><a href="#cb1-57" aria-hidden="true" tabindex="-1"></a>  ax.set_title(<span class="ss">f"$N = </span><span class="sc">{</span>n<span class="sc">}</span><span class="ss">$, $S = </span><span class="sc">{</span>n<span class="op">-</span><span class="dv">1</span><span class="sc">}</span><span class="ss">$"</span>)</span>
<span id="cb1-58"><a href="#cb1-58" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> i, noise <span class="kw">in</span> <span class="bu">enumerate</span>([<span class="fl">0.01</span>, <span class="fl">0.1</span>, <span class="fl">0.25</span>, <span class="fl">0.45</span>]):</span>
<span id="cb1-59"><a href="#cb1-59" aria-hidden="true" tabindex="-1"></a>    like <span class="op">=</span> LikelihoodArgs(</span>
<span id="cb1-60"><a href="#cb1-60" aria-hidden="true" tabindex="-1"></a>      n<span class="op">=</span>n,</span>
<span id="cb1-61"><a href="#cb1-61" aria-hidden="true" tabindex="-1"></a>      s<span class="op">=</span>n <span class="op">-</span> <span class="dv">1</span>,</span>
<span id="cb1-62"><a href="#cb1-62" aria-hidden="true" tabindex="-1"></a>      c0<span class="op">=</span>noise,</span>
<span id="cb1-63"><a href="#cb1-63" aria-hidden="true" tabindex="-1"></a>      c1<span class="op">=</span><span class="dv">1</span><span class="op">-</span>noise,</span>
<span id="cb1-64"><a href="#cb1-64" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb1-65"><a href="#cb1-65" aria-hidden="true" tabindex="-1"></a>    posterior <span class="op">=</span> posterior_t(bias<span class="op">=</span><span class="fl">0.5</span>, like<span class="op">=</span>like)</span>
<span id="cb1-66"><a href="#cb1-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-67"><a href="#cb1-67" aria-hidden="true" tabindex="-1"></a>    ax.plot([<span class="dv">0</span>, <span class="dv">1</span>], posterior, label<span class="op">=</span><span class="ss">f"</span><span class="sc">{</span>noise<span class="sc">:.2f}</span><span class="ss">"</span>, c<span class="op">=</span><span class="ss">f"C</span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb1-68"><a href="#cb1-68" aria-hidden="true" tabindex="-1"></a>    ax.scatter([<span class="dv">0</span>, <span class="dv">1</span>], posterior, c<span class="op">=</span><span class="ss">f"C</span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">"</span>, s<span class="op">=</span><span class="dv">4</span>)</span>
<span id="cb1-69"><a href="#cb1-69" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb1-70"><a href="#cb1-70" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> axs[<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb1-71"><a href="#cb1-71" aria-hidden="true" tabindex="-1"></a>ax.legend(frameon<span class="op">=</span><span class="va">False</span>, bbox_to_anchor<span class="op">=</span>(<span class="fl">1.05</span>, <span class="dv">1</span>))</span>
<span id="cb1-72"><a href="#cb1-72" aria-hidden="true" tabindex="-1"></a>fig.tight_layout()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stderr">
<pre><code>/home/pawel/micromamba/envs/data-science/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html
  from .autonotebook import tqdm as notebook_tqdm
An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="beta-bernoulli_files/figure-html/cell-2-output-2.png" class="img-fluid"></p>
</div>
</div>
<p>This formula is elegant: we can use it to solve the denoising problem, i.e., to infer the value of the <span class="math inline">\(T\)</span> variable. Why did we introduce the <span class="math inline">\(\theta\)</span> variable with its own prior? We may be interested not only in inferring a missing coin toss, but also in learning about the unknown bias <span class="math inline">\(\theta\)</span> of the coin from this experiment.</p>
<p>For example, if <span class="math inline">\(T\)</span> were directly observed, we would have <span class="math inline">\(P(\theta \mid T) = \mathrm{Beta}(\theta \mid \alpha + T, \beta + (1-T) )\)</span>:</p>
<div class="cell" data-execution_count="2">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>fig, axs <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">3</span>, figsize<span class="op">=</span>(<span class="dv">5</span>, <span class="fl">2.3</span>), dpi<span class="op">=</span><span class="dv">200</span>, sharex<span class="op">=</span><span class="va">True</span>, sharey<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> ax <span class="kw">in</span> axs:</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>  ax.spines[[<span class="st">'top'</span>, <span class="st">'right'</span>]].set_visible(<span class="va">False</span>)</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>  ax.set_xlim([<span class="op">-</span><span class="fl">0.02</span>, <span class="fl">1.02</span>])</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>bins <span class="op">=</span> jnp.linspace(<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">31</span>)</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>key <span class="op">=</span> jax.random.PRNGKey(<span class="dv">1</span>)</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>key, <span class="op">*</span>subkeys <span class="op">=</span> jax.random.split(key, <span class="dv">4</span>)</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> axs[<span class="dv">0</span>]</span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="st">"Prior</span><span class="ch">\n</span><span class="st">Beta(1, 1)"</span>)</span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>samples <span class="op">=</span> dist.Beta(<span class="dv">1</span>, <span class="dv">1</span>).sample(subkeys[<span class="dv">0</span>], sample_shape<span class="op">=</span>(<span class="dv">10_000</span>,))</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>ax.hist(samples, bins<span class="op">=</span>bins, density<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> axs[<span class="dv">1</span>]</span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="st">"Posterior for $T=0$</span><span class="ch">\n</span><span class="st">Beta(1, 2)"</span>)</span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a>samples <span class="op">=</span> dist.Beta(<span class="dv">1</span>, <span class="dv">2</span>).sample(subkeys[<span class="dv">0</span>], sample_shape<span class="op">=</span>(<span class="dv">10_000</span>,))</span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a>ax.hist(samples, bins<span class="op">=</span>bins, density<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> axs[<span class="dv">2</span>]</span>
<span id="cb3-23"><a href="#cb3-23" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="st">"Posterior for $T=1$</span><span class="ch">\n</span><span class="st">Beta(2, 1)"</span>)</span>
<span id="cb3-24"><a href="#cb3-24" aria-hidden="true" tabindex="-1"></a>samples <span class="op">=</span> dist.Beta(<span class="dv">2</span>, <span class="dv">1</span>).sample(subkeys[<span class="dv">0</span>], sample_shape<span class="op">=</span>(<span class="dv">10_000</span>,))</span>
<span id="cb3-25"><a href="#cb3-25" aria-hidden="true" tabindex="-1"></a>ax.hist(samples, bins<span class="op">=</span>bins, density<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb3-26"><a href="#cb3-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-27"><a href="#cb3-27" aria-hidden="true" tabindex="-1"></a>fig.tight_layout()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="beta-bernoulli_files/figure-html/cell-3-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>As <span class="math inline">\(T\)</span> is not directly observed, we have to look at <span class="math inline">\(P(\theta \mid \mathbf{X})\)</span>: <span class="math display">\[\begin{align*}
  P(\theta \mid \mathbf{X}) &amp;= \sum_{t} P(\theta, T=t \mid \mathbf{X}) \\
  &amp;= \sum_{t} P(\theta \mid T=t,  \mathbf{X})P(T=t\mid \mathbf{X})\\
  &amp;= \sum_{t} P(\theta \mid T=t) P(T=t\mid \mathbf{X}).
\end{align*}
\]</span></p>
<p>This is therefore a mixture of <span class="math inline">\(\mathrm{Beta}(\alpha+1, \beta)\)</span> and <span class="math inline">\(\mathrm{Beta}(\alpha, \beta + 1)\)</span> distributions:</p>
<div class="cell" data-execution_count="3">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>_noise <span class="op">=</span> <span class="fl">0.2</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> create_posterior_theta(</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>  prior: BetaPriorArgs,</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>  like: LikelihoodArgs,</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>):</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>  mixing <span class="op">=</span> dist.Categorical(</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>    probs<span class="op">=</span>posterior_t(</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>      bias<span class="op">=</span>prior.mean, like<span class="op">=</span>like</span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>  ))</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> dist.MixtureSameFamily(</span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>    mixing_distribution<span class="op">=</span>mixing,</span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a>    component_distribution<span class="op">=</span>dist.Beta(</span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a>      <span class="co"># alpha</span></span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a>      concentration1<span class="op">=</span>jnp.array([prior.alpha, prior.alpha <span class="op">+</span> <span class="dv">1</span>]),</span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a>      <span class="co"># beta</span></span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a>      concentration0<span class="op">=</span>jnp.array([prior.beta <span class="op">+</span> <span class="dv">1</span>, prior.beta]),</span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a>fig, axs <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">3</span>, figsize<span class="op">=</span>(<span class="dv">5</span>, <span class="fl">2.3</span>), dpi<span class="op">=</span><span class="dv">200</span>, sharex<span class="op">=</span><span class="va">True</span>, sharey<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> ax <span class="kw">in</span> axs:</span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a>  ax.spines[[<span class="st">'top'</span>, <span class="st">'right'</span>]].set_visible(<span class="va">False</span>)</span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a>  ax.set_xlim([<span class="op">-</span><span class="fl">0.02</span>, <span class="fl">1.02</span>])</span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-27"><a href="#cb4-27" aria-hidden="true" tabindex="-1"></a>key, <span class="op">*</span>subkeys <span class="op">=</span> jax.random.split(key, <span class="dv">4</span>)</span>
<span id="cb4-28"><a href="#cb4-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-29"><a href="#cb4-29" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> axs[<span class="dv">0</span>]</span>
<span id="cb4-30"><a href="#cb4-30" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="st">"$N=S=1$"</span>)</span>
<span id="cb4-31"><a href="#cb4-31" aria-hidden="true" tabindex="-1"></a>d <span class="op">=</span> create_posterior_theta(</span>
<span id="cb4-32"><a href="#cb4-32" aria-hidden="true" tabindex="-1"></a>  prior<span class="op">=</span>BetaPriorArgs(alpha<span class="op">=</span><span class="fl">1.0</span>, beta<span class="op">=</span><span class="fl">1.0</span>),</span>
<span id="cb4-33"><a href="#cb4-33" aria-hidden="true" tabindex="-1"></a>  like<span class="op">=</span>LikelihoodArgs(</span>
<span id="cb4-34"><a href="#cb4-34" aria-hidden="true" tabindex="-1"></a>    n<span class="op">=</span><span class="dv">1</span>, s<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb4-35"><a href="#cb4-35" aria-hidden="true" tabindex="-1"></a>    c0<span class="op">=</span>_noise, c1<span class="op">=</span><span class="dv">1</span><span class="op">-</span>_noise,</span>
<span id="cb4-36"><a href="#cb4-36" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb4-37"><a href="#cb4-37" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb4-38"><a href="#cb4-38" aria-hidden="true" tabindex="-1"></a>samples <span class="op">=</span> d.sample(subkeys[<span class="dv">0</span>], sample_shape<span class="op">=</span>(<span class="dv">10_000</span>,))</span>
<span id="cb4-39"><a href="#cb4-39" aria-hidden="true" tabindex="-1"></a>ax.hist(samples, bins<span class="op">=</span>bins, density<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb4-40"><a href="#cb4-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-41"><a href="#cb4-41" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> axs[<span class="dv">1</span>]</span>
<span id="cb4-42"><a href="#cb4-42" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="st">"$N=S=2$"</span>)</span>
<span id="cb4-43"><a href="#cb4-43" aria-hidden="true" tabindex="-1"></a>d <span class="op">=</span> create_posterior_theta(</span>
<span id="cb4-44"><a href="#cb4-44" aria-hidden="true" tabindex="-1"></a>  prior<span class="op">=</span>BetaPriorArgs(alpha<span class="op">=</span><span class="fl">1.0</span>, beta<span class="op">=</span><span class="fl">1.0</span>),</span>
<span id="cb4-45"><a href="#cb4-45" aria-hidden="true" tabindex="-1"></a>  like<span class="op">=</span>LikelihoodArgs(</span>
<span id="cb4-46"><a href="#cb4-46" aria-hidden="true" tabindex="-1"></a>    n<span class="op">=</span><span class="dv">2</span>, s<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb4-47"><a href="#cb4-47" aria-hidden="true" tabindex="-1"></a>    c0<span class="op">=</span>_noise, c1<span class="op">=</span><span class="dv">1</span><span class="op">-</span>_noise,</span>
<span id="cb4-48"><a href="#cb4-48" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb4-49"><a href="#cb4-49" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb4-50"><a href="#cb4-50" aria-hidden="true" tabindex="-1"></a>samples <span class="op">=</span> d.sample(subkeys[<span class="dv">0</span>], sample_shape<span class="op">=</span>(<span class="dv">10_000</span>,))</span>
<span id="cb4-51"><a href="#cb4-51" aria-hidden="true" tabindex="-1"></a>ax.hist(samples, bins<span class="op">=</span>bins, density<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb4-52"><a href="#cb4-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-53"><a href="#cb4-53" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> axs[<span class="dv">2</span>]</span>
<span id="cb4-54"><a href="#cb4-54" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="st">"$N=S=100$"</span>)</span>
<span id="cb4-55"><a href="#cb4-55" aria-hidden="true" tabindex="-1"></a>d <span class="op">=</span> create_posterior_theta(</span>
<span id="cb4-56"><a href="#cb4-56" aria-hidden="true" tabindex="-1"></a>  prior<span class="op">=</span>BetaPriorArgs(alpha<span class="op">=</span><span class="fl">1.0</span>, beta<span class="op">=</span><span class="fl">1.0</span>),</span>
<span id="cb4-57"><a href="#cb4-57" aria-hidden="true" tabindex="-1"></a>  like<span class="op">=</span>LikelihoodArgs(</span>
<span id="cb4-58"><a href="#cb4-58" aria-hidden="true" tabindex="-1"></a>    n<span class="op">=</span><span class="dv">100</span>, s<span class="op">=</span><span class="dv">100</span>,</span>
<span id="cb4-59"><a href="#cb4-59" aria-hidden="true" tabindex="-1"></a>    c0<span class="op">=</span>_noise, c1<span class="op">=</span><span class="dv">1</span><span class="op">-</span>_noise,</span>
<span id="cb4-60"><a href="#cb4-60" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb4-61"><a href="#cb4-61" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb4-62"><a href="#cb4-62" aria-hidden="true" tabindex="-1"></a>samples <span class="op">=</span> d.sample(subkeys[<span class="dv">0</span>], sample_shape<span class="op">=</span>(<span class="dv">10_000</span>,))</span>
<span id="cb4-63"><a href="#cb4-63" aria-hidden="true" tabindex="-1"></a>ax.hist(samples, bins<span class="op">=</span>bins, density<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb4-64"><a href="#cb4-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-65"><a href="#cb4-65" aria-hidden="true" tabindex="-1"></a>fig.tight_layout()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="beta-bernoulli_files/figure-html/cell-4-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>Importantly, we see that the posterior on <span class="math inline">\(\theta\)</span> will not shrink even for large <span class="math inline">\(N\)</span>: increasing <span class="math inline">\(N\)</span> affects how well we can determine the outcome of the toss <span class="math inline">\(T\)</span>. But even if we observe <span class="math inline">\(T\)</span> perfectly, it’s only one toss, so it does not give too much information on the bias <span class="math inline">\(\theta\)</span>.</p>
<p>Let’s now look at this problem also from a bit different perspective. If we want to update <span class="math inline">\(\theta\)</span> directly from the observed data <span class="math inline">\(\mathbf{X}\)</span>, we can marginalise <span class="math inline">\(T\)</span> out in the likelihood: <span class="math display">\[\begin{align*}
  P(\mathbf{X} \mid \theta ) &amp;= \sum_{t} P(\mathbf{X} \mid T=t) P(T=t\mid \theta) \\
  &amp;= \theta P(\mathbf{X} \mid T=1) + (1-\theta) P( \mathbf{X}\mid T=0) \\
  &amp;= \theta c_1^{S}(1-c_1)^{N-S} + (1-\theta) c_0^S(1-c_0)^{N-S}.
\end{align*}
\]</span></p>
<p>We have now only one continuous variable and we could use Hamiltonian Monte Carlo to sample from the distribution <span class="math inline">\(P(\theta \mid \mathbf{X})\)</span>. We have already determined it analytically, as a mixture of beta distributions, but let’s quickly compare the results:</p>
<div class="cell" data-execution_count="4">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> numpyro.infer <span class="im">import</span> MCMC, NUTS</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> create_model(like: LikelihoodArgs, prior: BetaPriorArgs):</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>  theta <span class="op">=</span> numpyro.sample(<span class="st">"theta"</span>, </span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>    dist.Beta(concentration1<span class="op">=</span>prior.alpha, concentration0<span class="op">=</span>prior.beta)</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>  log1 <span class="op">=</span> _binomial_loglikelihood(n<span class="op">=</span>like.n, s<span class="op">=</span>like.s, bias<span class="op">=</span>like.c1) <span class="op">+</span> jnp.log(theta)</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>  log0 <span class="op">=</span> _binomial_loglikelihood(n<span class="op">=</span>like.n, s<span class="op">=</span>like.s, bias<span class="op">=</span>like.c0) <span class="op">+</span> jnp.log1p(<span class="op">-</span>theta)</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>  numpyro.factor(<span class="st">"loglikelihood"</span>, jnp.logaddexp(log1, log0))</span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>prior_args <span class="op">=</span> BetaPriorArgs(</span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a>  alpha<span class="op">=</span><span class="fl">1.0</span>,</span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a>  beta<span class="op">=</span><span class="fl">1.0</span>,</span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a>like_args <span class="op">=</span> LikelihoodArgs(</span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a>  n<span class="op">=</span><span class="dv">3</span>,</span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a>  s<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a>  c0<span class="op">=</span><span class="fl">0.1</span>,</span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true" tabindex="-1"></a>  c1<span class="op">=</span><span class="fl">0.9</span>,</span>
<span id="cb5-23"><a href="#cb5-23" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb5-24"><a href="#cb5-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-25"><a href="#cb5-25" aria-hidden="true" tabindex="-1"></a>fig, axs <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">3</span>, figsize<span class="op">=</span>(<span class="dv">5</span>, <span class="fl">2.3</span>), dpi<span class="op">=</span><span class="dv">200</span>, sharex<span class="op">=</span><span class="va">True</span>, sharey<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb5-26"><a href="#cb5-26" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> ax <span class="kw">in</span> axs:</span>
<span id="cb5-27"><a href="#cb5-27" aria-hidden="true" tabindex="-1"></a>  ax.spines[[<span class="st">'top'</span>, <span class="st">'right'</span>]].set_visible(<span class="va">False</span>)</span>
<span id="cb5-28"><a href="#cb5-28" aria-hidden="true" tabindex="-1"></a>  ax.set_xlim([<span class="op">-</span><span class="fl">0.02</span>, <span class="fl">1.02</span>])</span>
<span id="cb5-29"><a href="#cb5-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-30"><a href="#cb5-30" aria-hidden="true" tabindex="-1"></a>key, <span class="op">*</span>subkeys <span class="op">=</span> jax.random.split(key, <span class="dv">4</span>)</span>
<span id="cb5-31"><a href="#cb5-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-32"><a href="#cb5-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-33"><a href="#cb5-33" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> axs[<span class="dv">0</span>]</span>
<span id="cb5-34"><a href="#cb5-34" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="st">"$P(</span><span class="ch">\\</span><span class="st">theta </span><span class="ch">\\</span><span class="st">mid T=1)$"</span>)</span>
<span id="cb5-35"><a href="#cb5-35" aria-hidden="true" tabindex="-1"></a>samples <span class="op">=</span> dist.Beta(<span class="dv">2</span>, <span class="dv">1</span>).sample(subkeys[<span class="dv">0</span>], sample_shape<span class="op">=</span>(<span class="dv">100_000</span>,))</span>
<span id="cb5-36"><a href="#cb5-36" aria-hidden="true" tabindex="-1"></a>ax.hist(samples, bins<span class="op">=</span>bins, density<span class="op">=</span><span class="va">True</span>, color<span class="op">=</span><span class="st">"C2"</span>)</span>
<span id="cb5-37"><a href="#cb5-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-38"><a href="#cb5-38" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> axs[<span class="dv">1</span>]</span>
<span id="cb5-39"><a href="#cb5-39" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="st">"$P(</span><span class="ch">\\</span><span class="st">theta</span><span class="ch">\\</span><span class="st">mid </span><span class="ch">\\</span><span class="st">mathbf</span><span class="sc">{X}</span><span class="st">)$</span><span class="ch">\n</span><span class="st">(explicit)"</span>)</span>
<span id="cb5-40"><a href="#cb5-40" aria-hidden="true" tabindex="-1"></a>d <span class="op">=</span> create_posterior_theta(</span>
<span id="cb5-41"><a href="#cb5-41" aria-hidden="true" tabindex="-1"></a>  like<span class="op">=</span>like_args,</span>
<span id="cb5-42"><a href="#cb5-42" aria-hidden="true" tabindex="-1"></a>  prior<span class="op">=</span>prior_args,</span>
<span id="cb5-43"><a href="#cb5-43" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb5-44"><a href="#cb5-44" aria-hidden="true" tabindex="-1"></a>samples <span class="op">=</span> d.sample(subkeys[<span class="dv">1</span>], sample_shape<span class="op">=</span>(<span class="dv">100_000</span>,))</span>
<span id="cb5-45"><a href="#cb5-45" aria-hidden="true" tabindex="-1"></a>ax.hist(samples, bins<span class="op">=</span>bins, density<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb5-46"><a href="#cb5-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-47"><a href="#cb5-47" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> axs[<span class="dv">2</span>]</span>
<span id="cb5-48"><a href="#cb5-48" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="st">"$P(</span><span class="ch">\\</span><span class="st">theta</span><span class="ch">\\</span><span class="st">mid </span><span class="ch">\\</span><span class="st">mathbf</span><span class="sc">{X}</span><span class="st">)$</span><span class="ch">\n</span><span class="st">(HMC)"</span>)</span>
<span id="cb5-49"><a href="#cb5-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-50"><a href="#cb5-50" aria-hidden="true" tabindex="-1"></a>nuts_kernel <span class="op">=</span> NUTS(create_model)</span>
<span id="cb5-51"><a href="#cb5-51" aria-hidden="true" tabindex="-1"></a>mcmc <span class="op">=</span> MCMC(nuts_kernel, num_warmup<span class="op">=</span><span class="dv">1000</span>, num_samples<span class="op">=</span><span class="dv">10_000</span>, num_chains<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb5-52"><a href="#cb5-52" aria-hidden="true" tabindex="-1"></a>mcmc.run(subkeys[<span class="dv">2</span>], like<span class="op">=</span>like_args, prior<span class="op">=</span>prior_args)</span>
<span id="cb5-53"><a href="#cb5-53" aria-hidden="true" tabindex="-1"></a>samples <span class="op">=</span> mcmc.get_samples()[<span class="st">"theta"</span>]</span>
<span id="cb5-54"><a href="#cb5-54" aria-hidden="true" tabindex="-1"></a>ax.hist(samples, bins<span class="op">=</span>bins, density<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stderr">
<pre><code>/tmp/ipykernel_11590/1934137946.py:51: UserWarning: There are not enough devices to run parallel chains: expected 2 but got 1. Chains will be drawn sequentially. If you are running MCMC in CPU, consider using `numpyro.set_host_device_count(2)` at the beginning of your program. You can double-check how many devices are available in your system using `jax.local_device_count()`.
  mcmc = MCMC(nuts_kernel, num_warmup=1000, num_samples=10_000, num_chains=2)
  0%|          | 0/11000 [00:00&lt;?, ?it/s]warmup:   0%|          | 1/11000 [00:00&lt;2:48:23,  1.09it/s, 3 steps of size 1.40e+01. acc. prob=0.98]warmup:   6%|▌         | 642/11000 [00:01&lt;00:11, 863.61it/s, 1 steps of size 1.58e+00. acc. prob=0.79]sample:  12%|█▏        | 1288/11000 [00:01&lt;00:05, 1765.94it/s, 1 steps of size 9.13e-01. acc. prob=0.91]sample:  18%|█▊        | 1948/11000 [00:01&lt;00:03, 2670.27it/s, 1 steps of size 9.13e-01. acc. prob=0.91]sample:  24%|██▎       | 2601/11000 [00:01&lt;00:02, 3484.36it/s, 3 steps of size 9.13e-01. acc. prob=0.91]sample:  30%|██▉       | 3261/11000 [00:01&lt;00:01, 4205.38it/s, 1 steps of size 9.13e-01. acc. prob=0.92]sample:  36%|███▌      | 3911/11000 [00:01&lt;00:01, 4774.65it/s, 3 steps of size 9.13e-01. acc. prob=0.91]sample:  42%|████▏     | 4572/11000 [00:01&lt;00:01, 5254.93it/s, 3 steps of size 9.13e-01. acc. prob=0.91]sample:  47%|████▋     | 5217/11000 [00:01&lt;00:01, 5580.35it/s, 1 steps of size 9.13e-01. acc. prob=0.91]sample:  53%|█████▎    | 5858/11000 [00:01&lt;00:00, 5771.69it/s, 1 steps of size 9.13e-01. acc. prob=0.91]sample:  59%|█████▉    | 6499/11000 [00:01&lt;00:00, 5951.85it/s, 1 steps of size 9.13e-01. acc. prob=0.91]sample:  65%|██████▌   | 7157/11000 [00:02&lt;00:00, 6131.07it/s, 3 steps of size 9.13e-01. acc. prob=0.91]sample:  71%|███████   | 7801/11000 [00:02&lt;00:00, 6191.86it/s, 3 steps of size 9.13e-01. acc. prob=0.91]sample:  77%|███████▋  | 8442/11000 [00:02&lt;00:00, 6142.81it/s, 3 steps of size 9.13e-01. acc. prob=0.91]sample:  82%|████████▏ | 9072/11000 [00:02&lt;00:00, 6188.51it/s, 3 steps of size 9.13e-01. acc. prob=0.91]sample:  88%|████████▊ | 9711/11000 [00:02&lt;00:00, 6247.36it/s, 3 steps of size 9.13e-01. acc. prob=0.91]sample:  94%|█████████▍| 10366/11000 [00:02&lt;00:00, 6336.80it/s, 3 steps of size 9.13e-01. acc. prob=0.91]sample: 100%|██████████| 11000/11000 [00:02&lt;00:00, 4188.79it/s, 1 steps of size 9.13e-01. acc. prob=0.91]
  0%|          | 0/11000 [00:00&lt;?, ?it/s]warmup:   6%|▌         | 665/11000 [00:00&lt;00:01, 6642.86it/s, 3 steps of size 1.31e+00. acc. prob=0.79]sample:  12%|█▏        | 1330/11000 [00:00&lt;00:01, 6536.01it/s, 3 steps of size 1.01e+00. acc. prob=0.89]sample:  18%|█▊        | 1984/11000 [00:00&lt;00:01, 6444.17it/s, 1 steps of size 1.01e+00. acc. prob=0.89]sample:  24%|██▍       | 2629/11000 [00:00&lt;00:01, 6401.43it/s, 1 steps of size 1.01e+00. acc. prob=0.89]sample:  30%|██▉       | 3270/11000 [00:00&lt;00:01, 6373.51it/s, 3 steps of size 1.01e+00. acc. prob=0.89]sample:  36%|███▌      | 3917/11000 [00:00&lt;00:01, 6403.57it/s, 3 steps of size 1.01e+00. acc. prob=0.89]sample:  42%|████▏     | 4568/11000 [00:00&lt;00:00, 6437.18it/s, 1 steps of size 1.01e+00. acc. prob=0.89]sample:  47%|████▋     | 5219/11000 [00:00&lt;00:00, 6457.23it/s, 3 steps of size 1.01e+00. acc. prob=0.89]sample:  53%|█████▎    | 5869/11000 [00:00&lt;00:00, 6468.53it/s, 1 steps of size 1.01e+00. acc. prob=0.89]sample:  59%|█████▉    | 6516/11000 [00:01&lt;00:00, 6441.67it/s, 3 steps of size 1.01e+00. acc. prob=0.89]sample:  65%|██████▌   | 7189/11000 [00:01&lt;00:00, 6529.38it/s, 3 steps of size 1.01e+00. acc. prob=0.89]sample:  71%|███████▏  | 7858/11000 [00:01&lt;00:00, 6576.41it/s, 3 steps of size 1.01e+00. acc. prob=0.89]sample:  77%|███████▋  | 8522/11000 [00:01&lt;00:00, 6593.16it/s, 3 steps of size 1.01e+00. acc. prob=0.89]sample:  84%|████████▎ | 9197/11000 [00:01&lt;00:00, 6638.41it/s, 3 steps of size 1.01e+00. acc. prob=0.89]sample:  90%|████████▉ | 9868/11000 [00:01&lt;00:00, 6658.19it/s, 1 steps of size 1.01e+00. acc. prob=0.89]sample:  96%|█████████▌| 10540/11000 [00:01&lt;00:00, 6674.45it/s, 3 steps of size 1.01e+00. acc. prob=0.89]sample: 100%|██████████| 11000/11000 [00:01&lt;00:00, 6546.98it/s, 3 steps of size 1.01e+00. acc. prob=0.89]</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="4">
<pre><code>(array([0.24749999, 0.28499999, 0.31799995, 0.39600002, 0.43200003,
        0.5084998 , 0.55500003, 0.64650004, 0.63000004, 0.77400005,
        0.81750005, 0.79799933, 0.86250005, 0.86850005, 1.00800096,
        1.08899909, 1.08749909, 1.2570012 , 1.22699898, 1.25250119,
        1.26899894, 1.34400128, 1.36049886, 1.4324988 , 1.53300146,
        1.48199876, 1.48800142, 1.60799866, 1.74450166, 1.6785016 ]),
 array([0.        , 0.03333334, 0.06666667, 0.10000001, 0.13333334,
        0.16666667, 0.20000002, 0.23333335, 0.26666668, 0.30000001,
        0.33333334, 0.36666667, 0.40000004, 0.43333337, 0.4666667 ,
        0.5       , 0.53333336, 0.56666672, 0.60000002, 0.63333338,
        0.66666669, 0.70000005, 0.73333335, 0.76666671, 0.80000007,
        0.83333337, 0.86666673, 0.90000004, 0.9333334 , 0.9666667 ,
        1.        ]),
 &lt;BarContainer object of 30 artists&gt;)</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="beta-bernoulli_files/figure-html/cell-5-output-3.png" class="img-fluid"></p>
</div>
</div>
<p>HMC agrees well with the mixture of beta distributions. We see that <span class="math inline">\(P(\theta \mid \mathbf{X})\)</span> is slightly more diffuse compared with the case when <span class="math inline">\(T\)</span> is directly observed.</p>
</section>
<section id="a-bit-different-model" class="level3">
<h3 class="anchored" data-anchor-id="a-bit-different-model">A bit different model</h3>
<p>In the model above, for any single observation <span class="math inline">\(X_n\)</span> we had <span class="math display">\[
P(X_n\mid \theta) = \theta c_1^{X_n}(1-c_1)^{1-X_n} + (1-\theta) c_0^{X_n}(1-c_0)^{1-X_n}.
\]</span></p>
<p>However, the joint likelihood was given by</p>
<p><span class="math display">\[\begin{align*}
P(\mathbf{X} \mid \theta) &amp;= \theta \prod_{n} c_1^{X_n}(1-c_1)^{1-X_n} + (1-\theta) \prod_{n} c_0^{X_n}(1-c_0)^{1-X_n} \\
&amp;= \theta c_1^S (1-c_1)^{N-S} + (1-\theta)c_0^S(1-c_0)^S,
\end{align*}
\]</span></p>
<p>which is <em>not</em> the product of <span class="math inline">\(\prod_n P(X_n\mid \theta)\)</span>, because all the variables <span class="math inline">\(X_n\)</span> were noisy observations of a single coin toss outcome <span class="math inline">\(T\)</span>.</p>
<p>Let’s now consider a deceptively similar model: <span class="math display">\[\begin{align*}
  \phi &amp;\sim \mathrm{Beta}(\alpha, \beta)\\
  U_n \mid \phi &amp;\sim \mathrm{Bernoulli}(\phi) \\
  Y_n \mid U_n &amp;\sim \mathrm{Bernoulli}(c_0(1-U_n) + c_1U_n)
\end{align*}
\]</span></p>
<p>In this case, for each observation <span class="math inline">\(Y_n\)</span>, we have <em>a new coin toss</em> <span class="math inline">\(U_n\)</span>.</p>
<p>We have <span class="math inline">\(U_n\sim \mathrm{Bernoulli}(\alpha/(\alpha + \beta))\)</span>, so that <span class="math inline">\(\mathrm{law}\, U_n = \mathrm{law}\, T\)</span>. Similarly, <span class="math inline">\(P(Y_n \mid \phi)\)</span> and <span class="math inline">\(P(X_n \mid \theta)\)</span> will be very similar: <span class="math display">\[\begin{align*}
P(Y_n \mid \phi) &amp;= \sum_{u} P( Y_n \mid U_n=u ) P(U_n=u \mid \phi) \\
&amp;= \phi c_1^{Y_n}(1-c_1)^{1-Y_n} + (1-\phi) c_0^{Y_n} (1-c_1)^{1-Y_n}.
\end{align*}
\]</span></p>
<p>We see that for <span class="math inline">\(X_n = Y_n\)</span> and <span class="math inline">\(\theta = \phi\)</span> the expressions are exactly the same.</p>
<p>For <span class="math inline">\(N=1\)</span> there is no real difference between these two models. However, for <span class="math inline">\(N\ge 2\)</span> a difference appears, because throws <span class="math inline">\(U_n\)</span> are independent and we have <span class="math display">\[
P(\mathbf{Y} \mid \phi) = \prod_{n} \left( \phi c_1^{Y_n}(1-c_1)^{1-Y_n} + (1-\phi) c_0^{Y_n} (1-c_1)^{1-Y_n} \right).
\]</span></p>
<p>This is substantially <em>different</em> from <span class="math inline">\(P(\mathbf{X}\mid \theta)\)</span>.</p>
<p>Perhaps the following perspective is useful: the new model, with variables <span class="math inline">\(U_n\)</span> marginalised out, corresponds to the following:</p>
<p><span class="math display">\[\begin{align*}
  \phi &amp;\sim \mathrm{Beta}(\alpha, \beta)\\
  Y_n \mid \phi &amp;\sim \mathrm{Bernoulli}(c_0(1-\phi) + c_1\phi)
\end{align*}
\]</span></p>
<p>which is different from the model with a shared latent variable <span class="math inline">\(T\)</span>:</p>
<p><span class="math display">\[\begin{align*}
  \theta &amp;\sim \mathrm{Beta}(\alpha, \beta)\\
  T\mid \theta &amp;\sim \mathrm{Bernoulli}(\theta)\\
  X_n \mid T &amp;\sim \mathrm{Bernoulli}(c_0(1-T) + c_1T)
\end{align*}
\]</span></p>
<p>Hence, although the likelihood functions agree for any <em>single observation</em>, i.e., for every <span class="math inline">\(x\in \{0, 1\}\)</span>, the likelihood functions <span class="math inline">\(P(X_n=x\mid \theta)\)</span> and <span class="math inline">\(P(Y_n=x\mid \phi)\)</span> are the same, the likelihood functions constructed using <em>all</em> observed variables, <span class="math inline">\(P(\mathbf{X}\mid \theta)\)</span> and <span class="math inline">\(P(\mathbf{Y}\mid \phi)\)</span>, are usually <em>different</em>.</p>
<p>Also, the posteriors <span class="math inline">\(P(\theta \mid \mathbf{X})\)</span> and <span class="math inline">\(P(\phi \mid \mathbf{Y})\)</span> do differ: <span class="math inline">\(\phi\)</span> treats each outcome <span class="math inline">\(Y_n\)</span> independently, so that the posterior can shrink quickly if <span class="math inline">\(N\)</span> is large.</p>
<p>Compare this with the posterior on <span class="math inline">\(\theta\)</span>, which assumes that all <span class="math inline">\(X_n\)</span> are noisy versions of a single throw <span class="math inline">\(T\)</span>, so it knows that there is little information about <span class="math inline">\(\theta\)</span> even if <span class="math inline">\(N\)</span> is large: the posterior will always be a mixture of <span class="math inline">\(\mathrm{Beta}(\alpha+1, \beta)\)</span> and <span class="math inline">\(\mathrm{Beta}(\alpha, \beta+1)\)</span>. In particular, if <span class="math inline">\(\alpha = \beta = 1\)</span>, the posterior on <span class="math inline">\(\theta\)</span> will still be very diffuse.</p>
<p>For example, consider <span class="math inline">\(T = 1\)</span>. We toss the coin <span class="math inline">\(N = 10\)</span> times, but due to the noise <span class="math inline">\(c_0 = 1 - c_1 = 0.1\)</span> we observed <span class="math inline">\(S=8\)</span>. We have the following <span class="math inline">\(P(T=1\mid \mathbf{X})\)</span>:</p>
<div class="cell" data-execution_count="5">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>prior <span class="op">=</span> BetaPriorArgs(<span class="fl">1.0</span>, <span class="fl">1.0</span>)</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>like <span class="op">=</span> LikelihoodArgs(</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>  n<span class="op">=</span><span class="dv">10</span>,</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>  s<span class="op">=</span><span class="dv">8</span>,</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>  c0<span class="op">=</span><span class="fl">0.1</span>,</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>  c1<span class="op">=</span><span class="fl">0.9</span>,</span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a>posterior_t_value <span class="op">=</span> posterior_t(</span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a>  bias<span class="op">=</span>prior.mean,</span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a>  like<span class="op">=</span>like,</span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"P(T=1 | X) = </span><span class="sc">{</span>posterior_t_value[<span class="dv">1</span>]<span class="sc">:.7f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>P(T=1 | X) = 0.9999981</code></pre>
</div>
</div>
<p>How would the posteriors on <span class="math inline">\(\theta\)</span> and <span class="math inline">\(\phi\)</span> look like? We will plot the above value as a dashed line:</p>
<div class="cell" data-execution_count="6">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> new_model(prior: BetaPriorArgs, like: LikelihoodArgs):</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>  phi <span class="op">=</span> numpyro.sample(<span class="st">"phi"</span>, dist.Beta(prior.alpha, prior.beta))</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>  bias <span class="op">=</span> like.c0 <span class="op">*</span> (<span class="dv">1</span> <span class="op">-</span> phi) <span class="op">+</span> like.c1 <span class="op">*</span> phi</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>  numpyro.factor(<span class="st">"loglikelihood"</span>,</span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>    _binomial_loglikelihood(n<span class="op">=</span>like.n, s<span class="op">=</span>like.s, bias<span class="op">=</span>bias)</span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a>fig, axs <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">3</span>, figsize<span class="op">=</span>(<span class="dv">5</span>, <span class="fl">2.3</span>), dpi<span class="op">=</span><span class="dv">200</span>, sharex<span class="op">=</span><span class="va">True</span>, sharey<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> ax <span class="kw">in</span> axs:</span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a>  ax.spines[[<span class="st">'top'</span>, <span class="st">'right'</span>]].set_visible(<span class="va">False</span>)</span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a>  ax.set_xlim([<span class="op">-</span><span class="fl">0.02</span>, <span class="fl">1.02</span>])</span>
<span id="cb10-14"><a href="#cb10-14" aria-hidden="true" tabindex="-1"></a>  ax.axvline(posterior_t_value[<span class="dv">1</span>], c<span class="op">=</span><span class="st">"white"</span>, linestyle<span class="op">=</span><span class="st">"--"</span>, linewidth<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb10-15"><a href="#cb10-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-16"><a href="#cb10-16" aria-hidden="true" tabindex="-1"></a>key, <span class="op">*</span>subkeys <span class="op">=</span> jax.random.split(key, <span class="dv">4</span>)</span>
<span id="cb10-17"><a href="#cb10-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-18"><a href="#cb10-18" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> axs[<span class="dv">0</span>]</span>
<span id="cb10-19"><a href="#cb10-19" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="st">"$P(</span><span class="ch">\\</span><span class="st">theta </span><span class="ch">\\</span><span class="st">mid T=1)$"</span>)</span>
<span id="cb10-20"><a href="#cb10-20" aria-hidden="true" tabindex="-1"></a>samples <span class="op">=</span> dist.Beta(<span class="dv">2</span>, <span class="dv">1</span>).sample(subkeys[<span class="dv">0</span>], sample_shape<span class="op">=</span>(<span class="dv">100_000</span>,))</span>
<span id="cb10-21"><a href="#cb10-21" aria-hidden="true" tabindex="-1"></a>ax.hist(samples, bins<span class="op">=</span>bins, density<span class="op">=</span><span class="va">True</span>, color<span class="op">=</span><span class="st">"C2"</span>)</span>
<span id="cb10-22"><a href="#cb10-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-23"><a href="#cb10-23" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> axs[<span class="dv">1</span>]</span>
<span id="cb10-24"><a href="#cb10-24" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="st">"$P(</span><span class="ch">\\</span><span class="st">theta</span><span class="ch">\\</span><span class="st">mid </span><span class="ch">\\</span><span class="st">mathbf</span><span class="sc">{X}</span><span class="st">=</span><span class="ch">\\</span><span class="st">mathbf</span><span class="sc">{x}</span><span class="st">)$"</span>)</span>
<span id="cb10-25"><a href="#cb10-25" aria-hidden="true" tabindex="-1"></a>d <span class="op">=</span> create_posterior_theta(</span>
<span id="cb10-26"><a href="#cb10-26" aria-hidden="true" tabindex="-1"></a>  like<span class="op">=</span>like_args,</span>
<span id="cb10-27"><a href="#cb10-27" aria-hidden="true" tabindex="-1"></a>  prior<span class="op">=</span>prior_args,</span>
<span id="cb10-28"><a href="#cb10-28" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb10-29"><a href="#cb10-29" aria-hidden="true" tabindex="-1"></a>samples <span class="op">=</span> d.sample(subkeys[<span class="dv">1</span>], sample_shape<span class="op">=</span>(<span class="dv">100_000</span>,))</span>
<span id="cb10-30"><a href="#cb10-30" aria-hidden="true" tabindex="-1"></a>ax.hist(samples, bins<span class="op">=</span>bins, density<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb10-31"><a href="#cb10-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-32"><a href="#cb10-32" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> axs[<span class="dv">2</span>]</span>
<span id="cb10-33"><a href="#cb10-33" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="st">"$P(</span><span class="ch">\\</span><span class="st">phi</span><span class="ch">\\</span><span class="st">mid </span><span class="ch">\\</span><span class="st">mathbf</span><span class="sc">{Y}</span><span class="st">=</span><span class="ch">\\</span><span class="st">mathbf</span><span class="sc">{x}</span><span class="st">)$"</span>)</span>
<span id="cb10-34"><a href="#cb10-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-35"><a href="#cb10-35" aria-hidden="true" tabindex="-1"></a>nuts_kernel <span class="op">=</span> NUTS(new_model)</span>
<span id="cb10-36"><a href="#cb10-36" aria-hidden="true" tabindex="-1"></a>mcmc <span class="op">=</span> MCMC(nuts_kernel, num_warmup<span class="op">=</span><span class="dv">1000</span>, num_samples<span class="op">=</span><span class="dv">5_000</span>, num_chains<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb10-37"><a href="#cb10-37" aria-hidden="true" tabindex="-1"></a>mcmc.run(subkeys[<span class="dv">2</span>], like<span class="op">=</span>like, prior<span class="op">=</span>prior)</span>
<span id="cb10-38"><a href="#cb10-38" aria-hidden="true" tabindex="-1"></a>samples <span class="op">=</span> mcmc.get_samples()[<span class="st">"phi"</span>]</span>
<span id="cb10-39"><a href="#cb10-39" aria-hidden="true" tabindex="-1"></a>ax.hist(samples, bins<span class="op">=</span>bins, density<span class="op">=</span><span class="va">True</span>, color<span class="op">=</span><span class="st">"C4"</span>)</span>
<span id="cb10-40"><a href="#cb10-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-41"><a href="#cb10-41" aria-hidden="true" tabindex="-1"></a>fig.tight_layout()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stderr">
<pre><code>/tmp/ipykernel_11590/3633385242.py:36: UserWarning: There are not enough devices to run parallel chains: expected 2 but got 1. Chains will be drawn sequentially. If you are running MCMC in CPU, consider using `numpyro.set_host_device_count(2)` at the beginning of your program. You can double-check how many devices are available in your system using `jax.local_device_count()`.
  mcmc = MCMC(nuts_kernel, num_warmup=1000, num_samples=5_000, num_chains=2)
  0%|          | 0/6000 [00:00&lt;?, ?it/s]warmup:   0%|          | 1/6000 [00:00&lt;1:28:40,  1.13it/s, 3 steps of size 8.10e+00. acc. prob=0.68]warmup:  11%|█         | 655/6000 [00:00&lt;00:05, 908.21it/s, 3 steps of size 1.76e+00. acc. prob=0.79]sample:  22%|██▏       | 1312/6000 [00:01&lt;00:02, 1844.48it/s, 7 steps of size 7.26e-01. acc. prob=0.93]sample:  33%|███▎      | 1972/6000 [00:01&lt;00:01, 2753.18it/s, 3 steps of size 7.26e-01. acc. prob=0.92]sample:  44%|████▍     | 2632/6000 [00:01&lt;00:00, 3577.19it/s, 3 steps of size 7.26e-01. acc. prob=0.92]sample:  55%|█████▍    | 3290/6000 [00:01&lt;00:00, 4280.01it/s, 3 steps of size 7.26e-01. acc. prob=0.92]sample:  66%|██████▌   | 3951/6000 [00:01&lt;00:00, 4863.02it/s, 3 steps of size 7.26e-01. acc. prob=0.92]sample:  77%|███████▋  | 4603/6000 [00:01&lt;00:00, 5298.99it/s, 3 steps of size 7.26e-01. acc. prob=0.92]sample:  88%|████████▊ | 5264/6000 [00:01&lt;00:00, 5655.87it/s, 3 steps of size 7.26e-01. acc. prob=0.92]sample:  99%|█████████▉| 5930/6000 [00:01&lt;00:00, 5936.81it/s, 3 steps of size 7.26e-01. acc. prob=0.92]sample: 100%|██████████| 6000/6000 [00:01&lt;00:00, 3335.62it/s, 3 steps of size 7.26e-01. acc. prob=0.92]
  0%|          | 0/6000 [00:00&lt;?, ?it/s]warmup:  11%|█▏        | 677/6000 [00:00&lt;00:00, 6768.72it/s, 1 steps of size 5.60e-01. acc. prob=0.79]sample:  23%|██▎       | 1354/6000 [00:00&lt;00:00, 6768.16it/s, 3 steps of size 7.83e-01. acc. prob=0.92]sample:  34%|███▍      | 2034/6000 [00:00&lt;00:00, 6781.42it/s, 1 steps of size 7.83e-01. acc. prob=0.92]sample:  45%|████▌     | 2714/6000 [00:00&lt;00:00, 6787.05it/s, 7 steps of size 7.83e-01. acc. prob=0.92]sample:  57%|█████▋    | 3393/6000 [00:00&lt;00:00, 6778.94it/s, 3 steps of size 7.83e-01. acc. prob=0.92]sample:  68%|██████▊   | 4072/6000 [00:00&lt;00:00, 6782.54it/s, 3 steps of size 7.83e-01. acc. prob=0.92]sample:  79%|███████▉  | 4752/6000 [00:00&lt;00:00, 6786.99it/s, 7 steps of size 7.83e-01. acc. prob=0.92]sample:  91%|█████████ | 5431/6000 [00:00&lt;00:00, 6787.16it/s, 1 steps of size 7.83e-01. acc. prob=0.92]sample: 100%|██████████| 6000/6000 [00:00&lt;00:00, 6781.78it/s, 1 steps of size 7.83e-01. acc. prob=0.92]</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="beta-bernoulli_files/figure-html/cell-7-output-2.png" class="img-fluid"></p>
</div>
</div>
<p>As we expected, the posterior on <span class="math inline">\(\phi\)</span> is much more precise than posterior on <span class="math inline">\(\theta\)</span>. It also is shifted towards the value <span class="math inline">\(T=1\)</span>, so (in this case) it behaves as sort of an approximation to <span class="math inline">\(T\)</span>.</p>
</section>
</section>
<section id="which-model-is-better-for-denoising" class="level2">
<h2 class="anchored" data-anchor-id="which-model-is-better-for-denoising">Which model is better for denoising?</h2>
<p>Both models actually answer different questions: the first model tries to estimate <span class="math inline">\(T\)</span>, a single coin toss, and slightly updates the information about this coin bias, <span class="math inline">\(\theta\)</span>.</p>
<p>The second model assumes independent coin tosses, where the bias is controlled by <span class="math inline">\(\phi\)</span>. As such, it can quickly shrink the posterior on <span class="math inline">\(\phi\)</span>. Moreover, it can be used to answer the question to impute individual coin tosses, <span class="math inline">\(P(\mathbf{U} \mid \mathbf{Y})\)</span>.</p>
<p>Let’s think what could happen if we fitted each model to the data generated from the other one: this is working with misspecified models (in the <span class="math inline">\(\mathcal M\)</span>-complete setting).</p>
<p>Consider a setting where we have a lot of data, <span class="math inline">\(N\gg 1\)</span> and the false positive and false negative rates are small, with <span class="math inline">\(c_0 \ll c_1\)</span>. If the data come from the second model, with individual variables <span class="math inline">\(U_n\sim \mathrm{Bernoulli}(\phi)\)</span>, and we have <span class="math inline">\(Y_n\approx U_n\)</span>, then the posterior on <span class="math inline">\(T\)</span> will have most mass on the maximum likelihood solution: either <span class="math inline">\(0\)</span> (which should happen for <span class="math inline">\(\phi \ll 0.5\)</span>) or <span class="math inline">\(1\)</span> (for <span class="math inline">\(\phi \gg 0.5\)</span>). This model will be underfitting and the predictive distribution from this model will be quite bad: new <span class="math inline">\(Y_{N+1}\)</span> would again be an approximation to <span class="math inline">\(U_{N+1}\)</span>, which is sampled from <span class="math inline">\(\mathrm{Bernoulli}(\phi)\)</span>, but the model would just return a noisy version of the inferred <span class="math inline">\(T\)</span>.</p>
<p>On the other hand, if we have a lot of data from the first model (with a single <span class="math inline">\(T\)</span> variable) and we fit the second model, the posterior on <span class="math inline">\(\phi\)</span> may have most of the mass near <span class="math inline">\(0\)</span> or <span class="math inline">\(1\)</span>, depending on the true value of <span class="math inline">\(T\)</span>. Hence, although <span class="math inline">\(U_n\sim \mathrm{Bernoulli}(\phi)\)</span> are sampled independently, once <span class="math inline">\(\phi\)</span> is near the true value of <span class="math inline">\(T\)</span> (<span class="math inline">\(0\)</span> or <span class="math inline">\(1\)</span>), they can all be approximately equal to <span class="math inline">\(T\)</span>.</p>
<p>So, when there is a lot of data, noting where most of the mass of <span class="math inline">\(\phi\)</span> lies can be a good approximation to the maximum likelihood of <span class="math inline">\(T\)</span>.</p>
<p>Of course, these <span class="math inline">\(N\gg 1\)</span> settings only tell what happens when we have a lot of data and we didn’t discuss the uncertainty: can we use <span class="math inline">\(\phi\)</span> to get well-calibrated uncertainty on <span class="math inline">\(T\)</span>?</p>
<p>I generally expect that the <span class="math inline">\(\phi\)</span> model can be a bit better in terms of handling slight misspecification, but doing inference directly on <span class="math inline">\(T\)</span> will provide better results in terms of uncertainty quantification for small <span class="math inline">\(N\)</span>. But this is just a hypothesis: extensive simulations are not for today.</p>
</section>
<section id="appendix" class="level2">
<h2 class="anchored" data-anchor-id="appendix">Appendix</h2>
<section id="list-of-distributions" class="level3">
<h3 class="anchored" data-anchor-id="list-of-distributions">List of distributions</h3>
<section id="binomial-distribution" class="level4">
<h4 class="anchored" data-anchor-id="binomial-distribution">Binomial distribution</h4>
<p>The simplest choice: we have a coin with bias <span class="math inline">\(b\)</span> and we toss it <span class="math inline">\(N\)</span> times: <span class="math display">\[\begin{align*}
  X_n &amp;\sim \mathrm{Bernoulli}(b) \text{ for } n = 1, \dotsc, N\\
  S &amp;= X_1 + \cdots + X_N
\end{align*}
\]</span></p>
<p>Then, we have <span class="math inline">\(S \sim \mathrm{Binomial}(N, b)\)</span>. As the individual throws are independent, it’s easy to prove that <span class="math display">\[
\mathbb E[S] = Nb, \quad \mathbb V[S] = Nb(1-b).
\]</span></p>
</section>
<section id="finite-mixture-of-binomial-distributions" class="level4">
<h4 class="anchored" data-anchor-id="finite-mixture-of-binomial-distributions">Finite mixture of binomial distributions</h4>
<p>As <a href="#a-few-coins">above</a>, consider <span class="math inline">\(K\)</span> coins with biases <span class="math inline">\(b_1, \dotsc, b_K\)</span> and a dice used to choose the coin which will be tossed. This is a finite mixture of binomial distributions: <span class="math display">\[\begin{align*}
  D &amp;\sim \mathrm{Categorical}(d_1, \dotsc, d_K)\\
  S \mid D &amp;\sim \mathrm{Binomial}(N, b_D)
\end{align*}
\]</span></p>
<p>In this case the expectation is exactly what one can expect: <span class="math display">\[
\mathbb E[S] = \sum_{k=1}^K d_k \cdot Nb_k = N\bar b,
\]</span> where <span class="math inline">\(\bar b = d_1b_1 + \cdots + d_Kb_K\)</span>.</p>
<p>However, the formula for variance is more complex: from <a href="https://en.wikipedia.org/wiki/Law_of_total_variance">the law of total variance</a>:</p>
<p><span class="math display">\[\begin{align*}
\mathbb V[S] &amp;= \mathbb E[\mathbb V[S\mid B]] + \mathbb V[ \mathbb E[S\mid B] ] \\
&amp;= \sum_{k=1}^K d_k N b_k(1-b_k) + \mathbb V[NB(1-B)]
\end{align*}
\]</span></p>
</section>
<section id="beta-binomial-distribution" class="level4">
<h4 class="anchored" data-anchor-id="beta-binomial-distribution">Beta-binomial distribution</h4>
<p>Similarly as <a href="#even-more-coins">here</a>, we can consider an infinite collection of coins, chosen from the beta distribution. Once we pick a coin, we toss it <span class="math inline">\(N\)</span> times:</p>
<p><span class="math display">\[\begin{align*}
  B &amp;\sim \mathrm{Beta}(\alpha, \beta)\\
  S \mid B &amp;\sim \mathrm{Binomial}(N, B)
\end{align*}
\]</span></p>
<p>The marginal distribution is called the <a href="https://en.wikipedia.org/wiki/Beta-binomial_distribution">beta-binomial distribution</a>: <span class="math display">\[
S \sim \mathrm{BetaBinomial}(N, \alpha, \beta).
\]</span></p>
<p>It’s easy to prove that <span class="math display">\[
\mathbb E[S] = N \frac{\alpha}{\alpha + \beta},
\]</span> but I don’t know an easy derivation of the formula for the variance: <span class="math display">\[
\mathbb V[S] = Nb(1-b)\cdot (1 + (N-1)\rho),
\]</span> where <span class="math inline">\(b=\alpha/(\alpha + \beta)\)</span> and <span class="math inline">\(\rho=1/(1 + \alpha + \beta)\)</span>.</p>
<p>Hence, choosing the coin first incurs additional variance (compared to the binomial distribution).</p>
</section>
<section id="poisson-binomial-distribution" class="level4">
<h4 class="anchored" data-anchor-id="poisson-binomial-distribution">Poisson binomial distribution</h4>
<p>That was quite a few examples. Let’s do one more: the <a href="https://en.wikipedia.org/wiki/Poisson_binomial_distribution">Poisson binomial distribution</a>, because it is fun.</p>
<p>In this case one has <span class="math inline">\(N\)</span> coins with biases <span class="math inline">\(b_1, \dotsc, b_N\)</span> and tosses each of them exactly once: <span class="math display">\[\begin{align*}
  X_n &amp;\sim \mathrm{Bernoulli}(b_n) \text{ for } n=1, \dotsc, N\\
  S &amp;= X_1 + \cdots + X_N.
\end{align*}
\]</span></p>
<p>We see that if all biases are equal, this reduces to the binomial distribution. However, this one is more flexible, as the expectation and variance are given now by <span class="math display">\[
  \mathbb E[S] = \sum_{n=1}^N b_n, \quad \mathbb  V[S] = \sum_{n=1}^N b_n(1-b_n).
\]</span></p>
</section>
</section>
<section id="beta-bernoulli-sparsity-magic" class="level3">
<h3 class="anchored" data-anchor-id="beta-bernoulli-sparsity-magic">Beta-Bernoulli sparsity magic</h3>
<p>Consider the following prior on coefficients in a linear model: <span class="math display">\[\begin{align*}
  \gamma &amp;\sim \mathrm{Beta}(\alpha, \beta)\\
  \theta_k \mid \gamma &amp;\sim \gamma\, Q_0 + (1-\gamma)\, Q_1 \text{ for } k = 1, \dotsc, K
\end{align*}
\]</span></p>
<p>where <span class="math inline">\(Q_1\)</span> is e.g., a <span class="math inline">\(\mathrm{Normal}(0, 10^2)\)</span> distribution corresponding to “slab” component and <span class="math inline">\(Q_0\)</span>, e.g., <span class="math inline">\(\mathrm{Normal}\left(0, 0.01^2\right)\)</span> is the “spike” component.</p>
<p>Intuitively, we expect that fraction <span class="math inline">\(\gamma\)</span> of the parameters will be shrunk to <span class="math inline">\(0\)</span> by the spike component <span class="math inline">\(Q_0\)</span> and the rest (the <span class="math inline">\(1-\gamma\)</span> fraction) of the parameters will actually be used to predict values.</p>
<p>Michael Betancourt wrote an <a href="https://betanalpha.github.io/assets/case_studies/modeling_sparsity.html">amazing tutorial</a> in which he introduces local latent variables, <span class="math inline">\(\lambda_k\)</span>, that control whether <span class="math inline">\(\theta_k\)</span> should be shrunk or not:</p>
<p><span class="math display">\[\begin{align*}
  \lambda_k &amp;\sim \mathrm{Beta}(\alpha, \beta) \text{ for } k = 1, \dotsc, K\\
  \theta_k \mid \lambda_k &amp;\sim \lambda_k \, Q_0 + (1-\lambda_k)\, Q_1 \text{ for } k = 1, \dotsc, K.
\end{align*}
\]</span></p>
<p>Using small letters for PDFs, we can marginalize variables <span class="math inline">\(\lambda_k\)</span> as follows: <span class="math display">\[
  p(\mathbf{\theta}) = \prod_{k=1}^K p(\theta_k) = \prod_{k=1}^K \left( \int p(\theta_k \mid \lambda_k) \, \mathrm{d}P(\lambda_k) \right)
\]</span> and <span class="math display">\[\begin{align*}
  p(\theta_k) &amp;= \int p(\theta_k \mid \lambda_k) \, \mathrm{d}P(\lambda_k) \\
  &amp;= q_0(\theta_k) \int \lambda_k\, \mathrm{Beta}(\lambda_k \mid \alpha, \beta) \, \mathrm{d}\lambda_k + q_1(\theta_k) \int (1-\lambda_k)\, \mathrm{Beta}(\lambda_k \mid \alpha, \beta)\, \mathrm{d} \lambda_k \\
  &amp;= q_0(\theta_k) \frac{\alpha}{\alpha + \beta} + q_1(\theta_k) \left( 1 - \frac{\alpha}{\alpha + \beta} \right),
\end{align*}
\]</span> so that <span class="math display">\[
  \theta_k \sim \gamma\, Q_0 + (1-\gamma)\, Q_1,
\]</span></p>
<p>where <span class="math inline">\(\gamma = \alpha / (\alpha + \beta)\)</span>.</p>
<p>Beta-Bernoulli distribution offers the following perspective: draw latent indicator variables <span class="math inline">\(T_k \mid \lambda_k \sim \mathrm{Bernoulli}(\lambda_k)\)</span>, so that <span class="math inline">\(\theta_k \mid T_k \sim T_k\, Q_0 + (1-T_k) \, Q_1\)</span>.</p>
<p>We recognize that <span class="math inline">\(T_k \sim \mathrm{BetaBernoulli}(\alpha, \beta)\)</span> which is just <span class="math inline">\(\mathrm{Bernoulli}(\gamma)\)</span> for <span class="math inline">\(\gamma =\alpha/(\alpha+\beta)\)</span>. By integrating out <span class="math inline">\(T_k\)</span> variables (which is just trivial summation!), we have <span class="math display">\[
  \theta_k \sim \gamma\, Q_0 + (1-\gamma)\, Q_1.
\]</span></p>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>