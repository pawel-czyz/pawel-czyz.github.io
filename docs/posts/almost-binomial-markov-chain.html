<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Paweł Czyż">
<meta name="dcterms.date" content="2024-01-19">
<meta name="description" content="Everybody knows how to toss a coin. This time we’ll toss it a few times.">

<title>Paweł Czyż - An almost binomial Markov chain</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">Paweł Czyż</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../index.html" rel="" target="">
 <span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../blog.html" rel="" target="">
 <span class="menu-text">Blog</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../publications.html" rel="" target="">
 <span class="menu-text">Publications</span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools ms-auto">
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#a-bernoulli-random-variable" id="toc-a-bernoulli-random-variable" class="nav-link active" data-scroll-target="#a-bernoulli-random-variable">A Bernoulli random variable</a></li>
  <li><a href="#a-bit-lazy-coin-tossing" id="toc-a-bit-lazy-coin-tossing" class="nav-link" data-scroll-target="#a-bit-lazy-coin-tossing">A bit lazy coin tossing</a>
  <ul class="collapse">
  <li><a href="#markov-chain" id="toc-markov-chain" class="nav-link" data-scroll-target="#markov-chain">Markov chain</a></li>
  </ul></li>
  <li><a href="#markov-chain-monte-carlo" id="toc-markov-chain-monte-carlo" class="nav-link" data-scroll-target="#markov-chain-monte-carlo">Markov chain Monte Carlo</a></li>
  <li><a href="#how-does-it-differ-from-beta-binomial" id="toc-how-does-it-differ-from-beta-binomial" class="nav-link" data-scroll-target="#how-does-it-differ-from-beta-binomial">How does it differ from beta-binomial?</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">An almost binomial Markov chain</h1>
</div>

<div>
  <div class="description">
    Everybody knows how to toss a coin. This time we’ll toss it a few times.
  </div>
</div>


<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Paweł Czyż </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">January 19, 2024</p>
    </div>
  </div>
  
    
  </div>
  

</header>

<section id="a-bernoulli-random-variable" class="level2">
<h2 class="anchored" data-anchor-id="a-bernoulli-random-variable">A Bernoulli random variable</h2>
<p>Recall that <span class="math inline">\(Y\sim \mathrm{Bernoulli}(p)\)</span> if <span class="math inline">\(Y\)</span> can attain values from the set <span class="math inline">\(\{0, 1\}\)</span> with probability <span class="math inline">\(P(Y=1) = p\)</span> and <span class="math inline">\(P(Y=0) = 1-p\)</span>. It’s easy to see that:</p>
<ul>
<li>For every <span class="math inline">\(k\ge 1\)</span> the random variables <span class="math inline">\(Y^k\)</span> and <span class="math inline">\(Y\)</span> are equal.</li>
<li>The expected value is <span class="math inline">\(\mathbb E[Y] = p\)</span>.</li>
<li>The variance is <span class="math inline">\(\mathbb{V}[Y]=\mathbb E[Y^2]-\mathbb E[Y]^2 = p-p^2 = p(1-p).\)</span> From <a href="https://en.wikipedia.org/wiki/AM%E2%80%93GM_inequality">AM-GM</a> we see that <span class="math inline">\(\mathbb{V}[Y] \le (1/2)^2=1/4\)</span>.</li>
</ul>
<p>Now, if we consider independent and identically distributed variables <span class="math inline">\(Y_1\sim \mathrm{Bernoulli}(p)\)</span>, <span class="math inline">\(Y_2\sim \mathrm{Bernoulli}(p)\)</span>, …, <span class="math inline">\(Y_n\sim \mathrm{Bernoulli}(p)\)</span>, we can define a new variable <span class="math inline">\(N_n = Y_1 + \cdots + Y_n\)</span> and an average <span class="math display">\[ \bar Y^{(n)} = \frac{N_n}{n}.\]</span></p>
<p>The random variable <span class="math inline">\(N_n\)</span> is distributed according to the binomial distribution and it’s easy to calculate the mean <span class="math inline">\(\mathbb E[N_n] = np\)</span> and variance <span class="math inline">\(\mathbb V[N_n] = np(1-p)\)</span>. Consequently, <span class="math inline">\(\mathbb E[ \bar Y^{(n)} ] = p\)</span> and <span class="math inline">\(\mathbb V[\bar Y^{(n)}] = np(1-p)/n^2 = p(1-p)/n \le 1/4n\)</span>.</p>
<p>Hence, we see that if we want to estimate <span class="math inline">\(p\)</span>, then <span class="math inline">\(\bar Y^{(n)}\)</span> is a reasonable estimator to use, and we can control its variance by choosing appropriate <span class="math inline">\(n\)</span>.</p>
<p>Thinking about very large <span class="math inline">\(n\)</span>, recall that the <a href="https://en.wikipedia.org/wiki/Law_of_large_numbers#Strong_law">strong law of large numbers</a> guarantees that <span class="math display">\[
P\left( \lim\limits_{n\to \infty} \bar Y^{(n)} = p \right) = 1.
\]</span></p>
</section>
<section id="a-bit-lazy-coin-tossing" class="level2">
<h2 class="anchored" data-anchor-id="a-bit-lazy-coin-tossing">A bit lazy coin tossing</h2>
<p>Above we defined a sequence of independent Bernoulli variables. Let’s introduce some dependency between them: define <span class="math display">\[
Y_1\sim \mathrm{Bernoulli}(p)
\]</span> and, for <span class="math inline">\(n\ge 1\)</span>, <span class="math display">\[
Y_{n+1}\mid Y_n \sim w\,\delta_{Y_n} +(1-w)\, \mathrm{Bernoulli}(p).
\]</span></p>
<p>Hence, to draw <span class="math inline">\(Y_1\)</span> we simply toss a coin, but to draw <span class="math inline">\(Y_2\)</span> we can be lazy with probability <span class="math inline">\(w\)</span> and use the sampled value <span class="math inline">\(Y_1\)</span> or, with probability <span class="math inline">\(1-w\)</span>, actually do the hard work of tossing a coin again.</p>
<p>Let’s think about the marginal distributions, i.e., we observe only the <span class="math inline">\(n\)</span>th coin toss. As <span class="math inline">\(Y_n\)</span> takes values in <span class="math inline">\(\{0, 1\}\)</span>, it has to be distributed according to <em>some</em> Bernoulli distribution.</p>
<p>Of course, we have <span class="math inline">\(\mathbb E[Y_1] = p\)</span>, but what is <span class="math inline">\(\mathbb E[Y_2]\)</span>? Using <a href="https://en.wikipedia.org/wiki/Law_of_total_expectation">the law of total expectation</a> we have <span class="math display">\[
\mathbb E[Y_2] = \mathbb E[ \mathbb E[Y_2\mid Y_1] ] = \mathbb E[ w Y_1 + (1-w)p ] = p.
\]</span> Interesting! Even if we have large <span class="math inline">\(w\)</span>, e.g., <span class="math inline">\(w=0.9\)</span>, we will still see <span class="math inline">\(Y_2=1\)</span> with original probability <span class="math inline">\(p\)</span>. More generally, we can prove by <a href="https://en.wikipedia.org/wiki/Mathematical_induction">induction</a> that that <span class="math inline">\(\mathbb E[Y_n] = p\)</span> for all <span class="math inline">\(n\ge 1\)</span>.</p>
<p>To calculate the variance, we could try <a href="https://en.wikipedia.org/wiki/Law_of_total_variance">the law of total variance</a>, but there is a simpler way: from the above observations we see that all the variables are distributed as <span class="math inline">\(Y_n\sim \mathrm{Bernoulli}(p)\)</span> (so they are identically distributed, but <em>not</em> independent for <span class="math inline">\(w&gt;0\)</span>) and the variance has to be <span class="math inline">\(\mathbb V[Y_n] = p(1-p)\)</span>.</p>
<p>Let’s now introduce variables <span class="math inline">\(N_n = Y_1 + \cdots + Y_n\)</span> and <span class="math inline">\(\bar Y^{(n)}=N_n/n\)</span>. As expectation is a linear operator, we know that <span class="math inline">\(\mathbb E[N_n] = np\)</span> and <span class="math inline">\(\mathbb E[\bar Y^{(n)}]=p\)</span>, but how <em>exactly</em> are these variables distributed? Or, at least, can we say anything about their variance?</p>
<p>It’s instructive to see what happens for <span class="math inline">\(w=1\)</span>: intuitively, we only tossed the coin once, and then just “copied” the result <span class="math inline">\(n\)</span> times, so that the sample size used to estimate <span class="math inline">\(\bar Y^{(n)}\)</span> is still one.</p>
<p>More formally, with probability 1 we have <span class="math inline">\(Y_1 = Y_2 = \cdots = Y_n\)</span>, so that <span class="math inline">\(N_n = nY_1\)</span> and <span class="math inline">\(\mathbb V[N_n] = n^2p(1-p)\)</span>. Then, also with probability 1, we also have <span class="math inline">\(\bar Y^{(n)}=Y_1\)</span> and <span class="math inline">\(\mathbb V[\bar Y^{(n)}]=p(1-p)\)</span>.</p>
<p>More generally, we have <span class="math display">\[
\mathbb V[N_n] = \sum_{i=1}^n \mathbb V[Y_i] + \sum_{i\neq j} \mathrm{cov}[Y_i, Y_j]
\]</span> and we can suspect that the covariance terms will be non-negative, usually incurring larger variance than a corresponding binomial distribution (obtained from independent draws). Let’s prove that.</p>
<section id="markov-chain" class="level3">
<h3 class="anchored" data-anchor-id="markov-chain">Markov chain</h3>
<p>We will be interested in covariance terms <span class="math display">\[\begin{align*}
\mathrm{cov}(Y_i, Y_{i+k}) &amp;= \mathbb E[Y_i\cdot Y_{i+k}] - \mathbb E[Y_i]\cdot \mathbb E[Y_{i+k}] \\
&amp;= P(Y_i=1, Y_{i+k}=1)-p^2 \\
&amp;= P(Y_i=1)P( Y_{i+k}=1\mid Y_i=1) -p^2 \\
&amp;= p\cdot P(Y_{i+k}=1 \mid Y_i=1) - p^2.
\end{align*}
\]</span></p>
<p>To calculate the probability <span class="math inline">\(P(Y_{i+k}=1\mid Y_i=1)\)</span> we need an observation: the sampling procedure defines a Markov chain with the transition matrix <span class="math display">\[
T = \begin{pmatrix}
    P(0\to 0) &amp; P(0 \to 1)\\
    P(1\to 0) &amp; P(1\to 1)
\end{pmatrix}
= \begin{pmatrix}
    w+(1-w)(1-p) &amp; p(1-w)\\
    (1-w)(1-p) &amp; w + p(1-w)
\end{pmatrix}.
\]</span></p>
<p>By induction and a handy identity <span class="math inline">\((1-x)(1+x+\cdots + x^{k-1}) = 1-x^{k}\)</span> one can prove that <span class="math display">\[
T^k = \begin{pmatrix}
    1-p(1-w^k) &amp; p(1-w^k)\\
    (1-p)(1-w^k) &amp; p+w^k(1-p),
\end{pmatrix}
\]</span> from which we can conveniently read <span class="math display">\[
P(Y_{i+k}=1\mid Y_i=1) = p+w^k(1-p)
\]</span> and <span class="math display">\[\mathrm{cov}(Y_i, Y_{i+k}) = w^k\cdot p(1-p).\]</span></p>
<p>Great, these terms are always non-negative! Let’s do a quick check: for <span class="math inline">\(w=0\)</span> the covariance terms vanish, resulting in <span class="math inline">\(\mathbb V[N_n]=np(1-p) + 0\)</span> and for <span class="math inline">\(w=1\)</span> we have <span class="math inline">\(\mathbb V[N_n] = np(1-p) + n(n-1)p(1-p)=n^2p(1-p)\)</span>.</p>
<p>For <span class="math inline">\(w\neq 1\)</span> we can use the same identity as before to get <span class="math display">\[\begin{align*}
    \mathbb V[N_n] &amp;= p(1-p)\cdot \left(n+\sum_{i=1}^n \sum_{k=1}^{n-i} w^k \right) \\
    &amp;= p(1-p)\left( n+ \frac{2 w \left(w^n-n w+n-1\right)}{(w-1)^2} \right)
\end{align*}
\]</span></p>
<p>Let’s numerically check whether this formula seems right:</p>
<div class="cell" data-execution_count="1">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> functools <span class="im">import</span> partial</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> jax</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> jax.numpy <span class="im">as</span> jnp</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> jax <span class="im">import</span> random, lax</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="at">@partial</span>(jax.jit, static_argnames<span class="op">=</span>[<span class="st">"n"</span>])</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> simulate_markov_chain(key, n: <span class="bu">int</span>, p: <span class="bu">float</span>, w: <span class="bu">float</span>) <span class="op">-&gt;</span> jnp.ndarray:</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>    keys <span class="op">=</span> random.split(key, n)</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> step(i, y):</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>        key_w, key_p <span class="op">=</span> random.split(keys[i])</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>        y_prev <span class="op">=</span> y[i<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>        mixture_sample <span class="op">=</span> random.bernoulli(key_w, w)</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>        y_next <span class="op">=</span> jnp.where(mixture_sample, y_prev, random.bernoulli(key_p, p))</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>        y <span class="op">=</span> y.at[i].<span class="bu">set</span>(y_next)</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> y</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>    y_init <span class="op">=</span> jnp.zeros(n, dtype<span class="op">=</span>jnp.int32)</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>    y_init <span class="op">=</span> y_init.at[<span class="dv">0</span>].<span class="bu">set</span>(random.bernoulli(keys[<span class="dv">0</span>], p))</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>    y_final <span class="op">=</span> lax.fori_loop(<span class="dv">1</span>, n, step, y_init)</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> y_final</span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> simulate_correlated_binomial(key, n: <span class="bu">int</span>, p: <span class="bu">float</span>, w: <span class="bu">float</span>) <span class="op">-&gt;</span> <span class="bu">int</span>: </span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> simulate_markov_chain(key<span class="op">=</span>key, n<span class="op">=</span>n, p<span class="op">=</span>p, w<span class="op">=</span>w).<span class="bu">sum</span>()</span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a><span class="at">@partial</span>(jax.jit, static_argnames<span class="op">=</span>[<span class="st">"n"</span>, <span class="st">"n_samples"</span>])</span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> sample_correlated_binomial(key, n: <span class="bu">int</span>, p: <span class="bu">float</span>, w: <span class="bu">float</span>, n_samples: <span class="bu">int</span> <span class="op">=</span> <span class="dv">1_000_000</span>) <span class="op">-&gt;</span> jnp.ndarray:</span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a>    keys <span class="op">=</span> random.split(key, n_samples)</span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> jax.vmap(partial(simulate_correlated_binomial, n<span class="op">=</span>n, p<span class="op">=</span>p, w<span class="op">=</span>w))(keys)</span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> variance_correlated_binomial(n: <span class="bu">int</span>, p: <span class="bu">float</span>, w: <span class="bu">float</span>) <span class="op">-&gt;</span> <span class="bu">float</span>:</span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a>    factor <span class="op">=</span> n<span class="op">**</span><span class="dv">2</span></span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> w <span class="op">&lt;</span> <span class="fl">1.0</span>:</span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a>        factor <span class="op">=</span> n <span class="op">+</span> ( <span class="dv">2</span> <span class="op">*</span> w <span class="op">*</span> (<span class="op">-</span><span class="dv">1</span> <span class="op">+</span> n <span class="op">-</span> n <span class="op">*</span> w <span class="op">+</span> w<span class="op">**</span>n)) <span class="op">/</span> (<span class="op">-</span><span class="dv">1</span> <span class="op">+</span> w)<span class="op">**</span><span class="dv">2</span></span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> p<span class="op">*</span>(<span class="dv">1</span><span class="op">-</span>p) <span class="op">*</span> factor</span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a>key <span class="op">=</span> random.PRNGKey(<span class="dv">2024</span><span class="op">-</span><span class="dv">1</span><span class="op">-</span><span class="dv">19</span>)</span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a>test_cases <span class="op">=</span> [</span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a>    (<span class="dv">10</span>, <span class="fl">0.5</span>, <span class="fl">0.5</span>),</span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a>    (<span class="dv">10</span>, <span class="fl">0.3</span>, <span class="fl">0.8</span>),</span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true" tabindex="-1"></a>    (<span class="dv">10</span>, <span class="fl">0.2</span>, <span class="fl">0.1</span>),</span>
<span id="cb1-45"><a href="#cb1-45" aria-hidden="true" tabindex="-1"></a>    (<span class="dv">5</span>, <span class="fl">0.4</span>, <span class="fl">0.3</span>),</span>
<span id="cb1-46"><a href="#cb1-46" aria-hidden="true" tabindex="-1"></a>    (<span class="dv">20</span>, <span class="fl">0.8</span>, <span class="fl">0.7</span>),</span>
<span id="cb1-47"><a href="#cb1-47" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb1-48"><a href="#cb1-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-49"><a href="#cb1-49" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> n, p, w <span class="kw">in</span> test_cases:</span>
<span id="cb1-50"><a href="#cb1-50" aria-hidden="true" tabindex="-1"></a>    key, subkey <span class="op">=</span> random.split(key)</span>
<span id="cb1-51"><a href="#cb1-51" aria-hidden="true" tabindex="-1"></a>    approx <span class="op">=</span> jnp.var(sample_correlated_binomial(subkey, n, p, w))</span>
<span id="cb1-52"><a href="#cb1-52" aria-hidden="true" tabindex="-1"></a>    exact <span class="op">=</span> variance_correlated_binomial(n, p, w)</span>
<span id="cb1-53"><a href="#cb1-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-54"><a href="#cb1-54" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Variance (appr.): </span><span class="sc">{</span>approx<span class="sc">:.2f}</span><span class="ss">"</span>)</span>
<span id="cb1-55"><a href="#cb1-55" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Variance (exact): </span><span class="sc">{</span>exact<span class="sc">:.2f}</span><span class="ss">"</span>)</span>
<span id="cb1-56"><a href="#cb1-56" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"-"</span><span class="op">*</span><span class="dv">23</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stderr">
<pre><code>An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Variance (appr.): 6.50
Variance (exact): 6.50
-----------------------
Variance (appr.): 11.40
Variance (exact): 11.40
-----------------------
Variance (appr.): 1.92
Variance (exact): 1.92
-----------------------
Variance (appr.): 1.93
Variance (exact): 1.94
-----------------------
Variance (appr.): 15.66
Variance (exact): 15.65
-----------------------</code></pre>
</div>
</div>
</section>
</section>
<section id="markov-chain-monte-carlo" class="level2">
<h2 class="anchored" data-anchor-id="markov-chain-monte-carlo">Markov chain Monte Carlo</h2>
<p>Recall that when the samples are independent, we can estimate <span class="math inline">\(p\)</span> via <span class="math inline">\(\bar Y^{(n)}\)</span> which is an unbiased estimator, i.e., <span class="math inline">\(\mathbb E[\bar Y^{(n)}] = p\)</span> and its variance is <span class="math inline">\(\mathbb V[\bar Y^{(n)}]=p(1-p)/n\le 1/4n\)</span>.</p>
<p>When we passed to a Markov chain introducing parameter <span class="math inline">\(w\)</span>, we also found out that <span class="math inline">\(\mathbb E[\bar Y^{(n)}]=p\)</span>. Moreover, for <span class="math inline">\(w&lt;1\)</span> (i.e., there’s some genuine sampling, rather than copying the first result) the variance of <span class="math inline">\(N_n\)</span> also grows as <span class="math inline">\(\mathcal O(n + w^n)=\mathcal O(n)\)</span>, so that <span class="math inline">\(\mathbb V[\bar Y^{(n)}] =\mathcal O(1/n)\)</span>, so that for a large <span class="math inline">\(n\)</span> the estimator <span class="math inline">\(\bar Y^{(n)}\)</span> can be a reliable estimator for <span class="math inline">\(p\)</span>. However, note that in the variance there’s a term <span class="math inline">\(1/(1-w)^2\)</span>, so that for <span class="math inline">\(w\)</span> close to <span class="math inline">\(1\)</span> one may have to use very, very, very large <span class="math inline">\(n\)</span> to make sure that the variance is small enough.</p>
<p>This Markov chain is in fact connected to Markov chain Monte Carlo samplers, used to sample from a given distribution.</p>
<p>The Markov chain <span class="math inline">\(Y_2, Y_3, \dotsc\)</span> has transition matrix <span class="math inline">\(T\)</span> and initial distribution of <span class="math inline">\(Y_1\)</span> (namely <span class="math inline">\(\mathrm{Bernoulli}(p)\)</span>). For <span class="math inline">\(0 &lt; p &lt; 1\)</span> and <span class="math inline">\(w &lt; 1\)</span> the transition matrix has positive entries, which implies that this Markov chain is ergodic (both irreducibility and aperiodicity are trivially satisfied in this case; more generally <a href="https://www.mathematik.uni-ulm.de/stochastik/lehre/ss06/markov/skript_engl/node10.html">quasi-positivity</a>, i.e., <span class="math inline">\(T^k\)</span> has positive entries for some <span class="math inline">\(k\ge 1\)</span>, is <a href="https://www.mathematik.uni-ulm.de/stochastik/lehre/ss06/markov/skript_engl/node12.html">equivalent to ergodicity</a>).</p>
<p>We can deduce two things. First, there’s a unique stationary distribution of this Markov chain. It can be found by solving the equations for the eigenvector <span class="math inline">\(T^{t}\pi=\pi\)</span>; in this case <span class="math inline">\(\pi=(1-p, p)\)</span> (what a surprise!), meaning that the stationary distribution is <span class="math inline">\(\mathrm{Bernoulli}(p)\)</span>.</p>
<p>Secondly, we can use the <a href="https://www.statslab.cam.ac.uk/~james/Markov/s110.pdf">ergodic theorem</a>. The ergodic theorem states that in this case for every function<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> <span class="math inline">\(f\colon \{0, 1\}\to \mathbb R\)</span> it holds that <span class="math display">\[
P\left(\lim\limits_{n\to\infty} \frac{1}{n}\sum_{i=1}^n f(Y_i) = \mathbb E[f] \right) = 1
\]</span> where the expectation <span class="math inline">\(\mathbb E[f]\)</span> is taken with respect to <span class="math inline">\(\pi\)</span>.</p>
<p>Note that for <span class="math inline">\(f(x) = x\)</span> we find out that with probability <span class="math inline">\(1\)</span> it holds that <span class="math inline">\(\lim\limits_{n\to \infty} \bar Y^{(n)} = p\)</span>.</p>
<p>Perhaps it’s worth commenting on why the stationary distribution is <span class="math inline">\(\mathrm{Bernoulli}(p)\)</span>. Consider any distribution <span class="math inline">\(\mathcal D\)</span> and a Markov chain <span class="math display">\[
Y_{n+1} \mid Y_n \sim w\, \delta_{Y_{n}} + (1-w)\, \mathcal D
\]</span> for <span class="math inline">\(w &lt; 1\)</span>. Intuitively, this Markov chain will either jump to a new location with the right probability, or stay at a current point by some additional time. This additional time depends only on <span class="math inline">\(w\)</span>, so that on average, at each point we spend the same time. Hence, it should not affect time averages over very, very, very long sequences. (However, as we have seen, large <span class="math inline">\(w\)</span> may imply large autocorrelation in the Markov chain and the chain would have to be extremely long to yield acceptable variance).</p>
<p>I think it should not be hard to formalize and prove the above observation, but it’s not for today. <a href="https://arxiv.org/abs/2110.07032">This review</a> could be useful for investigating this further.</p>
</section>
<section id="how-does-it-differ-from-beta-binomial" class="level2">
<h2 class="anchored" data-anchor-id="how-does-it-differ-from-beta-binomial">How does it differ from beta-binomial?</h2>
<p>Recall that a <a href="https://en.wikipedia.org/wiki/Beta-binomial_distribution">beta-binomial distribution</a> generates samples as follows:</p>
<ol type="1">
<li>Draw <span class="math inline">\(b\sim \mathrm{Beta}(\alpha, \beta)\)</span>;</li>
<li>Then, draw <span class="math inline">\(M \mid b \sim \mathrm{Binomial}(n, b)\)</span>.</li>
</ol>
<p>Hence, first a random coin is selected from a set of coins with different biases, and then it’s tossed <span class="math inline">\(n\)</span> times. This distribution has two degrees of freedom: <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span>, and allows one a more flexible control over both the mean and the variance. The mean is given by <span class="math display">\[
\mathbb E[M] = n\frac{\alpha}{\alpha + \beta},
\]</span> so if we write <span class="math inline">\(p = \alpha/(\alpha + \beta)\)</span>, we match the mean of a “corresponding” binomial distribution. The variance is given by <span class="math display">\[
\mathbb V[M] = np(1-p)\left(1 + \frac{n-1}{\alpha + \beta + 1} \right),
\]</span> so that for <span class="math inline">\(n \ge 2\)</span> we will see a larger variance than for a binomial distribution with corresponding mean.</p>
<p>We see that this variance is quadratic in <span class="math inline">\(n\)</span>, which is different from the formula for the variance of the almost binomial Markov chain. Nevertheless, we can ask ourselves a question whether beta-binomial can be a good approximation to the distribution studied before.</p>
<p>This intuition may be formalized in many ways, e.g., as minimization of statistical discrepancy measures, including total variation, various Wasserstain distances or <span class="math inline">\(f\)</span>-divergences. Instead, we will just match mean and variance.</p>
<p>So, of course, we will take <span class="math inline">\(p=\alpha/(\alpha + \beta)\)</span> and additionally solve for <span class="math inline">\(\mathbb V[M] = V\)</span>. The solution is then given by <span class="math display">\[\begin{align*}
\alpha &amp;= pR,\\
\beta &amp;= (1-p)R,\\
R &amp;= \frac{n^2 p(1-p)-V}{V - n p(1-p)}.
\end{align*}
\]</span></p>
<p>Now it’s coding time! We could use <a href="https://github.com/google/jax/issues/13327">TensorFlow Probability on JAX to sample from beta-binomial distribution</a>, but we will resort to core JAX.</p>
<div class="cell" data-execution_count="2">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt </span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>plt.style.use(<span class="st">"dark_background"</span>)</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> find_alpha_beta(n: <span class="bu">int</span>, p: <span class="bu">float</span>, variance: <span class="bu">float</span>) <span class="op">-&gt;</span> <span class="bu">tuple</span>[<span class="bu">float</span>, <span class="bu">float</span>]:</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>    num <span class="op">=</span> n<span class="op">**</span><span class="dv">2</span> <span class="op">*</span> p <span class="op">*</span> (<span class="dv">1</span><span class="op">-</span>p) <span class="op">-</span> variance</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>    den <span class="op">=</span> variance <span class="op">-</span> n <span class="op">*</span> p <span class="op">*</span> (<span class="dv">1</span><span class="op">-</span>p)</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>    r <span class="op">=</span> num <span class="op">/</span> den</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> r <span class="op">&lt;=</span> <span class="dv">0</span> <span class="kw">or</span> p <span class="op">&lt;=</span> <span class="dv">0</span> <span class="kw">or</span> p <span class="op">&gt;=</span> <span class="dv">1</span>:</span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>        <span class="cf">raise</span> <span class="pp">ValueError</span>(<span class="st">"Input results in non-positive alpha or beta"</span>)</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> p<span class="op">*</span>r, (<span class="dv">1</span><span class="op">-</span>p) <span class="op">*</span> r</span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a><span class="at">@partial</span>(jax.jit, static_argnames<span class="op">=</span>[<span class="st">"n"</span>])</span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> _sample_beta_binomial(key, n: <span class="bu">int</span>, alpha: <span class="bu">float</span>, beta: <span class="bu">float</span>) <span class="op">-&gt;</span> <span class="bu">int</span>:</span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a>    key_p, key_b <span class="op">=</span> random.split(key)</span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a>    p <span class="op">=</span> random.beta(key_p, a<span class="op">=</span>alpha, b<span class="op">=</span>beta)</span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a>    ber <span class="op">=</span> random.bernoulli(key_b, p<span class="op">=</span>p, shape<span class="op">=</span>(n,))</span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> jnp.<span class="bu">sum</span>(ber)</span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a><span class="at">@partial</span>(jax.jit, static_argnames<span class="op">=</span>[<span class="st">"n"</span>, <span class="st">"n_samples"</span>])</span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> sample_beta_binomial(key, n: <span class="bu">int</span>, alpha: <span class="bu">float</span>, beta: <span class="bu">float</span>, n_samples: <span class="bu">int</span> <span class="op">=</span> <span class="dv">1_000_000</span>) <span class="op">-&gt;</span> jnp.ndarray:</span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a>    keys <span class="op">=</span> random.split(key, n_samples)</span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> jax.vmap(partial(_sample_beta_binomial, n<span class="op">=</span>n, alpha<span class="op">=</span>alpha, beta<span class="op">=</span>beta))(keys)</span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-27"><a href="#cb4-27" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_compare(key, ax: plt.Axes, n: <span class="bu">int</span>, p: <span class="bu">float</span>, w: <span class="bu">float</span>, n_samples: <span class="bu">int</span> <span class="op">=</span> <span class="dv">1_000_000</span>, n_bins: <span class="bu">int</span> <span class="op">|</span> <span class="va">None</span> <span class="op">=</span> <span class="va">None</span>) <span class="op">-&gt;</span> <span class="va">None</span>:</span>
<span id="cb4-28"><a href="#cb4-28" aria-hidden="true" tabindex="-1"></a>    variance <span class="op">=</span> variance_correlated_binomial(n<span class="op">=</span>n, p<span class="op">=</span>p, w<span class="op">=</span>w)</span>
<span id="cb4-29"><a href="#cb4-29" aria-hidden="true" tabindex="-1"></a>    alpha, beta <span class="op">=</span> find_alpha_beta(n<span class="op">=</span>n, p<span class="op">=</span>p, variance<span class="op">=</span>variance)</span>
<span id="cb4-30"><a href="#cb4-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-31"><a href="#cb4-31" aria-hidden="true" tabindex="-1"></a>    key1, key2 <span class="op">=</span> random.split(key)</span>
<span id="cb4-32"><a href="#cb4-32" aria-hidden="true" tabindex="-1"></a>    sample_corr <span class="op">=</span> sample_correlated_binomial(key1, n<span class="op">=</span>n, p<span class="op">=</span>p, w<span class="op">=</span>w, n_samples<span class="op">=</span>n_samples)</span>
<span id="cb4-33"><a href="#cb4-33" aria-hidden="true" tabindex="-1"></a>    sample_betabin <span class="op">=</span> sample_beta_binomial(key2, n<span class="op">=</span>n, alpha<span class="op">=</span>alpha, beta<span class="op">=</span>beta, n_samples<span class="op">=</span>n_samples)</span>
<span id="cb4-34"><a href="#cb4-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-35"><a href="#cb4-35" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> n_bins <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb4-36"><a href="#cb4-36" aria-hidden="true" tabindex="-1"></a>        bins <span class="op">=</span> jnp.arange(<span class="op">-</span><span class="fl">0.5</span>, n <span class="op">+</span> <span class="fl">1.5</span>, <span class="dv">1</span>)</span>
<span id="cb4-37"><a href="#cb4-37" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb4-38"><a href="#cb4-38" aria-hidden="true" tabindex="-1"></a>        bins <span class="op">=</span> jnp.linspace(<span class="op">-</span><span class="fl">0.1</span>, n <span class="op">+</span> <span class="fl">0.1</span>, n_bins)</span>
<span id="cb4-39"><a href="#cb4-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-40"><a href="#cb4-40" aria-hidden="true" tabindex="-1"></a>    ax.hist(</span>
<span id="cb4-41"><a href="#cb4-41" aria-hidden="true" tabindex="-1"></a>        sample_corr, bins<span class="op">=</span>bins, density<span class="op">=</span><span class="va">True</span>, rasterized<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb4-42"><a href="#cb4-42" aria-hidden="true" tabindex="-1"></a>        color<span class="op">=</span><span class="st">"yellow"</span>,</span>
<span id="cb4-43"><a href="#cb4-43" aria-hidden="true" tabindex="-1"></a>        label<span class="op">=</span><span class="st">"Markov chain"</span>,</span>
<span id="cb4-44"><a href="#cb4-44" aria-hidden="true" tabindex="-1"></a>        histtype<span class="op">=</span><span class="st">"step"</span>,</span>
<span id="cb4-45"><a href="#cb4-45" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb4-46"><a href="#cb4-46" aria-hidden="true" tabindex="-1"></a>    ax.hist(</span>
<span id="cb4-47"><a href="#cb4-47" aria-hidden="true" tabindex="-1"></a>        sample_betabin, bins<span class="op">=</span>bins, density<span class="op">=</span><span class="va">True</span>, rasterized<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb4-48"><a href="#cb4-48" aria-hidden="true" tabindex="-1"></a>        color<span class="op">=</span><span class="st">"orange"</span>,</span>
<span id="cb4-49"><a href="#cb4-49" aria-hidden="true" tabindex="-1"></a>        label<span class="op">=</span><span class="st">"Beta-binomial"</span>,</span>
<span id="cb4-50"><a href="#cb4-50" aria-hidden="true" tabindex="-1"></a>        histtype<span class="op">=</span><span class="st">"step"</span>,</span>
<span id="cb4-51"><a href="#cb4-51" aria-hidden="true" tabindex="-1"></a>        linestyle<span class="op">=</span><span class="st">"--"</span></span>
<span id="cb4-52"><a href="#cb4-52" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb4-53"><a href="#cb4-53" aria-hidden="true" tabindex="-1"></a>    ax.spines[[<span class="st">"top"</span>, <span class="st">"right"</span>]].set_visible(<span class="va">False</span>)</span>
<span id="cb4-54"><a href="#cb4-54" aria-hidden="true" tabindex="-1"></a>    ax.set_xlabel(<span class="st">"Number of heads"</span>)</span>
<span id="cb4-55"><a href="#cb4-55" aria-hidden="true" tabindex="-1"></a>    ax.set_ylabel(<span class="st">"Probability"</span>)</span>
<span id="cb4-56"><a href="#cb4-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-57"><a href="#cb4-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-58"><a href="#cb4-58" aria-hidden="true" tabindex="-1"></a>fig, _axs <span class="op">=</span> plt.subplots(<span class="dv">2</span>, <span class="dv">2</span>, dpi<span class="op">=</span><span class="dv">250</span>)</span>
<span id="cb4-59"><a href="#cb4-59" aria-hidden="true" tabindex="-1"></a>axs <span class="op">=</span> _axs.ravel()</span>
<span id="cb4-60"><a href="#cb4-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-61"><a href="#cb4-61" aria-hidden="true" tabindex="-1"></a>key, <span class="op">*</span>keys <span class="op">=</span> random.split(key, <span class="dv">1</span> <span class="op">+</span> <span class="bu">len</span>(axs))</span>
<span id="cb4-62"><a href="#cb4-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-63"><a href="#cb4-63" aria-hidden="true" tabindex="-1"></a>params <span class="op">=</span> [</span>
<span id="cb4-64"><a href="#cb4-64" aria-hidden="true" tabindex="-1"></a>    <span class="co"># (n, p, w, n_bins)</span></span>
<span id="cb4-65"><a href="#cb4-65" aria-hidden="true" tabindex="-1"></a>    (<span class="dv">10</span>, <span class="fl">0.5</span>, <span class="fl">0.9</span>, <span class="va">None</span>),</span>
<span id="cb4-66"><a href="#cb4-66" aria-hidden="true" tabindex="-1"></a>    (<span class="dv">10</span>, <span class="fl">0.3</span>, <span class="fl">0.2</span>, <span class="va">None</span>),</span>
<span id="cb4-67"><a href="#cb4-67" aria-hidden="true" tabindex="-1"></a>    (<span class="dv">100</span>, <span class="fl">0.7</span>, <span class="fl">0.98</span>, <span class="dv">41</span>),</span>
<span id="cb4-68"><a href="#cb4-68" aria-hidden="true" tabindex="-1"></a>    (<span class="dv">100</span>, <span class="fl">0.3</span>, <span class="fl">0.6</span>, <span class="dv">41</span>),</span>
<span id="cb4-69"><a href="#cb4-69" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb4-70"><a href="#cb4-70" aria-hidden="true" tabindex="-1"></a><span class="cf">assert</span> <span class="bu">len</span>(params) <span class="op">==</span> <span class="bu">len</span>(axs)</span>
<span id="cb4-71"><a href="#cb4-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-72"><a href="#cb4-72" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> key, ax, (n, p, w, n_bins) <span class="kw">in</span> <span class="bu">zip</span>(keys, axs, params):</span>
<span id="cb4-73"><a href="#cb4-73" aria-hidden="true" tabindex="-1"></a>    plot_compare(</span>
<span id="cb4-74"><a href="#cb4-74" aria-hidden="true" tabindex="-1"></a>        key<span class="op">=</span>key,</span>
<span id="cb4-75"><a href="#cb4-75" aria-hidden="true" tabindex="-1"></a>        ax<span class="op">=</span>ax,</span>
<span id="cb4-76"><a href="#cb4-76" aria-hidden="true" tabindex="-1"></a>        n<span class="op">=</span>n,</span>
<span id="cb4-77"><a href="#cb4-77" aria-hidden="true" tabindex="-1"></a>        p<span class="op">=</span>p,</span>
<span id="cb4-78"><a href="#cb4-78" aria-hidden="true" tabindex="-1"></a>        w<span class="op">=</span>w,</span>
<span id="cb4-79"><a href="#cb4-79" aria-hidden="true" tabindex="-1"></a>        n_samples<span class="op">=</span><span class="dv">1_000_000</span>,</span>
<span id="cb4-80"><a href="#cb4-80" aria-hidden="true" tabindex="-1"></a>        n_bins<span class="op">=</span>n_bins,</span>
<span id="cb4-81"><a href="#cb4-81" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb4-82"><a href="#cb4-82" aria-hidden="true" tabindex="-1"></a>axs[<span class="dv">0</span>].legend(frameon<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb4-83"><a href="#cb4-83" aria-hidden="true" tabindex="-1"></a>fig.tight_layout()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="almost-binomial-markov-chain_files/figure-html/cell-3-output-1.png" class="img-fluid"></p>
</div>
</div>


</section>


<div id="quarto-appendix" class="default"><section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>Usually one has to add that the function is bounded. But we are working with a finite domain <span class="math inline">\(\{0, 1\}\)</span>, so literally every function is bounded.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>