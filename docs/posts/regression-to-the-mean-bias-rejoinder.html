<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Paweł Czyż">
<meta name="dcterms.date" content="2024-02-04">
<meta name="description" content="We take a closer look at an example where adding some bias can help with making predictions.">

<title>Paweł Czyż - Regression to the mean and biased predictions</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">Paweł Czyż</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../index.html" rel="" target="">
 <span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../blog.html" rel="" target="">
 <span class="menu-text">Blog</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../publications.html" rel="" target="">
 <span class="menu-text">Publications</span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools ms-auto">
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#the-example" id="toc-the-example" class="nav-link active" data-scroll-target="#the-example">The example</a></li>
  <li><a href="#biased-and-unbiased-estimators" id="toc-biased-and-unbiased-estimators" class="nav-link" data-scroll-target="#biased-and-unbiased-estimators">Biased and unbiased estimators</a>
  <ul class="collapse">
  <li><a href="#visualisations" id="toc-visualisations" class="nav-link" data-scroll-target="#visualisations">Visualisations</a></li>
  <li><a href="#twisting-the-problem" id="toc-twisting-the-problem" class="nav-link" data-scroll-target="#twisting-the-problem">Twisting the problem</a></li>
  </ul></li>
  <li><a href="#digression-simple-linear-regression-and-pca" id="toc-digression-simple-linear-regression-and-pca" class="nav-link" data-scroll-target="#digression-simple-linear-regression-and-pca">Digression: simple linear regression and PCA</a>
  <ul class="collapse">
  <li><a href="#simple-linear-regression" id="toc-simple-linear-regression" class="nav-link" data-scroll-target="#simple-linear-regression">Simple linear regression</a></li>
  <li><a href="#principal-component-analysis" id="toc-principal-component-analysis" class="nav-link" data-scroll-target="#principal-component-analysis">Principal component analysis</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Regression to the mean and biased predictions</h1>
</div>

<div>
  <div class="description">
    We take a closer look at an example where adding some bias can help with making predictions.
  </div>
</div>


<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Paweł Czyż </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">February 4, 2024</p>
    </div>
  </div>
  
    
  </div>
  

</header>

<p>In September 2008 <a href="https://projecteuclid.org/journals/bayesian-analysis/volume-3/issue-3"><em>Bayesian Analysis</em>, vol.&nbsp;3, issue 3</a> featured a wonderful discussion between five statistics superstars: <a href="http://www.stat.columbia.edu/~gelman/">Andrew Gelman</a>, <a href="https://en.wikipedia.org/wiki/Jos%C3%A9-Miguel_Bernardo">José M. Bernardo</a>, <a href="https://en.wikipedia.org/wiki/Joseph_Born_Kadane">Joseph B. Kadane</a>, <a href="http://www.senns.uk/">Stephen Senn</a> and <a href="https://www.stat.cmu.edu/~larry/">Larry Wasserman</a>.</p>
<p>The discussion is a great read on foundations of Bayesian statistics (and it’s open access!), but we will not summarise it today. Instead, let’s focus on an example from Andrew Gelman’s <a href="https://projecteuclid.org/journals/bayesian-analysis/volume-3/issue-3/Rejoinder/10.1214/08-BA318REJ.full"><em>Rejoinder</em></a> on regression to the mean and unbiased predictions.</p>
<section id="the-example" class="level2">
<h2 class="anchored" data-anchor-id="the-example">The example</h2>
<p>Andrew Gelman considers a problem in which one tries to estimate the height of adult daughter, <span class="math inline">\(Y\)</span>, from the height of her mother, <span class="math inline">\(X\)</span>. Consider an artificial scenario, where we have millions of data points, which we can use to estimate the joint probability distribution <span class="math inline">\((X, Y)\)</span> and it turns out to be bivariate normal<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> of the following form: <span class="math display">\[
\begin{pmatrix} X\\Y \end{pmatrix} \sim \mathcal N\left(\begin{pmatrix}\mu\\\mu\end{pmatrix}, \sigma^2\begin{pmatrix}  1 &amp; 0.5 \\ 0.5 &amp; 1\end{pmatrix}  \right)
\]</span> with known <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span>, say, <span class="math inline">\(\mu=160\)</span> and <span class="math inline">\(\sigma=10\)</span> in centimeters.</p>
<p>The marginal distributions on both <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are the same: <span class="math inline">\(\mathcal N(\mu, \sigma^2)\)</span>.</p>
<p>Let’s plot all of them:</p>
<div class="cell" data-execution_count="1">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>plt.style.use(<span class="st">"dark_background"</span>)</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> generate_data(</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>    rng,</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>    mu: <span class="bu">float</span>,</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>    sigma: <span class="bu">float</span>,</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>    n_points: <span class="bu">int</span>,</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>) <span class="op">-&gt;</span> np.ndarray:</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>    var <span class="op">=</span> np.square(sigma)</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> rng.multivariate_normal(</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>        mean<span class="op">=</span>(mu, mu),</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>        cov<span class="op">=</span>var <span class="op">*</span> np.asarray([</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>            [<span class="fl">1.0</span>, <span class="fl">0.5</span>],</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>            [<span class="fl">0.5</span>, <span class="fl">1.0</span>],</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>        ]),</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>        size<span class="op">=</span>n_points,</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>rng <span class="op">=</span> np.random.default_rng(<span class="dv">42</span>)</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>mu <span class="op">=</span> <span class="dv">160</span></span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>sigma <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> generate_data(rng, mu<span class="op">=</span>mu, sigma<span class="op">=</span>sigma, n_points<span class="op">=</span><span class="dv">5_000</span>)</span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>fig, axs <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">2</span>, figsize<span class="op">=</span>(<span class="fl">4.5</span>, <span class="fl">2.2</span>), dpi<span class="op">=</span><span class="dv">170</span>, sharex<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> axs[<span class="dv">0</span>]</span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a>ax.scatter(data[:, <span class="dv">0</span>], data[:, <span class="dv">1</span>], c<span class="op">=</span><span class="st">"C0"</span>, s<span class="op">=</span><span class="dv">1</span>, alpha<span class="op">=</span><span class="fl">0.1</span>, rasterized<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="st">"Joint"</span>)</span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">"$X$"</span>)</span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="st">"$Y$"</span>)</span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a>lim <span class="op">=</span> (mu <span class="op">-</span> <span class="dv">4</span><span class="op">*</span>sigma, mu <span class="op">+</span> <span class="dv">4</span><span class="op">*</span>sigma)</span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a>ax.set_xlim(<span class="op">*</span>lim)</span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a>ax.set_ylim(<span class="op">*</span>lim)</span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> axs[<span class="dv">1</span>]</span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a>ax.hist(data[:, <span class="dv">0</span>], color<span class="op">=</span><span class="st">"C1"</span>, bins<span class="op">=</span>np.linspace(<span class="op">*</span>lim, <span class="dv">21</span>), rasterized<span class="op">=</span><span class="va">True</span>, density<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="st">"Marginal"</span>)</span>
<span id="cb1-45"><a href="#cb1-45" aria-hidden="true" tabindex="-1"></a>ax.set_xlim(<span class="op">*</span>lim)</span>
<span id="cb1-46"><a href="#cb1-46" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">"$X$"</span>)</span>
<span id="cb1-47"><a href="#cb1-47" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="st">"PDF"</span>)</span>
<span id="cb1-48"><a href="#cb1-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-49"><a href="#cb1-49" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> ax <span class="kw">in</span> axs:</span>
<span id="cb1-50"><a href="#cb1-50" aria-hidden="true" tabindex="-1"></a>  ax.spines[[<span class="st">'top'</span>, <span class="st">'right'</span>]].set_visible(<span class="va">False</span>)</span>
<span id="cb1-51"><a href="#cb1-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-52"><a href="#cb1-52" aria-hidden="true" tabindex="-1"></a>fig.tight_layout()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="regression-to-the-mean-bias-rejoinder_files/figure-html/cell-2-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>The <a href="https://en.wikipedia.org/wiki/Multivariate_normal_distribution#Bivariate_case_2">conditional distributions</a> are also normal and take very similar form: <span class="math display">\[
Y\mid X=x \sim \mathcal N\big( \mu + 0.5(x-\mu), 0.75\sigma^2\big),
\]</span> and <span class="math display">\[
X\mid Y=y \sim \mathcal N\big( \mu + 0.5(y-\mu), 0.75\sigma^2\big).
\]</span></p>
<p>Let’s plot the conditional distribution in the fist panel. We will plot conditional distributions using mean <span class="math inline">\(\mathbb E[Y\mid X=x]\)</span> and the intervals representing one, two, and three standard deviations from it.</p>
<p>Then, in the second panel we will overlay it with points and the line <span class="math inline">\(y=x\)</span> (dashed red line).</p>
<div class="cell" data-execution_count="2">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>fig, axs <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">2</span>, figsize<span class="op">=</span>(<span class="fl">4.5</span>, <span class="fl">2.2</span>), dpi<span class="op">=</span><span class="dv">170</span>, sharex<span class="op">=</span><span class="va">True</span>, sharey<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>x_ax <span class="op">=</span> np.linspace(<span class="op">*</span>lim, <span class="dv">51</span>)</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>mean_pred <span class="op">=</span> mu <span class="op">+</span> <span class="fl">0.5</span> <span class="op">*</span> (x_ax<span class="op">-</span>mu)</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_conditional(ax):</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>    ax.set_xlim(<span class="op">*</span>lim)</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>    ax.set_ylim(<span class="op">*</span>lim)</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> n_sigma <span class="kw">in</span> [<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>]:</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>        band <span class="op">=</span> n_sigma <span class="op">*</span> np.sqrt(<span class="fl">0.75</span>) <span class="op">*</span> sigma</span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>        ax.fill_between(</span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>            x_ax,</span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>            mean_pred <span class="op">-</span> band,</span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>            mean_pred <span class="op">+</span> band,</span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>            alpha<span class="op">=</span><span class="fl">0.1</span>,</span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a>            color<span class="op">=</span><span class="st">"yellow"</span>,</span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a>            edgecolor<span class="op">=</span><span class="va">None</span>,</span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a>    ax.plot(</span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a>        x_ax,</span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a>        mean_pred,</span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a>        c<span class="op">=</span><span class="st">"white"</span>,</span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb2-26"><a href="#cb2-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-27"><a href="#cb2-27" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> axs[<span class="dv">0</span>]</span>
<span id="cb2-28"><a href="#cb2-28" aria-hidden="true" tabindex="-1"></a>plot_conditional(ax)</span>
<span id="cb2-29"><a href="#cb2-29" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">"$X$"</span>)</span>
<span id="cb2-30"><a href="#cb2-30" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="vs">r"$Y\mid X$"</span>)</span>
<span id="cb2-31"><a href="#cb2-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-32"><a href="#cb2-32" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> axs[<span class="dv">1</span>]</span>
<span id="cb2-33"><a href="#cb2-33" aria-hidden="true" tabindex="-1"></a>plot_conditional(ax)</span>
<span id="cb2-34"><a href="#cb2-34" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">"$X$"</span>)</span>
<span id="cb2-35"><a href="#cb2-35" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="st">"$Y$"</span>)</span>
<span id="cb2-36"><a href="#cb2-36" aria-hidden="true" tabindex="-1"></a>ax.scatter(data[:, <span class="dv">0</span>], data[:, <span class="dv">1</span>], c<span class="op">=</span><span class="st">"C0"</span>, s<span class="op">=</span><span class="dv">1</span>, alpha<span class="op">=</span><span class="fl">0.2</span>, rasterized<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb2-37"><a href="#cb2-37" aria-hidden="true" tabindex="-1"></a>ax.plot(x_ax, x_ax, c<span class="op">=</span><span class="st">"maroon"</span>, linestyle<span class="op">=</span><span class="st">"--"</span>)</span>
<span id="cb2-38"><a href="#cb2-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-39"><a href="#cb2-39" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> ax <span class="kw">in</span> axs:</span>
<span id="cb2-40"><a href="#cb2-40" aria-hidden="true" tabindex="-1"></a>  ax.spines[[<span class="st">'top'</span>, <span class="st">'right'</span>]].set_visible(<span class="va">False</span>)</span>
<span id="cb2-41"><a href="#cb2-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-42"><a href="#cb2-42" aria-hidden="true" tabindex="-1"></a>fig.tight_layout()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="regression-to-the-mean-bias-rejoinder_files/figure-html/cell-3-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>The <span class="math inline">\(y=x\)</span> has greater slope than <span class="math inline">\(\mathbb E[Y\mid X=x]\)</span> (namely, 1 is greater than 0.5), which is the usual <a href="https://en.wikipedia.org/wiki/Regression_toward_the_mean">regression to the mean</a>.</p>
</section>
<section id="biased-and-unbiased-estimators" class="level2">
<h2 class="anchored" data-anchor-id="biased-and-unbiased-estimators">Biased and unbiased estimators</h2>
<p>We have seen that we could do probabilistic prediction, returning the whole conditional distribution <span class="math inline">\(P(Y\mid X=x)\)</span>. Imagine however that a single point is required as the answer. We can take <span class="math inline">\(\mathbb E[Y\mid X=x]\)</span> as one estimator (solid line in the previous plot). More generally, for every number <span class="math inline">\(f\in \mathbb R\)</span> define an estimator <span class="math display">\[
\hat Y_f = \mu + f\cdot (X-\mu).
\]</span></p>
<p>We have the following:</p>
<ul>
<li>For <span class="math inline">\(f=0\)</span>, we have <span class="math inline">\(\hat Y_0 = \mu\)</span> is constantly predicting the mean.</li>
<li>For <span class="math inline">\(f=0.5\)</span>, <span class="math inline">\(\hat Y_{0.5} = 0.5(X + \mu)\)</span> is the regression to the mean we have seen above.</li>
<li>For <span class="math inline">\(f=1\)</span>, <span class="math inline">\(\hat Y_1 = X\)</span> returns the input, which we also have seen above.</li>
</ul>
<p>Let’s take a look at the bias and variance<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a> of these estimators.</p>
<p>We have <span class="math display">\[
(X\mid Y=y) \sim \mathcal N\big(\mu + 0.5(y-\mu), 0.75\sigma^2\big)
\]</span> and <span class="math inline">\(\hat Y_f = f\cdot X + \mu(1-f)\)</span>, meaning that <span class="math display">\[
\mathbb{V}[ \hat Y_f \mid Y=y ] = f^2 \cdot \mathbb{V}[X \mid Y=y] = 0.75 f^2\sigma^2
\]</span> and <span class="math display">\[
\begin{align*}\mathbb E[ \hat Y_f \mid Y=y ] &amp;= \mu(1-f) + f\cdot \mathbb E[X \mid Y=y ]\\ &amp;= \mu(1-f) + f\cdot ( \mu + 0.5(y-\mu) ) \\ &amp;=\mu-\mu f + f\mu + 0.5 f\cdot (y-\mu) \\
&amp;= \mu + 0.5f\cdot (y-\mu) \\ &amp;= 0.5 f\cdot y +\mu(1-0.5f)\end{align*}
\]</span></p>
<p>Hence, for <span class="math inline">\(f=0.5\)</span> we have <span class="math display">\[
\mathbb E[\hat Y_{0.5}\mid Y=y] = 0.25y + 0.75\mu,
\]</span> which is biased towards the mean.</p>
<p>To have an unbiased estimate, consider <span class="math inline">\(f=2\)</span>: <span class="math display">\[
\mathbb E[\hat Y_2\mid Y=y] = y,
\]</span> which however has the form <span class="math inline">\((\hat Y_2 \mid X=x) = \mu + 2(x-\mu)\)</span>, which amplifies the measured distance from the mean!</p>
<section id="visualisations" class="level3">
<h3 class="anchored" data-anchor-id="visualisations">Visualisations</h3>
<p>Let’s spend a minute designing the plots and then visualise the estimators.</p>
<p>The raw data are visualised by plotting <span class="math inline">\(Y\)</span> and <span class="math inline">\(X\)</span>. We can add the lines <span class="math inline">\(\hat Y_f\mid X=x\)</span> to them, to add some information on how <span class="math inline">\(\hat Y_f\)</span> (which is a deterministic function of <span class="math inline">\(X\)</span>) varies, and what the predictions are.</p>
<div class="cell" data-execution_count="3">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> yhat(x: np.ndarray, f: <span class="bu">float</span>) <span class="op">-&gt;</span> np.ndarray:</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> mu <span class="op">+</span> f <span class="op">*</span> (x <span class="op">-</span> mu)</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>(<span class="dv">3</span>, <span class="dv">3</span>), dpi<span class="op">=</span><span class="dv">150</span>)</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>ax.scatter(data[:, <span class="dv">0</span>], data[:, <span class="dv">1</span>], c<span class="op">=</span><span class="st">"C0"</span>, s<span class="op">=</span><span class="dv">1</span>, alpha<span class="op">=</span><span class="fl">0.2</span>, rasterized<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>fs <span class="op">=</span> [<span class="dv">0</span>, <span class="fl">0.5</span>, <span class="dv">1</span>, <span class="dv">2</span>]</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>colors <span class="op">=</span> [<span class="st">"orange"</span>, <span class="st">"white"</span>, <span class="st">"maroon"</span>, <span class="st">"purple"</span>]</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> f, col <span class="kw">in</span> <span class="bu">zip</span>(fs, colors):</span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>    ax.plot(</span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>        x_ax,</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>        yhat(x_ax, f),</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a>        color<span class="op">=</span>col,</span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a>        label<span class="op">=</span><span class="ss">f"$f=$</span><span class="sc">{</span>f<span class="sc">:.1f}</span><span class="ss">"</span>,</span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">"$X$"</span>)</span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="st">"$Y$"</span>)</span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a>ax.set_xlim(<span class="op">*</span>lim)</span>
<span id="cb3-23"><a href="#cb3-23" aria-hidden="true" tabindex="-1"></a>ax.set_ylim(<span class="op">*</span>lim)</span>
<span id="cb3-24"><a href="#cb3-24" aria-hidden="true" tabindex="-1"></a>ax.spines[[<span class="st">'top'</span>, <span class="st">'right'</span>]].set_visible(<span class="va">False</span>)</span>
<span id="cb3-25"><a href="#cb3-25" aria-hidden="true" tabindex="-1"></a>ax.legend(frameon<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb3-26"><a href="#cb3-26" aria-hidden="true" tabindex="-1"></a>fig.tight_layout()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="regression-to-the-mean-bias-rejoinder_files/figure-html/cell-4-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>Then, we can also look at the plot of <span class="math inline">\(\hat Y_f\)</span> and <span class="math inline">\(Y\)</span>. This will be a good illustration showing what bias and variance of these estimators actually mean. We will add a dashed “diagonal” line, <span class="math inline">\(\hat y_f = y\)</span>, to each of these plots.</p>
<div class="cell" data-execution_count="4">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>fig, axs <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="bu">len</span>(fs), figsize<span class="op">=</span>(<span class="dv">2</span> <span class="op">*</span> <span class="bu">len</span>(fs), <span class="dv">2</span>), dpi<span class="op">=</span><span class="dv">150</span>, sharex<span class="op">=</span><span class="va">True</span>, sharey<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> f, ax <span class="kw">in</span> <span class="bu">zip</span>(fs, axs):</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>    x, y <span class="op">=</span> data[:, <span class="dv">0</span>], data[:, <span class="dv">1</span>]</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>    y_ <span class="op">=</span> yhat(x, f)</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>    ax.scatter(y, y_, c<span class="op">=</span><span class="st">"C2"</span>, s<span class="op">=</span><span class="dv">1</span>, alpha<span class="op">=</span><span class="fl">0.2</span>, rasterized<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>    ax.plot(</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>        x_ax, x_ax</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>    ax.set_xlim(<span class="op">*</span>lim)</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>    ax.set_ylim(np.<span class="bu">min</span>(y_), np.<span class="bu">max</span>(y_))</span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>    ax.set_title(<span class="ss">f"$f=$</span><span class="sc">{</span>f<span class="sc">:.1f}</span><span class="ss">"</span>)</span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a>    ax.set_xlabel(<span class="st">"$Y$"</span>)</span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a>    ax.set_ylabel(<span class="ss">f"$\hat Y_f$"</span>)</span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> ax <span class="kw">in</span> axs:</span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a>    ax.spines[[<span class="st">'top'</span>, <span class="st">'right'</span>]].set_visible(<span class="va">False</span>)</span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a>fig.tight_layout()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stderr">
<pre><code>/tmp/ipykernel_43846/2941644956.py:11: UserWarning: Attempting to set identical low and high ylims makes transformation singular; automatically expanding.
  ax.set_ylim(np.min(y_), np.max(y_))</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="regression-to-the-mean-bias-rejoinder_files/figure-html/cell-5-output-2.png" class="img-fluid"></p>
</div>
</div>
<p>The bias can be seen in the following manner: for each value <span class="math inline">\(Y\)</span>, the values of <span class="math inline">\(\hat Y\)</span> corresponding to that <span class="math inline">\(Y\)</span> should be distributed in such a way that the mean lies on the line. This actually will be easier to see once we plot the difference <span class="math inline">\(\hat Y_f - \hat Y\)</span> and <span class="math inline">\(Y\)</span>.</p>
<div class="cell" data-execution_count="5">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>fig, axs <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="bu">len</span>(fs), figsize<span class="op">=</span>(<span class="dv">2</span> <span class="op">*</span> <span class="bu">len</span>(fs), <span class="dv">2</span>), dpi<span class="op">=</span><span class="dv">150</span>, sharex<span class="op">=</span><span class="va">True</span>, sharey<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> f, ax <span class="kw">in</span> <span class="bu">zip</span>(fs, axs):</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>    x, y <span class="op">=</span> data[:, <span class="dv">0</span>], data[:, <span class="dv">1</span>]</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>    y_ <span class="op">=</span> yhat(x, f)</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>    ax.scatter(y, y_ <span class="op">-</span> y, c<span class="op">=</span><span class="st">"C2"</span>, s<span class="op">=</span><span class="dv">1</span>, alpha<span class="op">=</span><span class="fl">0.2</span>, rasterized<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>    ax.plot(</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>        x_ax, np.zeros_like(x_ax)</span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>    ax.set_xlim(<span class="op">*</span>lim)</span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>    ax.set_title(<span class="ss">f"$f=$</span><span class="sc">{</span>f<span class="sc">:.1f}</span><span class="ss">"</span>)</span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a>    ax.set_xlabel(<span class="st">"$Y$"</span>)</span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a>    ax.set_ylabel(<span class="ss">f"$\hat Y_f - Y$"</span>)</span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> ax <span class="kw">in</span> axs:</span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a>    ax.spines[[<span class="st">'top'</span>, <span class="st">'right'</span>]].set_visible(<span class="va">False</span>)</span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a>fig.tight_layout()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="regression-to-the-mean-bias-rejoinder_files/figure-html/cell-6-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>We also see what how variance manifests: for <span class="math inline">\(Y\approx 160\)</span>, we have quite a range of corresponding <span class="math inline">\(\hat Y_2\)</span>. The estimator <span class="math inline">\(\hat Y_{0.5}\)</span> has clear bias for <span class="math inline">\(Y\)</span> far from the mean, but for most of the data points (which lie close the mean in this case) the prediction is reasonable.</p>
<p>In fact, let’s plot <span class="math inline">\(\mathbb E[|\hat Y_f -y | \mid y]\)</span> and <span class="math inline">\(\sqrt{\mathbb E[(\hat Y_f-y)^2\mid y]}\)</span> as a function of <span class="math inline">\(y\)</span>:</p>
<div class="cell" data-execution_count="6">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>n_y_hat_samples <span class="op">=</span> <span class="dv">10_000</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>y_ax <span class="op">=</span> np.linspace(mu <span class="op">-</span> <span class="dv">4</span> <span class="op">*</span> sigma, mu <span class="op">+</span> <span class="dv">4</span> <span class="op">*</span> sigma, <span class="dv">51</span>)</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Shape (n_y, n_y_hat_samples)</span></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>x_samples <span class="op">=</span> mu <span class="op">+</span> <span class="fl">0.5</span> <span class="op">*</span> (y_ax[:, <span class="va">None</span>] <span class="op">-</span> mu) <span class="op">+</span> rng.normal(loc<span class="op">=</span><span class="dv">0</span>, scale<span class="op">=</span>np.sqrt(<span class="fl">0.75</span>) <span class="op">*</span> sigma, size<span class="op">=</span>(y_ax.shape[<span class="dv">0</span>], n_y_hat_samples))</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>fig, axs <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">2</span>, figsize<span class="op">=</span>(<span class="fl">2.5</span><span class="op">*</span><span class="dv">2</span><span class="op">+</span><span class="dv">1</span>, <span class="fl">2.5</span>), dpi<span class="op">=</span><span class="dv">150</span>, sharex<span class="op">=</span><span class="va">True</span>, sharey<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> axs[<span class="dv">0</span>]</span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> f <span class="kw">in</span> fs:</span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>    preds <span class="op">=</span> yhat(x_samples, f)</span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> np.mean(np.<span class="bu">abs</span>(preds <span class="op">-</span> y_ax[:, <span class="va">None</span>]), axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a>    ax.plot(y_ax, loss, label<span class="op">=</span><span class="ss">f"$f=$</span><span class="sc">{</span>f<span class="sc">:.1f}</span><span class="ss">"</span>)</span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a><span class="co"># ax.legend(frameon=False)</span></span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">"$y$"</span>)</span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="vs">r"$E[|\hat Y_f-y| \mid y ]$"</span>)</span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-19"><a href="#cb7-19" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> axs[<span class="dv">1</span>]</span>
<span id="cb7-20"><a href="#cb7-20" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> f <span class="kw">in</span> fs:</span>
<span id="cb7-21"><a href="#cb7-21" aria-hidden="true" tabindex="-1"></a>    preds <span class="op">=</span> yhat(x_ax, f)</span>
<span id="cb7-22"><a href="#cb7-22" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> np.sqrt(np.mean(np.square(preds <span class="op">-</span> y_ax[:, <span class="va">None</span>]), axis<span class="op">=</span><span class="dv">1</span>))</span>
<span id="cb7-23"><a href="#cb7-23" aria-hidden="true" tabindex="-1"></a>    ax.plot(y_ax, loss, label<span class="op">=</span><span class="ss">f"$f=$</span><span class="sc">{</span>f<span class="sc">:.1f}</span><span class="ss">"</span>)</span>
<span id="cb7-24"><a href="#cb7-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-25"><a href="#cb7-25" aria-hidden="true" tabindex="-1"></a>ax.legend(frameon<span class="op">=</span><span class="va">False</span>, bbox_to_anchor<span class="op">=</span>(<span class="fl">1.05</span>, <span class="fl">1.0</span>))</span>
<span id="cb7-26"><a href="#cb7-26" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">"$y$"</span>)</span>
<span id="cb7-27"><a href="#cb7-27" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="vs">r"$\sqrt{E[(\hat Y_f-y)^2 \mid y ]}$"</span>)</span>
<span id="cb7-28"><a href="#cb7-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-29"><a href="#cb7-29" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> ax <span class="kw">in</span> axs:</span>
<span id="cb7-30"><a href="#cb7-30" aria-hidden="true" tabindex="-1"></a>    ax.spines[[<span class="st">'top'</span>, <span class="st">'right'</span>]].set_visible(<span class="va">False</span>)</span>
<span id="cb7-31"><a href="#cb7-31" aria-hidden="true" tabindex="-1"></a>fig.tight_layout()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="regression-to-the-mean-bias-rejoinder_files/figure-html/cell-7-output-1.png" class="img-fluid"></p>
</div>
</div>
</section>
<section id="twisting-the-problem" class="level3">
<h3 class="anchored" data-anchor-id="twisting-the-problem">Twisting the problem</h3>
<p>The above plots condition on unobserved parameter <span class="math inline">\(y\)</span>. Let’s think what happens when we observe the value <span class="math inline">\(X\)</span> and we want to know how far our estimate <span class="math inline">\(\hat Y_f\)</span> is from the unobserved value <span class="math inline">\(Y\)</span>. We can do a variant of one plots we have seen previously, where we put <span class="math inline">\(X\)</span> on the horizontal axis and <span class="math inline">\(\hat Y_f-Y\)</span> on the vertical one:</p>
<div class="cell" data-execution_count="7">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>fig, axs <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="bu">len</span>(fs), figsize<span class="op">=</span>(<span class="dv">2</span> <span class="op">*</span> <span class="bu">len</span>(fs), <span class="dv">2</span>), dpi<span class="op">=</span><span class="dv">150</span>, sharex<span class="op">=</span><span class="va">True</span>, sharey<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> f, ax <span class="kw">in</span> <span class="bu">zip</span>(fs, axs):</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>    x, y <span class="op">=</span> data[:, <span class="dv">0</span>], data[:, <span class="dv">1</span>]</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>    y_ <span class="op">=</span> yhat(x, f)</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>    ax.scatter(x, y_ <span class="op">-</span> y, c<span class="op">=</span><span class="st">"C2"</span>, s<span class="op">=</span><span class="dv">1</span>, alpha<span class="op">=</span><span class="fl">0.2</span>, rasterized<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>    ax.plot(</span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a>        x_ax, np.zeros_like(x_ax)</span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a>    ax.set_xlim(<span class="op">*</span>lim)</span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a>    ax.set_title(<span class="ss">f"$f=$</span><span class="sc">{</span>f<span class="sc">:.1f}</span><span class="ss">"</span>)</span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a>    ax.set_xlabel(<span class="st">"$X$"</span>)</span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a>    ax.set_ylabel(<span class="ss">f"$\hat Y_f - Y$"</span>)</span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> ax <span class="kw">in</span> axs:</span>
<span id="cb8-16"><a href="#cb8-16" aria-hidden="true" tabindex="-1"></a>    ax.spines[[<span class="st">'top'</span>, <span class="st">'right'</span>]].set_visible(<span class="va">False</span>)</span>
<span id="cb8-17"><a href="#cb8-17" aria-hidden="true" tabindex="-1"></a>fig.tight_layout()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="regression-to-the-mean-bias-rejoinder_files/figure-html/cell-8-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>We see that for all values of <span class="math inline">\(X\)</span>, the regression to the mean estimator, <span class="math inline">\(\hat Y_{0.5}\)</span>, works well. Let’s also take a look at the following losses, measuring how wrong our predictions will be on average for a given value of <span class="math inline">\(X\)</span>: <span class="math display">\[\begin{align*}
\ell_1(x) &amp;= \mathbb E\left[\left| Y_f - Y \right| \mid X=x \right], \\
\ell_2(x) &amp;= \sqrt{\mathbb E\left[\left( \hat Y_f - Y \right)^2 \mid X=x \right]}.
\end{align*}
\]</span></p>
<div class="cell" data-execution_count="8">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>n_y_samples <span class="op">=</span> <span class="dv">10_000</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Shape (n_x, n_y_samples)</span></span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>y_samples <span class="op">=</span> mu <span class="op">+</span> <span class="fl">0.5</span> <span class="op">*</span> (x_ax[:, <span class="va">None</span>] <span class="op">-</span> mu) <span class="op">+</span> rng.normal(loc<span class="op">=</span><span class="dv">0</span>, scale<span class="op">=</span>np.sqrt(<span class="fl">0.75</span>) <span class="op">*</span> sigma, size<span class="op">=</span>(x_ax.shape[<span class="dv">0</span>], n_y_samples))</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>fig, axs <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">2</span>, figsize<span class="op">=</span>(<span class="fl">2.5</span><span class="op">*</span><span class="dv">2</span> <span class="op">+</span> <span class="dv">1</span>, <span class="fl">2.5</span>), dpi<span class="op">=</span><span class="dv">150</span>, sharex<span class="op">=</span><span class="va">True</span>, sharey<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> axs[<span class="dv">0</span>]</span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> f <span class="kw">in</span> fs:</span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a>    preds <span class="op">=</span> yhat(x_ax, f)</span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> np.mean(np.<span class="bu">abs</span>(y_samples <span class="op">-</span> preds[:, <span class="va">None</span>]), axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a>    ax.plot(x_ax, loss, label<span class="op">=</span><span class="ss">f"$f=$</span><span class="sc">{</span>f<span class="sc">:.1f}</span><span class="ss">"</span>)</span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">"$x$"</span>)</span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="vs">r"$\ell_1$"</span>)</span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> axs[<span class="dv">1</span>]</span>
<span id="cb9-18"><a href="#cb9-18" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> f <span class="kw">in</span> fs:</span>
<span id="cb9-19"><a href="#cb9-19" aria-hidden="true" tabindex="-1"></a>    preds <span class="op">=</span> yhat(x_ax, f)</span>
<span id="cb9-20"><a href="#cb9-20" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> np.sqrt(np.mean(np.square(y_samples <span class="op">-</span> preds[:, <span class="va">None</span>]), axis<span class="op">=</span><span class="dv">1</span>))</span>
<span id="cb9-21"><a href="#cb9-21" aria-hidden="true" tabindex="-1"></a>    ax.plot(x_ax, loss, label<span class="op">=</span><span class="ss">f"$f=$</span><span class="sc">{</span>f<span class="sc">:.1f}</span><span class="ss">"</span>)</span>
<span id="cb9-22"><a href="#cb9-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-23"><a href="#cb9-23" aria-hidden="true" tabindex="-1"></a>ax.set_xlabel(<span class="st">"$x$"</span>)</span>
<span id="cb9-24"><a href="#cb9-24" aria-hidden="true" tabindex="-1"></a>ax.set_ylabel(<span class="vs">r"$\ell_2$"</span>)</span>
<span id="cb9-25"><a href="#cb9-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-26"><a href="#cb9-26" aria-hidden="true" tabindex="-1"></a>ax.legend(frameon<span class="op">=</span><span class="va">False</span>, bbox_to_anchor<span class="op">=</span>(<span class="fl">1.05</span>, <span class="fl">1.0</span>))</span>
<span id="cb9-27"><a href="#cb9-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-28"><a href="#cb9-28" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> ax <span class="kw">in</span> axs:</span>
<span id="cb9-29"><a href="#cb9-29" aria-hidden="true" tabindex="-1"></a>    ax.spines[[<span class="st">'top'</span>, <span class="st">'right'</span>]].set_visible(<span class="va">False</span>)</span>
<span id="cb9-30"><a href="#cb9-30" aria-hidden="true" tabindex="-1"></a>fig.tight_layout()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="regression-to-the-mean-bias-rejoinder_files/figure-html/cell-9-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>Even for values of <span class="math inline">\(X\)</span> quite far from the mean, <a href="https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff#Bias%E2%80%93variance_decomposition_of_mean_squared_error">some bias helps</a>! Overall, we see that regression to the mean is a sensible strategy in this case.</p>
<p>Also, using <span class="math inline">\(f=1\)</span> (i.e., <span class="math inline">\(\hat Y_f=X\)</span>) performs as well as <span class="math inline">\(f=0\)</span> (constant prediction <span class="math inline">\(\mu\)</span>). More precisely, observe that <span class="math display">\[
\left(Y - \hat Y_0 \mid X=x\right) = (Y-\mu \mid X=x) \sim \mathcal N\left( 0.5(x-\mu), 0.75 \sigma^2\right)
\]</span> and <span class="math display">\[
\left(Y - \hat Y_1 \mid X=x\right) = (Y-X \mid X=x) \sim \mathcal N\left( 0.5(\mu - x) , 0.75\sigma^2 \right)
\]</span></p>
<p>and the absolute value (or squaring) makes the losses exactly equal. This is also visible in the plot representing <span class="math inline">\(X\)</span> and <span class="math inline">\(Y_f - Y\)</span>.</p>
<p>As a final note, one can notice that for every <span class="math inline">\(x\)</span>, both <span class="math inline">\(\ell_1\)</span> and <span class="math inline">\(\ell_2\)</span> are minimised for <span class="math inline">\(f=0.5\)</span> using the following argument: <span class="math inline">\(f=0.5\)</span> predicts the mean of the conditional distribution <span class="math inline">\(Y\mid X=x\)</span>. The sum of squares (in this case we have actually expectation, but let’s not worry about averaging) is minimised for the mean. Similarly, median optimises the sum of absolute deviations, and each of the normal distributions representing <span class="math inline">\(Y-\hat Y_f \mid X=x\)</span> has mean equal to median.</p>
</section>
</section>
<section id="digression-simple-linear-regression-and-pca" class="level2">
<h2 class="anchored" data-anchor-id="digression-simple-linear-regression-and-pca">Digression: simple linear regression and PCA</h2>
<p>This section perhaps may be distracting and shouldn’t really be a part of this post, but I couldn’t resist: I still very much like this digression and I’m grateful for the opportunity to figure it out together with David.</p>
<section id="simple-linear-regression" class="level3">
<h3 class="anchored" data-anchor-id="simple-linear-regression">Simple linear regression</h3>
<p>What would happen if we fitted <a href="https://en.wikipedia.org/wiki/Simple_linear_regression">simple linear regression</a>, <span class="math inline">\(y=a+bx\)</span>? As we assume that we have a very large sample size, sample covariance is pretty much the same as the population covariance. Hence, the slope is given by <span class="math display">\[
b = \mathrm{Cov}(X, Y)/\mathbb{V}[X] = 0.5\sigma^2/\sigma^2 = 0.5
\]</span></p>
<p>and the intercept is <span class="math display">\[
    a = \mathbb E[Y] - b\cdot \mathbb E[X] = \mu - 0.5\mu = 0.5\mu,
\]</span> so that the line is <span class="math inline">\(y = 0.5(\mu + x)\)</span> and corresponds to the regression to the mean estimator <span class="math inline">\(\hat Y_{0.5}\)</span>. It shouldn’t be surprising: simple linear regression minimises the overall squared error. For each value <span class="math inline">\(x\)</span> of <span class="math inline">\(X\)</span> we actually know that it should be <span class="math inline">\(\mathbb E[Y\mid X=x]\)</span>, which is exactly <span class="math inline">\(\hat Y_{0.5}\)</span>.</p>
</section>
<section id="principal-component-analysis" class="level3">
<h3 class="anchored" data-anchor-id="principal-component-analysis">Principal component analysis</h3>
<p>What is the <a href="https://en.wikipedia.org/wiki/Principal_component_analysis">first principal component</a>? For PCA we center the data, so that the principal component will be passing through <span class="math inline">\((\mu, \mu)\)</span>. To find the slope, we need to find the eigenvector corresponding to the largest value of the covariance matrix of the data. We don’t really need to worry about the positive <span class="math inline">\(\sigma^2\)</span> factor, so let’s find the eigenvector of the matrix <span class="math display">\[
\begin{pmatrix}
    1 &amp; 0.5\\
    0.5 &amp; 1
\end{pmatrix}.
\]</span></p>
<p>The largest eigenvalue is <span class="math inline">\(1.5\)</span> with an eigenvector <span class="math inline">\((1, 1)\)</span> (and the other eigenvalue is <span class="math inline">\(0.5\)</span> with eigenvector, of course orthogonal, <span class="math inline">\((-1, 1)\)</span>). Hence, the line describing the principal component is given by <span class="math inline">\((x-\mu, y-\mu) = t (1, 1)\)</span>, where <span class="math inline">\(t\in \mathbb R\)</span>, which is the same line as <span class="math inline">\(y=x\)</span>. We see that this is essentially the <span class="math inline">\(\hat Y_1\)</span> estimator.</p>


</section>
</section>


<div id="quarto-appendix" class="default"><section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>Yes, height can’t be negative. But let’s ignore that: it’s a toy, but still informative, problem.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>It would be a wasted opportunity to not mention <a href="https://youtu.be/ZmbrsbYwRWw?feature=shared&amp;t=349">this wonderful joke</a>. The whole lecture is a true gem.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>